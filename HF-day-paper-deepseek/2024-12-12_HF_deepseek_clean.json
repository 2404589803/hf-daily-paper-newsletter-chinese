[
    {
        "paper": {
            "id": "2412.07760",
            "authors": [
                {
                    "_id": "67591885536995a5bc59a89e",
                    "user": {
                        "_id": "6530bf50f145530101ec03a2",
                        "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                        "isPro": false,
                        "fullname": "Jianhong Bai",
                        "user": "jianhongbai",
                        "type": "user"
                    },
                    "name": "Jianhong Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-11T09:19:50.897Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a89f",
                    "user": {
                        "_id": "63401c89f81b9d101361f712",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665146415483-63401c89f81b9d101361f712.png",
                        "isPro": false,
                        "fullname": "Richard",
                        "user": "menghanxia",
                        "type": "user"
                    },
                    "name": "Menghan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:24:51.075Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a0",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:24:43.182Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a1",
                    "user": {
                        "_id": "66c15837b186e4f6a0dac80c",
                        "avatarUrl": "/avatars/a3b75d6945f1608e64a2fcff887a5024.svg",
                        "isPro": false,
                        "fullname": "Ziyang Yuan",
                        "user": "ziyangy",
                        "type": "user"
                    },
                    "name": "Ziyang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:24:57.808Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a2",
                    "user": {
                        "_id": "63aef2cafcca84593e6682db",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Fu",
                        "user": "lemonaddie",
                        "type": "user"
                    },
                    "name": "Xiao Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-11T09:54:19.786Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a3",
                    "user": {
                        "_id": "6458b8d0990172cd1d703715",
                        "avatarUrl": "/avatars/55f0695e3cb9933c3903fde5a8f740d5.svg",
                        "isPro": false,
                        "fullname": "Zuozhu Liu",
                        "user": "Zuozhu",
                        "type": "user"
                    },
                    "name": "Zuozhu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:25:03.968Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a4",
                    "user": {
                        "_id": "66c46129d67297a9b93e03c5",
                        "avatarUrl": "/avatars/cffd8b07fa3655e240efc8e81f99d97d.svg",
                        "isPro": false,
                        "fullname": "Haoji Hu",
                        "user": "garland1979",
                        "type": "user"
                    },
                    "name": "Haoji Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:25:09.626Z",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a5",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67591885536995a5bc59a8a6",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "qq8933",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:24:11.595Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-10T18:55:17.000Z",
            "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse\n  Viewpoints",
            "summary": "Recent advancements in video diffusion models have shown exceptional\nabilities in simulating real-world dynamics and maintaining 3D consistency.\nThis progress inspires us to investigate the potential of these models to\nensure dynamic consistency across various viewpoints, a highly desirable\nfeature for applications such as virtual filming. Unlike existing methods\nfocused on multi-view generation of single objects for 4D reconstruction, our\ninterest lies in generating open-world videos from arbitrary viewpoints,\nincorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play\nmodule that enhances a pre-trained text-to-video model for multi-camera video\ngeneration, ensuring consistent content across different viewpoints.\nSpecifically, we introduce a multi-view synchronization module to maintain\nappearance and geometry consistency across these viewpoints. Given the scarcity\nof high-quality training data, we design a hybrid training scheme that\nleverages multi-camera images and monocular videos to supplement Unreal\nEngine-rendered multi-camera videos. Furthermore, our method enables intriguing\nextensions, such as re-rendering a video from novel viewpoints. We also release\na multi-view synchronized video dataset, named SynCamVideo-Dataset. Project\npage: https://jianhongbai.github.io/SynCamMaster/.",
            "upvotes": 36,
            "discussionId": "67591887536995a5bc59a94c"
        },
        "translation": "标题：SynCamMaster：从多样化视角同步生成多摄像头视频\n\n摘要：视频扩散模型的最新进展展示了其在模拟现实世界动态和保持3D一致性方面的卓越能力。这一进展激发了我们探索这些模型在确保不同视角间动态一致性方面的潜力，这是虚拟拍摄等应用中非常理想的一个特性。与现有专注于单个对象的多视角生成以进行4D重建的方法不同，我们的兴趣在于从任意视角生成开放世界视频，并结合6自由度相机姿态。为此，我们提出了一种即插即用模块，该模块增强了预训练的文本到视频模型，以实现多摄像头视频生成，确保不同视角间内容的连贯性。具体而言，我们引入了一个多视角同步模块，以保持这些视角间的外观和几何一致性。鉴于高质量训练数据的稀缺性，我们设计了一种混合训练方案，利用多摄像头图像和单目视频来补充Unreal Engine渲染的多摄像头视频。此外，我们的方法还支持一些有趣的扩展，例如从新视角重新渲染视频。我们还发布了一个名为SynCamVideo-Dataset的多视角同步视频数据集。项目页面：https://jianhongbai.github.io/SynCamMaster/。"
    },
    {
        "paper": {
            "id": "2412.08580",
            "authors": [
                {
                    "_id": "675a6094ca1f2b4cd6b2bfac",
                    "user": {
                        "_id": "63a06cebe648d425374f3ea3",
                        "avatarUrl": "/avatars/2e15a761aac16bfdc3da46dd5eef1adf.svg",
                        "isPro": false,
                        "fullname": "Zejian Li",
                        "user": "hyllbd",
                        "type": "user"
                    },
                    "name": "Zejian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:28:01.032Z",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfad",
                    "user": {
                        "_id": "66d1fb162e0412fa2aa4ffb0",
                        "avatarUrl": "/avatars/20620dd03d388f0262124f0c3523080d.svg",
                        "isPro": false,
                        "fullname": "Chenye Meng",
                        "user": "mengcy",
                        "type": "user"
                    },
                    "name": "Chenye Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:27:39.200Z",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfae",
                    "user": {
                        "_id": "6552d3f69fa2f7205a1f94a2",
                        "avatarUrl": "/avatars/8b857fe89586b25b54864da91ce1b35a.svg",
                        "isPro": false,
                        "fullname": "Yize Li",
                        "user": "yeezlee",
                        "type": "user"
                    },
                    "name": "Yize Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:28:06.791Z",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfaf",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb0",
                    "user": {
                        "_id": "63943c882b9483beb473ec25",
                        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
                        "isPro": false,
                        "fullname": "Shengyuan Zhang",
                        "user": "SYZhang0805",
                        "type": "user"
                    },
                    "name": "Shengyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:27:07.929Z",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb1",
                    "user": {
                        "_id": "6618ae26a7f8bb5a98ce4164",
                        "avatarUrl": "/avatars/5656997d49be6fd0f3fd793136098806.svg",
                        "isPro": false,
                        "fullname": "JIARUI MA",
                        "user": "MAJIARUI",
                        "type": "user"
                    },
                    "name": "Jiarui Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:28:15.616Z",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb2",
                    "name": "Jiayi Li",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb3",
                    "name": "Guang Yang",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb4",
                    "name": "Changyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb5",
                    "name": "Zhiyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb6",
                    "name": "Jinxiong Chang",
                    "hidden": false
                },
                {
                    "_id": "675a6094ca1f2b4cd6b2bfb7",
                    "user": {
                        "_id": "662a5c0649e03000ed1adab4",
                        "avatarUrl": "/avatars/961e4b9bf923923fcd871672d156f50c.svg",
                        "isPro": false,
                        "fullname": "Lingyun Sun",
                        "user": "slysun",
                        "type": "user"
                    },
                    "name": "Lingyun Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:25:33.476Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T17:57:10.000Z",
            "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\n  Image-Text Models with Structural Annotations",
            "summary": "Recent advances in text-to-image (T2I) generation have shown remarkable\nsuccess in producing high-quality images from text. However, existing T2I\nmodels show decayed performance in compositional image generation involving\nmultiple objects and intricate relationships. We attribute this problem to\nlimitations in existing datasets of image-text pairs, which lack precise\ninter-object relationship annotations with prompts only. To address this\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\nstructural annotations of scene graphs (SG), which precisely describe\nattributes and relationships of multiple objects, effectively representing the\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\nfoundation model SDXL-SG to incorporate structural annotation information into\nthe generation process. Extensive experiments show advanced models trained on\nour LAION-SG boast significant performance improvements in complex scene\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\nbenchmark that evaluates models on compositional image generation, establishing\na new standard for this domain.",
            "upvotes": 28,
            "discussionId": "675a609cca1f2b4cd6b2c12f"
        },
        "translation": "标题：LAION-SG：具有结构化注释的增强型大规模数据集，用于训练复杂图像-文本模型\n\n摘要：近期在文本到图像（T2I）生成领域的进展展示了从文本生成高质量图像的显著成功。然而，现有的T2I模型在涉及多个对象和复杂关系的组合图像生成任务中表现有所下降。我们认为这一问题源于现有图像-文本对数据集的局限性，这些数据集缺乏精确的跨对象关系注释，仅依赖于提示。为解决这一问题，我们构建了LAION-SG，这是一个具有高质量场景图（SG）结构注释的大规模数据集，能够精确描述多个对象的属性和关系，有效表示复杂场景中的语义结构。基于LAION-SG，我们训练了一个新的基础模型SDXL-SG，将结构化注释信息整合到生成过程中。大量实验表明，基于我们的LAION-SG训练的先进模型在复杂场景生成方面相较于现有数据集上的模型表现出显著的性能提升。我们还引入了CompSG-Bench，一个评估模型在组合图像生成任务上的基准，为该领域设立了新的标准。"
    },
    {
        "paper": {
            "id": "2412.08443",
            "authors": [
                {
                    "_id": "675a62917f9a1b78132efc33",
                    "user": {
                        "_id": "64d47a7a508a6313e33faedd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg",
                        "isPro": false,
                        "fullname": "Yuan Liu",
                        "user": "YuanLiuuuuuu",
                        "type": "user"
                    },
                    "name": "Yuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:28:51.562Z",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc34",
                    "name": "Le Tian",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc35",
                    "name": "Xiao Zhou",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc36",
                    "name": "Xinyu Gao",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc37",
                    "user": {
                        "_id": "663f288e9cb0add00b96bf45",
                        "avatarUrl": "/avatars/cd90b9490c439703cb3b52ccbc491199.svg",
                        "isPro": false,
                        "fullname": "kavioyu",
                        "user": "kavio",
                        "type": "user"
                    },
                    "name": "Kavio Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:29:26.282Z",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc38",
                    "user": {
                        "_id": "646aefc235c7a57f936cfa31",
                        "avatarUrl": "/avatars/fc55fadf7260f8d9e164ddafb5e2b8dc.svg",
                        "isPro": false,
                        "fullname": "Yang Yu",
                        "user": "yangyu1",
                        "type": "user"
                    },
                    "name": "Yang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T08:29:33.015Z",
                    "hidden": false
                },
                {
                    "_id": "675a62917f9a1b78132efc39",
                    "name": "Jie Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T15:08:25.000Z",
            "title": "POINTS1.5: Building a Vision-Language Model towards Real World\n  Applications",
            "summary": "Vision-language models have made significant strides recently, demonstrating\nsuperior performance across a range of tasks, e.g. optical character\nrecognition and complex diagram analysis. Building on this trend, we introduce\na new vision-language model, POINTS1.5, designed to excel in various real-world\napplications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several\nkey innovations: i) We replace the original CLIP vision encoder, which had a\nfixed image resolution, with a NaViT-style vision encoder that supports native\ndynamic high resolution. This allows POINTS1.5 to process images of any\nresolution without needing to split them into tiles. ii) We add bilingual\nsupport to POINTS1.5, significantly enhancing its capability in Chinese. Due to\nthe scarcity of open-source Chinese datasets for vision-language models, we\ncollect numerous images from the Internet and annotate them using a combination\nof manual and automatic methods. iii) We propose a set of rigorous filtering\nmethods for visual instruction tuning datasets. We comprehensively evaluate all\nthese filtering methods, and choose the most effective ones to obtain the final\nvisual instruction tuning set. Thanks to these innovations, POINTS1.5\nsignificantly outperforms POINTS1.0 and demonstrates strong performance across\na range of real-world applications. Notably, POINTS1.5-7B is trained on fewer\nthan 4 billion tokens and ranks first on the OpenCompass leaderboard among\nmodels with fewer than 10 billion parameters",
            "upvotes": 25,
            "discussionId": "675a62927f9a1b78132efc6f"
        },
        "translation": "标题：POINTS1.5：构建面向实际应用的视觉语言模型\n\n摘要：视觉语言模型近年来取得了显著进展，在光学字符识别和复杂图表分析等一系列任务中展现出卓越的性能。基于这一趋势，我们引入了一种新的视觉语言模型POINTS1.5，旨在在各种实际应用中表现出色。POINTS1.5是POINTS1.0的增强版本，并集成了几项关键创新：i）我们用支持原生动态高分辨率的NaViT风格视觉编码器替换了原有的固定图像分辨率的CLIP视觉编码器，这使得POINTS1.5能够处理任意分辨率的图像，而无需将其分割成图块。ii）我们为POINTS1.5增加了双语支持，显著提升了其中文处理能力。由于视觉语言模型的中文开源数据集稀缺，我们从互联网上收集了大量图像，并结合手动和自动方法对其进行标注。iii）我们提出了一套严格的视觉指令调优数据集过滤方法。我们对所有这些过滤方法进行了全面评估，并选择了最有效的方法来获得最终的视觉指令调优数据集。得益于这些创新，POINTS1.5在性能上显著超越了POINTS1.0，并在一系列实际应用中展现了强大的表现。值得注意的是，POINTS1.5-7B在不到40亿个token的训练数据上训练，并在参数少于100亿的模型中在OpenCompass排行榜上排名第一。"
    },
    {
        "paper": {
            "id": "2412.08486",
            "authors": [
                {
                    "_id": "675a9d12154448441ec42806",
                    "user": {
                        "_id": "62e0fcfe4db2175cd2710063",
                        "avatarUrl": "/avatars/715841ab309b67e310755274a29d6b61.svg",
                        "isPro": true,
                        "fullname": "Zijian Zhou",
                        "user": "franciszzj",
                        "type": "user"
                    },
                    "name": "Zijian Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-12T08:26:29.544Z",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42807",
                    "user": {
                        "_id": "633ef2d6905131c8f52cfb95",
                        "avatarUrl": "/avatars/415f61f80feaffca4e35e029b84ba5d2.svg",
                        "isPro": false,
                        "fullname": "Shikun Liu",
                        "user": "shikunl",
                        "type": "user"
                    },
                    "name": "Shikun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:03:35.757Z",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42808",
                    "name": "Xiao Han",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42809",
                    "name": "Haozhe Liu",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280a",
                    "name": "Kam Woh Ng",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280b",
                    "name": "Tian Xie",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280c",
                    "user": {
                        "_id": "62f8040c47d782a6e286ea8e",
                        "avatarUrl": "/avatars/f45224b851ae79fff9912f6e67159e52.svg",
                        "isPro": false,
                        "fullname": "Yuren Cong",
                        "user": "Yuren",
                        "type": "user"
                    },
                    "name": "Yuren Cong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:04:50.005Z",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280d",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280e",
                    "user": {
                        "_id": "650db809ff259b573e42e31d",
                        "avatarUrl": "/avatars/d6e4e4b67cb3ffa197c67197c816139b.svg",
                        "isPro": false,
                        "fullname": "Meng Meng Xu",
                        "user": "Wall-dandelion",
                        "type": "user"
                    },
                    "name": "Mengmeng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:04:56.730Z",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec4280f",
                    "name": "Juan-Manuel Pérez-Rúa",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42810",
                    "user": {
                        "_id": "64534d44e08584d0f5088b2c",
                        "avatarUrl": "/avatars/c5167abd763e186050d2a2e98a1a3b2e.svg",
                        "isPro": false,
                        "fullname": "Aditya Patel",
                        "user": "AdityaPatel",
                        "type": "user"
                    },
                    "name": "Aditya Patel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:05:15.461Z",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42811",
                    "name": "Tao Xiang",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42812",
                    "name": "Miaojing Shi",
                    "hidden": false
                },
                {
                    "_id": "675a9d12154448441ec42813",
                    "name": "Sen He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T15:51:14.000Z",
            "title": "Learning Flow Fields in Attention for Controllable Person Image\n  Generation",
            "summary": "Controllable person image generation aims to generate a person image\nconditioned on reference images, allowing precise control over the person's\nappearance or pose. However, prior methods often distort fine-grained textural\ndetails from the reference image, despite achieving high overall image quality.\nWe attribute these distortions to inadequate attention to corresponding regions\nin the reference image. To address this, we thereby propose learning flow\nfields in attention (Leffa), which explicitly guides the target query to attend\nto the correct reference key in the attention layer during training.\nSpecifically, it is realized via a regularization loss on top of the attention\nmap within a diffusion-based baseline. Our extensive experiments show that\nLeffa achieves state-of-the-art performance in controlling appearance (virtual\ntry-on) and pose (pose transfer), significantly reducing fine-grained detail\ndistortion while maintaining high image quality. Additionally, we show that our\nloss is model-agnostic and can be used to improve the performance of other\ndiffusion models.",
            "upvotes": 16,
            "discussionId": "675a9d18154448441ec42997"
        },
        "translation": "标题：基于注意力机制的可控人物图像生成中的流场学习\n\n摘要：可控人物图像生成旨在根据参考图像生成人物图像，允许对人物的外貌或姿态进行精确控制。然而，尽管先前的研究方法在整体图像质量上表现出色，但往往会导致参考图像中细粒度纹理细节的失真。我们认为这些失真源于对参考图像中相应区域的关注不足。为了解决这一问题，我们提出了在注意力机制中学习流场（Leffa），该方法在训练过程中明确引导目标查询在注意力层中关注正确的参考键。具体而言，这是通过在基于扩散的基线模型中对注意力图施加正则化损失来实现的。我们的广泛实验表明，Leffa在控制外貌（虚拟试穿）和姿态（姿态迁移）方面达到了最先进的性能，显著减少了细粒度细节的失真，同时保持了高图像质量。此外，我们还展示了我们的损失函数具有模型无关性，可以用于提升其他扩散模型的性能。"
    },
    {
        "paper": {
            "id": "2412.07744",
            "authors": [
                {
                    "_id": "67597f9444ec15bfe4c6be7d",
                    "user": {
                        "_id": "66a0b434968fb59eb2434a0a",
                        "avatarUrl": "/avatars/67538f97ebf1995c530563f411d3cd95.svg",
                        "isPro": false,
                        "fullname": "Ye",
                        "user": "zixuan-ye",
                        "type": "user"
                    },
                    "name": "Zixuan Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-11T13:39:09.531Z",
                    "hidden": false
                },
                {
                    "_id": "67597f9444ec15bfe4c6be7e",
                    "name": "Huijuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67597f9444ec15bfe4c6be7f",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:00:35.189Z",
                    "hidden": false
                },
                {
                    "_id": "67597f9444ec15bfe4c6be80",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67597f9444ec15bfe4c6be81",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "qq8933",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:00:50.168Z",
                    "hidden": false
                },
                {
                    "_id": "67597f9444ec15bfe4c6be82",
                    "user": {
                        "_id": "65f865cf806814aec87d45ce",
                        "avatarUrl": "/avatars/6b931c1efe20422a5985fce6a9e36aed.svg",
                        "isPro": false,
                        "fullname": "Wenhan Luo",
                        "user": "whluo",
                        "type": "user"
                    },
                    "name": "Wenhan Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:00:55.592Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-10T18:44:08.000Z",
            "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
            "summary": "Style control has been popular in video generation models. Existing methods\noften generate videos far from the given style, cause content leakage, and\nstruggle to transfer one video to the desired style. Our first observation is\nthat the style extraction stage matters, whereas existing methods emphasize\nglobal style but ignore local textures. In order to bring texture features\nwhile preventing content leakage, we filter content-related patches while\nretaining style ones based on prompt-patch similarity; for global style\nextraction, we generate a paired style dataset through model illusion to\nfacilitate contrastive learning, which greatly enhances the absolute style\nconsistency. Moreover, to fill in the image-to-video gap, we train a\nlightweight motion adapter on still videos, which implicitly enhances\nstylization extent, and enables our image-trained model to be seamlessly\napplied to videos. Benefited from these efforts, our approach, StyleMaster, not\nonly achieves significant improvement in both style resemblance and temporal\ncoherence, but also can easily generalize to video style transfer with a gray\ntile ControlNet. Extensive experiments and visualizations demonstrate that\nStyleMaster significantly outperforms competitors, effectively generating\nhigh-quality stylized videos that align with textual content and closely\nresemble the style of reference images. Our project page is at\nhttps://zixuan-ye.github.io/stylemaster",
            "upvotes": 14,
            "discussionId": "67597f9944ec15bfe4c6c09a"
        },
        "translation": "标题：风格大师：通过艺术生成与翻译实现视频风格化\n\n摘要：风格控制在视频生成模型中一直备受欢迎。现有方法往往生成的视频与给定风格相去甚远，导致内容泄露，并且在将一个视频转换为所需风格时表现不佳。我们的第一个观察结果是，风格提取阶段至关重要，而现有方法强调全局风格却忽略了局部纹理。为了在防止内容泄露的同时引入纹理特征，我们基于提示-块相似度过滤与内容相关的块，同时保留风格块；对于全局风格提取，我们通过模型幻觉生成配对风格数据集，以促进对比学习，从而大大增强了绝对风格一致性。此外，为了弥合图像到视频的差距，我们在静态视频上训练了一个轻量级运动适配器，这不仅隐式增强了风格化程度，还使得我们基于图像训练的模型能够无缝应用于视频。得益于这些努力，我们的方法——风格大师，不仅在风格相似性和时间一致性方面取得了显著改进，而且能够轻松泛化到使用灰色瓷砖ControlNet的视频风格转换。广泛的实验和可视化结果表明，风格大师显著优于竞争对手，能够有效生成高质量的风格化视频，这些视频与文本内容对齐，并且与参考图像的风格高度相似。我们的项目页面位于https://zixuan-ye.github.io/stylemaster。"
    },
    {
        "paper": {
            "id": "2412.08646",
            "authors": [
                {
                    "_id": "675a9f2184252325bb60445c",
                    "user": {
                        "_id": "630458fad14428368d1d0676",
                        "avatarUrl": "/avatars/05a0b0b75a6a9bdbc4185cc0880abf5e.svg",
                        "isPro": false,
                        "fullname": "LIU Jihao",
                        "user": "jjjjh",
                        "type": "user"
                    },
                    "name": "Jihao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-12T09:00:15.170Z",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb60445d",
                    "user": {
                        "_id": "66c8037c737ba92ae3fe0322",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
                        "isPro": false,
                        "fullname": "Zhiding Yu",
                        "user": "Zhiding",
                        "type": "user"
                    },
                    "name": "Zhiding Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:07:31.665Z",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb60445e",
                    "name": "Shiyi Lan",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb60445f",
                    "name": "Shihao Wang",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb604460",
                    "user": {
                        "_id": "65b8724123d948d884b379b1",
                        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
                        "isPro": false,
                        "fullname": "Rongyao Fang",
                        "user": "LucasFang",
                        "type": "user"
                    },
                    "name": "Rongyao Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:06:48.987Z",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb604461",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb604462",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:06:29.840Z",
                    "hidden": false
                },
                {
                    "_id": "675a9f2184252325bb604463",
                    "name": "Jose M. Alvare",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T18:59:54.000Z",
            "title": "StreamChat: Chatting with Streaming Video",
            "summary": "This paper presents StreamChat, a novel approach that enhances the\ninteraction capabilities of Large Multimodal Models (LMMs) with streaming video\ncontent. In streaming interaction scenarios, existing methods rely solely on\nvisual information available at the moment a question is posed, resulting in\nsignificant delays as the model remains unaware of subsequent changes in the\nstreaming video. StreamChat addresses this limitation by innovatively updating\nthe visual context at each decoding step, ensuring that the model utilizes\nup-to-date video content throughout the decoding process. Additionally, we\nintroduce a flexible and efficient crossattention-based architecture to process\ndynamic streaming inputs while maintaining inference efficiency for streaming\ninteractions. Furthermore, we construct a new dense instruction dataset to\nfacilitate the training of streaming interaction models, complemented by a\nparallel 3D-RoPE mechanism that encodes the relative temporal information of\nvisual and text tokens. Experimental results demonstrate that StreamChat\nachieves competitive performance on established image and video benchmarks and\nexhibits superior capabilities in streaming interaction scenarios compared to\nstate-of-the-art video LMM.",
            "upvotes": 12,
            "discussionId": "675a9f2184252325bb6044bc"
        },
        "translation": "标题：流聊：与流媒体视频对话\n摘要：本文介绍了StreamChat，这是一种新颖的方法，通过流媒体视频内容增强大型多模态模型（LMMs）的交互能力。在流媒体交互场景中，现有方法仅依赖于提问时刻可用的视觉信息，导致模型对流媒体视频后续变化的无知，从而产生显著的延迟。StreamChat通过在每个解码步骤中创新地更新视觉上下文，确保模型在整个解码过程中利用最新的视频内容，从而解决了这一限制。此外，我们引入了一种灵活且高效的基于交叉注意力的架构，以处理动态流输入，同时保持流交互的推理效率。此外，我们构建了一个新的密集指令数据集，以促进流交互模型的训练，并辅以一个并行的3D-RoPE机制，该机制编码了视觉和文本令牌的相对时间信息。实验结果表明，StreamChat在既定的图像和视频基准上实现了竞争性的性能，并在流交互场景中展现出优于现有最先进视频LMM的卓越能力。"
    },
    {
        "paper": {
            "id": "2412.05467",
            "authors": [
                {
                    "_id": "675b2a535b0522bba0448eaa",
                    "name": "Thibault Le Sellier De Chezelles",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eab",
                    "name": "Maxime Gasse",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eac",
                    "name": "Alexandre Drouin",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448ead",
                    "name": "Massimo Caccia",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eae",
                    "name": "Léo Boisvert",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eaf",
                    "name": "Megh Thakkar",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb0",
                    "name": "Tom Marty",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb1",
                    "name": "Rim Assouel",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb2",
                    "name": "Sahar Omidi Shayegan",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb3",
                    "name": "Lawrence Keunho Jang",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb4",
                    "name": "Xing Han Lù",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb5",
                    "name": "Ori Yoran",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb6",
                    "name": "Dehan Kong",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb7",
                    "name": "Frank F. Xu",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb8",
                    "name": "Siva Reddy",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eb9",
                    "name": "Quentin Cappart",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448eba",
                    "name": "Graham Neubig",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448ebb",
                    "name": "Ruslan Salakhutdinov",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448ebc",
                    "name": "Nicolas Chapados",
                    "hidden": false
                },
                {
                    "_id": "675b2a535b0522bba0448ebd",
                    "name": "Alexandre Lacoste",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-06T23:43:59.000Z",
            "title": "The BrowserGym Ecosystem for Web Agent Research",
            "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models.",
            "upvotes": 11,
            "discussionId": "675b2a555b0522bba0448fa9"
        },
        "translation": "标题：浏览器健身房生态系统：面向网页代理研究的统一环境\n\n摘要：浏览器健身房（BrowserGym）生态系统旨在应对日益增长的对于高效评估和基准测试网页代理的需求，特别是那些利用自动化和大型语言模型（LLMs）进行网页交互任务的代理。许多现有的基准测试存在碎片化和评估方法不一致的问题，这使得实现可靠的比较和可重复的结果变得困难。BrowserGym通过提供一个统一的、类似健身房的环境，具有明确的观察和动作空间，促进了跨不同基准的标准化评估。结合AgentLab，一个辅助代理创建、测试和分析的互补框架，BrowserGym提供了灵活性，以便集成新的基准测试，同时确保一致的评估和全面实验管理。这种标准化方法旨在减少开发网页代理的时间和复杂性，支持更可靠的比较，并促进对代理行为的深入分析，从而可能产生更具适应性和能力的代理，最终加速LLM驱动的自动化创新。作为支持证据，我们进行了首次大规模、多基准的网页代理实验，并在BrowserGym当前可用的所有基准上比较了6种最先进LLM的性能。我们的结果显示，OpenAI和Anthropic的最新模型之间存在显著差异，Claude-3.5-Sonnet在几乎所有基准上都领先，除了与视觉相关的任务，GPT-4o表现更优。尽管取得了这些进展，我们的结果强调，构建稳健和高效的网页代理仍然是一个重大挑战，这是由于现实世界网页环境的固有复杂性和当前模型的局限性。"
    },
    {
        "paper": {
            "id": "2412.06234",
            "authors": [
                {
                    "_id": "675932605456a65fb50aeed5",
                    "user": {
                        "_id": "6574932cd0ed8f5761069ac2",
                        "avatarUrl": "/avatars/2a6b6b9e8b01179eacc92d2a9cbf5df9.svg",
                        "isPro": false,
                        "fullname": "Seungtae",
                        "user": "stnamjef",
                        "type": "user"
                    },
                    "name": "Seungtae Nam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-12T08:22:38.514Z",
                    "hidden": false
                },
                {
                    "_id": "675932605456a65fb50aeed6",
                    "user": {
                        "_id": "634d10d4a252e0c53f99534c",
                        "avatarUrl": "/avatars/d8a486e105769ffee59cf996a4f51686.svg",
                        "isPro": false,
                        "fullname": "Sun Xiang yu",
                        "user": "xysun",
                        "type": "user"
                    },
                    "name": "Xiangyu Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:01:38.502Z",
                    "hidden": false
                },
                {
                    "_id": "675932605456a65fb50aeed7",
                    "user": {
                        "_id": "64aa93e799639cc312795ea8",
                        "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg",
                        "isPro": false,
                        "fullname": "Gyeongjin Kang",
                        "user": "lelady",
                        "type": "user"
                    },
                    "name": "Gyeongjin Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-11T09:19:29.174Z",
                    "hidden": false
                },
                {
                    "_id": "675932605456a65fb50aeed8",
                    "user": {
                        "_id": "66a4a1a7d8e85b03deddfa59",
                        "avatarUrl": "/avatars/56dbec2101717ad9471e08a03ae51f0c.svg",
                        "isPro": false,
                        "fullname": "Young geun Lee",
                        "user": "LeeYG",
                        "type": "user"
                    },
                    "name": "Younggeun Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:01:44.417Z",
                    "hidden": false
                },
                {
                    "_id": "675932605456a65fb50aeed9",
                    "user": {
                        "_id": "659b83ff2e1d57d594f714e7",
                        "avatarUrl": "/avatars/4072dc79d87eb3634392b6de3bc81a21.svg",
                        "isPro": false,
                        "fullname": "SeungJun Oh",
                        "user": "JustinOh",
                        "type": "user"
                    },
                    "name": "Seungjun Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:01:49.993Z",
                    "hidden": false
                },
                {
                    "_id": "675932605456a65fb50aeeda",
                    "user": {
                        "_id": "655e0141d36a195f663ee4b0",
                        "avatarUrl": "/avatars/64609d6fd1581e6cf3e057ee569d69a1.svg",
                        "isPro": false,
                        "fullname": "Eunbyung Park",
                        "user": "epark",
                        "type": "user"
                    },
                    "name": "Eunbyung Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:01:55.354Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T06:20:51.000Z",
            "title": "Generative Densification: Learning to Densify Gaussians for\n  High-Fidelity Generalizable 3D Reconstruction",
            "summary": "Generalized feed-forward Gaussian models have achieved significant progress\nin sparse-view 3D reconstruction by leveraging prior knowledge from large\nmulti-view datasets. However, these models often struggle to represent\nhigh-frequency details due to the limited number of Gaussians. While the\ndensification strategy used in per-scene 3D Gaussian splatting (3D-GS)\noptimization can be adapted to the feed-forward models, it may not be ideally\nsuited for generalized scenarios. In this paper, we propose Generative\nDensification, an efficient and generalizable method to densify Gaussians\ngenerated by feed-forward models. Unlike the 3D-GS densification strategy,\nwhich iteratively splits and clones raw Gaussian parameters, our method\nup-samples feature representations from the feed-forward models and generates\ntheir corresponding fine Gaussians in a single forward pass, leveraging the\nembedded prior knowledge for enhanced generalization. Experimental results on\nboth object-level and scene-level reconstruction tasks demonstrate that our\nmethod outperforms state-of-the-art approaches with comparable or smaller model\nsizes, achieving notable improvements in representing fine details.",
            "upvotes": 11,
            "discussionId": "675932625456a65fb50aef52"
        },
        "translation": "标题：生成式稠密化：通过学习稠密化高斯分布实现高保真泛化三维重建\n\n摘要：广义前馈高斯模型通过利用来自多视角大数据集的先验知识，在稀疏视角三维重建方面取得了显著进展。然而，这些模型由于高斯数量有限，往往难以表示高频细节。尽管场景专属的三维高斯喷洒（3D-GS）优化中的稠密化策略可以适应前馈模型，但它可能并不完全适用于广义场景。本文提出了一种生成式稠密化方法，这是一种高效且具有泛化能力的稠密化前馈模型生成高斯分布的方法。与3D-GS稠密化策略不同，后者通过迭代分裂和克隆原始高斯参数，我们的方法从前馈模型中上采样特征表示，并在单次前向传播中生成相应的高精度高斯分布，利用嵌入的先验知识增强泛化能力。在物体级和场景级重建任务的实验结果表明，我们的方法在模型尺寸相当或更小的情况下，显著优于现有最先进的方法，在表示细节方面取得了显著改进。"
    },
    {
        "paper": {
            "id": "2412.07825",
            "authors": [
                {
                    "_id": "675a65ea8e18d279de5b6a35",
                    "user": {
                        "_id": "625f81afe1994410eef1c36a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Wufei Ma",
                        "user": "wufeim",
                        "type": "user"
                    },
                    "name": "Wufei Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:03:03.299Z",
                    "hidden": false
                },
                {
                    "_id": "675a65ea8e18d279de5b6a36",
                    "name": "Haoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "675a65ea8e18d279de5b6a37",
                    "user": {
                        "_id": "661434dd0dcdede49138cd7c",
                        "avatarUrl": "/avatars/35f08bf08c5e9edfb3c78e280af718cb.svg",
                        "isPro": false,
                        "fullname": "Guofeng Zhang",
                        "user": "guofeng1123",
                        "type": "user"
                    },
                    "name": "Guofeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:02:35.026Z",
                    "hidden": false
                },
                {
                    "_id": "675a65ea8e18d279de5b6a38",
                    "user": {
                        "_id": "6504a2629310ce8c40f083c7",
                        "avatarUrl": "/avatars/e6c6bdd868729d9f66240b6957f6215c.svg",
                        "isPro": false,
                        "fullname": "Celso M de Melo",
                        "user": "cdemelo",
                        "type": "user"
                    },
                    "name": "Celso M de Melo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:02:28.868Z",
                    "hidden": false
                },
                {
                    "_id": "675a65ea8e18d279de5b6a39",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "675a65ea8e18d279de5b6a3a",
                    "user": {
                        "_id": "660c9ac4b202fcf3892f62fa",
                        "avatarUrl": "/avatars/7314fd5f3f642096d0e37d3194f1aa7e.svg",
                        "isPro": false,
                        "fullname": "Jieneng Chen",
                        "user": "jienengchen",
                        "type": "user"
                    },
                    "name": "Jieneng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-12T09:02:17.141Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-10T18:55:23.000Z",
            "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
            "summary": "3D spatial reasoning is the ability to analyze and interpret the positions,\norientations, and spatial relationships of objects within the 3D space. This\nallows models to develop a comprehensive understanding of the 3D scene,\nenabling their applicability to a broader range of areas, such as autonomous\nnavigation, robotics, and AR/VR. While large multi-modal models (LMMs) have\nachieved remarkable progress in a wide range of image and video understanding\ntasks, their capabilities to perform 3D spatial reasoning on diverse natural\nimages are less studied. In this work we present the first comprehensive 3D\nspatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual\nquestion-answer pairs across 12 question types. We conduct robust and thorough\nevaluation of 3D spatial reasoning capabilities by balancing the data\ndistribution and adopting a novel FlipEval strategy. To further study the\nrobustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench\nincludes two subsets with 3D spatial reasoning questions on paired images with\ncommon and uncommon viewpoints. We benchmark a wide range of open-sourced and\nproprietary LMMs, uncovering their limitations in various aspects of 3D\nawareness, such as height, orientation, location, and multi-object reasoning,\nas well as their degraded performance on images with uncommon camera\nviewpoints. Our 3DSRBench provide valuable findings and insights about the\nfuture development of LMMs with strong 3D reasoning capabilities. Our project\npage and dataset is available https://3dsrbench.github.io.",
            "upvotes": 11,
            "discussionId": "675a65f08e18d279de5b6bdc"
        },
        "translation": "标题：3DSRBench：一个综合的3D空间推理基准\n\n摘要：3D空间推理是指分析和解释物体在3D空间中的位置、方向和空间关系的能力。这使得模型能够全面理解3D场景，从而扩展其应用范围，如自动驾驶导航、机器人技术和增强现实/虚拟现实（AR/VR）。尽管大规模多模态模型（LMMs）在图像和视频理解任务中取得了显著进展，但它们在处理多样化的自然图像上的3D空间推理能力尚未得到充分研究。在本研究中，我们提出了首个综合的3D空间推理基准——3DSRBench，包含2,772个手动标注的视觉问答对，涵盖12种问题类型。我们通过平衡数据分布并采用一种新颖的FlipEval策略，进行了稳健且全面的3D空间推理能力评估。为进一步研究3D空间推理对相机3D视角的鲁棒性，我们的3DSRBench包括两个子集，分别包含常见和非常见视角的成对图像上的3D空间推理问题。我们对一系列开源和专有的LMMs进行了基准测试，揭示了它们在高度、方向、位置和多物体推理等方面的3D感知局限性，以及在非常见相机视角图像上的性能下降。我们的3DSRBench为未来开发具有强大3D推理能力的LMMs提供了宝贵的发现和见解。项目页面和数据集可在https://3dsrbench.github.io获取。"
    },
    {
        "paper": {
            "id": "2412.08629",
            "authors": [
                {
                    "_id": "675a8cf2c67a008f39d94c29",
                    "name": "Vladimir Kulikov",
                    "hidden": false
                },
                {
                    "_id": "675a8cf2c67a008f39d94c2a",
                    "name": "Matan Kleiner",
                    "hidden": false
                },
                {
                    "_id": "675a8cf2c67a008f39d94c2b",
                    "name": "Inbar Huberman-Spiegelglas",
                    "hidden": false
                },
                {
                    "_id": "675a8cf2c67a008f39d94c2c",
                    "name": "Tomer Michaeli",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T18:50:29.000Z",
            "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow\n  Models",
            "summary": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow\nmodel often involves inverting the image into its corresponding noise map.\nHowever, inversion by itself is typically insufficient for obtaining\nsatisfactory results, and therefore many methods additionally intervene in the\nsampling process. Such methods achieve improved results but are not seamlessly\ntransferable between model architectures. Here, we introduce FlowEdit, a\ntext-based editing method for pre-trained T2I flow models, which is\ninversion-free, optimization-free and model agnostic. Our method constructs an\nODE that directly maps between the source and target distributions\n(corresponding to the source and target text prompts) and achieves a lower\ntransport cost than the inversion approach. This leads to state-of-the-art\nresults, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples\nare available on the project's webpage.",
            "upvotes": 8,
            "discussionId": "675a8cf8c67a008f39d94dfa"
        },
        "translation": "标题：FlowEdit：基于预训练流模型的无逆向文本编辑方法\n\n摘要：使用预训练的文本到图像（T2I）扩散/流模型编辑真实图像通常涉及将图像逆向转换为其对应的噪声图。然而，仅靠逆向转换通常不足以获得令人满意的结果，因此许多方法还需要在采样过程中进行干预。这些方法虽然提高了结果质量，但并不适用于不同模型架构之间的无缝迁移。本文介绍了一种名为FlowEdit的文本编辑方法，适用于预训练的T2I流模型，该方法无需逆向转换、无需优化，并且与模型无关。我们的方法构建了一个常微分方程（ODE），直接在源分布和目标分布（分别对应于源文本提示和目标文本提示）之间进行映射，并且实现了比逆向方法更低的传输成本。这带来了最先进的结果，我们在Stable Diffusion 3和FLUX中展示了这一点。代码和示例可在项目网页上获取。"
    },
    {
        "paper": {
            "id": "2412.07797",
            "authors": [
                {
                    "_id": "675a5805e32fafc3f302070a",
                    "user": {
                        "_id": "647477a1e0b188d3cb20a910",
                        "avatarUrl": "/avatars/5da1cf4f7d727afc9320ed162bd38404.svg",
                        "isPro": false,
                        "fullname": "Fragile Merak",
                        "user": "Frag1le",
                        "type": "user"
                    },
                    "name": "Dongjie Fu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-12-12T03:27:03.907Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T08:30:43.000Z",
            "title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human\n  Motion Generation",
            "summary": "In the field of text-to-motion generation, Bert-type Masked Models (MoMask,\nMMM) currently produce higher-quality outputs compared to GPT-type\nautoregressive models (T2M-GPT). However, these Bert-type models often lack the\nstreaming output capability required for applications in video game and\nmultimedia environments, a feature inherent to GPT-type models. Additionally,\nthey demonstrate weaker performance in out-of-distribution generation. To\nsurpass the quality of BERT-type models while leveraging a GPT-type structure,\nwithout adding extra refinement models that complicate scaling data, we propose\na novel architecture, Mogo (Motion Only Generate Once), which generates\nhigh-quality lifelike 3D human motions by training a single transformer model.\nMogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual\nvector quantization variational autoencoder, which discretizes continuous\nmotion sequences with high precision; 2) Hierarchical Causal Transformer,\nresponsible for generating the base motion sequences in an autoregressive\nmanner while simultaneously inferring residuals across different layers.\nExperimental results demonstrate that Mogo can generate continuous and cyclic\nmotion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10\nseconds) length limitation of existing datasets like HumanML3D. On the\nHumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the\nGPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type\nmodel MMM (FID = 0.080). Furthermore, our model achieves the best quantitative\nperformance in out-of-distribution generation.",
            "upvotes": 7,
            "discussionId": "675a5807e32fafc3f3020808"
        },
        "translation": "标题：Mogo：用于高质量3D人体运动生成的RQ分层因果Transformer\n\n摘要：在文本到运动生成领域，Bert型掩码模型（如MoMask、MMM）相较于GPT型自回归模型（如T2M-GPT）目前能产生更高质量的输出。然而，这些Bert型模型通常缺乏在视频游戏和多媒体环境中所需的视频流输出能力，而这一特性是GPT型模型所固有的。此外，它们在分布外生成方面的表现较弱。为了在保持GPT型结构的同时超越Bert型模型的质量，且不增加额外的精炼模型来复杂化数据扩展，我们提出了一种新颖的架构——Mogo（仅运动生成一次），通过训练单一的Transformer模型生成高质量的逼真3D人体运动。Mogo仅由两个主要组件构成：1）RVQ-VAE，一种分层残差向量量化变分自编码器，能够高精度地离散化连续运动序列；2）分层因果Transformer，负责以自回归方式生成基础运动序列，同时跨不同层推断残差。实验结果表明，Mogo能够生成长达260帧（13秒）的连续和循环运动序列，超越了现有数据集如HumanML3D的196帧（10秒）长度限制。在HumanML3D测试集上，Mogo实现了0.079的FID分数，优于GPT型模型T2M-GPT（FID = 0.116）、AttT2M（FID = 0.112）以及Bert型模型MMM（FID = 0.080）。此外，我们的模型在分布外生成方面实现了最佳的定量性能。"
    },
    {
        "paper": {
            "id": "2412.06016",
            "authors": [
                {
                    "_id": "675b3e766b4e158cc439a67c",
                    "name": "Hyeonho Jeong",
                    "hidden": false
                },
                {
                    "_id": "675b3e766b4e158cc439a67d",
                    "name": "Chun-Hao Paul Huang",
                    "hidden": false
                },
                {
                    "_id": "675b3e766b4e158cc439a67e",
                    "name": "Jong Chul Ye",
                    "hidden": false
                },
                {
                    "_id": "675b3e766b4e158cc439a67f",
                    "name": "Niloy Mitra",
                    "hidden": false
                },
                {
                    "_id": "675b3e766b4e158cc439a680",
                    "name": "Duygu Ceylan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-08T18:21:00.000Z",
            "title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves\n  Video Generation",
            "summary": "While recent foundational video generators produce visually rich output, they\nstill struggle with appearance drift, where objects gradually degrade or change\ninconsistently across frames, breaking visual coherence. We hypothesize that\nthis is because there is no explicit supervision in terms of spatial tracking\nat the feature level. We propose Track4Gen, a spatially aware video generator\nthat combines video diffusion loss with point tracking across frames, providing\nenhanced spatial supervision on the diffusion features. Track4Gen merges the\nvideo generation and point tracking tasks into a single network by making\nminimal changes to existing video generation architectures. Using Stable Video\nDiffusion as a backbone, Track4Gen demonstrates that it is possible to unify\nvideo generation and point tracking, which are typically handled as separate\ntasks. Our extensive evaluations show that Track4Gen effectively reduces\nappearance drift, resulting in temporally stable and visually coherent video\ngeneration. Project page: hyeonho99.github.io/track4gen",
            "upvotes": 6,
            "discussionId": "675b3e7a6b4e158cc439a786"
        },
        "translation": "标题：Track4Gen：通过点跟踪提升视频生成质量\n\n摘要：尽管最近的基础视频生成器能够产生视觉上丰富的输出，但它们在处理外观漂移（appearance drift）问题时仍显不足，即物体在帧间逐渐退化或不一致变化，破坏了视觉连贯性。我们假设这是因为在特征级别缺乏明确的空间跟踪监督。为此，我们提出了Track4Gen，一种空间感知视频生成器，它将视频扩散损失与跨帧点跟踪相结合，为扩散特征提供了增强的空间监督。Track4Gen通过最小化对现有视频生成架构的修改，将视频生成和点跟踪任务合并到一个网络中。以Stable Video Diffusion为骨干，Track4Gen展示了统一视频生成和点跟踪（通常被视为独立任务）的可能性。我们的广泛评估表明，Track4Gen有效地减少了外观漂移，从而实现了时间上稳定且视觉连贯的视频生成。项目页面：hyeonho99.github.io/track4gen"
    },
    {
        "paper": {
            "id": "2412.06071",
            "authors": [
                {
                    "_id": "675ad3a39d2eead3eb196816",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "675ad3a39d2eead3eb196817",
                    "name": "Juyong Jiang",
                    "hidden": false
                },
                {
                    "_id": "675ad3a39d2eead3eb196818",
                    "user": {
                        "_id": "60d3b57ad7b174177faabd6e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg",
                        "isPro": true,
                        "fullname": "chansung park",
                        "user": "chansung",
                        "type": "user"
                    },
                    "name": "Chansung Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-12T13:40:54.230Z",
                    "hidden": false
                },
                {
                    "_id": "675ad3a39d2eead3eb196819",
                    "name": "Sunghun Kim",
                    "hidden": false
                },
                {
                    "_id": "675ad3a39d2eead3eb19681a",
                    "name": "Jing Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-08T21:26:22.000Z",
            "title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models",
            "summary": "The increasing sizes of large language models (LLMs) result in significant\ncomputational overhead and memory usage when adapting these models to specific\ntasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have\nbeen devised to mitigate these challenges by training a small set of parameters\nfor the task-specific updates of the model weights. Among PEFT methods, LoRA\nstands out for its simplicity and efficiency, inspiring the development of a\nseries of variants. However, LoRA and its successors disregard the knowledge\nthat is noisy or irrelevant to the targeted task, detrimentally impacting model\nperformance and leading to suboptimality. To address this limitation, we\nintroduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that\nleverages singular value decomposition (SVD) with knowledge-aware singular\nvalues to dynamically activate knowledge based on its relevance to the task at\nhand. We conduct extensive experiments across a range of LLMs on tasks spanning\nnatural language understanding (NLU), generation (NLG), instruction following,\nand commonsense reasoning. The experimental results demonstrate that KaSA\nconsistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks\nand 4 synthetic datasets, underscoring our method's efficacy and adaptability.\nThe source code of our method is available at\nhttps://github.com/juyongjiang/KaSA.",
            "upvotes": 4,
            "discussionId": "675ad3a59d2eead3eb196893"
        },
        "translation": "标题：KaSA：面向大语言模型的知识感知奇异值适应方法\n\n摘要：随着大语言模型（LLMs）规模的不断扩大，在将这些模型适应于特定任务或领域时，计算开销和内存使用显著增加。为了应对这些挑战，各种参数高效微调（PEFT）方法被提出，通过训练一小部分参数来实现模型权重的任务特定更新。在PEFT方法中，LoRA因其简单性和高效性脱颖而出，并激发了一系列变体的开发。然而，LoRA及其后续方法忽视了与目标任务无关或噪声化的知识，这严重影响了模型性能，导致次优结果。为了解决这一局限性，我们提出了知识感知奇异值适应（KaSA），这是一种PEFT方法，利用奇异值分解（SVD）与知识感知奇异值相结合，根据知识与当前任务的相关性动态激活知识。我们在涵盖自然语言理解（NLU）、生成（NLG）、指令遵循和常识推理的任务中，对一系列LLM进行了广泛的实验。实验结果表明，KaSA在16个基准测试和4个合成数据集上始终优于FFT和14种流行的PEFT基线方法，突显了我们方法的有效性和适应性。我们方法的源代码可在https://github.com/juyongjiang/KaSA获取。"
    },
    {
        "paper": {
            "id": "2412.09551",
            "authors": [
                {
                    "_id": "675b963adbb389af011b5a01",
                    "name": "Yihong Sun",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a02",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a03",
                    "name": "Liangzhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a04",
                    "name": "Jennifer J. Sun",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a05",
                    "name": "Yandong Li",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a06",
                    "name": "Xuhui Jia",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a07",
                    "name": "Hartwig Adam",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a08",
                    "name": "Bharath Hariharan",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a09",
                    "user": {
                        "_id": "650c249887dcda6616baa040",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tIVfxoAHAJ0sWyDMkaarA.jpeg",
                        "isPro": false,
                        "fullname": "Long Zhao",
                        "user": "garyzhao9012",
                        "type": "user"
                    },
                    "name": "Long Zhao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-13T02:13:07.970Z",
                    "hidden": false
                },
                {
                    "_id": "675b963adbb389af011b5a0a",
                    "name": "Ting Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-12T18:41:20.000Z",
            "title": "Video Creation by Demonstration",
            "summary": "We explore a novel video creation experience, namely Video Creation by\nDemonstration. Given a demonstration video and a context image from a different\nscene, we generate a physically plausible video that continues naturally from\nthe context image and carries out the action concepts from the demonstration.\nTo enable this capability, we present delta-Diffusion, a self-supervised\ntraining approach that learns from unlabeled videos by conditional future frame\nprediction. Unlike most existing video generation controls that are based on\nexplicit signals, we adopts the form of implicit latent control for maximal\nflexibility and expressiveness required by general videos. By leveraging a\nvideo foundation model with an appearance bottleneck design on top, we extract\naction latents from demonstration videos for conditioning the generation\nprocess with minimal appearance leakage. Empirically, delta-Diffusion\noutperforms related baselines in terms of both human preference and large-scale\nmachine evaluations, and demonstrates potentials towards interactive world\nsimulation. Sampled video generation results are available at\nhttps://delta-diffusion.github.io/.",
            "upvotes": 3,
            "discussionId": "675b963bdbb389af011b5a75"
        },
        "translation": "标题：基于示范的视频创作\n摘要：我们探索了一种新颖的视频创作体验，即基于示范的视频创作。给定一个示范视频和来自不同场景的上下文图像，我们生成一个物理上合理的视频，该视频从上下文图像自然延续，并执行示范中的动作概念。为了实现这一能力，我们提出了delta-Diffusion，一种自监督训练方法，通过条件未来帧预测从未标记的视频中学习。与大多数基于显式信号的现有视频生成控制方法不同，我们采用了隐式潜在控制形式，以满足通用视频所需的极大灵活性和表现力。通过利用具有外观瓶颈设计的视频基础模型，我们从示范视频中提取动作潜在变量，以最小化外观泄露的方式条件化生成过程。在实证研究中，delta-Diffusion在人类偏好和大规模机器评估方面均优于相关基线，并展示了向交互式世界模拟的潜力。样本视频生成结果可在https://delta-diffusion.github.io/ 查看。"
    },
    {
        "paper": {
            "id": "2412.06676",
            "authors": [
                {
                    "_id": "6759fcc41f49e61f776bb7ce",
                    "user": {
                        "_id": "65b7afaeb4ec059b974e9189",
                        "avatarUrl": "/avatars/08059754f2d299b76ab4649e8d768253.svg",
                        "isPro": false,
                        "fullname": "Roi Cohen",
                        "user": "roicohen9",
                        "type": "user"
                    },
                    "name": "Roi Cohen",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-11T21:01:39.374Z",
                    "hidden": false
                },
                {
                    "_id": "6759fcc41f49e61f776bb7cf",
                    "name": "Konstantin Dobler",
                    "hidden": false
                },
                {
                    "_id": "6759fcc41f49e61f776bb7d0",
                    "name": "Eden Biran",
                    "hidden": false
                },
                {
                    "_id": "6759fcc41f49e61f776bb7d1",
                    "name": "Gerard de Melo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T17:13:20.000Z",
            "title": "I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token",
            "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we propose a novel\ncalibration method that can be used to combat hallucinations. We add a special\n[IDK] (\"I don't know\") token to the model's vocabulary and introduce an\nobjective function that shifts probability mass to the [IDK] token for\nincorrect predictions. This approach allows the model to express uncertainty in\nits output explicitly. We evaluate our proposed method across multiple model\narchitectures and factual downstream tasks. We find that models trained with\nour method are able to express uncertainty in places where they would\npreviously make mistakes while suffering only a small loss of encoded\nknowledge. We further perform extensive ablation studies of multiple variations\nof our approach and provide a detailed analysis of the precision-recall\ntradeoff of our method.",
            "upvotes": 2,
            "discussionId": "6759fcc51f49e61f776bb814"
        },
        "translation": "标题：我不知道：使用[IDK]标记显式建模不确定性\n\n摘要：大型语言模型能够捕捉现实世界知识，使其在许多下游任务中表现出色。尽管最近取得了进展，这些模型仍然容易产生所谓的幻觉，导致它们输出不希望的和事实错误的内容。在这项工作中，我们提出了一种新颖的校准方法，用于对抗幻觉。我们在模型的词汇表中添加了一个特殊的[IDK]（“我不知道”）标记，并引入了一个目标函数，将概率质量转移到错误预测的[IDK]标记上。这种方法使模型能够在其输出中显式表达不确定性。我们在多种模型架构和事实下游任务中评估了我们提出的方法。我们发现，使用我们方法训练的模型能够在之前会出错的地方表达不确定性，同时仅遭受少量编码知识的损失。我们进一步进行了广泛的消融研究，探讨了我们方法的多种变体，并提供了关于我们方法精确率-召回率权衡的详细分析。"
    },
    {
        "paper": {
            "id": "2412.08503",
            "authors": [
                {
                    "_id": "675a91605f94292180d7856a",
                    "name": "Mingkun Lei",
                    "hidden": false
                },
                {
                    "_id": "675a91605f94292180d7856b",
                    "name": "Xue Song",
                    "hidden": false
                },
                {
                    "_id": "675a91605f94292180d7856c",
                    "name": "Beier Zhu",
                    "hidden": false
                },
                {
                    "_id": "675a91605f94292180d7856d",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "675a91605f94292180d7856e",
                    "name": "Chi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T16:13:23.000Z",
            "title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style\n  Elements",
            "summary": "Text-driven style transfer aims to merge the style of a reference image with\ncontent described by a text prompt. Recent advancements in text-to-image models\nhave improved the nuance of style transformations, yet significant challenges\nremain, particularly with overfitting to reference styles, limiting stylistic\ncontrol, and misaligning with textual content. In this paper, we propose three\ncomplementary strategies to address these issues. First, we introduce a\ncross-modal Adaptive Instance Normalization (AdaIN) mechanism for better\nintegration of style and text features, enhancing alignment. Second, we develop\na Style-based Classifier-Free Guidance (SCFG) approach that enables selective\ncontrol over stylistic elements, reducing irrelevant influences. Finally, we\nincorporate a teacher model during early generation stages to stabilize spatial\nlayouts and mitigate artifacts. Our extensive evaluations demonstrate\nsignificant improvements in style transfer quality and alignment with textual\nprompts. Furthermore, our approach can be integrated into existing style\ntransfer frameworks without fine-tuning.",
            "upvotes": 2,
            "discussionId": "675a91655f94292180d786e0"
        },
        "translation": "标题：StyleStudio：基于文本驱动的风格迁移与风格元素选择性控制\n\n摘要：文本驱动的风格迁移旨在将参考图像的风格与由文本提示描述的内容相结合。近年来，文本到图像模型的进步提高了风格转换的细微差别，但仍存在显著挑战，特别是对参考风格的过拟合、风格控制的局限性以及与文本内容的不一致。本文提出了三种互补策略来解决这些问题。首先，我们引入了一种跨模态的自适应实例归一化（AdaIN）机制，以更好地整合风格和文本特征，增强一致性。其次，我们开发了一种基于风格的无分类器指导（SCFG）方法，该方法能够实现对风格元素的选择性控制，减少无关影响。最后，我们在早期生成阶段引入了一个教师模型，以稳定空间布局并减少伪影。我们的广泛评估表明，在风格迁移质量和与文本提示的一致性方面有显著改进。此外，我们的方法可以集成到现有的风格迁移框架中，而无需微调。"
    },
    {
        "paper": {
            "id": "2412.07147",
            "authors": [
                {
                    "_id": "6758f7bfcbf493892e08e882",
                    "user": {
                        "_id": "6582c482f3006507ea10302a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
                        "isPro": false,
                        "fullname": "Bo Li",
                        "user": "liboaccn",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-11T09:20:03.698Z",
                    "hidden": false
                },
                {
                    "_id": "6758f7bfcbf493892e08e883",
                    "name": "Shaolin Zhu",
                    "hidden": false
                },
                {
                    "_id": "6758f7bfcbf493892e08e884",
                    "name": "Lijie Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-10T03:12:35.000Z",
            "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
            "summary": "Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.",
            "upvotes": 1,
            "discussionId": "6758f7c2cbf493892e08e908"
        },
        "translation": "标题：MIT-10M：一个大规模多语言图像翻译平行语料库\n\n摘要：图像翻译（IT）在多个领域具有巨大的潜力，能够将图像中的文本内容翻译成多种语言。然而，现有数据集往往在规模、多样性和质量上存在局限，阻碍了IT模型的发展和评估。为解决这一问题，我们引入了MIT-10M，这是一个包含超过1000万图像-文本对的大规模多语言图像翻译平行语料库，这些数据来源于真实世界数据，并经过了广泛的数据清洗和多语言翻译验证。该语料库包含84万张三种尺寸的图像，涵盖28个类别，任务难度分为三个级别，以及14种语言的图像-文本对，相较于现有数据集有显著提升。我们对MIT-10M进行了广泛的实验，以评估和训练模型。实验结果清楚地表明，我们的数据集在评估模型应对现实世界中具有挑战性和复杂性的图像翻译任务时，具有更高的适应性。此外，使用MIT-10M微调的模型的性能比基线模型提高了三倍，进一步证实了其优越性。"
    },
    {
        "paper": {
            "id": "2412.08467",
            "authors": [
                {
                    "_id": "675ad589659e5553260b6610",
                    "name": "Zun Wang",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6611",
                    "name": "Jialu Li",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6612",
                    "name": "Yicong Hong",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6613",
                    "name": "Songze Li",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6614",
                    "name": "Kunchang Li",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6615",
                    "name": "Shoubin Yu",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6616",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6617",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6618",
                    "name": "Yali Wang",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b6619",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "675ad589659e5553260b661a",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-11T15:32:24.000Z",
            "title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining\n  Data Flywheel",
            "summary": "Creating high-quality data for training robust language-instructed agents is\na long-lasting challenge in embodied AI. In this paper, we introduce a\nSelf-Refining Data Flywheel (SRDF) that generates high-quality and large-scale\nnavigational instruction-trajectory pairs by iteratively refining the data pool\nthrough the collaboration between two models, the instruction generator and the\nnavigator, without any human-in-the-loop annotation. Specifically, SRDF starts\nwith using a base generator to create an initial data pool for training a base\nnavigator, followed by applying the trained navigator to filter the data pool.\nThis leads to higher-fidelity data to train a better generator, which can, in\nturn, produce higher-quality data for training the next-round navigator. Such a\nflywheel establishes a data self-refining process, yielding a continuously\nimproved and highly effective dataset for large-scale language-guided\nnavigation learning. Our experiments demonstrate that after several flywheel\nrounds, the navigator elevates the performance boundary from 70% to 78% SPL on\nthe classic R2R test set, surpassing human performance (76%) for the first\ntime. Meanwhile, this process results in a superior generator, evidenced by a\nSPICE increase from 23.5 to 26.2, better than all previous VLN instruction\ngeneration methods. Finally, we demonstrate the scalability of our method\nthrough increasing environment and instruction diversity, and the\ngeneralization ability of our pre-trained navigator across various downstream\nnavigation tasks, surpassing state-of-the-art methods by a large margin in all\ncases.",
            "upvotes": 0,
            "discussionId": "675ad58b659e5553260b667a"
        },
        "translation": "标题：基于自精炼数据飞轮的引导语言导航学习\n\n摘要：在具身人工智能领域，为训练鲁棒的语言指令代理创建高质量数据一直是一个长期挑战。本文提出了一种自精炼数据飞轮（Self-Refining Data Flywheel, SRDF），通过指令生成器和导航器两个模型的协作，迭代精炼数据池，生成高质量和大规模的导航指令-轨迹对，且无需任何人工标注。具体而言，SRDF首先使用基础生成器创建初始数据池以训练基础导航器，然后利用训练好的导航器过滤数据池。这产生了更高保真度的数据，用于训练更好的生成器，进而生成更高质量的数据以训练下一轮导航器。这种飞轮机制建立了一个数据自精炼过程，产生了一个持续改进且高度有效的大规模语言引导导航学习数据集。我们的实验表明，经过几轮飞轮迭代后，导航器在经典R2R测试集上的性能边界从70%提升至78% SPL，首次超越人类表现（76%）。同时，这一过程产生了一个更优的生成器，SPICE得分从23.5提升至26.2，优于所有先前的视觉语言导航（VLN）指令生成方法。最后，我们通过增加环境和指令的多样性展示了方法的可扩展性，并证明了预训练导航器在各种下游导航任务中的泛化能力，在所有情况下均大幅超越了最先进的方法。"
    }  
]