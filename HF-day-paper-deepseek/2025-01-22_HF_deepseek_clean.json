[
  {
    "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
    "summary": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.",
    "translation": "标题：Condor：通过知识驱动的数据合成与精炼增强大语言模型对齐\n\n摘要：监督微调（SFT）数据的质量在提升大语言模型（LLMs）的对话能力方面起着至关重要的作用。然而，随着LLMs的不断进步，高质量人工标注的SFT数据的可用性已成为一个显著的瓶颈，这使得对合成训练数据的依赖变得更为必要。在本研究中，我们提出了Condor，一种新颖的两阶段合成数据生成框架，该框架结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20K Condor生成的样本上进行微调的基础模型，相较于其他模型，表现出了更优的性能。Condor中的额外精炼阶段进一步实现了不同规模（高达72B）LLMs的迭代自我改进，验证了我们方法的有效性。此外，我们对训练后合成数据扩展的研究揭示了性能提升的巨大未开发潜力，为未来的研究开辟了有前景的途径。",
    "url": "https://huggingface.co/papers/2501.12273",
    "arxiv_url": "https://arxiv.org/abs/2501.12273"
  }
]