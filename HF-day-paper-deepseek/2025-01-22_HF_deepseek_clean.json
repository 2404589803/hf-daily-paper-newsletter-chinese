[
  {
    "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "summary": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\nfor evaluating foundation models in video understanding. MMVU includes 3,000\nexpert-annotated questions spanning 27 subjects across four core disciplines:\nScience, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\nprior benchmarks, MMVU features three key advancements. First, it challenges\nmodels to apply domain-specific knowledge and perform expert-level reasoning to\nanalyze specialized-domain videos, moving beyond the basic visual perception\ntypically assessed in current video benchmarks. Second, each example is\nannotated by human experts from scratch. We implement strict data quality\ncontrols to ensure the high quality of the dataset. Finally, each example is\nenriched with expert-annotated reasoning rationals and relevant domain\nknowledge, facilitating in-depth analysis. We conduct an extensive evaluation\nof 32 frontier multimodal foundation models on MMVU. The latest\nSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest\nperformance among the tested models. However, they still fall short of matching\nhuman expertise. Through in-depth error analyses and case studies, we offer\nactionable insights for future advancements in expert-level,\nknowledge-intensive video understanding for specialized domains.",
    "translation": "标题：MMVU：专家级多学科视频理解评估\n\n摘要：我们介绍了MMVU，一个全面的专家级多学科基准，用于评估视频理解中的基础模型。MMVU包含3000个由专家标注的问题，涵盖四个核心学科的27个主题：科学、医疗保健、人文与社会科学以及工程学。与之前的基准相比，MMVU具有三个关键进展。首先，它挑战模型应用特定领域的知识并进行专家级推理来分析专业领域的视频，超越了当前视频基准中通常评估的基本视觉感知。其次，每个示例都由人类专家从头开始标注。我们实施了严格的数据质量控制，以确保数据集的高质量。最后，每个示例都丰富了专家标注的推理理由和相关领域知识，便于深入分析。我们对32个前沿多模态基础模型在MMVU上进行了广泛评估。最新的具备System-2能力的模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳。然而，它们仍然无法与人类专家的水平相匹配。通过深入的错误分析和案例研究，我们为未来在专家级、知识密集型的专业领域视频理解方面的进展提供了可行的见解。",
    "url": "https://huggingface.co/papers/2501.12380",
    "arxiv_url": "https://arxiv.org/abs/2501.12380"
  },
  {
    "title": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
    "summary": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
    "translation": "标题：Agent-R：通过迭代自训练训练语言模型代理进行反思\n\n摘要：大型语言模型（LLMs）代理在交互环境中解决复杂任务的作用日益重要。现有工作主要集中在通过从更强的专家进行行为克隆来提升性能，然而这种方法在现实应用中往往表现不佳，主要原因是无法从错误中恢复。然而，步骤级的批评数据难以收集且成本高昂。因此，自动化和动态构建自我批评数据集对于赋予模型智能代理能力至关重要。在本研究中，我们提出了一个迭代自训练框架，Agent-R，使语言代理能够即时反思。与传统方法根据正确性奖励或惩罚行为不同，Agent-R利用蒙特卡洛树搜索（MCTS）构建训练数据，从错误轨迹中恢复正确轨迹。代理反思的一个关键挑战在于需要及时修正，而不是等到整个轨迹结束。为了解决这个问题，我们引入了一种模型引导的批评构建机制：演员模型在失败轨迹中识别出第一个错误步骤（在其当前能力范围内）。从该步骤开始，我们将其与树中具有相同父节点的相邻正确路径拼接。这种策略使模型能够基于其当前策略学习反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代优化。我们的研究结果表明，Agent-R持续提高了模型从错误中恢复的能力，并实现了及时的错误纠正。在三个交互环境中的实验表明，Agent-R有效地使代理能够纠正错误行为，同时避免循环，与基线方法相比实现了更优的性能（+5.59%）。",
    "url": "https://huggingface.co/papers/2501.11425",
    "arxiv_url": "https://arxiv.org/abs/2501.11425"
  },
  {
    "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
    "summary": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.",
    "translation": "标题：Condor：通过知识驱动的数据合成与精炼增强大语言模型对齐\n\n摘要：监督微调（SFT）数据的质量在提升大语言模型（LLMs）的对话能力方面起着至关重要的作用。然而，随着LLMs的不断进步，高质量人工标注的SFT数据的可用性已成为一个显著的瓶颈，这导致了对合成训练数据的更大依赖。在本研究中，我们引入了Condor，一个新颖的两阶段合成数据生成框架，该框架结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20K Condor生成的样本上进行微调的基础模型，相较于其他模型，表现出了更优的性能。Condor中的额外精炼阶段进一步实现了不同规模（高达72B）LLMs的迭代自我改进，验证了我们方法的有效性。此外，我们对训练后合成数据扩展的研究揭示了性能改进的巨大未开发潜力，为未来的研究开辟了有前景的途径。",
    "url": "https://huggingface.co/papers/2501.12273",
    "arxiv_url": "https://arxiv.org/abs/2501.12273"
  },
  {
    "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
    "summary": "Smartphones have become indispensable in modern life, yet navigating complex\ntasks on mobile devices often remains frustrating. Recent advancements in large\nmultimodal model (LMM)-based mobile agents have demonstrated the ability to\nperceive and act in mobile environments. However, current approaches face\nsignificant limitations: they fall short in addressing real-world human needs,\nstruggle with reasoning-intensive and long-horizon tasks, and lack mechanisms\nto learn and improve from prior experiences. To overcome these challenges, we\nintroduce Mobile-Agent-E, a hierarchical multi-agent framework capable of\nself-evolution through past experience. By hierarchical, we mean an explicit\nseparation of high-level planning and low-level action execution. The framework\ncomprises a Manager, responsible for devising overall plans by breaking down\ncomplex tasks into subgoals, and four subordinate agents--Perceptor, Operator,\nAction Reflector, and Notetaker--which handle fine-grained visual perception,\nimmediate action execution, error verification, and information aggregation,\nrespectively. Mobile-Agent-E also features a novel self-evolution module which\nmaintains a persistent long-term memory comprising Tips and Shortcuts. Tips are\ngeneral guidance and lessons learned from prior tasks on how to effectively\ninteract with the environment. Shortcuts are reusable, executable sequences of\natomic operations tailored for specific subroutines. The inclusion of Tips and\nShortcuts facilitates continuous refinement in performance and efficiency.\nAlongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring\ncomplex mobile tasks requiring long-horizon, multi-app interactions. Empirical\nresults show that Mobile-Agent-E achieves a 22% absolute improvement over\nprevious state-of-the-art approaches across three foundation model backbones.\nProject page: https://x-plug.github.io/MobileAgent.",
    "translation": "标题：Mobile-Agent-E：面向复杂任务的自进化移动助手\n\n摘要：智能手机已成为现代生活中不可或缺的工具，然而在移动设备上处理复杂任务仍然常常令人沮丧。近年来，基于大型多模态模型（LMM）的移动代理在感知和操作移动环境方面取得了显著进展。然而，现有方法仍面临重大局限：它们难以满足现实世界中的人类需求，难以处理需要密集推理和长期规划的任务，并且缺乏从以往经验中学习和改进的机制。为应对这些挑战，我们提出了Mobile-Agent-E，一个能够通过过往经验实现自我进化的分层多代理框架。分层意味着将高层规划与低层操作执行明确分离。该框架包含一个负责将复杂任务分解为子目标并制定总体计划的Manager，以及四个下属代理——Perceptor（感知器）、Operator（操作器）、Action Reflector（动作反射器）和Notetaker（记录器），分别负责细粒度视觉感知、即时操作执行、错误验证和信息聚合。Mobile-Agent-E还引入了一个新颖的自我进化模块，该模块维护了一个包含Tips（技巧）和Shortcuts（快捷方式）的持久长期记忆。Tips是从先前任务中总结出的关于如何有效与环境交互的通用指导和经验教训。Shortcuts是为特定子程序定制的可重用、可执行的原子操作序列。Tips和Shortcuts的引入促进了性能和效率的持续优化。此外，我们还提出了Mobile-Eval-E，这是一个新的基准测试，包含需要长期规划和多应用交互的复杂移动任务。实验结果表明，在三个基础模型骨干上，Mobile-Agent-E相比之前的最先进方法实现了22%的绝对性能提升。项目页面：https://x-plug.github.io/MobileAgent。",
    "url": "https://huggingface.co/papers/2501.11733",
    "arxiv_url": "https://arxiv.org/abs/2501.11733"
  },
  {
    "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
    "summary": "This paper introduces UI-TARS, a native GUI agent model that solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots for context-aware understanding of UI elements and precise\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, involving multiple\nreasoning patterns such as task decomposition, reflection thinking, milestone\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\naddresses the data bottleneck by automatically collecting, filtering, and\nreflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns\nfrom its mistakes and adapts to unforeseen situations with minimal human\nintervention. We also analyze the evolution path of GUI agents to guide the\nfurther development of this domain.",
    "translation": "标题：UI-TARS：开创性的自动化GUI交互原生代理\n\n摘要：本文介绍了UI-TARS，一种原生GUI代理模型，仅以屏幕截图作为输入，并执行类似人类的交互操作（如键盘和鼠标操作）。与依赖高度封装的商业模型（如GPT-4o）以及专家设计的提示和工作流程的主流代理框架不同，UI-TARS是一个端到端模型，其性能优于这些复杂的框架。实验证明了其卓越的性能：UI-TARS在10多个评估感知、基础化和GUI任务执行的GUI代理基准测试中达到了SOTA性能。特别是在OSWorld基准测试中，UI-TARS在50步和15步中分别获得了24.6和22.7的分数，优于Claude（分别为22.0和14.9）。在AndroidWorld中，UI-TARS获得了46.6的分数，超过了GPT-4o（34.5）。UI-TARS融合了几项关键创新：（1）增强的感知：利用大规模的GUI截图数据集进行上下文感知的UI元素理解和精确的标注；（2）统一动作建模：将动作标准化为跨平台的统一空间，并通过大规模动作轨迹实现精确的基础化和交互；（3）系统-2推理：将深思熟虑的推理融入多步决策中，涉及任务分解、反思思维、里程碑识别等多种推理模式；（4）带有反思在线轨迹的迭代训练：通过在数百台虚拟机上自动收集、过滤和反思性地精炼新的交互轨迹，解决了数据瓶颈问题。通过迭代训练和反思调优，UI-TARS不断从错误中学习，并在最少的人工干预下适应不可预见的情况。我们还分析了GUI代理的演进路径，以指导该领域的进一步发展。",
    "url": "https://huggingface.co/papers/2501.12326",
    "arxiv_url": "https://arxiv.org/abs/2501.12326"
  },
  {
    "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
    "summary": "In this paper, we propose a novel audio-driven talking head method capable of\nsimultaneously generating highly expressive facial expressions and hand\ngestures. Unlike existing methods that focus on generating full-body or\nhalf-body poses, we investigate the challenges of co-speech gesture generation\nand identify the weak correspondence between audio features and full-body\ngestures as a key limitation. To address this, we redefine the task as a\ntwo-stage process. In the first stage, we generate hand poses directly from\naudio input, leveraging the strong correlation between audio signals and hand\nmovements. In the second stage, we employ a diffusion model to synthesize video\nframes, incorporating the hand poses generated in the first stage to produce\nrealistic facial expressions and body movements. Our experimental results\ndemonstrate that the proposed method outperforms state-of-the-art approaches,\nsuch as CyberHost and Vlogger, in terms of both visual quality and\nsynchronization accuracy. This work provides a new perspective on audio-driven\ngesture generation and a robust framework for creating expressive and natural\ntalking head animations.",
    "translation": "标题：EMO2：基于末端执行器引导的音频驱动虚拟形象视频生成\n\n摘要：本文提出了一种新颖的音频驱动说话头生成方法，能够同时生成高度丰富的面部表情和手势。与现有方法主要关注生成全身或半身姿态不同，我们研究了伴随语音手势生成的挑战，并发现音频特征与全身手势之间的弱相关性是一个关键限制。为了解决这一问题，我们将任务重新定义为两阶段过程。在第一阶段，我们直接从音频输入生成手部姿态，利用音频信号与手部运动之间的强相关性。在第二阶段，我们采用扩散模型合成视频帧，结合第一阶段生成的手部姿态，以生成逼真的面部表情和身体动作。实验结果表明，所提出的方法在视觉质量和同步精度方面均优于现有最先进的方法，如CyberHost和Vlogger。这项工作为音频驱动的手势生成提供了新的视角，并为创建富有表现力和自然的说话头动画提供了一个稳健的框架。",
    "url": "https://huggingface.co/papers/2501.10687",
    "arxiv_url": "https://arxiv.org/abs/2501.10687"
  },
  {
    "title": "Reasoning Language Models: A Blueprint",
    "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending large language models\n(LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary\nnature, and complex architectures - uniquely combining Reinforcement Learning\n(RL), search heuristics, and LLMs - present accessibility and scalability\nchallenges. To address these, we propose a comprehensive blueprint that\norganizes RLM components into a modular framework, based on a survey and\nanalysis of all RLM works. This blueprint incorporates diverse reasoning\nstructures (chains, trees, graphs, and nested forms), reasoning strategies\n(e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models\nand others), and supervision schemes (Output-Based and Process-Based\nSupervision). We also provide detailed mathematical formulations and\nalgorithmic specifications to simplify RLM implementation. By showing how\nschemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as\nspecial cases, we demonstrate the blueprint's versatility and unifying\npotential. To illustrate its utility, we introduce x1, a modular implementation\nfor rapid RLM prototyping and experimentation. Using x1 and a literature\nreview, we provide key insights, such as multi-phase training for policy and\nvalue models, and the importance of familiar training distributions. Finally,\nwe outline how RLMs can integrate with a broader LLM ecosystem, including tools\nand databases. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.",
    "translation": "标题：推理语言模型：一个蓝图\n\n摘要：推理语言模型（RLMs），也称为大型推理模型（LRMs），如OpenAI的o1和o3、DeepSeek-V3和阿里的QwQ，通过扩展大型语言模型（LLMs）并引入先进的推理机制，重新定义了AI的解决问题的能力。然而，它们的高成本、专有性质以及复杂的架构——独特地结合了强化学习（RL）、搜索启发式和LLMs——带来了可访问性和可扩展性的挑战。为了解决这些问题，我们提出了一个全面的蓝图，基于对所有RLM工作的调查和分析，将RLM组件组织成一个模块化框架。该蓝图包含了多样化的推理结构（链、树、图和嵌套形式）、推理策略（如蒙特卡洛树搜索、束搜索）、RL概念（策略、价值模型等）和监督方案（基于输出和基于过程的监督）。我们还提供了详细的数学公式和算法规范，以简化RLM的实现。通过展示LLaMA-Berry、QwQ、Journey Learning和Graph of Thoughts等方案如何作为特例适应，我们展示了蓝图的多样性和统一潜力。为了说明其实用性，我们引入了x1，一个用于快速RLM原型设计和实验的模块化实现。通过使用x1和文献综述，我们提供了关键见解，如策略和价值模型的多阶段训练，以及熟悉训练分布的重要性。最后，我们概述了RLMs如何与更广泛的LLM生态系统（包括工具和数据库）集成。我们的工作揭示了RLM构建的神秘面纱，普及了先进的推理能力，并促进了创新，旨在通过降低RLM开发和实验的门槛，缩小“富AI”和“穷AI”之间的差距。",
    "url": "https://huggingface.co/papers/2501.11223",
    "arxiv_url": "https://arxiv.org/abs/2501.11223"
  },
  {
    "title": "GPS as a Control Signal for Image Generation",
    "summary": "We show that the GPS tags contained in photo metadata provide a useful\ncontrol signal for image generation. We train GPS-to-image models and use them\nfor tasks that require a fine-grained understanding of how images vary within a\ncity. In particular, we train a diffusion model to generate images conditioned\non both GPS and text. The learned model generates images that capture the\ndistinctive appearance of different neighborhoods, parks, and landmarks. We\nalso extract 3D models from 2D GPS-to-image models through score distillation\nsampling, using GPS conditioning to constrain the appearance of the\nreconstruction from each viewpoint. Our evaluations suggest that our\nGPS-conditioned models successfully learn to generate images that vary based on\nlocation, and that GPS conditioning improves estimated 3D structure.",
    "translation": "标题：GPS作为图像生成的控制信号\n\n摘要：我们证明了照片元数据中包含的GPS标签为图像生成提供了有用的控制信号。我们训练了GPS到图像的模型，并将其用于需要精细理解城市内图像变化的任务。特别是，我们训练了一个扩散模型，以生成基于GPS和文本条件的图像。学习到的模型生成的图像捕捉了不同社区、公园和地标的独特外观。我们还通过分数蒸馏采样从2D GPS到图像模型中提取3D模型，使用GPS条件来约束每个视角下重建的外观。我们的评估表明，我们的GPS条件模型成功地学会了根据位置生成变化的图像，并且GPS条件改善了估计的3D结构。",
    "url": "https://huggingface.co/papers/2501.12390",
    "arxiv_url": "https://arxiv.org/abs/2501.12390"
  },
  {
    "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\n  Assets Generation",
    "summary": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\ngenerating high-resolution textured 3D assets. This system includes two\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\ncreate geometry that properly aligns with a given condition image, laying a\nsolid foundation for downstream applications. The texture synthesis model,\nbenefiting from strong geometric and diffusion priors, produces high-resolution\nand vibrant texture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\nplatform that simplifies the re-creation process of 3D assets. It allows both\nprofessional and amateur users to manipulate or even animate their meshes\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\noutperforms previous state-of-the-art models, including the open-source models\nand closed-source models in geometry details, condition alignment, texture\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\nin the open-source 3D community for large-scale foundation generative models.\nThe code and pre-trained weights of our models are available at:\nhttps://github.com/Tencent/Hunyuan3D-2",
    "translation": "标题：Hunyuan3D 2.0：面向高分辨率纹理3D资产生成的扩散模型扩展\n\n摘要：我们介绍了Hunyuan3D 2.0，这是一个先进的大规模3D合成系统，用于生成高分辨率的纹理3D资产。该系统包括两个基础组件：一个大规模形状生成模型——Hunyuan3D-DiT，以及一个大规模纹理合成模型——Hunyuan3D-Paint。形状生成模型基于可扩展的基于流的扩散变换器，旨在创建与给定条件图像正确对齐的几何形状，为下游应用奠定坚实基础。纹理合成模型得益于强大的几何和扩散先验，为生成或手工制作的网格生成高分辨率和生动的纹理贴图。此外，我们构建了Hunyuan3D-Studio——一个多功能、用户友好的生产平台，简化了3D资产的重建过程。它允许专业和业余用户高效地操作甚至动画化他们的网格。我们系统地评估了我们的模型，结果表明Hunyuan3D 2.0在几何细节、条件对齐、纹理质量等方面优于之前的最先进模型，包括开源模型和闭源模型。Hunyuan3D 2.0公开发布，旨在填补开源3D社区在大规模基础生成模型方面的空白。我们的模型的代码和预训练权重可在以下网址获取：https://github.com/Tencent/Hunyuan3D-2",
    "url": "https://huggingface.co/papers/2501.12202",
    "arxiv_url": "https://arxiv.org/abs/2501.12202"
  },
  {
    "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in\n  Realistic Environments",
    "summary": "Autonomous agents powered by large language models (LLMs) have the potential\nto enhance human capabilities, assisting with digital tasks from sending emails\nto performing data analysis. The abilities of existing LLMs at such tasks are\noften hindered by the lack of high-quality agent data from the corresponding\nenvironments they interact with. We propose Learn-by-interact, a data-centric\nframework to adapt LLM agents to any given environments without human\nannotations. Learn-by-interact synthesizes trajectories of agent-environment\ninteractions based on documentations, and constructs instructions by\nsummarizing or abstracting the interaction histories, a process called backward\nconstruction. We assess the quality of our synthetic data by using them in both\ntraining-based scenarios and training-free in-context learning (ICL), where we\ncraft innovative retrieval approaches optimized for agents. Extensive\nexperiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across\nrealistic coding, web, and desktop environments show the effectiveness of\nLearn-by-interact in various downstream agentic tasks -- baseline results are\nimproved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with\nCodestral-22B. We further demonstrate the critical role of backward\nconstruction, which provides up to 14.0\\% improvement for training. Our\nablation studies demonstrate the efficiency provided by our synthesized data in\nICL and the superiority of our retrieval pipeline over alternative approaches\nlike conventional retrieval-augmented generation (RAG). We expect that\nLearn-by-interact will serve as a foundation for agent data synthesis as LLMs\nare increasingly deployed at real-world environments.",
    "translation": "标题：交互学习：现实环境中自适应智能体的数据驱动框架\n\n摘要：由大型语言模型（LLMs）驱动的自主智能体有潜力增强人类能力，协助完成从发送电子邮件到数据分析等数字任务。然而，现有LLMs在这些任务中的能力往往受到缺乏高质量智能体数据的影响，这些数据来自它们所交互的相应环境。我们提出了“交互学习”（Learn-by-interact），这是一个以数据为中心的框架，旨在使LLM智能体适应任何给定环境，而无需人工标注。交互学习基于文档合成智能体与环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程称为反向构建。我们通过在基于训练的场景和无训练的上下文学习（ICL）中使用这些合成数据来评估其质量，其中我们设计了针对智能体优化的创新检索方法。在SWE-bench、WebArena、OSWorld和Spider2-V等现实编码、网络和桌面环境中的广泛实验表明，交互学习在各种下游智能体任务中的有效性——使用Claude-3.5的ICL基线结果提高了12.2%，使用Codestral-22B的训练基线结果提高了19.5%。我们进一步证明了反向构建的关键作用，它为训练提供了高达14.0%的改进。我们的消融研究表明，我们的合成数据在ICL中提供了效率，并且我们的检索管道优于传统的检索增强生成（RAG）等替代方法。我们期望，随着LLMs在现实世界环境中的部署越来越多，交互学习将成为智能体数据合成的基础。",
    "url": "https://huggingface.co/papers/2501.10893",
    "arxiv_url": "https://arxiv.org/abs/2501.10893"
  },
  {
    "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models",
    "summary": "This paper revisits the implementation of\nLoad-balancing Loss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E\nsum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i\nrepresents the frequency of expert i being selected, and p_i denotes the\naverage gating score of the expert i. Existing MoE training frameworks\nusually employ the parallel training strategy so that f_i and the LBL are\ncalculated within a micro-batch and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence (e.g., code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a global-batch to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize f_i across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n42.8B total parameters and 400B tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts.",
    "translation": "标题：细节中的魔鬼：在训练专用专家混合模型中实现负载均衡损失的探讨\n\n摘要：本文重新审视了在训练专家混合模型（Mixture-of-Experts, MoEs）时负载均衡损失（Load-balancing Loss, LBL）的实现。具体而言，MoEs的LBL定义为N_E sum_{i=1}^{N_E} f_i p_i，其中N_E是专家的总数，f_i表示专家i被选择的频率，p_i表示专家i的平均门控分数。现有的MoE训练框架通常采用并行训练策略，使得f_i和LBL在一个微批次内计算，然后在并行组之间进行平均。本质上，用于训练数十亿规模大型语言模型（LLMs）的微批次通常包含非常少的序列。因此，微批次的LBL几乎是在序列级别上，路由器被推动在每个序列内均匀分配令牌。在这种严格的约束下，即使是来自特定领域序列（例如代码）的令牌也会被均匀路由到所有专家，从而抑制了专家的专业化。在本研究中，我们提出使用全局批次来计算LBL，以放松这一约束。因为全局批次包含比微批次更多样化的序列，这将鼓励在语料库级别上的负载均衡。具体而言，我们引入了一个额外的通信步骤来同步微批次之间的f_i，然后使用它来计算LBL。通过在训练基于MoEs的LLMs（总参数高达42.8B，令牌数高达400B）上的实验，我们惊讶地发现，全局批次LBL策略在预训练困惑度和下游任务中都带来了显著的性能提升。我们的分析表明，全局批次LBL还大大提高了MoE专家的领域专业化程度。",
    "url": "https://huggingface.co/papers/2501.11873",
    "arxiv_url": "https://arxiv.org/abs/2501.11873"
  },
  {
    "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise",
    "summary": "Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code\nand model checkpoints are available on GitHub:\nhttps://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.",
    "translation": "标题：随流而动：使用实时扭曲噪声实现运动可控的视频扩散模型\n\n摘要：生成建模的目标是将随机噪声转化为结构化的输出。在本研究中，我们通过允许通过结构化潜在噪声采样来控制运动，从而增强了视频扩散模型。这仅通过数据的变化实现：我们对训练视频进行预处理以生成结构化噪声。因此，我们的方法与扩散模型设计无关，无需改变模型架构或训练流程。具体而言，我们提出了一种新颖的噪声扭曲算法，该算法足够快以实时运行，用从光流场导出的相关扭曲噪声替换随机时间高斯性，同时保留空间高斯性。我们算法的高效性使我们能够以最小的开销使用扭曲噪声微调现代视频扩散基础模型，并为广泛的用户友好运动控制提供一站式解决方案：局部物体运动控制、全局摄像机运动控制和运动转移。我们的扭曲噪声在时间一致性和空间高斯性之间的协调，使得在保持每帧像素质量的同时实现有效的运动控制。大量实验和用户研究证明了我们方法的优势，使其成为控制视频扩散模型中运动的稳健且可扩展的方法。视频结果可在我们的网页上查看：https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow。源代码和模型检查点可在GitHub上获取：https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow。",
    "url": "https://huggingface.co/papers/2501.08331",
    "arxiv_url": "https://arxiv.org/abs/2501.08331"
  },
  {
    "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "summary": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS.",
    "translation": "标题：视频深度任意：超长视频的一致性深度估计\n\n摘要：深度任意（Depth Anything）在单目深度估计方面取得了显著的成功，具有强大的泛化能力。然而，它在视频中存在时间不一致性问题，阻碍了其实际应用。已有多种方法通过利用视频生成模型或引入光流和相机姿态的先验来缓解这一问题。然而，这些方法仅适用于短视频（< 10秒），并且需要在质量和计算效率之间进行权衡。我们提出了视频深度任意（Video Depth Anything），用于在超长视频（超过几分钟）中进行高质量、一致性深度估计，而无需牺牲效率。我们的模型基于深度任意V2（Depth Anything V2），并将其头部替换为高效的时空头部。我们通过约束时间深度梯度设计了一种简单而有效的时间一致性损失，无需额外的几何先验。该模型在视频深度和未标记图像的联合数据集上进行训练，类似于深度任意V2。此外，还开发了一种基于关键帧的策略用于长视频推理。实验表明，我们的模型可以应用于任意长度的视频，而不会影响质量、一致性或泛化能力。在多个视频基准上的综合评估表明，我们的方法在零样本视频深度估计中达到了新的最先进水平。我们提供了不同规模的模型以支持各种场景，其中最小的模型能够在30 FPS下实现实时性能。",
    "url": "https://huggingface.co/papers/2501.12375",
    "arxiv_url": "https://arxiv.org/abs/2501.12375"
  }
]