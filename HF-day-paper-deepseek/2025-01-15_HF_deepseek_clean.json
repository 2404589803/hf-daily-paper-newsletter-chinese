[
  {
    "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
    "summary": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.",
    "translation": "标题：MiniMax-01：基于闪电注意力机制扩展基础模型\n\n摘要：我们推出了MiniMax-01系列，包括MiniMax-Text-01和MiniMax-VL-01，这些模型在性能上可与顶级模型相媲美，同时在处理更长上下文方面展现出卓越能力。其核心在于闪电注意力机制及其高效扩展。为了最大化计算能力，我们将其与专家混合模型（MoE）相结合，创建了一个包含32个专家、总计4560亿参数的模型，其中每个token激活459亿参数。我们为MoE和闪电注意力机制开发了优化的并行策略和高效的计算-通信重叠技术。这种方法使我们能够在跨越数百万token的上下文中，对具有数千亿参数的模型进行高效的训练和推理。MiniMax-Text-01的上下文窗口在训练时可达到100万token，在推理时可外推至400万token，且成本可控。我们的视觉-语言模型MiniMax-VL-01是通过继续训练5120亿视觉-语言token构建的。在标准和内部基准测试中的实验表明，我们的模型在性能上与GPT-4o和Claude-3.5-Sonnet等最先进模型相当，同时提供20-32倍长的上下文窗口。我们在https://github.com/MiniMax-AI上公开发布了MiniMax-01。",
    "url": "https://huggingface.co/papers/2501.08313",
    "arxiv_url": "https://arxiv.org/abs/2501.08313"
  },
  {
    "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following",
    "summary": "Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.",
    "translation": "标题：基于指令跟随的多模态AI辅助单细胞分析系统\n\n摘要：大型语言模型在解释复杂的自然语言指令方面表现出色，使其能够执行广泛的任务。在生命科学领域，单细胞RNA测序（scRNA-seq）数据被视为“细胞生物学的语言”，在单细胞水平上捕捉复杂的基因表达模式。然而，通过传统工具与这种“语言”进行交互通常效率低下且不直观，给研究人员带来了挑战。为了解决这些限制，我们提出了InstructCell，一种多模态AI辅助系统，利用自然语言作为媒介，实现更直接和灵活的单细胞分析。我们构建了一个全面的多模态指令数据集，将基于文本的指令与来自不同组织和物种的scRNA-seq数据配对。在此基础上，我们开发了一种多模态细胞语言架构，能够同时解释和处理这两种模态。InstructCell使研究人员能够使用简单的自然语言命令完成关键任务，如细胞类型注释、条件性伪细胞生成和药物敏感性预测。广泛的评估表明，InstructCell在各种实验条件下始终达到或超越现有单细胞基础模型的性能。更重要的是，InstructCell为探索复杂的单细胞数据提供了一个易于访问且直观的工具，降低了技术门槛，使研究人员能够获得更深入的生物学见解。",
    "url": "https://huggingface.co/papers/2501.08187",
    "arxiv_url": "https://arxiv.org/abs/2501.08187"
  },
  {
    "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
    "summary": "Derived from diffusion models, MangaNinjia specializes in the task of\nreference-guided line art colorization. We incorporate two thoughtful designs\nto ensure precise character detail transcription, including a patch shuffling\nmodule to facilitate correspondence learning between the reference color image\nand the target line art, and a point-driven control scheme to enable\nfine-grained color matching. Experiments on a self-collected benchmark\ndemonstrate the superiority of our model over current solutions in terms of\nprecise colorization. We further showcase the potential of the proposed\ninteractive point control in handling challenging cases, cross-character\ncolorization, multi-reference harmonization, beyond the reach of existing\nalgorithms.",
    "translation": "标题：MangaNinja：基于精确参考跟随的线稿上色\n\n摘要：MangaNinja源自扩散模型，专门用于参考引导的线稿上色任务。我们引入了两个深思熟虑的设计，以确保精确的角色细节转录，包括一个用于促进参考彩色图像与目标线稿之间对应学习的补丁混洗模块，以及一个点驱动控制方案，以实现细粒度的颜色匹配。在自收集的基准测试中，实验证明了我们的模型在精确上色方面优于当前解决方案。我们进一步展示了所提出的交互式点控制在处理具有挑战性的案例、跨角色上色、多参考协调等方面的潜力，这些是现有算法无法企及的。",
    "url": "https://huggingface.co/papers/2501.08332",
    "arxiv_url": "https://arxiv.org/abs/2501.08332"
  },
  {
    "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
    "summary": "The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.",
    "translation": "标题：扩散对抗后训练用于一步视频生成\n\n摘要：扩散模型广泛用于图像和视频生成，但其迭代生成过程缓慢且昂贵。虽然现有的蒸馏方法在图像领域展示了一步生成的潜力，但它们仍然存在显著的画质下降问题。在本研究中，我们提出了一种对抗后训练（APT）方法，该方法在扩散预训练之后针对真实数据进行训练，以实现一步视频生成。为了提高训练稳定性和质量，我们引入了对模型架构和训练程序的若干改进，以及一个近似的R1正则化目标。实验表明，我们的对抗后训练模型Seaweed-APT能够实时生成2秒、1280x720分辨率、24帧率的视频，仅需一次前向评估步骤。此外，我们的模型能够一步生成1024像素的图像，其质量可与最先进的方法相媲美。",
    "url": "https://huggingface.co/papers/2501.08316",
    "arxiv_url": "https://arxiv.org/abs/2501.08316"
  },
  {
    "title": "Democratizing Text-to-Image Masked Generative Models with Compact\n  Text-Aware One-Dimensional Tokens",
    "summary": "Image tokenizers form the foundation of modern text-to-image generative\nmodels but are notoriously difficult to train. Furthermore, most existing\ntext-to-image models rely on large-scale, high-quality private datasets, making\nthem challenging to replicate. In this work, we introduce Text-Aware\nTransformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful\nimage tokenizer that can utilize either discrete or continuous 1-dimensional\ntokens. TA-TiTok uniquely integrates textual information during the tokenizer\ndecoding stage (i.e., de-tokenization), accelerating convergence and enhancing\nperformance. TA-TiTok also benefits from a simplified, yet effective, one-stage\ntraining process, eliminating the need for the complex two-stage distillation\nused in previous 1-dimensional tokenizers. This design allows for seamless\nscalability to large datasets. Building on this, we introduce a family of\ntext-to-image Masked Generative Models (MaskGen), trained exclusively on open\ndata while achieving comparable performance to models trained on private data.\nWe aim to release both the efficient, strong TA-TiTok tokenizers and the\nopen-data, open-weight MaskGen models to promote broader access and democratize\nthe field of text-to-image masked generative models.",
    "translation": "标题：使用紧凑的文本感知一维标记实现文本到图像掩码生成模型的民主化\n\n摘要：图像标记器构成了现代文本到图像生成模型的基础，但其训练过程众所周知地困难。此外，大多数现有的文本到图像模型依赖于大规模、高质量的私有数据集，这使得它们难以复制。在这项工作中，我们引入了文本感知的基于变压器的一维标记器（TA-TiTok），这是一种高效且强大的图像标记器，可以利用离散或连续的一维标记。TA-TiTok在标记器解码阶段（即去标记化）独特地集成了文本信息，加速了收敛并提高了性能。TA-TiTok还受益于一个简化但有效的一阶段训练过程，消除了之前一维标记器中使用的复杂两阶段蒸馏的需求。这种设计使得能够无缝扩展到大型数据集。在此基础上，我们引入了一系列文本到图像的掩码生成模型（MaskGen），这些模型仅在开放数据上训练，同时实现了与在私有数据上训练的模型相当的性能。我们旨在发布高效的、强大的TA-TiTok标记器以及开放数据、开放权重的MaskGen模型，以促进更广泛的访问并实现文本到图像掩码生成模型领域的民主化。",
    "url": "https://huggingface.co/papers/2501.07730",
    "arxiv_url": "https://arxiv.org/abs/2501.07730"
  },
  {
    "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion\n  Priors",
    "summary": "Interactive image editing allows users to modify images through visual\ninteraction operations such as drawing, clicking, and dragging. Existing\nmethods construct such supervision signals from videos, as they capture how\nobjects change with various physical interactions. However, these models are\nusually built upon text-to-image diffusion models, so necessitate (i) massive\ntraining samples and (ii) an additional reference encoder to learn real-world\ndynamics and visual consistency. In this paper, we reformulate this task as an\nimage-to-video generation problem, so that inherit powerful video diffusion\npriors to reduce training costs and ensure temporal consistency. Specifically,\nwe introduce FramePainter as an efficient instantiation of this formulation.\nInitialized with Stable Video Diffusion, it only uses a lightweight sparse\ncontrol encoder to inject editing signals. Considering the limitations of\ntemporal attention in handling large motion between two frames, we further\npropose matching attention to enlarge the receptive field while encouraging\ndense correspondence between edited and source image tokens. We highlight the\neffectiveness and efficiency of FramePainter across various of editing signals:\nit domainantly outperforms previous state-of-the-art methods with far less\ntraining data, achieving highly seamless and coherent editing of images, \\eg,\nautomatically adjust the reflection of the cup. Moreover, FramePainter also\nexhibits exceptional generalization in scenarios not present in real-world\nvideos, \\eg, transform the clownfish into shark-like shape. Our code will be\navailable at https://github.com/YBYBZhang/FramePainter.",
    "translation": "标题：FramePainter：赋予交互式图像编辑视频扩散先验\n\n摘要：交互式图像编辑允许用户通过绘制、点击和拖动等视觉交互操作来修改图像。现有方法从视频中构建此类监督信号，因为它们捕捉了物体如何随各种物理交互而变化。然而，这些模型通常基于文本到图像的扩散模型，因此需要（i）大量的训练样本和（ii）额外的参考编码器来学习现实世界的动态和视觉一致性。在本文中，我们将此任务重新表述为图像到视频生成问题，从而继承强大的视频扩散先验，以减少训练成本并确保时间一致性。具体来说，我们引入了FramePainter作为这一表述的高效实例化。通过Stable Video Diffusion初始化，它仅使用轻量级的稀疏控制编码器来注入编辑信号。考虑到时间注意力在处理两帧之间大运动时的局限性，我们进一步提出了匹配注意力，以扩大感受野，同时鼓励编辑图像和源图像标记之间的密集对应。我们强调了FramePainter在各种编辑信号中的有效性和效率：它在训练数据远远少于之前最先进方法的情况下，显著优于这些方法，实现了高度无缝和连贯的图像编辑，例如自动调整杯子的反射。此外，FramePainter在现实世界视频中不存在的场景中也表现出卓越的泛化能力，例如将小丑鱼转变为鲨鱼形状。我们的代码将可在https://github.com/YBYBZhang/FramePainter获取。",
    "url": "https://huggingface.co/papers/2501.08225",
    "arxiv_url": "https://arxiv.org/abs/2501.08225"
  },
  {
    "title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players",
    "summary": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: https://github.com/pokerllm/pokerbench.",
    "translation": "标题：PokerBench：训练大型语言模型成为专业扑克玩家\n\n摘要：我们介绍了PokerBench——一个用于评估大型语言模型（LLMs）扑克游戏能力的基准。随着LLMs在传统自然语言处理任务中的卓越表现，它们在复杂、策略性游戏如扑克中的应用提出了新的挑战。扑克作为一种不完全信息游戏，需要多种技能，如数学、推理、规划、策略以及对博弈论和人类心理学的深刻理解。这使得扑克成为大型语言模型的理想前沿领域。PokerBench包含11,000个最重要的场景，分为翻牌前和翻牌后游戏，这些场景是与训练有素的扑克玩家合作开发的。我们评估了包括GPT-4、ChatGPT 3.5以及各种Llama和Gemma系列模型在内的知名模型，发现所有最先进的LLMs在玩最优扑克时表现不佳。然而，经过微调后，这些模型显示出显著的改进。我们通过让不同得分的模型相互竞争来验证PokerBench，证明在PokerBench上得分更高的模型在实际扑克游戏中具有更高的胜率。通过我们微调后的模型与GPT-4之间的游戏，我们还发现了简单监督微调在学习最优游戏策略方面的局限性，表明需要更先进的方法来有效训练语言模型在游戏中表现出色。因此，PokerBench提供了一个独特的基准，用于快速可靠地评估LLMs的扑克游戏能力，并作为一个全面的基准来研究LLMs在复杂游戏场景中的进展。数据集和代码将在以下网址提供：https://github.com/pokerllm/pokerbench。",
    "url": "https://huggingface.co/papers/2501.08328",
    "arxiv_url": "https://arxiv.org/abs/2501.08328"
  },
  {
    "title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data",
    "summary": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.",
    "translation": "标题：大型语言模型作为非结构化文本数据评判者的潜力与风险\n\n摘要：大型语言模型（LLMs）的快速发展在处理和总结非结构化文本数据方面展现了显著的能力。这对分析丰富的开放式数据集（如调查反馈）具有重要意义，LLMs有望高效提炼关键主题和情感。然而，随着组织越来越多地依赖这些强大的AI系统来理解文本反馈，一个关键问题随之而来：我们能否信任LLMs准确反映这些基于文本的数据集中的观点？尽管LLMs在生成类似人类的总结方面表现出色，但其输出可能无意中偏离原始反馈的真实内容。LLM生成的输出与数据中实际主题之间的差异可能导致决策失误，对组织产生深远影响。本研究探讨了LLMs作为评判模型评估其他LLMs生成总结的主题一致性的有效性。我们使用Anthropic Claude模型从开放式调查反馈中生成主题总结，并以亚马逊的Titan Express、Nova Pro和Meta的Llama作为LLM评判者。通过Cohen's kappa、Spearman's rho和Krippendorff's alpha等方法，将LLM作为评判者的方法与人类评估进行比较，验证了其作为传统以人为中心评估方法的可扩展替代方案。我们的研究结果表明，尽管LLMs作为评判者提供了与人类评估者相当的可扩展解决方案，但人类在检测细微、特定于上下文的细微差别方面可能仍具有优势。本研究为AI辅助文本分析领域的知识体系做出了贡献。我们讨论了局限性，并为未来研究提供了建议，强调在跨不同上下文和用例推广LLM评判模型时需要谨慎考虑。",
    "url": "https://huggingface.co/papers/2501.08167",
    "arxiv_url": "https://arxiv.org/abs/2501.08167"
  },
  {
    "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
    "summary": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
    "translation": "标题：HALoGEN：大型语言模型的幻觉及其发现之处\n\n摘要：尽管生成式大型语言模型（LLMs）在生成高质量和流畅文本方面表现出色，但它们也会产生幻觉：即与既定世界知识或提供的输入上下文不符的陈述。然而，测量幻觉可能具有挑战性，因为让人类即时验证模型生成既昂贵又耗时。在本研究中，我们发布了HALoGEN，一个全面的幻觉基准，包括：（1）10,923个涵盖九个领域的生成模型提示，包括编程、科学归因和摘要等；（2）每个用例的自动高精度验证器，将LLM生成分解为原子单元，并根据高质量知识源验证每个单元。我们使用此框架评估了来自14个语言模型的约150,000次生成，发现即使表现最佳的模型也充满了幻觉（有时高达86%的生成原子事实，具体取决于领域）。我们进一步定义了一种新的LLM幻觉错误分类，基于它们是否可能源于训练数据的错误回忆（A类错误）、训练数据中的错误知识（B类错误）或虚构（C类错误）。我们希望我们的框架为系统研究生成模型为何产生幻觉提供基础，并推动可信赖的大型语言模型的发展。",
    "url": "https://huggingface.co/papers/2501.08292",
    "arxiv_url": "https://arxiv.org/abs/2501.08292"
  },
  {
    "title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video\n  Description to Comprehensive Video Understanding",
    "summary": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)\ndesigned for generating detailed and accurate video descriptions, while also\nexhibiting superior general video understanding capabilities. Tarsier2 achieves\nsignificant advancements through three key upgrades: (1) Scaling pre-training\ndata from 11M to 40M video-text pairs, enriching both volume and diversity; (2)\nPerforming fine-grained temporal alignment during supervised fine-tuning; (3)\nUsing model-based sampling to automatically construct preference data and\napplying DPO training for optimization. Extensive experiments show that\nTarsier2-7B consistently outperforms leading proprietary models, including\nGPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K\nbenchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over\nGemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\%\nperformance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B\nalso sets new state-of-the-art results across 15 public benchmarks, spanning\ntasks such as video question-answering, video grounding, hallucination test,\nand embodied question-answering, demonstrating its versatility as a robust\ngeneralist vision-language model.",
    "translation": "标题：Tarsier2：从详细视频描述到全面视频理解的大型视觉语言模型进展\n\n摘要：我们介绍了Tarsier2，这是一种最先进的大型视觉语言模型（LVLM），旨在生成详细且准确的视频描述，同时展现出卓越的通用视频理解能力。Tarsier2通过三个关键升级实现了显著进展：（1）将预训练数据从1100万扩展到4000万视频-文本对，丰富了数据量和多样性；（2）在监督微调期间进行细粒度的时间对齐；（3）使用基于模型的采样自动构建偏好数据，并应用DPO训练进行优化。大量实验表明，Tarsier2-7B在详细视频描述任务中始终优于领先的专有模型，包括GPT-4o和Gemini 1.5 Pro。在DREAM-1K基准测试中，Tarsier2-7B的F1分数比GPT-4o提高了2.8%，比Gemini-1.5-Pro提高了5.8%。在人类并行评估中，Tarsier2-7B的表现优势比GPT-4o高出8.6%，比Gemini-1.5-Pro高出24.9%。Tarsier2-7B还在15个公共基准测试中创下了新的最先进结果，涵盖了视频问答、视频定位、幻觉测试和具身问答等任务，展示了其作为强大通用视觉语言模型的多功能性。",
    "url": "https://huggingface.co/papers/2501.07888",
    "arxiv_url": "https://arxiv.org/abs/2501.07888"
  }
]