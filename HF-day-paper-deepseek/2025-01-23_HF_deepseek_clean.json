[
  {
    "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in\n  Virtual 3D Spaces",
    "summary": "Virtual film production requires intricate decision-making processes,\nincluding scriptwriting, virtual cinematography, and precise actor positioning\nand actions. Motivated by recent advances in automated decision-making with\nlanguage agent-based societies, this paper introduces FilmAgent, a novel\nLLM-based multi-agent collaborative framework for end-to-end film automation in\nour constructed 3D virtual spaces. FilmAgent simulates various crew roles,\nincluding directors, screenwriters, actors, and cinematographers, and covers\nkey stages of a film production workflow: (1) idea development transforms\nbrainstormed ideas into structured story outlines; (2) scriptwriting elaborates\non dialogue and character actions for each scene; (3) cinematography determines\nthe camera setups for each shot. A team of agents collaborates through\niterative feedback and revisions, thereby verifying intermediate scripts and\nreducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key\naspects. Human evaluation shows that FilmAgent outperforms all baselines across\nall aspects and scores 3.98 out of 5 on average, showing the feasibility of\nmulti-agent collaboration in filmmaking. Further analysis reveals that\nFilmAgent, despite using the less advanced GPT-4o model, surpasses the\nsingle-agent o1, showing the advantage of a well-coordinated multi-agent\nsystem. Lastly, we discuss the complementary strengths and weaknesses of\nOpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
    "translation": "标题：FilmAgent：虚拟3D空间中端到端电影自动化的多智能体框架\n\n摘要：虚拟电影制作需要复杂的决策过程，包括剧本创作、虚拟摄影以及精确的演员定位和动作。受近期基于语言智能体的自动化决策进展的启发，本文提出了FilmAgent，一个基于大语言模型（LLM）的多智能体协作框架，用于在我们构建的3D虚拟空间中进行端到端的电影自动化制作。FilmAgent模拟了各种剧组角色，包括导演、编剧、演员和摄影师，并涵盖了电影制作工作流程的关键阶段：（1）创意开发将头脑风暴的想法转化为结构化的故事大纲；（2）剧本创作详细阐述每个场景的对话和角色动作；（3）摄影确定每个镜头的摄像机设置。一组智能体通过迭代反馈和修订进行协作，从而验证中间剧本并减少幻觉。我们评估了基于15个创意和4个关键方面生成的视频。人类评估显示，FilmAgent在所有方面均优于所有基线模型，平均得分为3.98（满分5分），展示了多智能体协作在电影制作中的可行性。进一步分析表明，尽管FilmAgent使用了较不先进的GPT-4o模型，但其表现优于单智能体o1，显示了协调良好的多智能体系统的优势。最后，我们讨论了OpenAI的文本到视频模型Sora与我们的FilmAgent在电影制作中的互补优势和劣势。",
    "url": "https://huggingface.co/papers/2501.12909",
    "arxiv_url": "https://arxiv.org/abs/2501.12909"
  },
  {
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning",
    "summary": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "translation": "标题：DeepSeek-R1：通过强化学习激励大语言模型的推理能力\n\n摘要：我们介绍了第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero是一个通过大规模强化学习（RL）训练的模型，无需监督微调（SFT）作为初步步骤，展示了显著的推理能力。通过RL，DeepSeek-R1-Zero自然涌现出许多强大且有趣的推理行为。然而，它也面临诸如可读性差和语言混合等挑战。为了解决这些问题并进一步提升推理性能，我们引入了DeepSeek-R1，它在RL之前结合了多阶段训练和冷启动数据。DeepSeek-R1在推理任务上的表现与OpenAI-o1-1217相当。为了支持研究社区，我们开源了DeepSeek-R1-Zero、DeepSeek-R1以及基于Qwen和Llama从DeepSeek-R1蒸馏出的六个密集模型（1.5B、7B、8B、14B、32B、70B）。",
    "url": "https://huggingface.co/papers/2501.12948",
    "arxiv_url": "https://arxiv.org/abs/2501.12948"
  },
  {
    "title": "Autonomy-of-Experts Models",
    "summary": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to\nspecific expert modules, activating only partial parameters and often\noutperforming dense models. We argue that the separation between the router's\ndecision-making and the experts' execution is a critical yet overlooked issue,\nleading to suboptimal expert selection and ineffective learning. To address\nthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which\nexperts autonomously select themselves to process inputs. AoE is based on the\ninsight that an expert is aware of its own capacity to effectively process a\ntoken, an awareness reflected in the scale of its internal activations. In AoE,\nrouters are removed; instead, experts pre-compute internal activations for\ninputs and are ranked based on their activation norms. Only the top-ranking\nexperts proceed with the forward pass, while the others abort. The overhead of\npre-computing activations is reduced through a low-rank weight factorization.\nThis self-evaluating-then-partner-comparing approach ensures improved expert\nselection and effective learning. We pre-train language models having 700M up\nto 4B parameters, demonstrating that AoE outperforms traditional MoE models\nwith comparable efficiency.",
    "translation": "标题：专家自主模型\n\n摘要：专家混合模型（Mixture-of-Experts, MoE）通常使用路由器将令牌分配给特定的专家模块，仅激活部分参数，并且往往优于密集模型。我们认为，路由器的决策与专家的执行之间的分离是一个关键但被忽视的问题，导致专家选择次优和学习效果不佳。为了解决这一问题，我们提出了专家自主模型（Autonomy-of-Experts, AoE），这是一种新颖的MoE范式，其中专家自主选择自己来处理输入。AoE基于一个洞察，即专家了解自己有效处理令牌的能力，这种意识反映在其内部激活的规模上。在AoE中，路由器被移除；相反，专家预先计算输入的内部激活，并根据其激活范数进行排名。只有排名最高的专家继续进行前向传播，而其他专家则中止。通过低秩权重分解，减少了预先计算激活的开销。这种先自我评估再伙伴比较的方法确保了改进的专家选择和有效的学习。我们预训练了参数从700M到4B的语言模型，证明AoE在效率相当的情况下优于传统的MoE模型。",
    "url": "https://huggingface.co/papers/2501.13074",
    "arxiv_url": "https://arxiv.org/abs/2501.13074"
  },
  {
    "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
    "summary": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%).",
    "translation": "标题：Kimi k1.5：利用大语言模型扩展强化学习\n\n摘要：通过下一个词预测进行语言模型预训练已被证明在计算扩展方面是有效的，但其受限于可用训练数据的数量。扩展强化学习（RL）为人工智能的持续改进开辟了新的方向，其前景在于大语言模型（LLMs）能够通过奖励驱动的探索来扩展其训练数据。然而，先前发表的工作尚未产生具有竞争力的结果。鉴于此，我们报告了Kimi k1.5的训练实践，这是我们最新通过RL训练的多模态LLM，包括其RL训练技术、多模态数据配方和基础设施优化。长上下文扩展和改进的策略优化方法是我们方法的关键要素，该方法建立了一个简单而有效的RL框架，而不依赖于更复杂的技术，如蒙特卡罗树搜索、价值函数和过程奖励模型。值得注意的是，我们的系统在多个基准测试和模态上实现了最先进的推理性能——例如，AIME上77.5分，MATH 500上96.2分，Codeforces上94百分位，MathVista上74.9分——与OpenAI的o1相当。此外，我们提出了有效的长到短方法，利用长链思维（long-CoT）技术改进短链思维（short-CoT）模型，产生了最先进的短链思维推理结果——例如，AIME上60.8分，MATH500上94.6分，LiveCodeBench上47.3分——大幅超越了现有的短链思维模型，如GPT-4o和Claude Sonnet 3.5（高达+550%）。",
    "url": "https://huggingface.co/papers/2501.12599",
    "arxiv_url": "https://arxiv.org/abs/2501.12599"
  },
  {
    "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
    "summary": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended\nreasoning processes similar to how humans ponder over complex problems. This\nreasoning paradigm significantly enhances the model's problem-solving abilities\nand has achieved promising results. However, long-thought reasoning process\nleads to a substantial increase in inference time. A pressing challenge is\nreducing the inference overhead of long-thought LLMs while ensuring accuracy.\nIn this paper, we experimentally demonstrate that long-thought reasoning models\nstruggle to effectively allocate token budgets based on problem difficulty and\nreasoning redundancies. To address this, we propose Length-Harmonizing\nFine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while\nmaintaining accuracy. This effective fine-tuning method first estimates the\nLLM's baseline performance through pre-sampling and then uses RL-style\nfine-tuning to encourage the model to generate shorter reasoning processes\nunder accuracy constraints. This allows the model to achieve efficient\nreasoning with lower redundancy while maintaining accuracy. Experiments on\nvarious mathematical reasoning benchmarks show that O1-Pruner not only\nsignificantly reduces inference overhead but also achieves higher accuracy,\nproviding a novel and promising solution to this challenge. Our code is coming\nsoon at https://github.com/StarDewXXX/O1-Pruner",
    "translation": "标题：O1-Pruner：用于O1类推理剪枝的长度协调微调\n\n摘要：最近，像OpenAI的O1这样的长思维推理大语言模型（LLMs）采用了类似于人类思考复杂问题的扩展推理过程。这种推理范式显著增强了模型的问题解决能力，并取得了令人瞩目的成果。然而，长思维推理过程导致推理时间大幅增加。一个紧迫的挑战是在确保准确性的同时减少长思维LLMs的推理开销。在本文中，我们通过实验证明，长思维推理模型难以根据问题难度和推理冗余有效分配token预算。为了解决这一问题，我们提出了长度协调微调（O1-Pruner），旨在最小化推理开销的同时保持准确性。这种有效的微调方法首先通过预采样估计LLM的基线性能，然后使用强化学习风格的微调来鼓励模型在准确性约束下生成更短的推理过程。这使得模型能够在保持准确性的同时以较低的冗余实现高效推理。在各种数学推理基准上的实验表明，O1-Pruner不仅显著减少了推理开销，还实现了更高的准确性，为这一挑战提供了一个新颖且有前景的解决方案。我们的代码即将发布在https://github.com/StarDewXXX/O1-Pruner。",
    "url": "https://huggingface.co/papers/2501.12570",
    "arxiv_url": "https://arxiv.org/abs/2501.12570"
  }
]