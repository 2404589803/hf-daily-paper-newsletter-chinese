[
    {
        "paper": {
            "id": "2501.03262",
            "authors": [
                {
                    "_id": "677e18a67edb3025daa99e09",
                    "user": {
                        "_id": "63f6c04ac96958470d1e9043",
                        "avatarUrl": "/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg",
                        "isPro": false,
                        "fullname": "Jian Hu",
                        "user": "chuyi777",
                        "type": "user"
                    },
                    "name": "Jian Hu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-08T06:18:15.147Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-04T02:08:06.000Z",
            "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language\n  Models",
            "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical\napproach for aligning large language models with human preferences, witnessing\nrapid algorithmic evolution through methods such as Proximal Policy\nOptimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave\nOne-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We\npresent REINFORCE++, an enhanced variant of the classical REINFORCE algorithm\nthat incorporates key optimization techniques from PPO while eliminating the\nneed for a critic network. REINFORCE++ achieves three primary objectives: (1)\nsimplicity (2) enhanced training stability, and (3) reduced computational\noverhead. Through extensive empirical evaluation, we demonstrate that\nREINFORCE++ exhibits superior stability compared to GRPO and achieves greater\ncomputational efficiency than PPO while maintaining comparable performance. The\nimplementation is available at https://github.com/OpenRLHF/OpenRLHF.",
            "upvotes": 53,
            "discussionId": "677e18a77edb3025daa99e4f"
        },
        "translation": "标题：REINFORCE++：一种简单高效的大语言模型对齐方法\n\n摘要：基于人类反馈的强化学习（RLHF）已成为将大语言模型与人类偏好对齐的关键方法，并通过近端策略优化（PPO）、直接偏好优化（DPO）、REINFORCE留一法（RLOO）、ReMax和群体相对策略优化（GRPO）等方法实现了快速的算法演进。我们提出了REINFORCE++，这是经典REINFORCE算法的增强版本，它结合了PPO的关键优化技术，同时消除了对评论者网络的需求。REINFORCE++实现了三个主要目标：（1）简化算法结构，（2）提高训练稳定性，以及（3）减少计算开销。通过广泛的实证评估，我们证明REINFORCE++在稳定性方面优于GRPO，并且在计算效率上优于PPO，同时保持了可比的性能。实现代码可在https://github.com/OpenRLHF/OpenRLHF获取。"
    },
    {
        "paper": {
            "id": "2501.03575",
            "authors": [
                {
                    "_id": "677dfaa14bf7f0d4734088a4",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088a6",
                    "user": {
                        "_id": "667b2e7f0ae3fef85fe33eb9",
                        "avatarUrl": "/avatars/033f74277bf934d8d9703e9a8c5a6716.svg",
                        "isPro": false,
                        "fullname": "Niket Agarwal",
                        "user": "niketa12",
                        "type": "user"
                    },
                    "name": "Niket Agarwal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:41:16.772Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088a7",
                    "user": {
                        "_id": "6517ec608a6d58d1d7ec8ec1",
                        "avatarUrl": "/avatars/4aac7fa6643f7f2d32e95cc991130ee9.svg",
                        "isPro": false,
                        "fullname": "Arslan Ali",
                        "user": "arslanali",
                        "type": "user"
                    },
                    "name": "Arslan Ali",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:41:24.513Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088a8",
                    "user": {
                        "_id": "675304737c4876b7a1475695",
                        "avatarUrl": "/avatars/7ee3c443f6a143b4a79d679fb7f60fe5.svg",
                        "isPro": false,
                        "fullname": "Maciej Bala",
                        "user": "mbalaNV",
                        "type": "user"
                    },
                    "name": "Maciej Bala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:41:34.024Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088a9",
                    "user": {
                        "_id": "66cd4e2564a8631d7328a637",
                        "avatarUrl": "/avatars/024592abd427a8109f85c49e52e3bb7e.svg",
                        "isPro": false,
                        "fullname": "Yogesh Balaji",
                        "user": "yogeshbalaji",
                        "type": "user"
                    },
                    "name": "Yogesh Balaji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:41:53.307Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088aa",
                    "name": "Erik Barker",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ab",
                    "name": "Tiffany Cai",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ac",
                    "user": {
                        "_id": "628d451386d23ad1560882c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628d451386d23ad1560882c4/UMxez0DEvX5qdP5ddqi-8.png",
                        "isPro": false,
                        "fullname": "Prithvijit Chattopadhyay",
                        "user": "prithv1",
                        "type": "user"
                    },
                    "name": "Prithvijit Chattopadhyay",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:43:22.176Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ad",
                    "user": {
                        "_id": "66f4cf1a03b5ba8a7f1f6522",
                        "avatarUrl": "/avatars/2768d6e37d3f280194cfb8ed274f6015.svg",
                        "isPro": false,
                        "fullname": "Yongxin Chen",
                        "user": "Ema11",
                        "type": "user"
                    },
                    "name": "Yongxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:43:31.201Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ae",
                    "user": {
                        "_id": "66f6510a1c8cb854dec2d05c",
                        "avatarUrl": "/avatars/c344d7f6747beec0c3bab0c023b7b3d4.svg",
                        "isPro": false,
                        "fullname": "Yin Cui",
                        "user": "yinc-nvidia",
                        "type": "user"
                    },
                    "name": "Yin Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:43:42.260Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088af",
                    "name": "Yifan Ding",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b0",
                    "name": "Daniel Dworakowski",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b1",
                    "user": {
                        "_id": "67227b67dfa07920c2985d22",
                        "avatarUrl": "/avatars/98f09002722950eadffe5c199d22bb4f.svg",
                        "isPro": false,
                        "fullname": "Jiaojiao Fan",
                        "user": "jjf233",
                        "type": "user"
                    },
                    "name": "Jiaojiao Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:44:52.955Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b2",
                    "name": "Michele Fenzi",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b3",
                    "user": {
                        "_id": "6353d3e95eac2d2efa7501f9",
                        "avatarUrl": "/avatars/4b063f54000bed4bfb1bfcc3cde1a09e.svg",
                        "isPro": false,
                        "fullname": "Francesco Ferroni",
                        "user": "fferroni",
                        "type": "user"
                    },
                    "name": "Francesco Ferroni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:45:05.039Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b4",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b5",
                    "name": "Dieter Fox",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b6",
                    "user": {
                        "_id": "67291c1a027e12eb38dc8a0c",
                        "avatarUrl": "/avatars/00a0ca6da11d2ffd14a83d28e57c01b4.svg",
                        "isPro": false,
                        "fullname": "Songwei Ge",
                        "user": "SongweiGe",
                        "type": "user"
                    },
                    "name": "Songwei Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:45:28.342Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b7",
                    "user": {
                        "_id": "6520e493b80dc49ba0f1e262",
                        "avatarUrl": "/avatars/af21c1ee154b2c9a0d56e69a07508ccb.svg",
                        "isPro": false,
                        "fullname": "Yunhao Ge",
                        "user": "yunhaog",
                        "type": "user"
                    },
                    "name": "Yunhao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:45:53.639Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b8",
                    "user": {
                        "_id": "647e8118770c299e56fc2bc8",
                        "avatarUrl": "/avatars/adf80f3473dda42450148789ae5c208f.svg",
                        "isPro": false,
                        "fullname": "Jinwei Gu",
                        "user": "jwgu",
                        "type": "user"
                    },
                    "name": "Jinwei Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:46:02.830Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088b9",
                    "name": "Siddharth Gururani",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ba",
                    "user": {
                        "_id": "62b20f6df4a72794189248fc",
                        "avatarUrl": "/avatars/87e1125868616d4f7d6ee1e5ec4499b4.svg",
                        "isPro": false,
                        "fullname": "Ethan He",
                        "user": "ethanhe",
                        "type": "user"
                    },
                    "name": "Ethan He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:46:14.910Z",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088bb",
                    "name": "Jiahui Huang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088bc",
                    "name": "Jacob Huffman",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088bd",
                    "name": "Pooya Jannaty",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088be",
                    "name": "Jingyi Jin",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088bf",
                    "name": "Seung Wook Kim",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c0",
                    "name": "Gergely Klár",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c1",
                    "name": "Grace Lam",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c2",
                    "name": "Shiyi Lan",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c3",
                    "name": "Laura Leal-Taixe",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c4",
                    "name": "Anqi Li",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c5",
                    "name": "Zhaoshuo Li",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c6",
                    "name": "Chen-Hsuan Lin",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c7",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c8",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088c9",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ca",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088cb",
                    "name": "Alice Luo",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088cc",
                    "name": "Qianli Ma",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088cd",
                    "name": "Hanzi Mao",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ce",
                    "name": "Kaichun Mo",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088cf",
                    "name": "Arsalan Mousavian",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d0",
                    "name": "Seungjun Nah",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d1",
                    "name": "Sriharsha Niverty",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d2",
                    "name": "David Page",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d3",
                    "name": "Despoina Paschalidou",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d4",
                    "name": "Zeeshan Patel",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d5",
                    "name": "Lindsey Pavao",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d6",
                    "name": "Morteza Ramezanali",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d7",
                    "name": "Fitsum Reda",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d8",
                    "name": "Xiaowei Ren",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088d9",
                    "name": "Vasanth Rao Naik Sabavat",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088da",
                    "name": "Ed Schmerling",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088db",
                    "name": "Stella Shi",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088dc",
                    "name": "Bartosz Stefaniak",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088dd",
                    "name": "Shitao Tang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088de",
                    "name": "Lyne Tchapmi",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088df",
                    "name": "Przemek Tredak",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e0",
                    "name": "Wei-Cheng Tseng",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e1",
                    "name": "Jibin Varghese",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e2",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e3",
                    "name": "Haoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e4",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e5",
                    "name": "Ting-Chun Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e6",
                    "name": "Fangyin Wei",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e7",
                    "name": "Xinyue Wei",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e8",
                    "name": "Jay Zhangjie Wu",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088e9",
                    "name": "Jiashu Xu",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ea",
                    "name": "Wei Yang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088eb",
                    "name": "Lin Yen-Chen",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ec",
                    "name": "Xiaohui Zeng",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ed",
                    "name": "Yu Zeng",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ee",
                    "name": "Jing Zhang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088ef",
                    "name": "Qinsheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088f0",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088f1",
                    "name": "Qingqing Zhao",
                    "hidden": false
                },
                {
                    "_id": "677dfaa14bf7f0d4734088f2",
                    "name": "Artur Zolkowski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T06:55:50.000Z",
            "title": "Cosmos World Foundation Model Platform for Physical AI",
            "summary": "Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make our platform open-source and our models open-weight with\npermissive licenses available via https://github.com/NVIDIA/Cosmos.",
            "upvotes": 35,
            "discussionId": "677dfaa84bf7f0d473408be8"
        },
        "translation": "标题：面向物理人工智能的Cosmos世界基础模型平台\n\n摘要：物理人工智能首先需要进行数字化训练。它需要一个自身的数字孪生体，即策略模型，以及一个世界的数字孪生体，即世界模型。在本文中，我们介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理人工智能系统构建定制化的世界模型。我们将世界基础模型定位为一种通用世界模型，可以通过微调转化为适用于下游应用的定制化世界模型。我们的平台涵盖了视频筛选流程、预训练的世界基础模型、预训练世界基础模型的训练后示例以及视频标记器。为了帮助物理人工智能开发者解决社会中最关键的问题，我们将平台开源，并通过https://github.com/NVIDIA/Cosmos提供开放权重的模型和宽松的许可。"
    },
    {
        "paper": {
            "id": "2501.02955",
            "authors": [
                {
                    "_id": "677dfb5d0310e9426191dd3e",
                    "user": {
                        "_id": "62ecd24cb8764c7738ef2793",
                        "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
                        "isPro": false,
                        "fullname": "Wenyi Hong",
                        "user": "wenyi",
                        "type": "user"
                    },
                    "name": "Wenyi Hong",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-08T04:13:19.132Z",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd3f",
                    "user": {
                        "_id": "65acc5afe2a2c8635614de43",
                        "avatarUrl": "/avatars/c5fce792792cc0b52ed7475d72460c58.svg",
                        "isPro": false,
                        "fullname": "Yean Cheng",
                        "user": "LiquidAmmonia",
                        "type": "user"
                    },
                    "name": "Yean Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:31.589Z",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd40",
                    "user": {
                        "_id": "6466d1640ed2f7a8cba87503",
                        "avatarUrl": "/avatars/652746e63dfeb5154ae7d34039d1a485.svg",
                        "isPro": false,
                        "fullname": "Zhuoyi Yang",
                        "user": "zyyangzy",
                        "type": "user"
                    },
                    "name": "Zhuoyi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:26:08.664Z",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd41",
                    "name": "Weihan Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd42",
                    "name": "Lefan Wang",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd43",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd44",
                    "user": {
                        "_id": "6406db5cd684369027166986",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg",
                        "isPro": false,
                        "fullname": "Shiyu Huang",
                        "user": "ShiyuHuang",
                        "type": "user"
                    },
                    "name": "Shiyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:27:03.140Z",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd45",
                    "user": {
                        "_id": "640e73bdfdeaae1390857b62",
                        "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg",
                        "isPro": false,
                        "fullname": "Yuxiao Dong",
                        "user": "yuxiaod",
                        "type": "user"
                    },
                    "name": "Yuxiao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:27:12.264Z",
                    "hidden": false
                },
                {
                    "_id": "677dfb5d0310e9426191dd46",
                    "user": {
                        "_id": "640dff05474aa6f89556677e",
                        "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
                        "isPro": false,
                        "fullname": "Jie Tang",
                        "user": "jerytang",
                        "type": "user"
                    },
                    "name": "Jie Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:27:19.977Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-06T11:57:38.000Z",
            "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models",
            "summary": "In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io .",
            "upvotes": 32,
            "discussionId": "677dfb5f0310e9426191de09"
        },
        "translation": "标题：MotionBench：面向视觉语言模型的细粒度视频运动理解基准测试与改进\n\n摘要：近年来，视觉语言模型（VLMs）在视频理解领域取得了显著进展。然而，当前基准测试中一个关键能力——细粒度运动理解——仍未得到充分探索。为填补这一空白，我们提出了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。MotionBench通过六种主要类别的运动导向问题类型来评估模型的运动级感知能力，并包含从多种来源收集的数据，确保了对现实世界视频内容的广泛代表性。实验结果表明，现有的VLMs在理解细粒度运动方面表现不佳。为了增强VLM在有限序列长度内感知细粒度运动的能力，我们进行了广泛的实验，回顾了针对视频特征压缩优化的VLM架构，并提出了一种新颖且高效的Through-Encoder（TE）融合方法。实验表明，更高的帧率输入和TE融合在运动理解方面带来了改进，但仍存在显著的提升空间。我们的基准测试旨在指导和激励开发更具能力的视频理解模型，强调细粒度运动理解的重要性。项目页面：https://motion-bench.github.io。"
    },
    {
        "paper": {
            "id": "2501.03895",
            "authors": [
                {
                    "_id": "677ded917e773a03180e90c9",
                    "user": {
                        "_id": "64803e5dc57f629056c601f1",
                        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
                        "isPro": false,
                        "fullname": "Shaolei Zhang",
                        "user": "zhangshaolei",
                        "type": "user"
                    },
                    "name": "Shaolei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:27:33.924Z",
                    "hidden": false
                },
                {
                    "_id": "677ded917e773a03180e90ca",
                    "user": {
                        "_id": "65b7573482d384513443875e",
                        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
                        "isPro": false,
                        "fullname": "Qingkai Fang",
                        "user": "poeroz",
                        "type": "user"
                    },
                    "name": "Qingkai Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:27:41.696Z",
                    "hidden": false
                },
                {
                    "_id": "677ded917e773a03180e90cb",
                    "name": "Zhe Yang",
                    "hidden": false
                },
                {
                    "_id": "677ded917e773a03180e90cc",
                    "user": {
                        "_id": "63b39f33922f26a27e7e93dd",
                        "avatarUrl": "/avatars/fa1562f8c44270647826b293f49483bb.svg",
                        "isPro": false,
                        "fullname": "Yang Feng",
                        "user": "fengyang0317",
                        "type": "user"
                    },
                    "name": "Yang Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:29:03.040Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T16:03:14.000Z",
            "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token",
            "summary": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.",
            "upvotes": 22,
            "discussionId": "677ded937e773a03180e9144"
        },
        "translation": "标题：LLaVA-Mini：基于单一视觉令牌的高效图像与视频大型多模态模型\n\n摘要：随着GPT-4o等实时大型多模态模型（LMMs）的出现，高效LMMs引起了广泛关注。LMM框架通常将视觉输入编码为视觉令牌（连续表示），并将其与文本指令整合到大型语言模型（LLMs）的上下文中，其中大规模参数和大量上下文令牌（主要是视觉令牌）导致巨大的计算开销。以往关于高效LMMs的研究主要集中在用较小模型替换LLM骨干，而忽视了令牌数量的关键问题。本文介绍了LLaVA-Mini，一种具有最少视觉令牌的高效LMM。为了实现视觉令牌的高压缩比同时保留视觉信息，我们首先分析了LMMs如何理解视觉令牌，发现大多数视觉令牌仅在LLM骨干的早期层中起关键作用，它们主要将视觉信息融合到文本令牌中。基于这一发现，LLaVA-Mini引入了模态预融合，提前将视觉信息融合到文本令牌中，从而促进输入到LLM骨干的视觉令牌的极端压缩至单一令牌。LLaVA-Mini是一个统一的大型多模态模型，能够高效地支持图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中，实验表明LLaVA-Mini仅使用1个视觉令牌（而非576个）即可超越LLaVA-v1.5。效率分析显示，LLaVA-Mini可以减少77%的浮点运算（FLOPs），在40毫秒内提供低延迟响应，并在24GB内存的GPU硬件上处理超过10,000帧的视频。"
    },
    {
        "paper": {
            "id": "2501.04001",
            "authors": [
                {
                    "_id": "677e2052dbff7b495e85ec95",
                    "user": {
                        "_id": "6391e41f2e73987364e6bcb2",
                        "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
                        "isPro": false,
                        "fullname": "Haobo Yuan",
                        "user": "HarborYuan",
                        "type": "user"
                    },
                    "name": "Haobo Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:27.046Z",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec96",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:39:14.906Z",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec97",
                    "user": {
                        "_id": "660d28db4215cc70372bc432",
                        "avatarUrl": "/avatars/a515303c6e29725ef3698bb695ffa743.svg",
                        "isPro": false,
                        "fullname": "Tao Zhang",
                        "user": "TaoZhang",
                        "type": "user"
                    },
                    "name": "Tao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:39:23.333Z",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec98",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec99",
                    "user": {
                        "_id": "638598a138f4aec99c50750e",
                        "avatarUrl": "/avatars/42a4aad213e04a0ded1ab7f81910e082.svg",
                        "isPro": false,
                        "fullname": "Shilin Xu",
                        "user": "shilinxu",
                        "type": "user"
                    },
                    "name": "Shilin Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:40:16.694Z",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec9a",
                    "name": "Shunping Ji",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec9b",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec9c",
                    "name": "Lu Qi",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec9d",
                    "user": {
                        "_id": "67298e44017b96a1d0101dc4",
                        "avatarUrl": "/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg",
                        "isPro": false,
                        "fullname": "Jiashi Feng",
                        "user": "jshfeng",
                        "type": "user"
                    },
                    "name": "Jiashi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:41:01.815Z",
                    "hidden": false
                },
                {
                    "_id": "677e2052dbff7b495e85ec9e",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T18:58:54.000Z",
            "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
            "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
            "upvotes": 18,
            "discussionId": "677e2056dbff7b495e85ede6"
        },
        "translation": "标题：Sa2VA：将SAM2与LLaVA结合用于图像和视频的密集基础理解\n\n摘要：本文提出了Sa2VA，这是首个用于图像和视频密集基础理解的统一模型。与现有的多模态大语言模型不同，后者通常局限于特定模态和任务，而Sa2VA支持广泛的图像和视频任务，包括参考分割和对话，且仅需最小的一次性指令调优。Sa2VA结合了SAM-2（一个基础视频分割模型）和LLaVA（一个先进的视觉语言模型），并将文本、图像和视频统一到一个共享的LLM令牌空间中。通过使用LLM，Sa2VA生成指令令牌，指导SAM-2生成精确的掩码，从而实现对静态和动态视觉内容的基础多模态理解。此外，我们引入了Ref-SAV，这是一个包含复杂视频场景中超过72k对象表达式的自动标注数据集，旨在提升模型性能。我们还手动验证了Ref-SAV数据集中的2k视频对象，以在复杂环境中进行参考视频对象分割的基准测试。实验表明，Sa2VA在多个任务中达到了最先进的水平，特别是在参考视频对象分割方面，突显了其在复杂现实应用中的潜力。"
    },
    {
        "paper": {
            "id": "2501.03847",
            "authors": [
                {
                    "_id": "677e11b751b4ae6d3c98fdca",
                    "name": "Zekai Gu",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdcb",
                    "user": {
                        "_id": "64be3a355b8d826146ed2001",
                        "avatarUrl": "/avatars/85a87270e7c76345e555803ab31c33d1.svg",
                        "isPro": false,
                        "fullname": "ruiyan",
                        "user": "ruiyan",
                        "type": "user"
                    },
                    "name": "Rui Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:47:05.657Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdcc",
                    "user": {
                        "_id": "64d3abeb1058ae59738ba8ce",
                        "avatarUrl": "/avatars/4a9de3db835ade2c20f0d2de678e85c4.svg",
                        "isPro": false,
                        "fullname": "jiahao lu",
                        "user": "jiahao97",
                        "type": "user"
                    },
                    "name": "Jiahao Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:46:55.864Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdcd",
                    "user": {
                        "_id": "634e6bdd2cd84978c48f3985",
                        "avatarUrl": "/avatars/2f6ae1f50666ca187a7a3c74b5e173f7.svg",
                        "isPro": false,
                        "fullname": "Peng Li",
                        "user": "pengHTYX",
                        "type": "user"
                    },
                    "name": "Peng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:29.526Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdce",
                    "user": {
                        "_id": "645223fb01d7bd9555ea399a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/ZED54WEISs5HxUd52Yge8.png",
                        "isPro": false,
                        "fullname": "Zhiyang Dou",
                        "user": "frankzydou",
                        "type": "user"
                    },
                    "name": "Zhiyang Dou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:47:13.361Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdcf",
                    "user": {
                        "_id": "635f8ed47c05eb9f59963d3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                        "isPro": false,
                        "fullname": "ChenyangSi",
                        "user": "ChenyangSi",
                        "type": "user"
                    },
                    "name": "Chenyang Si",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:47:21.815Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd0",
                    "user": {
                        "_id": "643ba2f725681c3afaa8f05e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Dong",
                        "user": "zhendongucb",
                        "type": "user"
                    },
                    "name": "Zhen Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:47:32.762Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd1",
                    "name": "Qifeng Liu",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd2",
                    "name": "Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd3",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:48:08.642Z",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd4",
                    "name": "Wenping Wang",
                    "hidden": false
                },
                {
                    "_id": "677e11b751b4ae6d3c98fdd5",
                    "name": "Yuan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T15:01:58.000Z",
            "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video\n  Generation Control",
            "summary": "Diffusion models have demonstrated impressive performance in generating\nhigh-quality videos from text prompts or images. However, precise control over\nthe video generation process, such as camera manipulation or content editing,\nremains a significant challenge. Existing methods for controlled video\ngeneration are typically limited to a single control type, lacking the\nflexibility to handle diverse control demands. In this paper, we introduce\nDiffusion as Shader (DaS), a novel approach that supports multiple video\ncontrol tasks within a unified architecture. Our key insight is that achieving\nversatile video control necessitates leveraging 3D control signals, as videos\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\ninputs, making the video diffusion process inherently 3D-aware. This innovation\nallows DaS to achieve a wide range of video controls by simply manipulating the\n3D tracking videos. A further advantage of using 3D tracking videos is their\nability to effectively link frames, significantly enhancing the temporal\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\nacross diverse tasks, including mesh-to-video generation, camera control,\nmotion transfer, and object manipulation.",
            "upvotes": 13,
            "discussionId": "677e11b851b4ae6d3c98fe56"
        },
        "translation": "标题：扩散作为着色器：面向多功能视频生成控制的三维感知视频扩散\n\n摘要：扩散模型在从文本提示或图像生成高质量视频方面展示了令人印象深刻的性能。然而，对视频生成过程的精确控制，如相机操作或内容编辑，仍然是一个重大挑战。现有的受控视频生成方法通常仅限于单一控制类型，缺乏处理多样化控制需求的灵活性。在本文中，我们介绍了扩散作为着色器（DaS），这是一种在统一架构内支持多种视频控制任务的新方法。我们的关键见解是，实现多功能视频控制需要利用三维控制信号，因为视频本质上是动态三维内容的二维渲染。与仅限于二维控制信号的现有方法不同，DaS利用三维跟踪视频作为控制输入，使视频扩散过程本质上具有三维感知能力。这一创新使得DaS能够通过简单地操作三维跟踪视频来实现广泛的视频控制。使用三维跟踪视频的另一个优势是其能够有效地链接帧，显著增强生成视频的时间一致性。仅使用8个H800 GPU在不到10k视频上进行3天的微调，DaS在多种任务中展示了强大的控制能力，包括网格到视频生成、相机控制、运动转移和对象操作。"
    },
    {
        "paper": {
            "id": "2501.03936",
            "authors": [
                {
                    "_id": "677e43433dfa51c15df22b7d",
                    "user": {
                        "_id": "64380d17819f3ab20d17595b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64380d17819f3ab20d17595b/LW_JVO54BCoYnKYZ-868o.png",
                        "isPro": false,
                        "fullname": "Zheng Hao",
                        "user": "Forceless",
                        "type": "user"
                    },
                    "name": "Hao Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:22.639Z",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b7e",
                    "name": "Xinyan Guan",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b7f",
                    "user": {
                        "_id": "6354fdcd525beaee68899e50",
                        "avatarUrl": "/avatars/1135c7e402510b2ad0fe33f60c8d842c.svg",
                        "isPro": false,
                        "fullname": "Hao Kong",
                        "user": "Danphnis",
                        "type": "user"
                    },
                    "name": "Hao Kong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:50:59.404Z",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b80",
                    "name": "Jia Zheng",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b81",
                    "user": {
                        "_id": "6711c702f858a456b4b9f3a4",
                        "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
                        "isPro": false,
                        "fullname": "Hongyu  Lin",
                        "user": "sanmusunrise",
                        "type": "user"
                    },
                    "name": "Hongyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:49:20.814Z",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b82",
                    "user": {
                        "_id": "6216496a9b34d2fb49144599",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
                        "isPro": false,
                        "fullname": "Yaojie Lu",
                        "user": "luyaojie",
                        "type": "user"
                    },
                    "name": "Yaojie Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T13:23:32.842Z",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b83",
                    "name": "Ben He",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b84",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:49:38.801Z",
                    "hidden": false
                },
                {
                    "_id": "677e43433dfa51c15df22b85",
                    "name": "Le Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T16:53:01.000Z",
            "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
            "summary": "Automatically generating presentations from documents is a challenging task\nthat requires balancing content quality, visual design, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, often overlooking visual design and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to understand their\nstructural patterns and content schemas, then drafts outlines and generates\nslides through code actions to ensure consistency and alignment. To\ncomprehensively evaluate the quality of generated presentations, we further\nintroduce PPTEval, an evaluation framework that assesses presentations across\nthree dimensions: Content, Design, and Coherence. Experiments show that\nPPTAgent significantly outperforms traditional automatic presentation\ngeneration methods across all three dimensions. The code and data are available\nat https://github.com/icip-cas/PPTAgent.",
            "upvotes": 10,
            "discussionId": "677e43443dfa51c15df22bd7"
        },
        "translation": "标题：PPTAgent：超越文本到幻灯片的演示文稿生成与评估\n\n摘要：从文档自动生成演示文稿是一项具有挑战性的任务，需要在内容质量、视觉设计和结构连贯性之间取得平衡。现有方法主要集中于单独改进和评估内容质量，往往忽视了视觉设计和结构连贯性，这限制了它们的实际应用性。为了解决这些局限性，我们提出了PPTAgent，它通过一种受人类工作流程启发的两阶段、基于编辑的方法，全面改进了演示文稿的生成。PPTAgent首先分析参考演示文稿以理解其结构模式和内容模式，然后通过代码操作起草大纲并生成幻灯片，以确保一致性和对齐。为了全面评估生成的演示文稿的质量，我们进一步引入了PPTEval，这是一个评估框架，从内容、设计和连贯性三个维度对演示文稿进行评估。实验表明，PPTAgent在所有三个维度上均显著优于传统的自动演示文稿生成方法。代码和数据可在https://github.com/icip-cas/PPTAgent获取。"
    },
    {
        "paper": {
            "id": "2501.04561",
            "authors": [
                {
                    "_id": "677f302c7bd81737ce7a9405",
                    "name": "Run Luo",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9406",
                    "name": "Ting-En Lin",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9407",
                    "name": "Haonan Zhang",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9408",
                    "name": "Yuchuan Wu",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9409",
                    "name": "Xiong Liu",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940a",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940b",
                    "name": "Yongbin Li",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940c",
                    "name": "Longze Chen",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940d",
                    "name": "Jiaming Li",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940e",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a940f",
                    "name": "Yangyi Chen",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9410",
                    "name": "Hamid Alinejad-Rokny",
                    "hidden": false
                },
                {
                    "_id": "677f302c7bd81737ce7a9411",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T15:18:09.000Z",
            "title": "OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment\n  across Language with Real-time Self-Aware Emotional Speech Synthesis",
            "summary": "Recent advancements in omnimodal learning have been achieved in understanding\nand generation across images, text, and speech, though mainly within\nproprietary models. Limited omnimodal datasets and the inherent challenges\nassociated with real-time emotional speech generation have hindered open-source\nprogress. To address these issues, we propose openomni, a two-stage training\nmethod combining omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model is further trained on text-image tasks to generalize\nfrom vision to speech in a (near) zero-shot manner, outperforming models\ntrained on tri-modal datasets. In the speech generation phase, a lightweight\ndecoder facilitates real-time emotional speech through training on speech tasks\nand preference learning. Experiments demonstrate that openomni consistently\nimproves across omnimodal, vision-language, and speech-language evaluations,\nenabling natural, emotion-rich dialogues and real-time emotional speech\ngeneration.",
            "upvotes": 9,
            "discussionId": "677f302d7bd81737ce7a944a"
        },
        "translation": "标题：OpenOmni：基于大语言模型的零样本全模态对齐与实时自感知情感语音合成\n\n摘要：尽管主要在专有模型中实现，全模态学习在图像、文本和语音的理解与生成方面取得了显著进展。有限的全模态数据集以及与实时情感语音生成相关的固有挑战阻碍了开源领域的进展。为解决这些问题，我们提出了openomni，一种结合全模态对齐和语音生成的两阶段训练方法，以开发一种先进的全模态大语言模型。在对齐阶段，预训练的语音模型在文本-图像任务上进一步训练，以（近乎）零样本的方式从视觉泛化到语音，其性能优于在三模态数据集上训练的模型。在语音生成阶段，通过在语音任务和偏好学习上的训练，轻量级解码器实现了实时情感语音生成。实验表明，openomni在全模态、视觉-语言和语音-语言评估中持续改进，能够实现自然、情感丰富的对话和实时情感语音生成。"
    },
    {
        "paper": {
            "id": "2501.03916",
            "authors": [
                {
                    "_id": "677e3b1ddbff7b495e8f0424",
                    "user": {
                        "_id": "64a3d1ddb3239f3e3892b24b",
                        "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                        "isPro": false,
                        "fullname": "Jiakang Yuan",
                        "user": "JiakangYuan",
                        "type": "user"
                    },
                    "name": "Jiakang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:52:54.625Z",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f0425",
                    "user": {
                        "_id": "65b88b92e0bde92c176a888a",
                        "avatarUrl": "/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg",
                        "isPro": false,
                        "fullname": "Xiangchao Yan",
                        "user": "yxc97",
                        "type": "user"
                    },
                    "name": "Xiangchao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:53:08.361Z",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f0426",
                    "user": {
                        "_id": "643df87f7cd64d872cb9fabd",
                        "avatarUrl": "/avatars/c53bfabcee08de448dde973915e8b31d.svg",
                        "isPro": false,
                        "fullname": "Botian Shi",
                        "user": "friskit",
                        "type": "user"
                    },
                    "name": "Botian Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:53:16.461Z",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f0427",
                    "name": "Tao Chen",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f0428",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f0429",
                    "user": {
                        "_id": "643dfd235aafbdca3a5792c0",
                        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
                        "isPro": false,
                        "fullname": "Bo Zhang",
                        "user": "BoZhang",
                        "type": "user"
                    },
                    "name": "Bo Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:24.813Z",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f042a",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f042b",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "677e3b1ddbff7b495e8f042c",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:53:56.271Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T16:31:10.000Z",
            "title": "Dolphin: Closed-loop Open-ended Auto-research through Thinking,\n  Practice, and Feedback",
            "summary": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we propose Dolphin, the\nfirst closed-loop open-ended auto-research framework to further build the\nentire process of human scientific research. Dolphin can generate research\nideas, perform experiments, and get feedback from experimental results to\ngenerate higher-quality ideas. More specifically, Dolphin first generates novel\nideas based on relevant papers which are ranked by the topic and task\nattributes. Then, the codes are automatically generated and debugged with the\nexception-traceback-guided local code structure. Finally, Dolphin automatically\nanalyzes the results of each idea and feeds the results back to the next round\nof idea generation. Experiments are conducted on the benchmark datasets of\ndifferent topics and results show that Dolphin can generate novel ideas\ncontinuously and complete the experiment in a loop. We highlight that Dolphin\ncan automatically propose methods that are comparable to the state-of-the-art\nin some tasks such as 2D image classification and 3D point classification.",
            "upvotes": 8,
            "discussionId": "677e3b1edbff7b495e8f049c"
        },
        "translation": "标题：海豚：通过思考、实践和反馈实现闭环开放式自动研究\n\n摘要：由于人工智能（AI）的发展，科学研究范式正在经历深刻的变革。最近的研究表明，各种AI辅助的研究方法可以通过改进数据分析、加速计算和促进新想法的生成，大幅提高研究效率。为了进一步迈向终极目标（即自动化科学研究），本文提出了海豚（Dolphin），这是第一个闭环开放式自动研究框架，旨在进一步构建人类科学研究的全过程。海豚能够生成研究想法、执行实验，并从实验结果中获得反馈以生成更高质量的想法。具体而言，海豚首先基于相关论文生成新想法，这些论文按主题和任务属性进行排序。然后，代码会自动生成并通过异常回溯引导的局部代码结构进行调试。最后，海豚自动分析每个想法的结果，并将结果反馈到下一轮的想法生成中。在不同主题的基准数据集上进行了实验，结果表明海豚能够持续生成新想法并在循环中完成实验。我们强调，海豚在某些任务（如2D图像分类和3D点分类）中能够自动提出与最先进方法相媲美的方法。"
    },
    {
        "paper": {
            "id": "2501.02790",
            "authors": [
                {
                    "_id": "677dea23e86d0754dc6e3f09",
                    "user": {
                        "_id": "605e8dfd5abeb13e714c4c18",
                        "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
                        "isPro": false,
                        "fullname": "yueqin yin",
                        "user": "yyqoni",
                        "type": "user"
                    },
                    "name": "Yueqin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:41.339Z",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0a",
                    "user": {
                        "_id": "677ebaf3fcaae73ddd6c8475",
                        "avatarUrl": "/avatars/98db20fe3bbbf7caffb5821a7242dc54.svg",
                        "isPro": false,
                        "fullname": "Shentao Yang",
                        "user": "shentaoyang",
                        "type": "user"
                    },
                    "name": "Shentao Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T21:08:25.725Z",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0b",
                    "name": "Yujia Xie",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0c",
                    "name": "Ziyi Yang",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0d",
                    "name": "Yuting Sun",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0e",
                    "name": "Hany Awadalla",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f0f",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "677dea23e86d0754dc6e3f10",
                    "name": "Mingyuan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-06T06:17:56.000Z",
            "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language\n  Model",
            "summary": "Reinforcement learning from human feedback (RLHF) has been widely adopted to\nalign language models (LMs) with human preference. Prior RLHF works typically\ntake a bandit formulation, which, though intuitive, ignores the sequential\nnature of LM generation and can suffer from the sparse reward issue. While\nrecent works propose dense token-level RLHF, treating each token as an action\nmay be oversubtle to proper reward assignment. In this paper, we seek to get\nthe best of both by training and utilizing a segment-level reward model, which\nassigns a reward to each semantically complete text segment that spans over a\nshort sequence of tokens. For reward learning, our method allows dynamic text\nsegmentation and compatibility with standard sequence-preference datasets. For\neffective RL-based LM training against segment reward, we generalize the\nclassical scalar bandit reward normalizers into location-aware normalizer\nfunctions and interpolate the segment reward for further densification. With\nthese designs, our method performs competitively on three popular RLHF\nbenchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation\nstudies are conducted to further demonstrate our method.",
            "upvotes": 6,
            "discussionId": "677dea24e86d0754dc6e3f49"
        },
        "translation": "标题：文本分段及其奖励学习以改进语言模型中的RLHF\n\n摘要：基于人类反馈的强化学习（RLHF）已被广泛采用，以使语言模型（LM）与人类偏好保持一致。以往的RLHF研究通常采用赌博机（bandit）形式，虽然直观，但忽略了LM生成的序列性质，并且可能受到稀疏奖励问题的影响。尽管最近的研究提出了密集的令牌级RLHF，但将每个令牌视为一个动作可能过于细微，难以正确分配奖励。在本文中，我们通过训练和利用分段级奖励模型，旨在兼顾两者优势，该模型为每个跨越短序列令牌的语义完整文本段分配奖励。在奖励学习方面，我们的方法允许动态文本分割，并与标准序列偏好数据集兼容。为了有效进行基于RL的LM训练以对抗分段奖励，我们将经典的标量赌博机奖励归一化器推广为位置感知归一化函数，并对分段奖励进行插值以进一步密集化。通过这些设计，我们的方法在三个流行的RLHF基准测试中表现优异：AlpacaEval 2.0、Arena-Hard和MT-Bench。我们还进行了消融研究，以进一步证明我们方法的有效性。"
    },
    {
        "paper": {
            "id": "2501.03714",
            "authors": [
                {
                    "_id": "677e48854bf7f0d4735973d3",
                    "user": {
                        "_id": "677e4ef9c27c05b2d178d775",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xQoYxKiRmlnCOslOaxjCl.png",
                        "isPro": false,
                        "fullname": "Sangwoon Kwak",
                        "user": "sangwoonkwak",
                        "type": "user"
                    },
                    "name": "Sangwoon Kwak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:51:17.553Z",
                    "hidden": false
                },
                {
                    "_id": "677e48854bf7f0d4735973d4",
                    "user": {
                        "_id": "660ca70399760c2ef3ab4450",
                        "avatarUrl": "/avatars/d1dd8492b8129554b0650d22d0731fed.svg",
                        "isPro": false,
                        "fullname": "Joonsoo Kim",
                        "user": "joons1991",
                        "type": "user"
                    },
                    "name": "Joonsoo Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:51:29.754Z",
                    "hidden": false
                },
                {
                    "_id": "677e48854bf7f0d4735973d5",
                    "user": {
                        "_id": "677e68cc8a7269e1779e4def",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PrMRHMrB4aEQPWV0pxNxX.png",
                        "isPro": false,
                        "fullname": "Jun Young Jeong",
                        "user": "shurek20",
                        "type": "user"
                    },
                    "name": "Jun Young Jeong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:51:42.670Z",
                    "hidden": false
                },
                {
                    "_id": "677e48854bf7f0d4735973d6",
                    "name": "Won-Sik Cheong",
                    "hidden": false
                },
                {
                    "_id": "677e48854bf7f0d4735973d7",
                    "name": "Jihyong Oh",
                    "hidden": false
                },
                {
                    "_id": "677e48854bf7f0d4735973d8",
                    "name": "Munchurl Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T11:43:13.000Z",
            "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval\n  Adjustment for Compact Dynamic 3D Gaussian Splatting",
            "summary": "3D Gaussian Splatting (3DGS) has made significant strides in scene\nrepresentation and neural rendering, with intense efforts focused on adapting\nit for dynamic scenes. Despite delivering remarkable rendering quality and\nspeed, existing methods struggle with storage demands and representing complex\nreal-world motions. To tackle these issues, we propose MoDecGS, a\nmemory-efficient Gaussian splatting framework designed for reconstructing novel\nviews in challenging scenarios with complex motions. We introduce GlobaltoLocal\nMotion Decomposition (GLMD) to effectively capture dynamic motions in a\ncoarsetofine manner. This approach leverages Global Canonical Scaffolds (Global\nCS) and Local Canonical Scaffolds (Local CS), extending static Scaffold\nrepresentation to dynamic video reconstruction. For Global CS, we propose\nGlobal Anchor Deformation (GAD) to efficiently represent global dynamics along\ncomplex motions, by directly deforming the implicit Scaffold attributes which\nare anchor position, offset, and local context features. Next, we finely adjust\nlocal motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.\nAdditionally, we introduce Temporal Interval Adjustment (TIA) to automatically\ncontrol the temporal coverage of each Local CS during training, allowing\nMoDecGS to find optimal interval assignments based on the specified number of\ntemporal segments. Extensive evaluations demonstrate that MoDecGS achieves an\naverage 70% reduction in model size over stateoftheart methods for dynamic 3D\nGaussians from realworld dynamic videos while maintaining or even improving\nrendering quality.",
            "upvotes": 6,
            "discussionId": "677e48884bf7f0d473597469"
        },
        "translation": "标题：MoDec-GS：全局到局部运动分解与时间间隔调整的紧凑动态3D高斯泼溅\n\n摘要：3D高斯泼溅（3DGS）在场景表示和神经渲染方面取得了显著进展，尤其是在将其应用于动态场景方面进行了大量努力。尽管现有方法在渲染质量和速度上表现出色，但在存储需求和表示复杂现实世界运动方面仍面临挑战。为解决这些问题，我们提出了MoDecGS，一种内存高效的高斯泼溅框架，旨在在具有复杂运动的挑战性场景中重建新视图。我们引入了全局到局部运动分解（GLMD），以粗到细的方式有效捕捉动态运动。该方法利用全局规范支架（Global CS）和局部规范支架（Local CS），将静态支架表示扩展到动态视频重建。对于Global CS，我们提出了全局锚点变形（GAD），通过直接变形隐式支架属性（包括锚点位置、偏移和局部上下文特征）来高效表示沿复杂运动的全局动态。接下来，我们通过局部高斯变形（LGD）显式地精细调整局部运动。此外，我们引入了时间间隔调整（TIA），在训练过程中自动控制每个Local CS的时间覆盖范围，使MoDecGS能够基于指定的时间段数量找到最佳间隔分配。大量评估表明，MoDecGS在保持甚至提高渲染质量的同时，相较于现有动态3D高斯方法，模型大小平均减少了70%。"
    },
    {
        "paper": {
            "id": "2501.03931",
            "authors": [
                {
                    "_id": "677df1e52f09432f00e506ae",
                    "user": {
                        "_id": "6418554a0956be7233a1023e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
                        "isPro": false,
                        "fullname": "zhang yuechen",
                        "user": "julianjuaner",
                        "type": "user"
                    },
                    "name": "Yuechen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-08T09:44:33.746Z",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506af",
                    "user": {
                        "_id": "662d968eac05b4f7c280d6f9",
                        "avatarUrl": "/avatars/59d55142f33f650d9116a04110db839e.svg",
                        "isPro": false,
                        "fullname": "YaoYang Liu",
                        "user": "LazySheeep",
                        "type": "user"
                    },
                    "name": "Yaoyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:55:13.065Z",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506b0",
                    "name": "Bin Xia",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506b1",
                    "user": {
                        "_id": "673a10f911b7efeeedabc252",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T7ySn7F0pTVCvRdcvMz3d.png",
                        "isPro": false,
                        "fullname": "Bohao Peng",
                        "user": "BoHao0326",
                        "type": "user"
                    },
                    "name": "Bohao Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:56:25.937Z",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506b2",
                    "name": "Zexin Yan",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506b3",
                    "user": {
                        "_id": "6447485270338c0376087917",
                        "avatarUrl": "/avatars/359d17963a6d1c5f1fddf027169332a5.svg",
                        "isPro": false,
                        "fullname": "Eric Lo",
                        "user": "ericlo",
                        "type": "user"
                    },
                    "name": "Eric Lo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:56:05.809Z",
                    "hidden": false
                },
                {
                    "_id": "677df1e52f09432f00e506b4",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T16:48:31.000Z",
            "title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion\n  Transformers",
            "summary": "We present Magic Mirror, a framework for generating identity-preserved videos\nwith cinematic-level quality and dynamic motion. While recent advances in video\ndiffusion models have shown impressive capabilities in text-to-video\ngeneration, maintaining consistent identity while producing natural motion\nremains challenging. Previous methods either require person-specific\nfine-tuning or struggle to balance identity preservation with motion diversity.\nBuilt upon Video Diffusion Transformers, our method introduces three key\ncomponents: (1) a dual-branch facial feature extractor that captures both\nidentity and structural features, (2) a lightweight cross-modal adapter with\nConditioned Adaptive Normalization for efficient identity integration, and (3)\na two-stage training strategy combining synthetic identity pairs with video\ndata. Extensive experiments demonstrate that Magic Mirror effectively balances\nidentity consistency with natural motion, outperforming existing methods across\nmultiple metrics while requiring minimal parameters added. The code and model\nwill be made publicly available at:\nhttps://github.com/dvlab-research/MagicMirror/",
            "upvotes": 6,
            "discussionId": "677df1e92f09432f00e507e3"
        },
        "translation": "标题：魔镜：基于视频扩散变换器的身份保持视频生成\n\n摘要：我们提出了魔镜（Magic Mirror）框架，用于生成具有电影级质量和动态运动的身份保持视频。尽管最近在视频扩散模型方面的进展在文本到视频生成方面展示了令人印象深刻的能力，但在生成自然运动的同时保持身份一致性仍然具有挑战性。先前的方法要么需要针对特定人物进行微调，要么难以在身份保持与运动多样性之间取得平衡。基于视频扩散变换器，我们的方法引入了三个关键组件：（1）双分支面部特征提取器，用于捕捉身份和结构特征；（2）轻量级跨模态适配器，结合条件自适应归一化以实现高效的身份集成；（3）结合合成身份对与视频数据的两阶段训练策略。大量实验表明，魔镜有效地平衡了身份一致性与自然运动，在多个指标上优于现有方法，同时仅需添加最少的参数。代码和模型将公开发布在：https://github.com/dvlab-research/MagicMirror/"
    },
    {
        "paper": {
            "id": "2501.02393",
            "authors": [
                {
                    "_id": "677cfd73f7545101cef8b48b",
                    "name": "Markus J. Buehler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-04T22:30:21.000Z",
            "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
            "summary": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.",
            "upvotes": 3,
            "discussionId": "677cfd74f7545101cef8b4f4"
        },
        "translation": "标题：图感知同构注意力：Transformer中的自适应动态机制\n\n摘要：我们提出了一种通过将图感知关系推理整合到注意力机制中来修改Transformer架构的方法，融合了图神经网络和语言建模的概念。基于注意力机制与图论之间的内在联系，我们将Transformer的注意力机制重新表述为图操作，并提出了图感知同构注意力（Graph-Aware Isomorphic Attention）。该方法利用先进的图建模策略，包括图同构网络（Graph Isomorphism Networks, GIN）和主邻域聚合（Principal Neighborhood Aggregation, PNA），以丰富关系结构的表示。我们的方法能够捕捉复杂的依赖关系并在任务间泛化，这一点通过减少泛化差距和提高学习性能得到了验证。此外，我们扩展了图感知注意力的概念，引入了稀疏GIN注意力（Sparse GIN-Attention），这是一种采用稀疏GIN的微调方法。通过将注意力矩阵解释为稀疏邻接图，该技术以最小的计算开销增强了预训练基础模型的适应性，赋予它们图感知能力。与低秩适应（LoRA）等其他方法相比，稀疏GIN注意力微调在训练动态和泛化性能上表现更优。我们讨论了传统注意力机制中潜在的类图结构，为理解Transformer提供了新的视角。通过将Transformer演化为用于关系推理的分层GIN模型，这一视角为基础模型的开发提出了深远的影响，使得设计能够动态适应局部和全局依赖关系的架构成为可能。在生物信息学、材料科学、语言建模等领域的应用中，这种关系与序列数据建模的融合将有助于开发可解释且可泛化的建模策略。"
    },
    {
        "paper": {
            "id": "2501.02260",
            "authors": [
                {
                    "_id": "677df32e9b2af6b36b22668e",
                    "user": {
                        "_id": "6629604bdf81c3e1d90ca5ec",
                        "avatarUrl": "/avatars/bb35569fe124de1374f149b940a4b56c.svg",
                        "isPro": false,
                        "fullname": "Mengting Wei",
                        "user": "mengtingwei",
                        "type": "user"
                    },
                    "name": "Mengting Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:54:19.846Z",
                    "hidden": false
                },
                {
                    "_id": "677df32e9b2af6b36b22668f",
                    "name": "Tuomas Varanka",
                    "hidden": false
                },
                {
                    "_id": "677df32e9b2af6b36b226690",
                    "user": {
                        "_id": "667690daf2312ba18e81a160",
                        "avatarUrl": "/avatars/f502d8ba6cf1de352032e62acab4022d.svg",
                        "isPro": false,
                        "fullname": "Xingxun JIANG",
                        "user": "Xingxun",
                        "type": "user"
                    },
                    "name": "Xingxun Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T13:54:36.033Z",
                    "hidden": false
                },
                {
                    "_id": "677df32e9b2af6b36b226691",
                    "name": "Huai-Qian Khor",
                    "hidden": false
                },
                {
                    "_id": "677df32e9b2af6b36b226692",
                    "name": "Guoying Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-04T11:28:49.000Z",
            "title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit\n  Control",
            "summary": "We address the problem of facial expression editing by controling the\nrelative variation of facial action-unit (AU) from the same person. This\nenables us to edit this specific person's expression in a fine-grained,\ncontinuous and interpretable manner, while preserving their identity, pose,\nbackground and detailed facial attributes. Key to our model, which we dub\nMagicFace, is a diffusion model conditioned on AU variations and an ID encoder\nto preserve facial details of high consistency. Specifically, to preserve the\nfacial details with the input identity, we leverage the power of pretrained\nStable-Diffusion models and design an ID encoder to merge appearance features\nthrough self-attention. To keep background and pose consistency, we introduce\nan efficient Attribute Controller by explicitly informing the model of current\nbackground and pose of the target. By injecting AU variations into a denoising\nUNet, our model can animate arbitrary identities with various AU combinations,\nyielding superior results in high-fidelity expression editing compared to other\nfacial expression editing works. Code is publicly available at\nhttps://github.com/weimengting/MagicFace.",
            "upvotes": 3,
            "discussionId": "677df32f9b2af6b36b2266dd"
        },
        "translation": "标题：MagicFace：基于动作单元控制的高保真面部表情编辑\n\n摘要：我们通过控制同一人面部动作单元（AU）的相对变化来解决面部表情编辑问题。这使得我们能够以细粒度、连续且可解释的方式编辑特定人物的表情，同时保留其身份、姿态、背景和详细的面部属性。我们提出的模型名为MagicFace，其核心是一个基于AU变化的条件扩散模型和一个用于保持高一致性面部细节的ID编码器。具体来说，为了保留输入身份的面部细节，我们利用预训练的Stable-Diffusion模型的能力，并设计了一个ID编码器，通过自注意力机制融合外观特征。为了保持背景和姿态的一致性，我们引入了一个高效的属性控制器，通过显式地告知模型当前目标的背景和姿态。通过将AU变化注入去噪UNet，我们的模型能够使用各种AU组合为任意身份生成动画，与其他面部表情编辑工作相比，在高保真表情编辑方面取得了优异的结果。代码公开在https://github.com/weimengting/MagicFace。"
    },
    {
        "paper": {
            "id": "2501.02376",
            "authors": [
                {
                    "_id": "677f28c33d2765f5655e948f",
                    "name": "Wenhao Wang",
                    "hidden": false
                },
                {
                    "_id": "677f28c33d2765f5655e9490",
                    "name": "Yifan Sun",
                    "hidden": false
                },
                {
                    "_id": "677f28c33d2765f5655e9491",
                    "name": "Zongxin Yang",
                    "hidden": false
                },
                {
                    "_id": "677f28c33d2765f5655e9492",
                    "name": "Zhentao Tan",
                    "hidden": false
                },
                {
                    "_id": "677f28c33d2765f5655e9493",
                    "name": "Zhengdong Hu",
                    "hidden": false
                },
                {
                    "_id": "677f28c33d2765f5655e9494",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-04T20:34:53.000Z",
            "title": "Generalizable Origin Identification for Text-Guided Image-to-Image\n  Diffusion Models",
            "summary": "Text-guided image-to-image diffusion models excel in translating images based\non textual prompts, allowing for precise and creative visual modifications.\nHowever, such a powerful technique can be misused for spreading misinformation,\ninfringing on copyrights, and evading content tracing. This motivates us to\nintroduce the task of origin IDentification for text-guided Image-to-image\nDiffusion models (ID^2), aiming to retrieve the original image of a given\ntranslated query. A straightforward solution to ID^2 involves training a\nspecialized deep embedding model to extract and compare features from both\nquery and reference images. However, due to visual discrepancy across\ngenerations produced by different diffusion models, this similarity-based\napproach fails when training on images from one model and testing on those from\nanother, limiting its effectiveness in real-world applications. To solve this\nchallenge of the proposed ID^2 task, we contribute the first dataset and a\ntheoretically guaranteed method, both emphasizing generalizability. The curated\ndataset, OriPID, contains abundant Origins and guided Prompts, which can be\nused to train and test potential IDentification models across various diffusion\nmodels. In the method section, we first prove the existence of a linear\ntransformation that minimizes the distance between the pre-trained Variational\nAutoencoder (VAE) embeddings of generated samples and their origins.\nSubsequently, it is demonstrated that such a simple linear transformation can\nbe generalized across different diffusion models. Experimental results show\nthat the proposed method achieves satisfying generalization performance,\nsignificantly surpassing similarity-based methods (+31.6% mAP), even those\nwith generalization designs.",
            "upvotes": 0,
            "discussionId": "677f28c63d2765f5655e95ae"
        },
        "translation": "标题：面向文本引导图像到图像扩散模型的通用来源识别\n\n摘要：文本引导的图像到图像扩散模型在基于文本提示的图像转换方面表现出色，能够实现精确且富有创意的视觉修改。然而，这种强大的技术可能被滥用于传播错误信息、侵犯版权和逃避内容追踪。这促使我们提出了文本引导图像到图像扩散模型的来源识别任务（ID^2），旨在检索给定转换查询的原始图像。ID^2的一个直接解决方案是训练一个专门的深度嵌入模型，以提取和比较查询图像和参考图像的特征。然而，由于不同扩散模型生成的图像之间存在视觉差异，这种基于相似性的方法在从一个模型的图像训练并在另一个模型的图像上测试时失效，限制了其在实际应用中的有效性。为了解决这一挑战，我们贡献了第一个数据集和一种理论上保证的方法，两者都强调通用性。精心策划的数据集OriPID包含丰富的原始图像和引导提示，可用于训练和测试跨各种扩散模型的潜在识别模型。在方法部分，我们首先证明存在一种线性变换，可以最小化生成样本与其原始图像的预训练变分自编码器（VAE）嵌入之间的距离。随后，证明了这种简单的线性变换可以推广到不同的扩散模型中。实验结果表明，所提出的方法实现了令人满意的泛化性能，显著超越了基于相似性的方法（+31.6% mAP），即使是那些具有泛化设计的方法。"
    }
]