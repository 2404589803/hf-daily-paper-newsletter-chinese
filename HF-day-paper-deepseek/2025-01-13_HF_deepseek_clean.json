[
  {
    "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
    "summary": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the\nissue of generating factually incorrect outputs in foundation models by\nretrieving external knowledge relevant to queries and incorporating it into\ntheir generation process. However, existing RAG approaches have primarily\nfocused on textual information, with some recent advancements beginning to\nconsider images, and they largely overlook videos, a rich source of multimodal\nknowledge capable of representing events, processes, and contextual details\nmore effectively than any other modality. While a few recent studies explore\nthe integration of videos in the response generation process, they either\npredefine query-associated videos without retrieving them according to queries,\nor convert videos into the textual descriptions without harnessing their\nmultimodal richness. To tackle these, we introduce VideoRAG, a novel framework\nthat not only dynamically retrieves relevant videos based on their relevance\nwith queries but also utilizes both visual and textual information of videos in\nthe output generation. Further, to operationalize this, our method revolves\naround the recent advance of Large Video Language Models (LVLMs), which enable\nthe direct processing of video content to represent it for retrieval and\nseamless integration of the retrieved videos jointly with queries. We\nexperimentally validate the effectiveness of VideoRAG, showcasing that it is\nsuperior to relevant baselines.",
    "translation": "标题：VideoRAG：基于视频语料库的检索增强生成\n\n摘要：检索增强生成（Retrieval-Augmented Generation, RAG）是一种强大的策略，通过检索与查询相关的外部知识并将其整合到生成过程中，来解决基础模型生成事实错误输出的问题。然而，现有的RAG方法主要集中在文本信息上，最近的一些进展开始考虑图像，但它们大多忽略了视频这一丰富的多模态知识来源，视频能够比其他任何模态更有效地表示事件、过程和上下文细节。尽管最近的一些研究探索了在响应生成过程中整合视频的方法，但它们要么预定义了与查询相关的视频而没有根据查询进行检索，要么将视频转换为文本描述而没有利用其多模态的丰富性。为了解决这些问题，我们提出了VideoRAG，这是一个新颖的框架，不仅根据查询动态检索相关视频，还在输出生成中利用视频的视觉和文本信息。此外，为了实现这一点，我们的方法围绕最近的大型视频语言模型（Large Video Language Models, LVLMs）的进展展开，这些模型能够直接处理视频内容以进行检索，并将检索到的视频与查询无缝整合。我们通过实验验证了VideoRAG的有效性，展示了其优于相关基线模型的表现。",
    "url": "https://huggingface.co/papers/2501.05874"
  },
  {
    "title": "Enabling Scalable Oversight via Self-Evolving Critic",
    "summary": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component.",
    "translation": "标题：通过自我演进的批评者实现可扩展的监督\n\n摘要：尽管大型语言模型（LLMs）表现出色，但其发展面临一个关键挑战：可扩展的监督。即在人类评估困难或LLMs超越人类的任务中提供有效反馈。虽然越来越多的人对使用LLMs进行批评感兴趣，但当前方法仍依赖于人类注释或更强大的模型，使得在没有外部监督的情况下增强批评能力的问题仍未解决。我们引入了SCRIT（自我演进的批评者），一个能够实现批评能力真正自我演进的框架。技术上，SCRIT通过训练合成数据进行自我改进，这些数据由基于对比的自我批评者生成，该批评者使用参考解决方案进行逐步批评，并通过自我验证机制确保批评质量，该机制通过纠正结果来保证批评质量。使用最强大的LLMs之一Qwen2.5-72B-Instruct实现，SCRIT在批评纠正和错误识别基准上实现了高达10.3%的改进。我们的分析表明，SCRIT的性能随着数据和模型规模的增加而正向扩展，优于其他方法，并且其自我验证组件对其性能至关重要。",
    "url": "https://huggingface.co/papers/2501.05727"
  },
  {
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric\n  Interaction Primitives as Spatial Constraints",
    "summary": "The development of general robotic systems capable of manipulating in\nunstructured environments is a significant challenge. While Vision-Language\nModels(VLM) excel in high-level commonsense reasoning, they lack the\nfine-grained 3D spatial understanding required for precise manipulation tasks.\nFine-tuning VLM on robotic datasets to create Vision-Language-Action\nModels(VLA) is a potential solution, but it is hindered by high data collection\ncosts and generalization issues. To address these challenges, we propose a\nnovel object-centric representation that bridges the gap between VLM's\nhigh-level reasoning and the low-level precision required for manipulation. Our\nkey insight is that an object's canonical space, defined by its functional\naffordances, provides a structured and semantically meaningful way to describe\ninteraction primitives, such as points and directions. These primitives act as\na bridge, translating VLM's commonsense reasoning into actionable 3D spatial\nconstraints. In this context, we introduce a dual closed-loop, open-vocabulary\nrobotic manipulation system: one loop for high-level planning through primitive\nresampling, interaction rendering and VLM checking, and another for low-level\nexecution via 6D pose tracking. This design ensures robust, real-time control\nwithout requiring VLM fine-tuning. Extensive experiments demonstrate strong\nzero-shot generalization across diverse robotic manipulation tasks,\nhighlighting the potential of this approach for automating large-scale\nsimulation data generation.",
    "translation": "标题：OmniManip：通过以对象为中心的交互原语作为空间约束实现通用机器人操作\n\n摘要：开发能够在非结构化环境中进行操作的通用机器人系统是一个重大挑战。尽管视觉-语言模型（VLM）在高层常识推理方面表现出色，但它们缺乏精确操作任务所需的细粒度三维空间理解能力。通过在机器人数据集上微调VLM以创建视觉-语言-动作模型（VLA）是一种潜在的解决方案，但这一方法受到高数据收集成本和泛化问题的阻碍。为了解决这些挑战，我们提出了一种新颖的以对象为中心的表示方法，该方法在VLM的高层推理与操作所需的低层精度之间架起了一座桥梁。我们的关键见解是，由对象的功能性可供性定义的规范空间提供了一种结构化和语义上有意义的方式来描述交互原语，如点和方向。这些原语充当桥梁，将VLM的常识推理转化为可操作的三维空间约束。在此背景下，我们引入了一种双闭环、开放词汇的机器人操作系统：一个闭环通过原语重采样、交互渲染和VLM检查进行高层规划，另一个闭环通过六维姿态跟踪进行低层执行。这种设计确保了无需VLM微调的鲁棒实时控制。大量实验展示了在各种机器人操作任务中的强大零样本泛化能力，突显了该方法在自动化大规模仿真数据生成方面的潜力。",
    "url": "https://huggingface.co/papers/2501.03841"
  },
  {
    "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
    "summary": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available.",
    "translation": "标题：LlamaV-o1：重新思考大语言模型中的逐步视觉推理\n\n摘要：推理是解决复杂多步骤问题的基本能力，尤其是在需要顺序逐步理解的视觉环境中。现有方法缺乏一个全面的框架来评估视觉推理，并且没有强调逐步解决问题。为此，我们提出了一个全面的框架，通过三个关键贡献来推进大语言模型（LLMs）中的逐步视觉推理。首先，我们引入了一个专门设计用于评估多步骤推理任务的视觉推理基准。该基准提出了八种不同类别的多样化挑战，从复杂的视觉感知到科学推理，总共包含超过4k个推理步骤，从而能够对LLMs在多个步骤中执行准确且可解释的视觉推理能力进行稳健评估。其次，我们提出了一种新颖的指标，以单个步骤的粒度评估视觉推理的质量，强调正确性和逻辑一致性。与传统的最终任务准确性指标相比，所提出的指标提供了对推理性能更深入的洞察。第三，我们提出了一种新的多模态视觉推理模型，名为LlamaV-o1，该模型使用多步骤课程学习方法进行训练，其中任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1专为多步骤推理设计，并通过结构化的训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并且在推理扩展过程中比闭源专有模型表现更好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分为67.3，绝对增益为3.8%，同时在推理扩展过程中速度快5倍。我们的基准、模型和代码均已公开。",
    "url": "https://huggingface.co/papers/2501.06186"
  },
  {
    "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
    "summary": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks.",
    "translation": "标题：多智能体微调：通过多样化推理链实现自我改进\n\n摘要：近年来，大型语言模型（LLMs）取得了显著的性能提升，但其根本上受限于基础训练数据。为了在训练数据之外进一步提升模型性能，最近的研究探索了如何利用LLMs生成合成数据以实现自主自我改进。然而，连续的自我改进步骤可能会达到收益递减的临界点。在本研究中，我们提出了一种互补的自我改进方法，即对语言模型的多智能体社会进行微调。一组从相同基础模型出发的语言模型，通过模型间的多智能体交互生成的数据进行独立更新，从而实现各自的专门化。通过在独立的数据集上训练每个模型，我们展示了这种方法如何实现模型间的专门化和模型集合的多样化。因此，我们的整体系统能够保留多样化的推理链，并在比单智能体自我改进方法更多的微调轮次中实现自主改进。我们在广泛的推理任务中定量展示了该方法的有效性。",
    "url": "https://huggingface.co/papers/2501.05707"
  }
]