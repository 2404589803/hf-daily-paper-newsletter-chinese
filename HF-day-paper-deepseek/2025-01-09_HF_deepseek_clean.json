[
    {
        "paper": {
            "id": "2501.04519",
            "authors": [
                {
                    "_id": "677f3f364be6eaf5077001f6",
                    "name": "Xinyu Guan",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001f7",
                    "user": {
                        "_id": "62b0009c72043b05d29492b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                        "isPro": false,
                        "fullname": "Li Lyna Zhang",
                        "user": "lynazhang",
                        "type": "user"
                    },
                    "name": "Li Lyna Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T10:14:40.859Z",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001f8",
                    "user": {
                        "_id": "662d015a2d4c0e85da85ff0c",
                        "avatarUrl": "/avatars/ff38e82d1371fe9e69bacb9b04cfe444.svg",
                        "isPro": false,
                        "fullname": "Yifei Liu",
                        "user": "YF-L",
                        "type": "user"
                    },
                    "name": "Yifei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:58:00.452Z",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001f9",
                    "user": {
                        "_id": "632bc663eafe8eca5e9bfdbc",
                        "avatarUrl": "/avatars/787553c73e9a96adc5219e67acd29c00.svg",
                        "isPro": false,
                        "fullname": "Ning Shang",
                        "user": "J-shang",
                        "type": "user"
                    },
                    "name": "Ning Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T10:16:39.284Z",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001fa",
                    "name": "Youran Sun",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001fb",
                    "name": "Yi Zhu",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001fc",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "677f3f364be6eaf5077001fd",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T14:12:57.000Z",
            "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking",
            "summary": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.",
            "upvotes": 116,
            "discussionId": "677f3f374be6eaf507700262"
        },
        "translation": "标题：rStar-Math：小型语言模型通过自我进化的深度思考掌握数学推理\n\n摘要：我们提出了rStar-Math，以证明小型语言模型（SLMs）无需从优越模型中进行蒸馏，即可媲美甚至超越OpenAI o1的数学推理能力。rStar-Math通过蒙特卡洛树搜索（MCTS）进行“深度思考”来实现这一目标，其中数学策略SLM在基于SLM的过程奖励模型的指导下进行测试时搜索。rStar-Math引入了三项创新来解决训练这两个SLM的挑战：（1）一种新颖的代码增强的CoT数据合成方法，通过广泛的MCTS滚动生成逐步验证的推理轨迹，用于训练策略SLM；（2）一种新颖的过程奖励模型训练方法，避免了简单的步骤级评分注释，从而产生更有效的过程偏好模型（PPM）；（3）一种自我进化方案，其中策略SLM和PPM从零开始构建，并通过迭代进化提高推理能力。通过对747k个数学问题的数百万个合成解决方案进行4轮自我进化，rStar-Math将SLMs的数学推理能力提升到了最先进的水平。在MATH基准测试中，它将Qwen2.5-Math-7B从58.8%提高到90.0%，将Phi3-mini-3.8B从41.4%提高到86.4%，分别比o1-preview高出+4.5%和+0.9%。在美国数学奥林匹克（AIME）中，rStar-Math平均解决了53.3%（8/15）的问题，跻身于最优秀的高中数学学生前20%。代码和数据将在https://github.com/microsoft/rStar上提供。"
    },
    {
        "paper": {
            "id": "2501.04682",
            "authors": [
                {
                    "_id": "677f4e0d05ab3422540b88ff",
                    "name": "Violet Xiang",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8900",
                    "name": "Charlie Snell",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8901",
                    "name": "Kanishk Gandhi",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8902",
                    "user": {
                        "_id": "611a7ec4289467cafea62d13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/611a7ec4289467cafea62d13/pck-0fmPQkoU7yzh6-WoL.jpeg",
                        "isPro": false,
                        "fullname": "Alon Albalak",
                        "user": "alon-albalak",
                        "type": "user"
                    },
                    "name": "Alon Albalak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:57:57.972Z",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8903",
                    "user": {
                        "_id": "6511ee845b7e52b0251fdee9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6511ee845b7e52b0251fdee9/hTIwiIYBGOVnIrxtpri83.png",
                        "isPro": false,
                        "fullname": "Anikait Singh",
                        "user": "Asap7772",
                        "type": "user"
                    },
                    "name": "Anikait Singh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:59.745Z",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8904",
                    "name": "Chase Blagden",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8905",
                    "name": "Duy Phung",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8906",
                    "name": "Rafael Rafailov",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8907",
                    "user": {
                        "_id": "61aa15fd8a9625ebfe284286",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61aa15fd8a9625ebfe284286/KaGzIeijcgcN15JErCqft.jpeg",
                        "isPro": false,
                        "fullname": "nathan lile",
                        "user": "nlile",
                        "type": "user"
                    },
                    "name": "Nathan Lile",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:07:02.418Z",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8908",
                    "name": "Dakota Mahan",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b8909",
                    "name": "Louis Castricato",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b890a",
                    "name": "Jan-Philipp Franken",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b890b",
                    "name": "Nick Haber",
                    "hidden": false
                },
                {
                    "_id": "677f4e0d05ab3422540b890c",
                    "name": "Chelsea Finn",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T18:42:48.000Z",
            "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Though",
            "summary": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
            "upvotes": 46,
            "discussionId": "677f4e1105ab3422540b8a2e"
        },
        "translation": "标题：迈向LLMs中的系统2推理：学习如何通过元思维链进行思考\n\n摘要：我们提出了一种新颖的框架，即元思维链（Meta-CoT），该框架通过显式建模达到特定思维链所需的底层推理，扩展了传统的思维链（CoT）。我们展示了来自最先进模型的实证证据，这些模型表现出与上下文搜索一致的行为，并探索了通过过程监督、合成数据生成和搜索算法生成Meta-CoT的方法。最后，我们概述了一个具体的训练模型以生成Meta-CoT的管道，结合了指令调优与线性化搜索轨迹和强化学习后训练。最后，我们讨论了开放的研究问题，包括扩展法则、验证器角色以及发现新颖推理算法的潜力。这项工作为在LLMs中实现Meta-CoT提供了理论和实践路线图，为人工智能中更强大和类人的推理铺平了道路。"
    },
    {
        "paper": {
            "id": "2501.04686",
            "authors": [
                {
                    "_id": "677f62ca407edfda4408dd07",
                    "user": {
                        "_id": "6548956fe49bd8d58e8adf0e",
                        "avatarUrl": "/avatars/24c54b14c253c87d4b7438193f16ce28.svg",
                        "isPro": false,
                        "fullname": "Ruilin",
                        "user": "Antimage01",
                        "type": "user"
                    },
                    "name": "Ruilin Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:31.575Z",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd08",
                    "user": {
                        "_id": "64ae0f825d48838462023c9b",
                        "avatarUrl": "/avatars/d3f348c1428376aea490339e94d4c239.svg",
                        "isPro": false,
                        "fullname": "Zheng Zhuofan",
                        "user": "fun6668",
                        "type": "user"
                    },
                    "name": "Zhuofan Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T11:09:45.251Z",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd09",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd0a",
                    "user": {
                        "_id": "66b1eb17652012ddfb59e41e",
                        "avatarUrl": "/avatars/b1c8ce44e7a7d789a3eb2e29e2ddfebe.svg",
                        "isPro": false,
                        "fullname": "Yiyao Yu",
                        "user": "yiyaoyu",
                        "type": "user"
                    },
                    "name": "Yiyao Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:19:06.927Z",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd0b",
                    "name": "Xinzhe Ni",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd0c",
                    "name": "Zicheng Lin",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd0d",
                    "name": "Jin Zeng",
                    "hidden": false
                },
                {
                    "_id": "677f62ca407edfda4408dd0e",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:02:25.883Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T18:49:41.000Z",
            "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
            "summary": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.",
            "upvotes": 37,
            "discussionId": "677f62cb407edfda4408dd5c"
        },
        "translation": "标题：URSA：多模态数学中的思维链推理理解与验证\n\n摘要：思维链（Chain-of-thought, CoT）推理已广泛应用于大型语言模型（LLMs）的数学推理中。最近，对CoT轨迹引入的衍生过程监督引发了关于在测试时增强扩展能力的讨论，从而提升了这些模型的潜力。然而，在多模态数学推理中，高质量CoT训练数据的稀缺阻碍了现有模型实现高精度的CoT推理，并限制了测试时推理潜力的发挥。在本研究中，我们提出了一种三模块合成策略，该策略整合了CoT蒸馏、轨迹格式重写和格式统一，从而生成了一个高质量的多模态数学CoT推理指令微调数据集MMathCoT-1M。我们在多个多模态数学基准上全面验证了训练后的URSA-7B模型的最先进（SOTA）性能。对于测试时的扩展，我们引入了一种数据合成策略，该策略自动生成过程注释数据集DualMath-1.1M，重点关注解释和逻辑。通过在DualMath-1.1M上进一步训练URSA-7B，我们从CoT推理能力过渡到强大的监督能力。训练后的URSA-RM-7B作为验证器，有效提升了URSA-7B在测试时的性能。URSA-RM-7B还展示了出色的分布外（OOD）验证能力，体现了其泛化能力。模型权重、训练数据和代码将开源。"
    },
    {
        "paper": {
            "id": "2501.04227",
            "authors": [
                {
                    "_id": "677f4d709e9ddbcae5ce5364",
                    "name": "Samuel Schmidgall",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce5365",
                    "user": {
                        "_id": "632181ab2284fb8b5d39fc07",
                        "avatarUrl": "/avatars/467c7982b6b9b25689ebf677926f6ce0.svg",
                        "isPro": false,
                        "fullname": "Yusheng Su",
                        "user": "yushengsu",
                        "type": "user"
                    },
                    "name": "Yusheng Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:22:29.469Z",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce5366",
                    "name": "Ze Wang",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce5367",
                    "name": "Ximeng Sun",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce5368",
                    "user": {
                        "_id": "66a966af504d582c8c65a2ee",
                        "avatarUrl": "/avatars/710df9f4637b9188c8d0d58d488caa9b.svg",
                        "isPro": false,
                        "fullname": "Jialian Wu",
                        "user": "jialianww",
                        "type": "user"
                    },
                    "name": "Jialian Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:01.650Z",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce5369",
                    "user": {
                        "_id": "66f31375a3a9e600302f25d7",
                        "avatarUrl": "/avatars/c049575a5d17eeb8b76a5be5de7a5392.svg",
                        "isPro": false,
                        "fullname": "Xiaodong Yu",
                        "user": "xiaodongyu-amd",
                        "type": "user"
                    },
                    "name": "Xiaodong Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:23:43.262Z",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce536a",
                    "user": {
                        "_id": "6328885bb0910efc277f42a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663600974191-6328885bb0910efc277f42a0.jpeg",
                        "isPro": false,
                        "fullname": "JiangLiu",
                        "user": "JiangLiu",
                        "type": "user"
                    },
                    "name": "Jiang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:23:35.567Z",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce536b",
                    "user": {
                        "_id": "6569f6cb44ce94a70187f407",
                        "avatarUrl": "/avatars/414ecc9739ed23f9f395bd7cfa65055f.svg",
                        "isPro": false,
                        "fullname": "Zicheng Liu",
                        "user": "ZCLiu35",
                        "type": "user"
                    },
                    "name": "Zicheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:23:28.447Z",
                    "hidden": false
                },
                {
                    "_id": "677f4d709e9ddbcae5ce536c",
                    "user": {
                        "_id": "65adc9d086f88a686be41215",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65adc9d086f88a686be41215/xizVHuZPkE0Gu8_ulx0Fm.jpeg",
                        "isPro": false,
                        "fullname": "Emad Barsoum",
                        "user": "ebarsoum",
                        "type": "user"
                    },
                    "name": "Emad Barsoum",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:23:21.569Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T01:58:42.000Z",
            "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
            "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.",
            "upvotes": 36,
            "discussionId": "677f4d739e9ddbcae5ce5412"
        },
        "translation": "标题：智能体实验室：利用LLM智能体作为研究助手\n\n摘要：历史上，科学发现一直是一个漫长且成本高昂的过程，从最初的构想到最终的结果需要投入大量的时间和资源。为了加速科学发现、降低研究成本并提高研究质量，我们引入了智能体实验室，这是一个基于LLM的自主框架，能够完成整个研究过程。该框架接受人类提供的研究构想，并经历三个阶段——文献综述、实验和报告撰写，以产生全面的研究成果，包括代码库和研究报告，同时允许用户在每一阶段提供反馈和指导。我们部署了智能体实验室，并使用了多种最先进的LLM，邀请多位研究人员通过参与调查来评估其质量，提供人类反馈以指导研究过程，然后评估最终论文。我们发现：（1）由o1-preview驱动的智能体实验室产生了最佳的研究成果；（2）与现有方法相比，生成的机器学习代码能够达到最先进的性能；（3）人类参与，在每一阶段提供反馈，显著提高了研究的整体质量；（4）智能体实验室显著降低了研究费用，与之前的自主研究方法相比，实现了84%的降低。我们希望智能体实验室能够使研究人员将更多精力投入到创造性构思上，而不是低级的编码和写作，最终加速科学发现。"
    },
    {
        "paper": {
            "id": "2501.04306",
            "authors": [
                {
                    "_id": "677f72d01178cdb06e266bc2",
                    "user": {
                        "_id": "658d2075a6d1ddb400769196",
                        "avatarUrl": "/avatars/9e6030a9cb488943fc130eb55c2982bb.svg",
                        "isPro": false,
                        "fullname": "Ziming Luo",
                        "user": "Ziming2000",
                        "type": "user"
                    },
                    "name": "Ziming Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:18.730Z",
                    "hidden": false
                },
                {
                    "_id": "677f72d01178cdb06e266bc3",
                    "user": {
                        "_id": "646a11791556443f24b582e9",
                        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                        "isPro": false,
                        "fullname": "Zonglin Yang",
                        "user": "ZonglinY",
                        "type": "user"
                    },
                    "name": "Zonglin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:03.156Z",
                    "hidden": false
                },
                {
                    "_id": "677f72d01178cdb06e266bc4",
                    "user": {
                        "_id": "66cfe76a22493660a2fddf2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WQtmUTIQaqFVryT2YY8zZ.jpeg",
                        "isPro": false,
                        "fullname": "Zexin Xu",
                        "user": "Ason-jay",
                        "type": "user"
                    },
                    "name": "Zexin Xu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-09T06:55:13.005Z",
                    "hidden": false
                },
                {
                    "_id": "677f72d01178cdb06e266bc5",
                    "user": {
                        "_id": "6408ec2ebf3e9a4bb2e0b7e9",
                        "avatarUrl": "/avatars/5a34796d778ac526aae8f3059c5e8c8d.svg",
                        "isPro": false,
                        "fullname": "Wei Yang",
                        "user": "weiyang15",
                        "type": "user"
                    },
                    "name": "Wei Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:27.358Z",
                    "hidden": false
                },
                {
                    "_id": "677f72d01178cdb06e266bc6",
                    "user": {
                        "_id": "66c4154fb83a7e94d588ade3",
                        "avatarUrl": "/avatars/9f939a4045e96529aee7d124447624a3.svg",
                        "isPro": false,
                        "fullname": "Xinya Du",
                        "user": "xinyadu",
                        "type": "user"
                    },
                    "name": "Xinya Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:57:46.573Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T06:44:02.000Z",
            "title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
            "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR",
            "upvotes": 19,
            "discussionId": "677f72d11178cdb06e266c0a"
        },
        "translation": "标题：LLM4SR：大语言模型在科学研究中的应用综述\n\n摘要：近年来，大语言模型（LLMs）的快速发展已经改变了科学研究的格局，为研究周期的各个阶段提供了前所未有的支持。本文首次系统地综述了LLMs如何革新科学研究过程。我们分析了LLMs在研究的四个关键阶段中所扮演的独特角色：假设发现、实验规划与实施、科学写作以及同行评审。我们的综述全面展示了针对特定任务的方法论和评估基准。通过识别当前的挑战并提出未来的研究方向，本综述不仅突出了LLMs的变革潜力，还旨在激励和指导研究人员和实践者利用LLMs推动科学探究。相关资源可在以下仓库获取：https://github.com/du-nlp-lab/LLM4SR"
    },
    {
        "paper": {
            "id": "2501.04575",
            "authors": [
                {
                    "_id": "677f5089a0843d06966ac68e",
                    "user": {
                        "_id": "66dd4feb14f4776a44b071f2",
                        "avatarUrl": "/avatars/8b06fa3019999abb762de2b8d9961e2b.svg",
                        "isPro": false,
                        "fullname": "Yuhang Liu",
                        "user": "CausalLLMs",
                        "type": "user"
                    },
                    "name": "Yuhang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:38.607Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac68f",
                    "user": {
                        "_id": "64245f2c089d5fae56b4549a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                        "isPro": false,
                        "fullname": "Pengxiang Li",
                        "user": "pengxiang",
                        "type": "user"
                    },
                    "name": "Pengxiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:37.729Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac690",
                    "user": {
                        "_id": "63909970937867f0cb400b09",
                        "avatarUrl": "/avatars/a360d5d5a3605e7312b078312eee79b6.svg",
                        "isPro": false,
                        "fullname": "zishu wei",
                        "user": "shu06",
                        "type": "user"
                    },
                    "name": "Zishu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:47.683Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac691",
                    "user": {
                        "_id": "6719f1ad725123d503b5ef3c",
                        "avatarUrl": "/avatars/08e1be1f4afa1b6e1501a15cdb786a47.svg",
                        "isPro": false,
                        "fullname": "Congkai Xie",
                        "user": "congkai",
                        "type": "user"
                    },
                    "name": "Congkai Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:24:53.767Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac692",
                    "user": {
                        "_id": "65897684f8b453e1f57cdb26",
                        "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
                        "isPro": false,
                        "fullname": "huxueyu",
                        "user": "huxueyu",
                        "type": "user"
                    },
                    "name": "Xueyu Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:25:09.450Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac693",
                    "user": {
                        "_id": "65601713baf1f5f902292bb6",
                        "avatarUrl": "/avatars/57c65cff29ffb4f25b943b5baccdc795.svg",
                        "isPro": false,
                        "fullname": "Xinchen Xu",
                        "user": "SHU-Xuxc",
                        "type": "user"
                    },
                    "name": "Xinchen Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:25:15.603Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac694",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac695",
                    "user": {
                        "_id": "650dde4ce14eeb01d42b37a1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
                        "isPro": false,
                        "fullname": "Xiaotian Han",
                        "user": "xiaotianhan",
                        "type": "user"
                    },
                    "name": "Xiaotian Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:57:55.175Z",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac696",
                    "name": "Hongxia Yang",
                    "hidden": false
                },
                {
                    "_id": "677f5089a0843d06966ac697",
                    "name": "Fei Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T15:45:21.000Z",
            "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection",
            "summary": "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce InfiGUIAgent, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. InfiGUIAgent achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\nhttps://github.com/Reallm-Labs/InfiGUIAgent.",
            "upvotes": 15,
            "discussionId": "677f5089a0843d06966ac6e3"
        },
        "translation": "标题：InfiGUIAgent：具备原生推理与反思能力的多模态通用GUI代理\n\n摘要：由多模态大语言模型（MLLMs）驱动的图形用户界面（GUI）代理在计算机和手机等计算设备上的任务自动化方面展现出巨大潜力。然而，现有代理在多步推理和对文本注释的依赖方面面临挑战，限制了其有效性。我们提出了InfiGUIAgent，这是一种基于MLLM的GUI代理，通过两阶段的监督微调管道进行训练。第一阶段增强了基本技能，如GUI理解和基础，而第二阶段则利用合成数据整合了层次推理和期望-反思推理技能，以实现代理的原生推理能力。InfiGUIAgent在多个GUI基准测试中表现出色，突显了原生推理技能在增强自动化任务中GUI交互方面的影响。相关资源可在https://github.com/Reallm-Labs/InfiGUIAgent获取。"
    },
    {
        "paper": {
            "id": "2501.02772",
            "authors": [
                {
                    "_id": "677ccfefb35098e1340b036f",
                    "user": {
                        "_id": "64392c42abdc6ce5351f3ea3",
                        "avatarUrl": "/avatars/aafa7bd7f4eb11278924876e23b524d9.svg",
                        "isPro": false,
                        "fullname": "Haoyu Liu",
                        "user": "noobimp",
                        "type": "user"
                    },
                    "name": "Haoyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-08T21:29:31.230Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0370",
                    "user": {
                        "_id": "632bd2f72d6a805eeb4bc601",
                        "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
                        "isPro": false,
                        "fullname": "HUANG SHAOHAN",
                        "user": "buaahsh",
                        "type": "user"
                    },
                    "name": "Shaohan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:26:59.222Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0371",
                    "user": {
                        "_id": "62f1db70a39cf6f5c63fc5e5",
                        "avatarUrl": "/avatars/ff4a0f4f4ddab4cc87cf3b3abea0f232.svg",
                        "isPro": true,
                        "fullname": "Jianfeng Liu",
                        "user": "docacola",
                        "type": "user"
                    },
                    "name": "Jianfeng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:27:21.174Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0372",
                    "name": "Yuefeng Zhan",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0373",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0374",
                    "user": {
                        "_id": "642da49dc1adada5b345c72b",
                        "avatarUrl": "/avatars/5fa8d345a7248246454444999756105b.svg",
                        "isPro": false,
                        "fullname": "deng",
                        "user": "weiweideng",
                        "type": "user"
                    },
                    "name": "Weiwei Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:27:50.251Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0375",
                    "user": {
                        "_id": "64b88a7449bde5d9481e4755",
                        "avatarUrl": "/avatars/45a963517ec2c54c81ee80449d655014.svg",
                        "isPro": false,
                        "fullname": "feng sun",
                        "user": "forserty",
                        "type": "user"
                    },
                    "name": "Feng Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:27:55.984Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0376",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:01.282Z",
                    "hidden": false
                },
                {
                    "_id": "677ccfefb35098e1340b0377",
                    "name": "Qi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-06T05:29:00.000Z",
            "title": "GeAR: Generation Augmented Retrieval",
            "summary": "Document retrieval techniques form the foundation for the development of\nlarge-scale information systems. The prevailing methodology is to construct a\nbi-encoder and compute the semantic similarity. However, such scalar similarity\nis difficult to reflect enough information and impedes our comprehension of the\nretrieval results. In addition, this computational process mainly emphasizes\nthe global semantics and ignores the fine-grained semantic relationship between\nthe query and the complex text in the document. In this paper, we propose a new\nmethod called Generation Augmented Retrieval\n(GeAR) that incorporates well-designed fusion and decoding modules.\nThis enables GeAR to generate the relevant text from documents based on the\nfused representation of the query and the document, thus learning to \"focus on\"\nthe fine-grained information. Also when used as a retriever, GeAR does not add\nany computational burden over bi-encoders. To support the training of the new\nframework, we have introduced a pipeline to efficiently synthesize high-quality\ndata by utilizing large language models. GeAR exhibits competitive retrieval\nand localization performance across diverse scenarios and datasets. Moreover,\nthe qualitative analysis and the results generated by GeAR provide novel\ninsights into the interpretation of retrieval results. The code, data, and\nmodels will be released after completing technical review to facilitate future\nresearch.",
            "upvotes": 11,
            "discussionId": "677ccff2b35098e1340b0403"
        },
        "translation": "标题：GeAR：生成增强检索\n\n摘要：文档检索技术是大型信息系统发展的基础。当前的主流方法是构建双编码器并计算语义相似度。然而，这种标量相似度难以反映足够的信息，阻碍了我们对检索结果的理解。此外，这一计算过程主要强调全局语义，而忽略了查询与文档中复杂文本之间的细粒度语义关系。本文提出了一种称为生成增强检索（GeAR）的新方法，该方法结合了精心设计的融合和解码模块。这使得GeAR能够基于查询和文档的融合表示生成相关文本，从而学会“关注”细粒度信息。同时，当用作检索器时，GeAR不会增加双编码器的计算负担。为了支持新框架的训练，我们引入了一种利用大型语言模型高效合成高质量数据的流程。GeAR在多种场景和数据集中展示了具有竞争力的检索和定位性能。此外，GeAR的定性分析和生成结果为检索结果的解释提供了新的见解。代码、数据和模型将在完成技术审查后发布，以促进未来的研究。"
    },
    {
        "paper": {
            "id": "2501.04689",
            "authors": [
                {
                    "_id": "677f73632fcceb4c315524de",
                    "name": "Zixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "677f73632fcceb4c315524df",
                    "user": {
                        "_id": "64b7f06fda8017900e893eb4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7f06fda8017900e893eb4/3VcQFjERXyAjHQFLMFEJt.jpeg",
                        "isPro": false,
                        "fullname": "Mark Boss",
                        "user": "mboss",
                        "type": "user"
                    },
                    "name": "Mark Boss",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:00.640Z",
                    "hidden": false
                },
                {
                    "_id": "677f73632fcceb4c315524e0",
                    "name": "Aaryaman Vasishta",
                    "hidden": false
                },
                {
                    "_id": "677f73632fcceb4c315524e1",
                    "name": "James M. Rehg",
                    "hidden": false
                },
                {
                    "_id": "677f73632fcceb4c315524e2",
                    "user": {
                        "_id": "630e7cfe3fc17ffc50f6e835",
                        "avatarUrl": "/avatars/0b742ff094a09f9374fafcd97ab9e002.svg",
                        "isPro": false,
                        "fullname": "Varun Jampani",
                        "user": "varunjampani",
                        "type": "user"
                    },
                    "name": "Varun Jampani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:30:32.287Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T18:52:03.000Z",
            "title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single\n  Images",
            "summary": "We study the problem of single-image 3D object reconstruction. Recent works\nhave diverged into two directions: regression-based modeling and generative\nmodeling. Regression methods efficiently infer visible surfaces, but struggle\nwith occluded regions. Generative methods handle uncertain regions better by\nmodeling distributions, but are computationally expensive and the generation is\noften misaligned with visible surfaces. In this paper, we present SPAR3D, a\nnovel two-stage approach aiming to take the best of both directions. The first\nstage of SPAR3D generates sparse 3D point clouds using a lightweight point\ndiffusion model, which has a fast sampling speed. The second stage uses both\nthe sampled point cloud and the input image to create highly detailed meshes.\nOur two-stage design enables probabilistic modeling of the ill-posed\nsingle-image 3D task while maintaining high computational efficiency and great\noutput fidelity. Using point clouds as an intermediate representation further\nallows for interactive user edits. Evaluated on diverse datasets, SPAR3D\ndemonstrates superior performance over previous state-of-the-art methods, at an\ninference speed of 0.7 seconds. Project page with code and model:\nhttps://spar3d.github.io",
            "upvotes": 10,
            "discussionId": "677f73642fcceb4c31552573"
        },
        "translation": "标题：SPAR3D：基于单张图像的稳定点感知三维物体重建\n\n摘要：我们研究了单张图像的三维物体重建问题。最近的研究分为两个方向：基于回归的建模和生成建模。回归方法能够有效地推断可见表面，但在处理遮挡区域时存在困难。生成方法通过建模分布更好地处理不确定区域，但计算成本高且生成结果通常与可见表面对齐不佳。本文提出了一种新颖的两阶段方法SPAR3D，旨在结合这两个方向的优点。SPAR3D的第一阶段使用轻量级点扩散模型生成稀疏的三维点云，具有快速的采样速度。第二阶段利用采样的点云和输入图像生成高度细节化的网格。我们的两阶段设计能够在保持高计算效率和输出保真度的同时，对不适定的单张图像三维任务进行概率建模。使用点云作为中间表示进一步允许用户进行交互式编辑。在多个数据集上的评估表明，SPAR3D在0.7秒的推理速度下，表现出优于现有最先进方法的性能。项目页面包含代码和模型：https://spar3d.github.io"
    },
    {
        "paper": {
            "id": "2501.04144",
            "authors": [
                {
                    "_id": "677f9188fb22a3b146287a6c",
                    "user": {
                        "_id": "646f9a9d5b8225e31ae5a408",
                        "avatarUrl": "/avatars/b6210993b51bb650ea8aa8c04e5fc351.svg",
                        "isPro": false,
                        "fullname": "kamwoh",
                        "user": "kamwoh",
                        "type": "user"
                    },
                    "name": "Kam Woh Ng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:28.347Z",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a6d",
                    "user": {
                        "_id": "64244f3b089d5fae56b3d487",
                        "avatarUrl": "/avatars/d23beaacff2eb1dda84f684a99e61b4f.svg",
                        "isPro": false,
                        "fullname": "George Young",
                        "user": "jingyang",
                        "type": "user"
                    },
                    "name": "Jing Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:37.012Z",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a6e",
                    "user": {
                        "_id": "668c0d460eeb9552cbd45989",
                        "avatarUrl": "/avatars/a69dd6959c0d2df7c08c321d2a4b4e98.svg",
                        "isPro": false,
                        "fullname": "Jia Wei Sii",
                        "user": "siijiawei",
                        "type": "user"
                    },
                    "name": "Jia Wei Sii",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:43.193Z",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a6f",
                    "user": {
                        "_id": "62cc7a38376917c0223dd24b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1657566065867-noauth.png",
                        "isPro": false,
                        "fullname": "JiankangDeng",
                        "user": "JiankangDeng",
                        "type": "user"
                    },
                    "name": "Jiankang Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:49.888Z",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a70",
                    "user": {
                        "_id": "65785c327ee8616c7c179e64",
                        "avatarUrl": "/avatars/eb0a788aa05c5071e73885005f9f59c6.svg",
                        "isPro": false,
                        "fullname": "Chee Seng Chan",
                        "user": "cschan1313",
                        "type": "user"
                    },
                    "name": "Chee Seng Chan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:28:55.545Z",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a71",
                    "name": "Yi-Zhe Song",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a72",
                    "name": "Tao Xiang",
                    "hidden": false
                },
                {
                    "_id": "677f9188fb22a3b146287a73",
                    "user": {
                        "_id": "647ef9e7aa8c04bbf9362a8a",
                        "avatarUrl": "/avatars/dc23e15acf7ee7f3b9f1a68c2716a66d.svg",
                        "isPro": false,
                        "fullname": "Xiatian Zhu",
                        "user": "Xiatian-Zhu",
                        "type": "user"
                    },
                    "name": "Xiatian Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:29:33.382Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T21:14:11.000Z",
            "title": "Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation",
            "summary": "In this paper, we push the boundaries of fine-grained 3D generation into\ntruly creative territory. Current methods either lack intricate details or\nsimply mimic existing objects -- we enable both. By lifting 2D fine-grained\nunderstanding into 3D through multi-view diffusion and modeling part latents as\ncontinuous distributions, we unlock the ability to generate entirely new, yet\nplausible parts through interpolation and sampling. A self-supervised feature\nconsistency loss further ensures stable generation of these unseen parts. The\nresult is the first system capable of creating novel 3D objects with\nspecies-specific details that transcend existing examples. While we demonstrate\nour approach on birds, the underlying framework extends beyond things that can\nchirp! Code will be released at https://github.com/kamwoh/chirpy3d.",
            "upvotes": 9,
            "discussionId": "677f918cfb22a3b146287b7a"
        },
        "translation": "标题：Chirpy3D：基于连续部件潜变量的创造性3D鸟类生成\n\n摘要：在本文中，我们将细粒度3D生成的边界推向了真正的创造性领域。现有方法要么缺乏精细的细节，要么只是简单地模仿现有物体——而我们实现了这两者。通过多视角扩散将2D细粒度理解提升到3D，并将部件潜变量建模为连续分布，我们解锁了通过插值和采样生成全新但合理的部件的能力。自监督特征一致性损失进一步确保了这些未见部件的稳定生成。其结果是第一个能够创建具有物种特异性细节的新颖3D物体的系统，这些细节超越了现有示例。虽然我们在鸟类上展示了我们的方法，但底层框架可以扩展到不仅仅是能够鸣叫的物体！代码将在https://github.com/kamwoh/chirpy3d发布。"
    },
    {
        "paper": {
            "id": "2501.03271",
            "authors": [
                {
                    "_id": "677f70b49fd1b42991dbf051",
                    "user": {
                        "_id": "637537dc08eebfdd0a36e561",
                        "avatarUrl": "/avatars/70f800e80bac457649b1a4deaf6291ba.svg",
                        "isPro": false,
                        "fullname": "Amitava Das",
                        "user": "dramitavadas",
                        "type": "user"
                    },
                    "name": "Amitava Das",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:32:21.610Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf052",
                    "name": "Suranjana Trivedy",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf053",
                    "user": {
                        "_id": "64731b1371f07ae738cffed6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64731b1371f07ae738cffed6/BoEiEgNrNZFXZPG5guWsu.jpeg",
                        "isPro": false,
                        "fullname": "Danush Khanna",
                        "user": "danushkhanna",
                        "type": "user"
                    },
                    "name": "Danush Khanna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:32:34.248Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf054",
                    "user": {
                        "_id": "65f84a28e67fd04a14b4bd2d",
                        "avatarUrl": "/avatars/951b43457820a4486d7e09df357718fd.svg",
                        "isPro": false,
                        "fullname": "Rajarshi Roy",
                        "user": "dextrr",
                        "type": "user"
                    },
                    "name": "Rajarshi Roy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:32:40.821Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf055",
                    "name": "Gurpreet Singh",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf056",
                    "user": {
                        "_id": "638f1a6cc4444c6ca86fc4b4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1a6cc4444c6ca86fc4b4/t_2zcBUdcyNvS9KlcybPx.jpeg",
                        "isPro": false,
                        "fullname": "Basab Ghosh",
                        "user": "basab1142",
                        "type": "user"
                    },
                    "name": "Basab Ghosh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:57:48.729Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf057",
                    "name": "Yaswanth Narsupalli",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf058",
                    "user": {
                        "_id": "6517bfecce732bf33a29d04b",
                        "avatarUrl": "/avatars/b6534a540fa10199df8f9acc497083d5.svg",
                        "isPro": false,
                        "fullname": "Vinija Jain",
                        "user": "Vinija",
                        "type": "user"
                    },
                    "name": "Vinija Jain",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:33:06.982Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf059",
                    "user": {
                        "_id": "65492730a3d1682f79f3ab7a",
                        "avatarUrl": "/avatars/fc62605bdf10947dab393305c51ace96.svg",
                        "isPro": false,
                        "fullname": "Vasu Sharma",
                        "user": "vasusharma55",
                        "type": "user"
                    },
                    "name": "Vasu Sharma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:33:14.847Z",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf05a",
                    "name": "Aishwarya Naresh Reganti",
                    "hidden": false
                },
                {
                    "_id": "677f70b49fd1b42991dbf05b",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:06.459Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-05T00:08:52.000Z",
            "title": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich\n  Paradigm for Direct Preference Optimization",
            "summary": "The rapid rise of large language models (LLMs) has unlocked many applications\nbut also underscores the challenge of aligning them with diverse values and\npreferences. Direct Preference Optimization (DPO) is central to alignment but\nconstrained by fixed divergences and limited feature transformations. We\npropose DPO-Kernels, which integrates kernel methods to address these issues\nthrough four key contributions: (i) Kernelized Representations with polynomial,\nRBF, Mahalanobis, and spectral kernels for richer transformations, plus a\nhybrid loss combining embedding-based and probability-based objectives; (ii)\nDivergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya,\nWasserstein, and f-divergences) for greater stability; (iii) Data-Driven\nSelection metrics that automatically choose the best kernel-divergence pair;\nand (iv) a Hierarchical Mixture of Kernels for both local precision and global\nmodeling. Evaluations on 12 datasets demonstrate state-of-the-art performance\nin factuality, safety, reasoning, and instruction following. Grounded in\nHeavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization\nfor LLMs, offering a comprehensive resource for further alignment research.",
            "upvotes": 5,
            "discussionId": "677f70b89fd1b42991dbf199"
        },
        "translation": "标题：DPO核：一种语义感知、核增强和发散丰富的直接偏好优化范式\n\n摘要：大型语言模型（LLMs）的迅速崛起解锁了许多应用，但也凸显了将其与多样化的价值观和偏好对齐的挑战。直接偏好优化（DPO）是对齐的核心，但受到固定发散和有限特征转换的限制。我们提出了DPO-Kernels，它通过整合核方法来解决这些问题，具体包括四个关键贡献：（i）核化表示，使用多项式、RBF、马氏距离和谱核进行更丰富的转换，并结合基于嵌入和基于概率目标的混合损失；（ii）发散替代方案（Jensen-Shannon、Hellinger、Renyi、Bhattacharyya、Wasserstein和f-发散）以提高稳定性；（iii）数据驱动选择指标，自动选择最佳的核-发散对；（iv）核的层次混合，用于局部精度和全局建模。在12个数据集上的评估表明，在事实性、安全性、推理和指令遵循方面达到了最先进的性能。基于重尾自正则化，DPO-Kernels保持了LLMs的强大泛化能力，为进一步的对齐研究提供了全面的资源。"
    },
    {
        "paper": {
            "id": "2501.04694",
            "authors": [
                {
                    "_id": "677f68d5b8f3572fc35d6046",
                    "name": "Yaoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6047",
                    "user": {
                        "_id": "650be23ec4e52db6a4db63ef",
                        "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg",
                        "isPro": false,
                        "fullname": "Haoling Li",
                        "user": "Ringo1110",
                        "type": "user"
                    },
                    "name": "Haoling Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T10:06:15.412Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6048",
                    "user": {
                        "_id": "641a9a4b05290a135041a3ed",
                        "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
                        "isPro": false,
                        "fullname": "Pluto",
                        "user": "CharonBony",
                        "type": "user"
                    },
                    "name": "Xin Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-09T06:12:38.212Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6049",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604a",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-09T19:57:52.589Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604b",
                    "user": {
                        "_id": "6327cafc46ffcfc27cfa4c9a",
                        "avatarUrl": "/avatars/4daea27b432ff428c41d590669f7330c.svg",
                        "isPro": false,
                        "fullname": "Wenxiang Hu",
                        "user": "wenxcs",
                        "type": "user"
                    },
                    "name": "Wenxiang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:31:18.292Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604c",
                    "name": "Zhongxin Guo",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604d",
                    "user": {
                        "_id": "64c66647725ffa04b2fd6c94",
                        "avatarUrl": "/avatars/620f63f27fa1e90423b0dc22aa8e5809.svg",
                        "isPro": false,
                        "fullname": "yangyu huang",
                        "user": "yangyu90",
                        "type": "user"
                    },
                    "name": "Yangyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:31:33.079Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604e",
                    "name": "Ying Xin",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d604f",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:31:49.764Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6050",
                    "name": "Jinsong Su",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6051",
                    "user": {
                        "_id": "6358c7db856b319a29bdb26b",
                        "avatarUrl": "/avatars/c806e76b6342c1b788bd1dc337f086b8.svg",
                        "isPro": false,
                        "fullname": "QiChen",
                        "user": "QiChen00",
                        "type": "user"
                    },
                    "name": "Qi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:32:03.897Z",
                    "hidden": false
                },
                {
                    "_id": "677f68d5b8f3572fc35d6052",
                    "user": {
                        "_id": "67366efb049bfa3a9084e8d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Bny5bUc6qFQ_RWpEKpRAW.jpeg",
                        "isPro": false,
                        "fullname": "Scarlett Li",
                        "user": "lisijia0504",
                        "type": "user"
                    },
                    "name": "Scarlett Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-09T20:32:10.147Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T18:58:15.000Z",
            "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
            "summary": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
            "upvotes": 4,
            "discussionId": "677f68d6b8f3572fc35d6091"
        },
        "translation": "标题：EpiCoder：在代码生成中涵盖多样性和复杂性\n\n摘要：有效的指令调优对于优化代码大语言模型（LLMs）至关重要，它能够使模型行为与用户期望保持一致，并提升模型在实际应用中的性能。然而，现有的大多数方法主要关注代码片段，这些片段局限于特定功能和固定结构，限制了合成数据的复杂性和多样性。为了解决这些局限性，我们引入了一种基于特征树的新型合成框架，该框架受到抽象语法树（AST）的启发。与AST仅捕捉代码的语法结构不同，我们的框架能够建模代码元素之间的语义关系，从而生成更加细致和多样化的数据。特征树从原始数据中构建，并通过迭代优化以增加提取特征的数量和多样性。这一过程能够识别代码中更复杂的模式和关系。通过控制深度和广度对子树进行采样，我们的框架能够精确调整生成代码的复杂性，支持从简单的函数级操作到复杂的多文件场景的广泛任务。我们对广泛使用的基础模型进行了微调，创建了EpiCoder系列，在多个基准测试中实现了函数和文件级别的最先进性能。值得注意的是，实证证据表明，我们的方法在合成高度复杂的仓库级代码数据方面显示出显著潜力。通过软件工程原则和LLM-as-a-judge方法对数据复杂性和多样性进行严格评估，进一步分析阐明了该方法的优势。"
    },
    {
        "paper": {
            "id": "2501.04652",
            "authors": [
                {
                    "_id": "677fdb64a907a4645d4ad7e2",
                    "user": {
                        "_id": "607f060442beb4da0f990182",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
                        "isPro": false,
                        "fullname": "Patrice Bechard",
                        "user": "patricebechard",
                        "type": "user"
                    },
                    "name": "Patrice Béchard",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-09T14:24:03.741Z",
                    "hidden": false
                },
                {
                    "_id": "677fdb64a907a4645d4ad7e3",
                    "user": {
                        "_id": "64820d2bd8662b0714a2a3cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64820d2bd8662b0714a2a3cd/AHp4bGT05PNlaIGue5gDw.png",
                        "isPro": false,
                        "fullname": "Orlando Marquez",
                        "user": "marquezo",
                        "type": "user"
                    },
                    "name": "Orlando Marquez Ayala",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-09T18:53:32.679Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T18:05:30.000Z",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "summary": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.",
            "upvotes": 2,
            "discussionId": "677fdb65a907a4645d4ad80b"
        },
        "translation": "标题：多任务检索器微调用于领域特定且高效的RAG\n\n摘要：检索增强生成（RAG）在部署大型语言模型（LLMs）时已变得无处不在，因为它可以解决生成幻觉或过时信息等典型限制。然而，在构建现实世界的RAG应用时，会出现一些实际问题。首先，检索到的信息通常是领域特定的。由于微调LLMs在计算上非常昂贵，因此微调检索器以提高LLM输入中包含的数据质量更为可行。其次，随着更多应用部署在同一现实世界系统中，无法负担部署单独的检索器。此外，这些RAG应用通常检索不同类型的数据。我们的解决方案是在各种领域特定任务上对小型检索器编码器进行指令微调，以便部署一个编码器来服务多种用例，从而实现低成本、可扩展性和速度。我们展示了该编码器如何泛化到领域外设置以及现实世界企业用例中未见过的检索任务。"
    }
]