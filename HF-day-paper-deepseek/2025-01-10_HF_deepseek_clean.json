[
    {
        "paper": {
            "id": "2501.05441",
            "authors": [
                {
                    "_id": "6780827489ff720d2e028eb8",
                    "user": {
                        "_id": "656988c35958c68e3180961e",
                        "avatarUrl": "/avatars/7e7916a9c9d2502774dd7727c1a03049.svg",
                        "isPro": false,
                        "fullname": "Yiwen Huang",
                        "user": "Eva1209",
                        "type": "user"
                    },
                    "name": "Yiwen Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:25:22.968Z",
                    "hidden": false
                },
                {
                    "_id": "6780827489ff720d2e028eb9",
                    "user": {
                        "_id": "620573d0522e40b4a18d8763",
                        "avatarUrl": "/avatars/9353c064ef8ccac84d0397411d38fa90.svg",
                        "isPro": false,
                        "fullname": "Aaron Gokaslan",
                        "user": "Skylion007",
                        "type": "user"
                    },
                    "name": "Aaron Gokaslan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:25:29.960Z",
                    "hidden": false
                },
                {
                    "_id": "6780827489ff720d2e028eba",
                    "user": {
                        "_id": "640ea1c43c82bd463ee80e19",
                        "avatarUrl": "/avatars/3213d2e8a433a207dfb96a35f0f52a92.svg",
                        "isPro": false,
                        "fullname": "Volodymyr Kuleshov",
                        "user": "kuleshov",
                        "type": "user"
                    },
                    "name": "Volodymyr Kuleshov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:25:37.188Z",
                    "hidden": false
                },
                {
                    "_id": "6780827489ff720d2e028ebb",
                    "user": {
                        "_id": "6581b07b4466994ea8491941",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6581b07b4466994ea8491941/dsKpoHwnyTqCaSRMV3evn.png",
                        "isPro": false,
                        "fullname": "James Tompkin",
                        "user": "jamestompkin",
                        "type": "user"
                    },
                    "name": "James Tompkin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:25:45.080Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-09T18:53:06.000Z",
            "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
            "summary": "There is a widely-spread claim that GANs are difficult to train, and GAN\narchitectures in the literature are littered with empirical tricks. We provide\nevidence against this claim and build a modern GAN baseline in a more\nprincipled manner. First, we derive a well-behaved regularized relativistic GAN\nloss that addresses issues of mode dropping and non-convergence that were\npreviously tackled via a bag of ad-hoc tricks. We analyze our loss\nmathematically and prove that it admits local convergence guarantees, unlike\nmost existing relativistic losses. Second, our new loss allows us to discard\nall ad-hoc tricks and replace outdated backbones used in common GANs with\nmodern architectures. Using StyleGAN2 as an example, we present a roadmap of\nsimplification and modernization that results in a new minimalist baseline --\nR3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,\nImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against\nstate-of-the-art GANs and diffusion models.",
            "upvotes": 27,
            "discussionId": "6780827a89ff720d2e029207"
        },
        "translation": "标题：GAN已死；GAN永存！一个现代的GAN基线\n\n摘要：有一种广泛传播的说法认为GANs难以训练，并且文献中的GAN架构充斥着经验技巧。我们提供了反对这一说法的证据，并以更有原则的方式构建了一个现代的GAN基线。首先，我们推导出一个表现良好的正则化相对论GAN损失函数，解决了之前通过一系列临时技巧处理的模式丢失和不收敛问题。我们对该损失函数进行了数学分析，并证明其具有局部收敛保证，这与大多数现有的相对论损失函数不同。其次，我们的新损失函数使我们能够摒弃所有临时技巧，并用现代架构替换常见GANs中使用的过时骨干网络。以StyleGAN2为例，我们提出了一个简化和现代化的路线图，从而产生了一个新的极简基线——R3GAN。尽管方法简单，但我们的方法在FFHQ、ImageNet、CIFAR和Stacked MNIST数据集上超越了StyleGAN2，并与最先进的GANs和扩散模型相比表现优异。"
    },
    {
        "paper": {
            "id": "2501.05453",
            "authors": [
                {
                    "_id": "67809ce9063dd44ffb1de7a7",
                    "user": {
                        "_id": "63895b3a43d8b0797a0d0406",
                        "avatarUrl": "/avatars/04d9952dff70f85d4e481ec80ae818cb.svg",
                        "isPro": false,
                        "fullname": "Jathushan Rajasegaran",
                        "user": "brjathu",
                        "type": "user"
                    },
                    "name": "Jathushan Rajasegaran",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:22:39.142Z",
                    "hidden": false
                },
                {
                    "_id": "67809ce9063dd44ffb1de7a8",
                    "name": "Ilija Radosavovic",
                    "hidden": false
                },
                {
                    "_id": "67809ce9063dd44ffb1de7a9",
                    "user": {
                        "_id": "667f3f8c5595354e745a1eb8",
                        "avatarUrl": "/avatars/7bdf766b61709e318c86795b2f48d424.svg",
                        "isPro": false,
                        "fullname": "Rahul Ravishankar",
                        "user": "rravishankar",
                        "type": "user"
                    },
                    "name": "Rahul Ravishankar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:23:24.331Z",
                    "hidden": false
                },
                {
                    "_id": "67809ce9063dd44ffb1de7aa",
                    "user": {
                        "_id": "62f6942205ca68c0e0ff7b97",
                        "avatarUrl": "/avatars/b0600de531b4c88f4aa7c30c5ceb06e1.svg",
                        "isPro": false,
                        "fullname": "Yossi Gandelsman",
                        "user": "yossig",
                        "type": "user"
                    },
                    "name": "Yossi Gandelsman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:23:33.999Z",
                    "hidden": false
                },
                {
                    "_id": "67809ce9063dd44ffb1de7ab",
                    "name": "Christoph Feichtenhofer",
                    "hidden": false
                },
                {
                    "_id": "67809ce9063dd44ffb1de7ac",
                    "user": {
                        "_id": "65369a95605a07338de78ab0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
                        "isPro": false,
                        "fullname": "Jitendra Malik ",
                        "user": "jitendra1995",
                        "type": "user"
                    },
                    "name": "Jitendra Malik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:24:49.391Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-09T18:59:58.000Z",
            "title": "An Empirical Study of Autoregressive Pre-training from Videos",
            "summary": "We empirically study autoregressive pre-training from videos. To perform our\nstudy, we construct a series of autoregressive video models, called Toto. We\ntreat videos as sequences of visual tokens and train transformer models to\nautoregressively predict future tokens. Our models are pre-trained on a diverse\ndataset of videos and images comprising over 1 trillion visual tokens. We\nexplore different architectural, training, and inference design choices. We\nevaluate the learned visual representations on a range of downstream tasks\nincluding image recognition, video classification, object tracking, and\nrobotics. Our results demonstrate that, despite minimal inductive biases,\nautoregressive pre-training leads to competitive performance across all\nbenchmarks. Finally, we find that scaling our video models results in similar\nscaling curves to those seen in language models, albeit with a different rate.\nMore details at https://brjathu.github.io/toto/",
            "upvotes": 17,
            "discussionId": "67809ced063dd44ffb1de947"
        },
        "translation": "标题：基于视频的自回归预训练实证研究\n\n摘要：本文对基于视频的自回归预训练进行了实证研究。为了开展研究，我们构建了一系列自回归视频模型，称为Toto。我们将视频视为视觉标记的序列，并训练Transformer模型以自回归方式预测未来的标记。我们的模型在一个包含超过1万亿视觉标记的多样化视频和图像数据集上进行了预训练。我们探索了不同的架构、训练和推理设计选择。我们在一系列下游任务上评估了学习到的视觉表示，包括图像识别、视频分类、目标跟踪和机器人技术。我们的结果表明，尽管具有最小的归纳偏差，自回归预训练在所有基准测试中都表现出竞争力。最后，我们发现，扩展我们的视频模型会产生与语言模型相似的扩展曲线，尽管速率不同。更多详情请访问https://brjathu.github.io/toto/"
    },
    {
        "paper": {
            "id": "2501.04003",
            "authors": [
                {
                    "_id": "6780c27b7a8cd9c2c506a074",
                    "user": {
                        "_id": "67097d0c02d531812edad345",
                        "avatarUrl": "/avatars/b3a48c0f7c8e37bcfb0749007cd25608.svg",
                        "isPro": false,
                        "fullname": "Shaoyuan Xie",
                        "user": "shaoyuanxie",
                        "type": "user"
                    },
                    "name": "Shaoyuan Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:34:11.760Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a075",
                    "user": {
                        "_id": "62df78222d89ce551ce0f71d",
                        "avatarUrl": "/avatars/89fba294cff2d2f941d121c1923e4c76.svg",
                        "isPro": false,
                        "fullname": "Lingdong Kong",
                        "user": "ldkong",
                        "type": "user"
                    },
                    "name": "Lingdong Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-10T08:20:30.948Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a076",
                    "user": {
                        "_id": "652965773a416e1f2173443b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                        "isPro": false,
                        "fullname": "Yuhao Dong",
                        "user": "THUdyh",
                        "type": "user"
                    },
                    "name": "Yuhao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:34:17.783Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a077",
                    "user": {
                        "_id": "65bcaa3f88b6228542e9caa3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bcaa3f88b6228542e9caa3/DcaY3Ioic6lKfd8Rk3A1D.jpeg",
                        "isPro": false,
                        "fullname": "Sima",
                        "user": "Chonghao",
                        "type": "user"
                    },
                    "name": "Chonghao Sima",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:34:27.775Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a078",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:34:33.723Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a079",
                    "name": "Qi Alfred Chen",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a07a",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:35:28.051Z",
                    "hidden": false
                },
                {
                    "_id": "6780c27b7a8cd9c2c506a07b",
                    "name": "Liang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T18:59:55.000Z",
            "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the\n  Reliability, Data, and Metric Perspectives",
            "summary": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in\ntheir use for autonomous driving, particularly in generating interpretable\ndriving decisions through natural language. However, the assumption that VLMs\ninherently provide visually grounded, reliable, and interpretable explanations\nfor driving remains largely unexamined. To address this gap, we introduce\nDriveBench, a benchmark dataset designed to evaluate VLM reliability across 17\nsettings (clean, corrupted, and text-only inputs), encompassing 19,200 frames,\n20,498 question-answer pairs, three question types, four mainstream driving\ntasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often\ngenerate plausible responses derived from general knowledge or textual cues\nrather than true visual grounding, especially under degraded or missing visual\ninputs. This behavior, concealed by dataset imbalances and insufficient\nevaluation metrics, poses significant risks in safety-critical scenarios like\nautonomous driving. We further observe that VLMs struggle with multi-modal\nreasoning and display heightened sensitivity to input corruptions, leading to\ninconsistencies in performance. To address these challenges, we propose refined\nevaluation metrics that prioritize robust visual grounding and multi-modal\nunderstanding. Additionally, we highlight the potential of leveraging VLMs'\nawareness of corruptions to enhance their reliability, offering a roadmap for\ndeveloping more trustworthy and interpretable decision-making systems in\nreal-world autonomous driving contexts. The benchmark toolkit is publicly\naccessible.",
            "upvotes": 10,
            "discussionId": "6780c27d7a8cd9c2c506a10d"
        },
        "translation": "标题：视觉语言模型是否已准备好用于自动驾驶？从可靠性、数据和指标角度的实证研究\n\n摘要：视觉语言模型（VLMs）的最新进展引发了人们对其在自动驾驶中应用的兴趣，特别是在通过自然语言生成可解释的驾驶决策方面。然而，关于VLMs是否能够固有地提供视觉基础、可靠且可解释的驾驶解释的假设在很大程度上仍未得到验证。为了填补这一空白，我们引入了DriveBench，这是一个基准数据集，旨在评估VLMs在17种设置（干净、损坏和纯文本输入）下的可靠性，涵盖19,200帧图像、20,498个问答对、三种问题类型、四项主流驾驶任务以及总共12个流行的VLMs。我们的研究结果表明，VLMs通常生成基于一般知识或文本线索的看似合理的响应，而非真正的视觉基础，特别是在视觉输入退化或缺失的情况下。这种行为被数据集不平衡和评估指标不足所掩盖，在自动驾驶等安全关键场景中构成重大风险。我们进一步观察到，VLMs在多模态推理方面存在困难，并且对输入损坏表现出更高的敏感性，导致性能不一致。为了应对这些挑战，我们提出了改进的评估指标，优先考虑稳健的视觉基础和多模态理解。此外，我们强调了利用VLMs对损坏的感知来增强其可靠性的潜力，为在现实世界的自动驾驶环境中开发更可信和可解释的决策系统提供了路线图。基准工具包可公开访问。"
    },
    {
        "paper": {
            "id": "2501.05032",
            "authors": [
                {
                    "_id": "6780bd677a8cd9c2c5053b7d",
                    "user": {
                        "_id": "6468ce47e134d050a58aa89c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6468ce47e134d050a58aa89c/ApFcPlOzgI6Cjr0SYPpk6.png",
                        "isPro": false,
                        "fullname": "Yağız Çalık",
                        "user": "Weyaxi",
                        "type": "user"
                    },
                    "name": "Ethem Yağız Çalık",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-10T06:28:04.751Z",
                    "hidden": false
                },
                {
                    "_id": "6780bd677a8cd9c2c5053b7e",
                    "user": {
                        "_id": "63da3d7ae697e5898cb86854",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Talha Rüzgar Akkuş",
                        "user": "Q-bert",
                        "type": "user"
                    },
                    "name": "Talha Rüzgar Akkuş",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-10T06:28:32.323Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-09T07:44:06.000Z",
            "title": "Enhancing Human-Like Responses in Large Language Models",
            "summary": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.",
            "upvotes": 8,
            "discussionId": "6780bd687a8cd9c2c5053bc1"
        },
        "translation": "标题：增强大型语言模型中的人类化响应能力\n\n摘要：本文探讨了使大型语言模型（LLMs）更加人类化的最新进展。我们重点关注增强人工智能系统中自然语言理解、对话连贯性和情感智能的技术。研究评估了多种方法，包括使用多样化数据集进行微调、融入心理学原理以及设计能更好模仿人类推理模式的模型。我们的研究结果表明，这些增强不仅改善了用户交互体验，还为跨不同领域的AI应用开辟了新的可能性。未来的工作将解决这些人类化属性带来的伦理影响和潜在偏见。"
    },
    {
        "paper": {
            "id": "2501.05122",
            "authors": [
                {
                    "_id": "6780ce57c759e70936ba6da5",
                    "user": {
                        "_id": "613f291fe5a812eff913808e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613f291fe5a812eff913808e/aQUoRv6tllC2j-jg6meUt.jpeg",
                        "isPro": false,
                        "fullname": "Gregor Geigle",
                        "user": "Gregor",
                        "type": "user"
                    },
                    "name": "Gregor Geigle",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T13:35:02.224Z",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6da6",
                    "name": "Florian Schneider",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6da7",
                    "name": "Carolin Holtermann",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6da8",
                    "name": "Chris Biemann",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6da9",
                    "name": "Radu Timofte",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6daa",
                    "name": "Anne Lauscher",
                    "hidden": false
                },
                {
                    "_id": "6780ce57c759e70936ba6dab",
                    "name": "Goran Glavaš",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-09T10:26:14.000Z",
            "title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language\n  Model",
            "summary": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly\non English data, which makes them struggle to understand non-English input and\nfail to generate output in the desired target language. Existing efforts\nmitigate these issues by adding multilingual training data, but do so in a\nlargely ad-hoc manner, lacking insight into how different training mixes tip\nthe scale for different groups of languages. In this work, we present a\ncomprehensive investigation into the training strategies for massively\nmultilingual LVLMs. First, we conduct a series of multi-stage experiments\nspanning 13 downstream vision-language tasks and 43 languages, systematically\nexamining: (1) the number of training languages that can be included without\ndegrading English performance and (2) optimal language distributions of\npre-training as well as (3) instruction-tuning data. Further, we (4)\ninvestigate how to improve multilingual text-in-image understanding, and\nintroduce a new benchmark for the task. Surprisingly, our analysis reveals that\none can (i) include as many as 100 training languages simultaneously (ii) with\nas little as 25-50\\% of non-English data, to greatly improve multilingual\nperformance while retaining strong English performance. We further find that\n(iii) including non-English OCR data in pre-training and instruction-tuning is\nparamount for improving multilingual text-in-image understanding. Finally, we\nput all our findings together and train Centurio, a 100-language LVLM, offering\nstate-of-the-art performance in an evaluation covering 14 tasks and 56\nlanguages.",
            "upvotes": 5,
            "discussionId": "6780ce5ac759e70936ba6e7b"
        },
        "translation": "标题：Centurio：大型视觉语言模型多语言能力的驱动因素研究\n\n摘要：迄今为止，大多数大型视觉语言模型（LVLMs）主要是在英语数据上进行训练的，这使得它们在理解非英语输入和生成目标语言输出方面表现不佳。现有的研究通过增加多语言训练数据来缓解这些问题，但大多采用临时的方式，缺乏对不同训练组合如何影响不同语言群体的深入理解。在本研究中，我们对大规模多语言LVLMs的训练策略进行了全面调查。首先，我们进行了一系列多阶段实验，涵盖13个下游视觉语言任务和43种语言，系统地考察了：（1）在不降低英语性能的情况下可以包含的训练语言数量；（2）预训练数据的最佳语言分布；（3）指令调优数据的最佳语言分布。此外，我们还（4）研究了如何提高多语言图像中文本的理解能力，并为此任务引入了一个新的基准。令人惊讶的是，我们的分析表明，可以（i）同时包含多达100种训练语言，（ii）仅使用25-50%的非英语数据，就能显著提高多语言性能，同时保持强大的英语性能。我们还发现，（iii）在预训练和指令调优中包含非英语OCR数据对于提高多语言图像中文本的理解能力至关重要。最后，我们将所有发现整合在一起，训练了Centurio，一个支持100种语言的LVLM，在涵盖14个任务和56种语言的评估中提供了最先进的性能。"
    },
    {
        "paper": {
            "id": "2501.04377",
            "authors": [
                {
                    "_id": "6780a7fdaf887f31d643cb55",
                    "user": {
                        "_id": "6600cb94e95908a2a60f3abd",
                        "avatarUrl": "/avatars/82cdb1ef4ab4caacfa8d9f4c4f538222.svg",
                        "isPro": false,
                        "fullname": "keyekun",
                        "user": "keyekun",
                        "type": "user"
                    },
                    "name": "Yekun Ke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:26:56.992Z",
                    "hidden": false
                },
                {
                    "_id": "6780a7fdaf887f31d643cb56",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "6780a7fdaf887f31d643cb57",
                    "name": "Yingyu Liang",
                    "hidden": false
                },
                {
                    "_id": "6780a7fdaf887f31d643cb58",
                    "user": {
                        "_id": "6335604ea01bd734f72316b0",
                        "avatarUrl": "/avatars/4c6611dabd492106ffb2e82fd680d983.svg",
                        "isPro": false,
                        "fullname": "Zhizhou Sha",
                        "user": "JamesSand",
                        "type": "user"
                    },
                    "name": "Zhizhou Sha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:26:20.704Z",
                    "hidden": false
                },
                {
                    "_id": "6780a7fdaf887f31d643cb59",
                    "user": {
                        "_id": "64b769d7fa7eabaae5fb7f2f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b769d7fa7eabaae5fb7f2f/AbxVphanApZDfj3cOI1tb.jpeg",
                        "isPro": false,
                        "fullname": "Zhenmei Shi",
                        "user": "Zhenmei",
                        "type": "user"
                    },
                    "name": "Zhenmei Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:26:12.739Z",
                    "hidden": false
                },
                {
                    "_id": "6780a7fdaf887f31d643cb5a",
                    "name": "Zhao Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T09:34:15.000Z",
            "title": "On Computational Limits and Provably Efficient Criteria of Visual\n  Autoregressive Models: A Fine-Grained Complexity Analysis",
            "summary": "Recently, Visual Autoregressive (VAR) Models introduced a\ngroundbreaking advancement in the field of image generation, offering a\nscalable approach through a coarse-to-fine \"next-scale prediction\" paradigm.\nHowever, the state-of-the-art algorithm of VAR models in [Tian,\nJiang, Yuan, Peng and Wang, NeurIPS 2024] takes O(n^4) time, which is\ncomputationally inefficient. In this work, we analyze the computational limits\nand efficiency criteria of VAR Models through a fine-grained\ncomplexity lens. Our key contribution is identifying the conditions under which\nVAR computations can achieve sub-quadratic time complexity.\nSpecifically, we establish a critical threshold for the norm of input matrices\nused in VAR attention mechanisms. Above this threshold, assuming the\nStrong Exponential Time Hypothesis (SETH) from fine-grained\ncomplexity theory, a sub-quartic time algorithm for VAR models is\nimpossible. To substantiate our theoretical findings, we present efficient\nconstructions leveraging low-rank approximations that align with the derived\ncriteria. This work initiates the study of the computational efficiency of the\nVAR model from a theoretical perspective. Our technique will shed\nlight on advancing scalable and efficient image generation in VAR\nframeworks.",
            "upvotes": 5,
            "discussionId": "6780a7feaf887f31d643cb83"
        },
        "translation": "标题：视觉自回归模型的计算极限与可证明高效性标准：细粒度复杂性分析\n\n摘要：最近，视觉自回归（VAR）模型在图像生成领域引入了一项突破性进展，通过从粗到细的“下一尺度预测”范式提供了一种可扩展的方法。然而，[Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024]中提出的VAR模型的最先进算法需要O(n^4)的时间，这在计算上是低效的。在本研究中，我们通过细粒度复杂性视角分析了VAR模型的计算极限和效率标准。我们的关键贡献是确定了在何种条件下VAR计算可以实现次二次时间复杂度。具体而言，我们为VAR注意力机制中使用的输入矩阵的范数建立了一个关键阈值。超过该阈值时，假设细粒度复杂性理论中的强指数时间假设（SETH），VAR模型的次四次时间算法是不可能的。为了证实我们的理论发现，我们提出了利用低秩近似的高效构造，这些构造符合我们推导的标准。本研究从理论角度开启了VAR模型计算效率的研究。我们的技术将为在VAR框架中推进可扩展且高效的图像生成提供新的见解。"
    },
    {
        "paper": {
            "id": "2501.03489",
            "authors": [
                {
                    "_id": "678047e05e9a7b1c28c857be",
                    "user": {
                        "_id": "670ec3f6db1a6bcfe832e0a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
                        "isPro": false,
                        "fullname": "Nandan Kumar Jha",
                        "user": "nandan523",
                        "type": "user"
                    },
                    "name": "Nandan Kumar Jha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T08:36:07.110Z",
                    "hidden": false
                },
                {
                    "_id": "678047e05e9a7b1c28c857bf",
                    "name": "Brandon Reagen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-07T03:17:47.000Z",
            "title": "Entropy-Guided Attention for Private LLMs",
            "summary": "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps://github.com/Nandan91/entropy-guided-attention-llm{entropy-guided-llm}.",
            "upvotes": 5,
            "discussionId": "678047e15e9a7b1c28c8580f"
        },
        "translation": "标题：基于熵引导的注意力机制用于私有大语言模型\n\n摘要：专有语言模型的普及引发了严重的隐私问题，这促使私有推理（PI）技术的进步，其中计算直接在加密数据上进行，而不会泄露用户的敏感信息。尽管PI提供了一个有前景的解决方案，但其实际部署受到显著的通信和延迟开销的阻碍，这些开销主要源于非线性操作。为了解决这一问题，我们引入了一个信息论框架来表征仅解码器语言模型中非线性的作用，为优化适应PI需求的Transformer架构奠定了原则性基础。通过利用香农熵作为定量度量，我们揭示了非线性之前未被探索的双重意义：除了确保训练稳定性外，它们对于保持注意力头多样性至关重要。具体而言，我们发现移除非线性会触发两种关键故障模式：深层中的{\\em熵崩溃}导致训练不稳定，以及浅层中的{\\em熵过载}导致多头注意力（MHA）表示能力的未充分利用。我们提出了一种基于熵引导的注意力机制，并结合一种新颖的熵正则化技术来缓解熵过载。此外，我们探索了适用于PI的层归一化替代方案，以防止熵崩溃并稳定减少非线性的LLM训练。我们的研究弥合了信息论与架构设计之间的差距，确立了熵动力学作为开发高效PI架构的原则性指南。代码和实现可在https://github.com/Nandan91/entropy-guided-attention-llm{entropy-guided-llm}获取。"
    },
    {
        "paper": {
            "id": "2501.05040",
            "authors": [
                {
                    "_id": "6780943c53cd2d16be1e9aae",
                    "user": {
                        "_id": "62d00ff8dd7bdfc5e5c553c6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d00ff8dd7bdfc5e5c553c6/u9Be7K16IS2Hc5OqKctEA.jpeg",
                        "isPro": false,
                        "fullname": "chengxing xie",
                        "user": "yitianlian",
                        "type": "user"
                    },
                    "name": "Chengxing Xie",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-10T03:32:24.510Z",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9aaf",
                    "user": {
                        "_id": "61cd553d3dd34ba1985e075b",
                        "avatarUrl": "/avatars/5c39e4ac0892decc3af8e08109469196.svg",
                        "isPro": false,
                        "fullname": "Bowen Li",
                        "user": "bowenli",
                        "type": "user"
                    },
                    "name": "Bowen Li",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-10T09:31:08.408Z",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9ab0",
                    "user": {
                        "_id": "65570843c4865c852d541688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AEGlyozNi3YGSbdcJLmOL.jpeg",
                        "isPro": false,
                        "fullname": "Chang Gao",
                        "user": "changgy",
                        "type": "user"
                    },
                    "name": "Chang Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T16:10:27.384Z",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9ab1",
                    "name": "He Du",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9ab2",
                    "name": "Wai Lam",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9ab3",
                    "name": "Difan Zou",
                    "hidden": false
                },
                {
                    "_id": "6780943c53cd2d16be1e9ab4",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-09T07:54:24.000Z",
            "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: a code file retrieval module\nand a code editing module. The retrieval module employs BM25 along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthe code editing module utilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110K GitHub issues\nalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on the SWE-Bench Lite and Verified\nbenchmarks, achieving state-of-the-art performance among open-source models\nwith scores of 23.3% and 30.2%, respectively. These outcomes highlight the\nefficacy of our approach. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer.",
            "upvotes": 4,
            "discussionId": "6780944053cd2d16be1e9c10"
        },
        "translation": "标题：SWE-Fixer：训练开源大型语言模型以实现高效GitHub问题解决\n\n摘要：大型语言模型（LLMs）在各种复杂任务中展示了显著的熟练度。LLMs的一个重要应用是解决软件工程挑战，特别是通过修复用户报告的问题来解决GitHub上的实际任务。然而，目前许多方法依赖于专有的LLMs，这限制了可重复性、可访问性和透明度。用于解决软件工程问题的LLMs的关键组件以及如何有效增强其能力仍不明确。为了解决这些挑战，我们引入了SWE-Fixer，一种新颖的开源LLM，旨在有效且高效地解决GitHub问题。SWE-Fixer包含两个基本模块：代码文件检索模块和代码编辑模块。检索模块采用BM25结合轻量级LLM模型实现从粗到细的文件检索。随后，代码编辑模块利用另一个LLM模型为识别出的文件生成补丁。然后，为了弥补公开数据集的不足，我们编制了一个包含110K GitHub问题及其相应补丁的广泛数据集，并分别训练SWE-Fixer的两个模块。我们在SWE-Bench Lite和Verified基准上评估了我们的方法，分别在开源模型中取得了23.3%和30.2%的最先进性能。这些结果突显了我们方法的有效性。我们将在https://github.com/InternLM/SWE-Fixer上公开我们的模型、数据集和代码。"
    },
    {
        "paper": {
            "id": "2501.04828",
            "authors": [
                {
                    "_id": "6780c9488299cfc4289f3e32",
                    "user": {
                        "_id": "676e79527f6800a084e0c579",
                        "avatarUrl": "/avatars/088b9f76d2882985c5b5656a6230c2c1.svg",
                        "isPro": false,
                        "fullname": "Saziye Betül Özates",
                        "user": "sbozates",
                        "type": "user"
                    },
                    "name": "Şaziye Betül Özateş",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T10:58:00.713Z",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e33",
                    "user": {
                        "_id": "663a2ad20301bc198ed03ec0",
                        "avatarUrl": "/avatars/7a8c7b40ac3933085ad2ccdc033a70cc.svg",
                        "isPro": false,
                        "fullname": "Tarık Emre Tıraş",
                        "user": "temretiras",
                        "type": "user"
                    },
                    "name": "Tarık Emre Tıraş",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-10T08:20:23.780Z",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e34",
                    "name": "Ece Elif Adak",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e35",
                    "name": "Berat Doğan",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e36",
                    "user": {
                        "_id": "66816ea081998a6a08eb6606",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66816ea081998a6a08eb6606/MWkJMVLxaQC7eS_ff1WMD.png",
                        "isPro": false,
                        "fullname": "Fatih Burak Karagöz",
                        "user": "fatihburakkaragoz",
                        "type": "user"
                    },
                    "name": "Fatih Burak Karagöz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T11:01:15.324Z",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e37",
                    "user": {
                        "_id": "655b99be973f30a0a579f9f8",
                        "avatarUrl": "/avatars/657b4d6204edda6c1a1a2fa037a88c6e.svg",
                        "isPro": false,
                        "fullname": "Efe Genç",
                        "user": "tcTHEBESTMAN",
                        "type": "user"
                    },
                    "name": "Efe Eren Genç",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-10T11:01:27.195Z",
                    "hidden": false
                },
                {
                    "_id": "6780c9488299cfc4289f3e38",
                    "name": "Esma F. Bilgin Taşdemir",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-08T20:29:00.000Z",
            "title": "Building Foundations for Natural Language Processing of Historical\n  Turkish: Resources and Models",
            "summary": "This paper introduces foundational resources and models for natural language\nprocessing (NLP) of historical Turkish, a domain that has remained\nunderexplored in computational linguistics. We present the first named entity\nrecognition (NER) dataset, HisTR and the first Universal Dependencies treebank,\nOTA-BOUN for a historical form of the Turkish language along with\ntransformer-based models trained using these datasets for named entity\nrecognition, dependency parsing, and part-of-speech tagging tasks.\nAdditionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of\ntransliterated historical Turkish texts that spans a wide range of historical\nperiods. Our experimental results show significant improvements in the\ncomputational analysis of historical Turkish, achieving promising results in\ntasks that require understanding of historical linguistic structures. They also\nhighlight existing challenges, such as domain adaptation and language\nvariations across time periods. All of the presented resources and models are\nmade available at https://huggingface.co/bucolin to serve as a benchmark for\nfuture progress in historical Turkish NLP.",
            "upvotes": 3,
            "discussionId": "6780c9498299cfc4289f3e78"
        },
        "translation": "标题：构建历史土耳其语自然语言处理的基础：资源与模型\n\n摘要：本文介绍了用于历史土耳其语自然语言处理（NLP）的基础资源和模型，这一领域在计算语言学中尚未得到充分探索。我们提出了首个命名实体识别（NER）数据集HisTR，以及首个适用于历史土耳其语的通用依存树库OTA-BOUN，并利用这些数据集训练了基于Transformer的模型，用于命名实体识别、依存句法分析和词性标注任务。此外，我们还引入了奥斯曼文本语料库（OTC），这是一个涵盖广泛历史时期的、经过清理的转写历史土耳其语文本语料库。我们的实验结果表明，在历史土耳其语的计算分析方面取得了显著进展，在需要理解历史语言结构的任务中取得了令人鼓舞的结果。这些结果也突显了现有的挑战，如领域适应和跨时期语言变化。所有提供的资源和模型均在https://huggingface.co/bucolin上公开，作为未来历史土耳其语NLP进展的基准。"
    }
]