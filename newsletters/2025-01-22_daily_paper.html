<h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-01-22 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：13</li>
<li>热门领域：LLM, RL, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. MMVU：专家级多学科视频理解评估</h3>
<p><strong>原文标题：</strong> MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</p>
<p><strong>摘要：</strong>
我们介绍了MMVU，一个全面的专家级多学科基准，用于评估视频理解中的基础模型。MMVU包含3000个由专家标注的问题，涵盖四个核心学科的27个主题：科学、医疗保健、人文与社会科学以及工程学。与之前的基准相比，MMVU具有三个关键进展。首先，它挑战模型应用特定领域的知识并进行专家级推理来分析专业领域的视频，超越了当前视频基准中通常评估的基本视觉感知。其次，每个示例都由人类专家从头开始标注。我们实施了严格的数据质量控制，以确保数据集的高质量。最后，每个示例都丰富了专家标注的推理理由和相关领域知识，便于深入分析。我们对32个前沿多模态基础模型在MMVU上进行了广泛评估。最新的具备System-2能力的模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳。然而，它们仍然无法与人类专家的水平相匹配。通过深入的错误分析和案例研究，我们为未来在专家级、知识密集型的专业领域视频理解方面的进展提供了可行的见解。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12380">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12380">arXiv</a></p>
<hr />
<h3>2. Agent-R：通过迭代自训练训练语言模型代理进行反思</h3>
<p><strong>原文标题：</strong> Agent-R: Training Language Model Agents to Reflect via Iterative
  Self-Training</p>
<p><strong>摘要：</strong>
大型语言模型（LLMs）代理在交互环境中解决复杂任务的作用日益重要。现有工作主要集中在通过从更强的专家进行行为克隆来提升性能，然而这种方法在现实应用中往往表现不佳，主要原因是无法从错误中恢复。然而，步骤级的批评数据难以收集且成本高昂。因此，自动化和动态构建自我批评数据集对于赋予模型智能代理能力至关重要。在本研究中，我们提出了一个迭代自训练框架，Agent-R，使语言代理能够即时反思。与传统方法根据正确性奖励或惩罚行为不同，Agent-R利用蒙特卡洛树搜索（MCTS）构建训练数据，从错误轨迹中恢复正确轨迹。代理反思的一个关键挑战在于需要及时修正，而不是等到整个轨迹结束。为了解决这个问题，我们引入了一种模型引导的批评构建机制：演员模型在失败轨迹中识别出第一个错误步骤（在其当前能力范围内）。从该步骤开始，我们将其与树中具有相同父节点的相邻正确路径拼接。这种策略使模型能够基于其当前策略学习反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代优化。我们的研究结果表明，Agent-R持续提高了模型从错误中恢复的能力，并实现了及时的错误纠正。在三个交互环境中的实验表明，Agent-R有效地使代理能够纠正错误行为，同时避免循环，与基线方法相比实现了更优的性能（+5.59%）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.11425">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.11425">arXiv</a></p>
<hr />
<h3>3. Condor：通过知识驱动的数据合成与精炼增强大语言模型对齐</h3>
<p><strong>原文标题：</strong> Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and
  Refinement</p>
<p><strong>摘要：</strong>
监督微调（SFT）数据的质量在提升大语言模型（LLMs）的对话能力方面起着至关重要的作用。然而，随着LLMs的不断进步，高质量人工标注的SFT数据的可用性已成为一个显著的瓶颈，这导致了对合成训练数据的更大依赖。在本研究中，我们引入了Condor，一个新颖的两阶段合成数据生成框架，该框架结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20K Condor生成的样本上进行微调的基础模型，相较于其他模型，表现出了更优的性能。Condor中的额外精炼阶段进一步实现了不同规模（高达72B）LLMs的迭代自我改进，验证了我们方法的有效性。此外，我们对训练后合成数据扩展的研究揭示了性能改进的巨大未开发潜力，为未来的研究开辟了有前景的途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12273">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12273">arXiv</a></p>
<hr />
<h3>4. Mobile-Agent-E：面向复杂任务的自进化移动助手</h3>
<p><strong>原文标题：</strong> Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</p>
<p><strong>摘要：</strong>
智能手机已成为现代生活中不可或缺的工具，然而在移动设备上处理复杂任务仍然常常令人沮丧。近年来，基于大型多模态模型（LMM）的移动代理在感知和操作移动环境方面取得了显著进展。然而，现有方法仍面临重大局限：它们难以满足现实世界中的人类需求，难以处理需要密集推理和长期规划的任务，并且缺乏从以往经验中学习和改进的机制。为应对这些挑战，我们提出了Mobile-Agent-E，一个能够通过过往经验实现自我进化的分层多代理框架。分层意味着将高层规划与低层操作执行明确分离。该框架包含一个负责将复杂任务分解为子目标并制定总体计划的Manager，以及四个下属代理——Perceptor（感知器）、Operator（操作器）、Action Reflector（动作反射器）和Notetaker（记录器），分别负责细粒度视觉感知、即时操作执行、错误验证和信息聚合。Mobile-Agent-E还引入了一个新颖的自我进化模块，该模块维护了一个包含Tips（技巧）和Shortcuts（快捷方式）的持久长期记忆。Tips是从先前任务中总结出的关于如何有效与环境交互的通用指导和经验教训。Shortcuts是为特定子程序定制的可重用、可执行的原子操作序列。Tips和Shortcuts的引入促进了性能和效率的持续优化。此外，我们还提出了Mobile-Eval-E，这是一个新的基准测试，包含需要长期规划和多应用交互的复杂移动任务。实验结果表明，在三个基础模型骨干上，Mobile-Agent-E相比之前的最先进方法实现了22%的绝对性能提升。项目页面：https://x-plug.github.io/MobileAgent。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.11733">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.11733">arXiv</a></p>
<hr />
<h3>5. UI-TARS：开创性的自动化GUI交互原生代理</h3>
<p><strong>原文标题：</strong> UI-TARS: Pioneering Automated GUI Interaction with Native Agents</p>
<p><strong>摘要：</strong>
本文介绍了UI-TARS，一种原生GUI代理模型，仅以屏幕截图作为输入，并执行类似人类的交互操作（如键盘和鼠标操作）。与依赖高度封装的商业模型（如GPT-4o）以及专家设计的提示和工作流程的主流代理框架不同，UI-TARS是一个端到端模型，其性能优于这些复杂的框架。实验证明了其卓越的性能：UI-TARS在10多个评估感知、基础化和GUI任务执行的GUI代理基准测试中达到了SOTA性能。特别是在OSWorld基准测试中，UI-TARS在50步和15步中分别获得了24.6和22.7的分数，优于Claude（分别为22.0和14.9）。在AndroidWorld中，UI-TARS获得了46.6的分数，超过了GPT-4o（34.5）。UI-TARS融合了几项关键创新：（1）增强的感知：利用大规模的GUI截图数据集进行上下文感知的UI元素理解和精确的标注；（2）统一动作建模：将动作标准化为跨平台的统一空间，并通过大规模动作轨迹实现精确的基础化和交互；（3）系统-2推理：将深思熟虑的推理融入多步决策中，涉及任务分解、反思思维、里程碑识别等多种推理模式；（4）带有反思在线轨迹的迭代训练：通过在数百台虚拟机上自动收集、过滤和反思性地精炼新的交互轨迹，解决了数据瓶颈问题。通过迭代训练和反思调优，UI-TARS不断从错误中学习，并在最少的人工干预下适应不可预见的情况。我们还分析了GUI代理的演进路径，以指导该领域的进一步发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12326">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12326">arXiv</a></p>
<hr />
<h3>6. EMO2：基于末端执行器引导的音频驱动虚拟形象视频生成</h3>
<p><strong>原文标题：</strong> EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</p>
<p><strong>摘要：</strong>
本文提出了一种新颖的音频驱动说话头生成方法，能够同时生成高度丰富的面部表情和手势。与现有方法主要关注生成全身或半身姿态不同，我们研究了伴随语音手势生成的挑战，并发现音频特征与全身手势之间的弱相关性是一个关键限制。为了解决这一问题，我们将任务重新定义为两阶段过程。在第一阶段，我们直接从音频输入生成手部姿态，利用音频信号与手部运动之间的强相关性。在第二阶段，我们采用扩散模型合成视频帧，结合第一阶段生成的手部姿态，以生成逼真的面部表情和身体动作。实验结果表明，所提出的方法在视觉质量和同步精度方面均优于现有最先进的方法，如CyberHost和Vlogger。这项工作为音频驱动的手势生成提供了新的视角，并为创建富有表现力和自然的说话头动画提供了一个稳健的框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.10687">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.10687">arXiv</a></p>
<hr />
<h3>7. 推理语言模型：一个蓝图</h3>
<p><strong>原文标题：</strong> Reasoning Language Models: A Blueprint</p>
<p><strong>摘要：</strong>
推理语言模型（RLMs），也称为大型推理模型（LRMs），如OpenAI的o1和o3、DeepSeek-V3和阿里的QwQ，通过扩展大型语言模型（LLMs）并引入先进的推理机制，重新定义了AI的解决问题的能力。然而，它们的高成本、专有性质以及复杂的架构——独特地结合了强化学习（RL）、搜索启发式和LLMs——带来了可访问性和可扩展性的挑战。为了解决这些问题，我们提出了一个全面的蓝图，基于对所有RLM工作的调查和分析，将RLM组件组织成一个模块化框架。该蓝图包含了多样化的推理结构（链、树、图和嵌套形式）、推理策略（如蒙特卡洛树搜索、束搜索）、RL概念（策略、价值模型等）和监督方案（基于输出和基于过程的监督）。我们还提供了详细的数学公式和算法规范，以简化RLM的实现。通过展示LLaMA-Berry、QwQ、Journey Learning和Graph of Thoughts等方案如何作为特例适应，我们展示了蓝图的多样性和统一潜力。为了说明其实用性，我们引入了x1，一个用于快速RLM原型设计和实验的模块化实现。通过使用x1和文献综述，我们提供了关键见解，如策略和价值模型的多阶段训练，以及熟悉训练分布的重要性。最后，我们概述了RLMs如何与更广泛的LLM生态系统（包括工具和数据库）集成。我们的工作揭示了RLM构建的神秘面纱，普及了先进的推理能力，并促进了创新，旨在通过降低RLM开发和实验的门槛，缩小“富AI”和“穷AI”之间的差距。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.11223">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.11223">arXiv</a></p>
<hr />
<h3>8. GPS作为图像生成的控制信号</h3>
<p><strong>原文标题：</strong> GPS as a Control Signal for Image Generation</p>
<p><strong>摘要：</strong>
我们证明了照片元数据中包含的GPS标签为图像生成提供了有用的控制信号。我们训练了GPS到图像的模型，并将其用于需要精细理解城市内图像变化的任务。特别是，我们训练了一个扩散模型，以生成基于GPS和文本条件的图像。学习到的模型生成的图像捕捉了不同社区、公园和地标的独特外观。我们还通过分数蒸馏采样从2D GPS到图像模型中提取3D模型，使用GPS条件来约束每个视角下重建的外观。我们的评估表明，我们的GPS条件模型成功地学会了根据位置生成变化的图像，并且GPS条件改善了估计的3D结构。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12390">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12390">arXiv</a></p>
<hr />
<h3>9. Hunyuan3D 2.0：面向高分辨率纹理3D资产生成的扩散模型扩展</h3>
<p><strong>原文标题：</strong> Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D
  Assets Generation</p>
<p><strong>摘要：</strong>
我们介绍了Hunyuan3D 2.0，这是一个先进的大规模3D合成系统，用于生成高分辨率的纹理3D资产。该系统包括两个基础组件：一个大规模形状生成模型——Hunyuan3D-DiT，以及一个大规模纹理合成模型——Hunyuan3D-Paint。形状生成模型基于可扩展的基于流的扩散变换器，旨在创建与给定条件图像正确对齐的几何形状，为下游应用奠定坚实基础。纹理合成模型得益于强大的几何和扩散先验，为生成或手工制作的网格生成高分辨率和生动的纹理贴图。此外，我们构建了Hunyuan3D-Studio——一个多功能、用户友好的生产平台，简化了3D资产的重建过程。它允许专业和业余用户高效地操作甚至动画化他们的网格。我们系统地评估了我们的模型，结果表明Hunyuan3D 2.0在几何细节、条件对齐、纹理质量等方面优于之前的最先进模型，包括开源模型和闭源模型。Hunyuan3D 2.0公开发布，旨在填补开源3D社区在大规模基础生成模型方面的空白。我们的模型的代码和预训练权重可在以下网址获取：https://github.com/Tencent/Hunyuan3D-2</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12202">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12202">arXiv</a></p>
<hr />
<h3>10. 交互学习：现实环境中自适应智能体的数据驱动框架</h3>
<p><strong>原文标题：</strong> Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in
  Realistic Environments</p>
<p><strong>摘要：</strong>
由大型语言模型（LLMs）驱动的自主智能体有潜力增强人类能力，协助完成从发送电子邮件到数据分析等数字任务。然而，现有LLMs在这些任务中的能力往往受到缺乏高质量智能体数据的影响，这些数据来自它们所交互的相应环境。我们提出了“交互学习”（Learn-by-interact），这是一个以数据为中心的框架，旨在使LLM智能体适应任何给定环境，而无需人工标注。交互学习基于文档合成智能体与环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程称为反向构建。我们通过在基于训练的场景和无训练的上下文学习（ICL）中使用这些合成数据来评估其质量，其中我们设计了针对智能体优化的创新检索方法。在SWE-bench、WebArena、OSWorld和Spider2-V等现实编码、网络和桌面环境中的广泛实验表明，交互学习在各种下游智能体任务中的有效性——使用Claude-3.5的ICL基线结果提高了12.2%，使用Codestral-22B的训练基线结果提高了19.5%。我们进一步证明了反向构建的关键作用，它为训练提供了高达14.0%的改进。我们的消融研究表明，我们的合成数据在ICL中提供了效率，并且我们的检索管道优于传统的检索增强生成（RAG）等替代方法。我们期望，随着LLMs在现实世界环境中的部署越来越多，交互学习将成为智能体数据合成的基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.10893">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.10893">arXiv</a></p>
<hr />
<h3>11. 细节中的魔鬼：在训练专用专家混合模型中实现负载均衡损失的探讨</h3>
<p><strong>原文标题：</strong> Demons in the Detail: On Implementing Load Balancing Loss for Training
  Specialized Mixture-of-Expert Models</p>
<p><strong>摘要：</strong>
本文重新审视了在训练专家混合模型（Mixture-of-Experts, MoEs）时负载均衡损失（Load-balancing Loss, LBL）的实现。具体而言，MoEs的LBL定义为N_E sum_{i=1}^{N_E} f_i p_i，其中N_E是专家的总数，f_i表示专家i被选择的频率，p_i表示专家i的平均门控分数。现有的MoE训练框架通常采用并行训练策略，使得f_i和LBL在一个微批次内计算，然后在并行组之间进行平均。本质上，用于训练数十亿规模大型语言模型（LLMs）的微批次通常包含非常少的序列。因此，微批次的LBL几乎是在序列级别上，路由器被推动在每个序列内均匀分配令牌。在这种严格的约束下，即使是来自特定领域序列（例如代码）的令牌也会被均匀路由到所有专家，从而抑制了专家的专业化。在本研究中，我们提出使用全局批次来计算LBL，以放松这一约束。因为全局批次包含比微批次更多样化的序列，这将鼓励在语料库级别上的负载均衡。具体而言，我们引入了一个额外的通信步骤来同步微批次之间的f_i，然后使用它来计算LBL。通过在训练基于MoEs的LLMs（总参数高达42.8B，令牌数高达400B）上的实验，我们惊讶地发现，全局批次LBL策略在预训练困惑度和下游任务中都带来了显著的性能提升。我们的分析表明，全局批次LBL还大大提高了MoE专家的领域专业化程度。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.11873">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.11873">arXiv</a></p>
<hr />
<h3>12. 随流而动：使用实时扭曲噪声实现运动可控的视频扩散模型</h3>
<p><strong>原文标题：</strong> Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using
  Real-Time Warped Noise</p>
<p><strong>摘要：</strong>
生成建模的目标是将随机噪声转化为结构化的输出。在本研究中，我们通过允许通过结构化潜在噪声采样来控制运动，从而增强了视频扩散模型。这仅通过数据的变化实现：我们对训练视频进行预处理以生成结构化噪声。因此，我们的方法与扩散模型设计无关，无需改变模型架构或训练流程。具体而言，我们提出了一种新颖的噪声扭曲算法，该算法足够快以实时运行，用从光流场导出的相关扭曲噪声替换随机时间高斯性，同时保留空间高斯性。我们算法的高效性使我们能够以最小的开销使用扭曲噪声微调现代视频扩散基础模型，并为广泛的用户友好运动控制提供一站式解决方案：局部物体运动控制、全局摄像机运动控制和运动转移。我们的扭曲噪声在时间一致性和空间高斯性之间的协调，使得在保持每帧像素质量的同时实现有效的运动控制。大量实验和用户研究证明了我们方法的优势，使其成为控制视频扩散模型中运动的稳健且可扩展的方法。视频结果可在我们的网页上查看：https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow。源代码和模型检查点可在GitHub上获取：https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.08331">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.08331">arXiv</a></p>
<hr />
<h3>13. 视频深度任意：超长视频的一致性深度估计</h3>
<p><strong>原文标题：</strong> Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</p>
<p><strong>摘要：</strong>
深度任意（Depth Anything）在单目深度估计方面取得了显著的成功，具有强大的泛化能力。然而，它在视频中存在时间不一致性问题，阻碍了其实际应用。已有多种方法通过利用视频生成模型或引入光流和相机姿态的先验来缓解这一问题。然而，这些方法仅适用于短视频（&lt; 10秒），并且需要在质量和计算效率之间进行权衡。我们提出了视频深度任意（Video Depth Anything），用于在超长视频（超过几分钟）中进行高质量、一致性深度估计，而无需牺牲效率。我们的模型基于深度任意V2（Depth Anything V2），并将其头部替换为高效的时空头部。我们通过约束时间深度梯度设计了一种简单而有效的时间一致性损失，无需额外的几何先验。该模型在视频深度和未标记图像的联合数据集上进行训练，类似于深度任意V2。此外，还开发了一种基于关键帧的策略用于长视频推理。实验表明，我们的模型可以应用于任意长度的视频，而不会影响质量、一致性或泛化能力。在多个视频基准上的综合评估表明，我们的方法在零样本视频深度估计中达到了新的最先进水平。我们提供了不同规模的模型以支持各种场景，其中最小的模型能够在30 FPS下实现实时性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.12375">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.12375">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-01-22_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>