<h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-01-17 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：10</li>
<li>热门领域：LLM, Transformer, RL, AIGC</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 超越去噪步长扩展的扩散模型推理时扩展</h3>
<p><strong>原文标题：</strong> Inference-Time Scaling for Diffusion Models beyond Scaling Denoising
  Steps</p>
<p><strong>摘要：</strong>
生成模型在各个领域产生了重大影响，这主要归功于它们通过增加数据、计算资源和模型规模在训练过程中扩展的能力，这一现象由扩展定律所表征。最近的研究开始探索大型语言模型（LLMs）在推理时的扩展行为，揭示了在推理过程中通过增加计算量如何进一步提升性能。与LLMs不同，扩散模型天生具备通过调整去噪步数来灵活调整推理时计算的能力，尽管性能提升通常在几十步后趋于平缓。在本研究中，我们探索了扩散模型在增加去噪步数之外的推理时扩展行为，并研究了如何通过增加计算量进一步提升生成性能。具体而言，我们考虑了一个旨在为扩散采样过程识别更好噪声的搜索问题。我们沿着两个轴构建设计空间：用于提供反馈的验证器，以及用于寻找更好噪声候选的算法。通过在类条件和文本条件图像生成基准上的广泛实验，我们的研究结果表明，增加推理时计算量显著提高了扩散模型生成样本的质量，并且由于图像的复杂性，可以特别选择框架中的组件组合以适应不同的应用场景。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09732">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09732">arXiv</a></p>
<hr />
<h3>2. OmniThink：通过思维扩展机器写作的知识边界</h3>
<p><strong>原文标题：</strong> OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking</p>
<p><strong>摘要：</strong>
使用大型语言模型进行机器写作通常依赖于检索增强生成。然而，这些方法仍然局限于模型预定义的范围之内，限制了生成内容丰富的信息。具体而言，传统的检索信息往往缺乏深度和实用性，并且存在冗余问题，这对生成文章的质量产生了负面影响，导致文章内容浅显、重复且缺乏原创性。为了解决这些问题，我们提出了OmniThink，一种模拟人类迭代扩展和反思过程的机器写作框架。OmniThink的核心思想是模拟学习者在逐步加深对主题理解过程中的认知行为。实验结果表明，OmniThink在不损害连贯性和深度等指标的情况下，提高了生成文章的知识密度。人类评估和专家反馈进一步凸显了OmniThink在解决生成长篇文章实际挑战中的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09751">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09751">arXiv</a></p>
<hr />
<h3>3. 探索高级患者模拟器中的问诊-诊断关系</h3>
<p><strong>原文标题：</strong> Exploring the Inquiry-Diagnosis Relationship with Advanced Patient
  Simulators</p>
<p><strong>摘要：</strong>
在线医疗咨询（OMC）限制医生仅通过问诊收集患者信息，这使得原本复杂的诊断顺序决策过程更加具有挑战性。近年来，大型语言模型的快速发展显示出改变OMC的巨大潜力。然而，大多数研究主要集中在相对充足信息条件下提高诊断准确性，而对咨询过程中的“问诊”阶段关注有限。这种关注不足导致“问诊”与“诊断”之间的关系未能得到充分探索。在本文中，我们首先从真实的医患对话中提取患者互动策略，并使用这些策略指导训练一个高度模拟现实行为的患者模拟器。通过将病历输入我们的患者模拟器以模拟患者反应，我们进行了大量实验来探索咨询过程中“问诊”与“诊断”之间的关系。实验结果表明，问诊和诊断遵循李比希定律：无论诊断能力如何，问诊质量差都会限制诊断效果，反之亦然。此外，实验揭示了各种模型在问诊表现上的显著差异。为了探究这一现象，我们将问诊过程分为四种类型：（1）主诉问诊；（2）已知症状的详细说明；（3）伴随症状的问诊；（4）收集家族或病史。我们分析了不同模型在这四种类型中的问诊分布，以探讨其表现显著差异的原因。我们计划在https://github.com/LIO-H-ZEN/PatientSimulator上开源我们的患者模拟器的权重和相关代码。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09484">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09484">arXiv</a></p>
<hr />
<h3>4. 视觉分词器在重建与生成任务中的规模化探索</h3>
<p><strong>原文标题：</strong> Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation</p>
<p><strong>摘要：</strong>
通过自编码实现的视觉分词技术，通过将像素压缩至潜在空间，为当前最先进的图像和视频生成模型提供了强大支持。尽管基于Transformer的生成器的规模化是近期进展的核心，但分词器组件本身却很少被规模化，这留下了关于自编码器设计选择如何影响其重建目标及下游生成性能的开放性问题。我们的工作旨在探索自编码器的规模化，以填补这一空白。为了促进这一探索，我们用增强的视觉Transformer架构（ViTok）替代了典型的卷积骨干网络。我们在远超ImageNet-1K的大规模图像和视频数据集上训练ViTok，消除了分词器规模化的数据限制。我们首先研究了自编码器瓶颈的规模化如何影响重建和生成，发现虽然它与重建高度相关，但其与生成的关系更为复杂。接着，我们探讨了分别规模化自编码器的编码器和解码器对重建和生成性能的影响。关键的是，我们发现规模化编码器对重建或生成的增益微乎其微，而规模化解码器则提升了重建性能，但对生成的好处则参差不齐。基于我们的探索，我们设计了ViTok作为一个轻量级自编码器，在ImageNet-1K和COCO重建任务（256p和512p）上实现了与最先进自编码器相竞争的性能，同时在UCF-101的16帧128p视频重建上超越了现有自编码器，所有这些都使用了2-5倍更少的FLOPs。当与扩散Transformer结合时，ViTok在ImageNet-1K的图像生成上展示了竞争力，并在UCF-101的类条件视频生成上设定了新的最先进基准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09755">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09755">arXiv</a></p>
<hr />
<h3>5. RLHS：通过事后模拟缓解RLHF中的错位问题</h3>
<p><strong>原文标题：</strong> RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</p>
<p><strong>摘要：</strong>
生成式人工智能系统（如基础模型，FMs）必须与人类价值观良好对齐，以确保其行为是有帮助且可信的。尽管基于人类反馈的强化学习（RLHF）在利用人类判断优化模型性能方面显示出潜力，但现有的RLHF流程主要依赖于即时反馈，这可能无法准确反映交互对用户效用的下游影响。我们证明，基于评估者对下游后果的预见性估计的反馈会系统地引发古德哈特定律动态，激励诸如谄媚和欺骗等错位行为，并最终降低用户结果。为了缓解这一问题，我们提出通过将RLHF重新聚焦于事后反馈来解耦评估与预测。我们的理论分析表明，将评估者反馈条件化于下游观察可以缓解错位并提高预期的人类效用，即使这些观察是由AI系统本身模拟的。为了在实际的对齐算法中利用这一见解，我们引入了基于事后模拟的强化学习（RLHS），该方法首先模拟可能的下游后果，然后引出反馈以评估哪些行为在事后看来是真正有益的。我们将RLHS应用于两种广泛使用的在线和离线偏好优化方法——近端策略优化（PPO）和直接偏好优化（DPO）——并通过实验证明，这两种方法中的错位问题均显著减少。通过一项在线人类用户研究，我们表明，尽管RLHS仅使用模拟的事后反馈进行训练，但在帮助用户实现目标和获得更高满意度评分方面，RLHS始终优于RLHF。这些结果强调了关注长期后果（即使是模拟的）对于缓解RLHF中的错位问题的重要性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.08617">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.08617">arXiv</a></p>
<hr />
<h3>6. SynthLight：通过重新渲染合成人脸学习的扩散模型肖像重光照</h3>
<p><strong>原文标题：</strong> SynthLight: Portrait Relighting with Diffusion Model by Learning to
  Re-render Synthetic Faces</p>
<p><strong>摘要：</strong>
我们介绍了SynthLight，一种用于肖像重光照的扩散模型。我们的方法将图像重光照视为重新渲染问题，其中像素会根据环境光照条件的变化进行转换。使用基于物理的渲染引擎，我们合成了一个数据集，以模拟在不同光照条件下使用3D头部资产进行的光照条件转换。我们提出了两种训练和推理策略，以弥合合成图像和真实图像领域之间的差距：（1）利用无光照标签的真实人像进行多任务训练；（2）基于无分类器引导的推理时间扩散采样过程，利用输入肖像更好地保留细节。我们的方法能够推广到各种真实照片，并产生逼真的光照效果，包括镜面高光和投射阴影，同时保留主体的身份。我们在光舞台数据上的定量实验展示了与最先进的重光照方法相当的结果。我们在野外图像上的定性结果展示了丰富且前所未有的光照效果。项目页面：https://vrroom.github.io/synthlight/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09756">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09756">arXiv</a></p>
<hr />
<h3>7. FAST：面向视觉-语言-动作模型的高效动作标记化方法</h3>
<p><strong>原文标题：</strong> FAST: Efficient Action Tokenization for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
自回归序列模型，如基于Transformer的视觉-语言-动作（VLA）策略，在捕捉复杂且可泛化的机器人行为方面非常有效。然而，这类模型要求我们对连续动作信号进行标记化选择，这决定了模型预测的离散符号如何映射到连续的机器人动作。我们发现，当前基于简单的每维度、每时间步分箱方案的机器人动作标记化方法，在处理高频机器人数据以学习灵巧技能时通常表现不佳。为了解决这一挑战，我们提出了一种基于离散余弦变换的压缩式机器人动作标记化方案。我们的标记化方法，即频域动作序列标记化（FAST），使我们能够训练自回归VLA模型，以应对标准离散化方法完全失效的高灵巧性和高频率任务。基于FAST，我们发布了FAST+，一个在100万条真实机器人动作轨迹上训练的通用机器人动作标记器。它可以作为黑箱标记器，适用于各种机器人动作序列，涵盖不同的动作空间和控制频率。最后，我们展示了当与pi0 VLA结合时，我们的方法能够扩展到在1万小时机器人数据上进行训练，并与扩散VLA的性能相匹配，同时将训练时间减少多达5倍。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09747">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09747">arXiv</a></p>
<hr />
<h3>8. 迈向大规模推理模型：基于大语言模型的强化推理研究综述</h3>
<p><strong>原文标题：</strong> Towards Large Reasoning Models: A Survey of Reinforced Reasoning with
  Large Language Models</p>
<p><strong>摘要：</strong>
语言长期以来被视为人类推理的重要工具。大语言模型（LLMs）的突破引发了利用这些模型解决复杂推理任务的广泛研究兴趣。研究者们已超越了简单的自回归标记生成，引入了“思维”概念——即代表推理过程中间步骤的标记序列。这一创新范式使LLMs能够模拟复杂的人类推理过程，如树搜索和反思性思维。最近，一种新兴的“学习推理”趋势应用强化学习（RL）来训练LLMs掌握推理过程。该方法通过试错搜索算法自动生成高质量的推理轨迹，显著扩展了LLMs的推理能力，并提供了更多的训练数据。此外，最近的研究表明，在测试时推理过程中鼓励LLMs使用更多标记进行“思考”可以进一步提高推理准确性。因此，训练时和测试时的扩展相结合，展示了一个新的研究前沿——通向大规模推理模型的路径。OpenAI的o1系列的引入标志着这一研究方向的重要里程碑。在本综述中，我们全面回顾了LLM推理的最新进展。我们首先介绍了LLMs的基础背景，然后探讨了推动大规模推理模型发展的关键技术组件，重点关注自动数据构建、学习推理技术和测试时扩展。我们还分析了构建大规模推理模型的流行开源项目，并总结了开放挑战和未来研究方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09686">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09686">arXiv</a></p>
<hr />
<h3>9. AnyStory：面向文本到图像生成中统一单主体与多主体个性化的研究</h3>
<p><strong>原文标题：</strong> AnyStory: Towards Unified Single and Multiple Subject Personalization in
  Text-to-Image Generation</p>
<p><strong>摘要：</strong>
近年来，大规模生成模型在文本到图像生成方面展现了卓越的能力。然而，生成具有特定主体的高保真个性化图像仍面临挑战，尤其是在涉及多个主体的情况下。本文提出AnyStory，一种统一的主体个性化生成方法。AnyStory不仅实现了单主体的高保真个性化，还实现了多主体的高保真个性化，且不牺牲主体保真度。具体而言，AnyStory以“编码-路由”的方式对主体个性化问题进行建模。在编码步骤中，AnyStory利用一个通用且强大的图像编码器，即ReferenceNet，结合CLIP视觉编码器，实现主体特征的高保真编码。在路由步骤中，AnyStory利用解耦的实例感知主体路由器，准确感知并预测潜在空间中对应主体的可能位置，并指导主体条件的注入。详细的实验结果展示了我们的方法在保留主体细节、对齐文本描述以及多主体个性化方面的优异性能。项目页面位于https://aigcdesigngroup.github.io/AnyStory/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09503">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09503">arXiv</a></p>
<hr />
<h3>10. CaPa：用于高效4K纹理网格生成的雕刻与绘制合成方法</h3>
<p><strong>原文标题：</strong> CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation</p>
<p><strong>摘要：</strong>
从文本或视觉输入合成高质量的3D资产已成为现代生成建模的核心目标。尽管3D生成算法层出不穷，但它们经常面临多视角不一致、生成速度慢、保真度低以及表面重建问题等挑战。虽然一些研究已经解决了部分问题，但全面的解决方案仍然难以实现。本文介绍了CaPa，一种雕刻与绘制框架，能够高效生成高保真度的3D资产。CaPa采用两阶段过程，将几何生成与纹理合成解耦。首先，一个3D潜在扩散模型在多视角输入的指导下生成几何，确保跨视角的结构一致性。随后，利用一种新颖的、模型无关的空间解耦注意力机制，该框架为给定几何合成了高分辨率纹理（最高可达4K）。此外，我们提出了一种3D感知的遮挡修复算法，用于填充未纹理化的区域，从而在整个模型中实现一致的结果。该流程在不到30秒的时间内生成高质量的3D资产，为商业应用提供即用型输出。实验结果表明，CaPa在纹理保真度和几何稳定性方面均表现出色，为实用、可扩展的3D资产生成树立了新标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.09433">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.09433">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="audio/2025-01-17_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>