<h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-01-08 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：RL, LLM, GPT, Diffusion, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. REINFORCE++：一种简单高效的大规模语言模型对齐方法</h3>
<p><strong>原文标题：</strong> REINFORCE++: A Simple and Efficient Approach for Aligning Large Language
  Models</p>
<p><strong>摘要：</strong>
基于人类反馈的强化学习（RLHF）已成为将大规模语言模型与人类偏好对齐的关键方法，其算法通过近端策略优化（PPO）、直接偏好优化（DPO）、REINFORCE留一法（RLOO）、ReMax和组相对策略优化（GRPO）等方法迅速演进。我们提出了REINFORCE++，这是经典REINFORCE算法的增强版本，它结合了PPO的关键优化技术，同时消除了对评论者网络的需求。REINFORCE++实现了三个主要目标：（1）简化，（2）增强训练稳定性，以及（3）减少计算开销。通过广泛的实证评估，我们证明REINFORCE++在稳定性上优于GRPO，并在计算效率上超过PPO，同时保持可比的性能。实现代码可在https://github.com/OpenRLHF/OpenRLHF获取。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>2. 面向物理人工智能的Cosmos世界基础模型平台</h3>
<p><strong>原文标题：</strong> Cosmos World Foundation Model Platform for Physical AI</p>
<p><strong>摘要：</strong>
物理人工智能首先需要进行数字化训练。它需要一个自身的数字孪生体，即策略模型，以及一个世界的数字孪生体，即世界模型。在本文中，我们介绍了Cosmos世界基础模型平台，旨在帮助开发者为他们的物理人工智能系统构建定制化的世界模型。我们将世界基础模型定位为一种通用世界模型，可以通过微调转化为适用于下游应用的定制化世界模型。我们的平台涵盖了视频筛选流程、预训练的世界基础模型、预训练世界基础模型的训练后示例以及视频标记器。为了帮助物理人工智能开发者解决社会中最关键的问题，我们将平台开源，并通过https://github.com/NVIDIA/Cosmos提供开放权重的模型和宽松的许可。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>3. MotionBench：面向视觉语言模型的细粒度视频运动理解基准测试与改进</h3>
<p><strong>原文标题：</strong> MotionBench: Benchmarking and Improving Fine-grained Video Motion
  Understanding for Vision Language Models</p>
<p><strong>摘要：</strong>
近年来，视觉语言模型（VLMs）在视频理解领域取得了显著进展。然而，当前基准测试中一个关键能力——细粒度运动理解——仍未得到充分探索。为填补这一空白，我们提出了MotionBench，这是一个旨在评估视频理解模型细粒度运动理解能力的综合评估基准。MotionBench通过六种主要类别的运动导向问题类型评估模型的运动级感知能力，并包含从多种来源收集的数据，确保广泛代表现实世界视频内容。实验结果表明，现有VLMs在理解细粒度运动方面表现不佳。为了增强VLM在有限LLM序列长度内感知细粒度运动的能力，我们进行了广泛的实验，回顾了针对视频特征压缩优化的VLM架构，并提出了一种新颖且高效的Through-Encoder（TE）融合方法。实验表明，更高的帧率输入和TE融合在运动理解方面带来了改进，但仍存在很大的提升空间。我们的基准测试旨在指导和激励开发更强大的视频理解模型，强调细粒度运动理解的重要性。项目页面：https://motion-bench.github.io。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>4. LLaVA-Mini：基于单一视觉标记的高效图像与视频大型多模态模型</h3>
<p><strong>原文标题：</strong> LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One
  Vision Token</p>
<p><strong>摘要：</strong>
随着GPT-4o等实时大型多模态模型（LMMs）的出现，高效LMMs引起了广泛关注。LMM框架通常将视觉输入编码为视觉标记（连续表示），并将其与文本指令整合到大型语言模型（LLMs）的上下文中，其中大规模参数和大量上下文标记（主要是视觉标记）导致了巨大的计算开销。以往关于高效LMMs的研究主要集中在用较小模型替换LLM骨干，而忽视了标记数量的关键问题。本文提出了LLaVA-Mini，一种具有最少视觉标记的高效LMM。为了在保留视觉信息的同时实现视觉标记的高压缩比，我们首先分析了LMMs如何理解视觉标记，发现大多数视觉标记仅在LLM骨干的早期层中起关键作用，主要将视觉信息融合到文本标记中。基于这一发现，LLaVA-Mini引入了模态预融合，提前将视觉信息融合到文本标记中，从而促进输入到LLM骨干的视觉标记压缩为单一标记。LLaVA-Mini是一种统一的大型多模态模型，能够高效支持图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中，实验表明LLaVA-Mini仅使用1个视觉标记（而非576个）即可超越LLaVA-v1.5。效率分析显示，LLaVA-Mini能够减少77%的浮点运算次数，在40毫秒内提供低延迟响应，并在24GB内存的GPU硬件上处理超过10,000帧的视频。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>5. Sa2VA：将SAM2与LLaVA结合以实现图像和视频的密集基础理解</h3>
<p><strong>原文标题：</strong> Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of
  Images and Videos</p>
<p><strong>摘要：</strong>
本研究提出了Sa2VA，这是首个能够对图像和视频进行密集基础理解的统一模型。与现有的多模态大型语言模型不同，这些模型通常局限于特定的模态和任务，而Sa2VA支持广泛的图像和视频任务，包括参考分割和对话，只需进行最小的一次性指令调优。Sa2VA将SAM-2（一个基础视频分割模型）与LLaVA（一个先进的视觉语言模型）相结合，并将文本、图像和视频统一到一个共享的LLM令牌空间中。通过使用LLM，Sa2VA生成指令令牌，指导SAM-2生成精确的掩码，从而实现对静态和动态视觉内容的基础多模态理解。此外，我们引入了Ref-SAV，这是一个自动标注的数据集，包含超过72k个复杂视频场景中的对象表达，旨在提升模型性能。我们还手动验证了Ref-SAV数据集中的2k个视频对象，以在复杂环境中进行参考视频对象分割的基准测试。实验表明，Sa2VA在多个任务中达到了最先进的水平，特别是在参考视频对象分割方面，突显了其在复杂现实世界应用中的潜力。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>6. 扩散作为着色器：面向多功能视频生成控制的3D感知视频扩散模型</h3>
<p><strong>原文标题：</strong> Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video
  Generation Control</p>
<p><strong>摘要：</strong>
扩散模型在从文本提示或图像生成高质量视频方面展现了令人印象深刻的性能。然而，对视频生成过程的精确控制，如摄像机操作或内容编辑，仍然是一个重大挑战。现有的受控视频生成方法通常局限于单一控制类型，缺乏处理多样化控制需求的灵活性。在本文中，我们提出了扩散作为着色器（Diffusion as Shader, DaS），这是一种在统一架构内支持多种视频控制任务的新方法。我们的关键见解是，实现多功能视频控制需要利用3D控制信号，因为视频本质上是动态3D内容的2D渲染。与局限于2D控制信号的现有方法不同，DaS利用3D跟踪视频作为控制输入，使视频扩散过程本质上具有3D感知能力。这一创新使得DaS能够通过简单地操作3D跟踪视频来实现广泛的视频控制。使用3D跟踪视频的另一个优势是它们能够有效地链接帧，显著增强生成视频的时间一致性。仅在使用不到10k视频的8个H800 GPU上进行3天的微调后，DaS在多种任务中展示了强大的控制能力，包括网格到视频生成、摄像机控制、运动转移和对象操作。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>7. OpenOmni：基于大语言模型的零样本全模态对齐与实时自感知情感语音合成</h3>
<p><strong>原文标题：</strong> OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment
  across Language with Real-time Self-Aware Emotional Speech Synthesis</p>
<p><strong>摘要：</strong>
尽管主要在专有模型中实现，全模态学习在图像、文本和语音的理解与生成方面取得了显著进展。有限的全模态数据集以及与实时情感语音生成相关的固有挑战阻碍了开源领域的进展。为解决这些问题，我们提出了OpenOmni，一种结合全模态对齐和语音生成的两阶段训练方法，旨在开发一种先进的全模态大语言模型。在对齐阶段，预训练的语音模型在文本-图像任务上进一步训练，以（近乎）零样本的方式从视觉泛化到语音，其性能优于在三模态数据集上训练的模型。在语音生成阶段，通过在语音任务和偏好学习上的训练，轻量级解码器实现了实时情感语音生成。实验表明，OpenOmni在全模态、视觉-语言和语音-语言评估中持续改进，能够实现自然、情感丰富的对话和实时情感语音生成。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>8. PPTAgent：超越文本到幻灯片的演示文稿生成与评估</h3>
<p><strong>原文标题：</strong> PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides</p>
<p><strong>摘要：</strong>
从文档自动生成演示文稿是一项具有挑战性的任务，需要在内容质量、视觉设计和结构连贯性之间取得平衡。现有方法主要集中于单独改进和评估内容质量，往往忽视了视觉设计和结构连贯性，这限制了它们的实际应用。为了解决这些局限性，我们提出了PPTAgent，它通过受人类工作流程启发的两阶段、基于编辑的方法，全面改进演示文稿生成。PPTAgent首先分析参考演示文稿以理解其结构模式和内容模式，然后通过代码操作起草大纲并生成幻灯片，以确保一致性和对齐。为了全面评估生成演示文稿的质量，我们进一步引入了PPTEval，这是一个评估框架，从内容、设计和连贯性三个维度评估演示文稿。实验表明，PPTAgent在所有三个维度上均显著优于传统的自动演示文稿生成方法。代码和数据可在https://github.com/icip-cas/PPTAgent获取。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>9. 海豚：通过思考、实践和反馈实现闭环开放式自动研究</h3>
<p><strong>原文标题：</strong> Dolphin: Closed-loop Open-ended Auto-research through Thinking,
  Practice, and Feedback</p>
<p><strong>摘要：</strong>
由于人工智能（AI）的发展，科学研究范式正在经历深刻的变革。最近的研究表明，各种AI辅助的研究方法可以通过改进数据分析、加速计算和促进新想法的生成，大幅提高研究效率。为了进一步迈向终极目标（即自动化科学研究），本文提出了海豚（Dolphin），这是第一个闭环开放式自动研究框架，旨在进一步构建人类科学研究的全过程。海豚能够生成研究想法、执行实验，并从实验结果中获得反馈以生成更高质量的想法。具体而言，海豚首先基于按主题和任务属性排序的相关论文生成新颖的想法。然后，代码会自动生成并通过异常回溯引导的局部代码结构进行调试。最后，海豚自动分析每个想法的结果，并将结果反馈到下一轮的想法生成中。在不同主题的基准数据集上进行了实验，结果表明海豚能够持续生成新颖的想法并在循环中完成实验。我们强调，海豚在某些任务（如2D图像分类和3D点分类）中能够自动提出与最先进方法相媲美的方法。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>10. 文本分段及其奖励学习以改进语言模型中的RLHF</h3>
<p><strong>原文标题：</strong> Segmenting Text and Learning Their Rewards for Improved RLHF in Language
  Model</p>
<p><strong>摘要：</strong>
基于人类反馈的强化学习（RLHF）已被广泛采用，以使语言模型（LM）与人类偏好保持一致。先前的RLHF工作通常采用赌博机（bandit）形式，虽然直观，但忽略了LM生成的序列性质，并可能受到稀疏奖励问题的影响。尽管最近的工作提出了密集的令牌级RLHF，但将每个令牌视为一个动作可能对适当的奖励分配过于细致。在本文中，我们通过训练和利用分段级奖励模型，寻求两者的最佳结合，该模型为每个跨越短序列令牌的语义完整文本段分配奖励。对于奖励学习，我们的方法允许动态文本分割，并与标准的序列偏好数据集兼容。为了有效地进行基于RL的LM训练以对抗分段奖励，我们将经典的标量赌博机奖励归一化器推广为位置感知的归一化函数，并对分段奖励进行插值以进一步密集化。通过这些设计，我们的方法在三个流行的RLHF基准测试中表现优异：AlpacaEval 2.0、Arena-Hard和MT-Bench。进行了消融研究以进一步证明我们的方法。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>11. MoDec-GS：全局到局部运动分解与时间间隔调整的紧凑动态3D高斯泼溅</h3>
<p><strong>原文标题：</strong> MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval
  Adjustment for Compact Dynamic 3D Gaussian Splatting</p>
<p><strong>摘要：</strong>
3D高斯泼溅（3DGS）在场景表示和神经渲染方面取得了显著进展，尤其是在适应动态场景方面投入了大量努力。尽管现有方法在渲染质量和速度上表现出色，但在存储需求和表示复杂现实世界运动方面仍面临挑战。为解决这些问题，我们提出了MoDecGS，一种内存高效的高斯泼溅框架，专为在具有复杂运动的挑战性场景中重建新视图而设计。我们引入了全局到局部运动分解（GLMD），以粗到细的方式有效捕捉动态运动。该方法利用全局规范支架（Global CS）和局部规范支架（Local CS），将静态支架表示扩展到动态视频重建。对于全局CS，我们提出了全局锚点变形（GAD），通过直接变形隐式支架属性（包括锚点位置、偏移和局部上下文特征）来高效表示沿复杂运动的全局动态。接着，我们通过局部CS的局部高斯变形（LGD）进行精细调整。此外，我们引入了时间间隔调整（TIA），在训练过程中自动控制每个局部CS的时间覆盖范围，使MoDecGS能够基于指定的时间段数量找到最优的时间间隔分配。大量评估表明，MoDecGS在保持甚至提高渲染质量的同时，相较于现有动态3D高斯方法，模型大小平均减少了70%。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>12. 魔镜：基于视频扩散变换器的身份保持视频生成</h3>
<p><strong>原文标题：</strong> Magic Mirror: ID-Preserved Video Generation in Video Diffusion
  Transformers</p>
<p><strong>摘要：</strong>
我们提出了魔镜（Magic Mirror），一个用于生成具有电影级质量和动态运动的身份保持视频的框架。尽管最近的视频扩散模型在文本到视频生成方面展示了令人印象深刻的能力，但在生成自然运动的同时保持身份一致性仍然具有挑战性。先前的方法要么需要对特定人物进行微调，要么难以在身份保持和运动多样性之间取得平衡。基于视频扩散变换器，我们的方法引入了三个关键组件：（1）一个双分支面部特征提取器，用于捕捉身份和结构特征；（2）一个轻量级的跨模态适配器，结合条件自适应归一化以实现高效的身份集成；（3）一个两阶段训练策略，结合合成身份对和视频数据。大量实验表明，魔镜有效地平衡了身份一致性和自然运动，在多个指标上优于现有方法，同时仅需添加最少的参数。代码和模型将公开发布在：https://github.com/dvlab-research/MagicMirror/</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>13. 图感知同构注意力：Transformer中的自适应动态机制</h3>
<p><strong>原文标题：</strong> Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers</p>
<p><strong>摘要：</strong>
我们提出了一种通过将图感知关系推理整合到注意力机制中来修改Transformer架构的方法，该方法融合了图神经网络和语言建模的概念。基于注意力机制与图论之间的内在联系，我们将Transformer的注意力机制重新表述为图操作，并提出了图感知同构注意力（Graph-Aware Isomorphic Attention）。该方法利用先进的图建模策略，包括图同构网络（Graph Isomorphism Networks, GIN）和主邻域聚合（Principal Neighborhood Aggregation, PNA），以丰富关系结构的表示。我们的方法能够捕捉复杂的依赖关系并在任务间泛化，这一点通过减少泛化差距和提高学习性能得到了验证。此外，我们扩展了图感知注意力的概念，引入了稀疏GIN注意力（Sparse GIN-Attention），这是一种采用稀疏GIN的微调方法。通过将注意力矩阵解释为稀疏邻接图，该技术以最小的计算开销增强了预训练基础模型的适应性，赋予它们图感知能力。与低秩适应（LoRA）等其他方法相比，稀疏GIN注意力微调在训练动态和泛化能力方面表现更优。我们讨论了传统注意力机制中潜在的类图结构，为理解Transformer提供了新的视角。通过将Transformer演化为用于关系推理的分层GIN模型，这一视角为基础模型开发提供了深刻的启示，使得设计能够动态适应局部和全局依赖关系的架构成为可能。在生物信息学、材料科学、语言建模等领域的应用中，这种关系与序列数据建模的结合有望带来可解释且可泛化的建模策略。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>14. MagicFace：基于动作单元控制的高保真面部表情编辑</h3>
<p><strong>原文标题：</strong> MagicFace: High-Fidelity Facial Expression Editing with Action-Unit
  Control</p>
<p><strong>摘要：</strong>
我们通过控制同一人面部动作单元（AU）的相对变化来解决面部表情编辑问题。这使得我们能够以细粒度、连续且可解释的方式编辑特定人物的表情，同时保留其身份、姿态、背景和详细的面部属性。我们提出的模型名为MagicFace，其核心是一个基于AU变化条件化的扩散模型和一个用于保持高一致性面部细节的ID编码器。具体来说，为了保留输入身份的面部细节，我们利用预训练的Stable-Diffusion模型的能力，并设计了一个ID编码器，通过自注意力机制融合外观特征。为了保持背景和姿态的一致性，我们引入了一个高效的属性控制器，明确告知模型当前目标的背景和姿态。通过将AU变化注入去噪UNet，我们的模型可以使用各种AU组合为任意身份生成动画，与其他面部表情编辑工作相比，在高保真表情编辑方面取得了卓越的结果。代码公开在https://github.com/weimengting/MagicFace。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>15. 文本引导图像到图像扩散模型的可泛化来源识别</h3>
<p><strong>原文标题：</strong> Generalizable Origin Identification for Text-Guided Image-to-Image
  Diffusion Models</p>
<p><strong>摘要：</strong>
文本引导的图像到图像扩散模型在基于文本提示的图像转换方面表现出色，允许进行精确和创造性的视觉修改。然而，这种强大的技术可能被滥用于传播错误信息、侵犯版权和逃避内容追踪。这促使我们引入了文本引导图像到图像扩散模型的来源识别任务（ID^2），旨在检索给定转换查询的原始图像。ID^2的一个直接解决方案是训练一个专门的深度嵌入模型，以提取和比较查询图像和参考图像的特征。然而，由于不同扩散模型生成的图像之间存在视觉差异，这种基于相似性的方法在从一个模型的图像训练并在另一个模型的图像上测试时失败，限制了其在实际应用中的有效性。为了解决ID^2任务中的这一挑战，我们贡献了第一个数据集和一个理论上保证的方法，两者都强调泛化能力。精心策划的数据集OriPID包含丰富的原始图像和引导提示，可用于训练和测试各种扩散模型的潜在识别模型。在方法部分，我们首先证明了存在一个线性变换，可以最小化生成样本的预训练变分自编码器（VAE）嵌入与其原始图像之间的距离。随后，证明了这种简单的线性变换可以在不同的扩散模型之间泛化。实验结果表明，所提出的方法实现了令人满意的泛化性能，显著超过了基于相似性的方法（+31.6% mAP），即使是那些具有泛化设计的方法。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="audio/2025-01-08_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>