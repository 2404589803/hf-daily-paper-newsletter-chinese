<h1>🤗 Hugging Face 每日论文速递 2025-01-08</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：RL, LLM</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. REINFORCE++：一种简单高效的大语言模型对齐方法</h3>
<p><strong>摘要：</strong>
基于人类反馈的强化学习（RLHF）已成为将大语言模型与人类偏好对齐的关键方法，并通过近端策略优化（PPO）、直接偏好优化（DPO）、REINFORCE留一法（RLOO）、ReMax和群体相对策略优化（GRPO）等方法实现了快速的算法演进。我们提出了REINFORCE++，这是经典REINFORCE算法的增强版本，它结合了PPO的关键优化技术，同时消除了对评论者网络的需求。REINFORCE++实现了三个主要目标：（1）简化算法结构，（2）提高训练稳定性，以及（3）减少计算开销。通过广泛的实证评估，我们证明REINFORCE++在稳定性方面优于GRPO，并且在计算效率上优于PPO，同时保持了可比的性能。实现代码可在https://github.com/OpenRLHF/OpenRLHF获取。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>2. 面向物理人工智能的Cosmos世界基础模型平台</h3>
<p><strong>摘要：</strong>
物理人工智能首先需要进行数字化训练。它需要一个自身的数字孪生体，即策略模型，以及一个世界的数字孪生体，即世界模型。在本文中，我们介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理人工智能系统构建定制化的世界模型。我们将世界基础模型定位为一种通用世界模型，可以通过微调转化为适用于下游应用的定制化世界模型。我们的平台涵盖了视频筛选流程、预训练的世界基础模型、预训练世界基础模型的训练后示例以及视频标记器。为了帮助物理人工智能开发者解决社会中最关键的问题，我们将平台开源，并通过https://github.com/NVIDIA/Cosmos提供开放权重的模型和宽松的许可。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>3. MotionBench：面向视觉语言模型的细粒度视频运动理解基准测试与改进</h3>
<p><strong>摘要：</strong>
近年来，视觉语言模型（VLMs）在视频理解领域取得了显著进展。然而，当前基准测试中一个关键能力——细粒度运动理解——仍未得到充分探索。为填补这一空白，我们提出了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。MotionBench通过六种主要类别的运动导向问题类型来评估模型的运动级感知能力，并包含从多种来源收集的数据，确保了对现实世界视频内容的广泛代表性。实验结果表明，现有的VLMs在理解细粒度运动方面表现不佳。为了增强VLM在有限序列长度内感知细粒度运动的能力，我们进行了广泛的实验，回顾了针对视频特征压缩优化的VLM架构，并提出了一种新颖且高效的Through-Encoder（TE）融合方法。实验表明，更高的帧率输入和TE融合在运动理解方面带来了改进，但仍存在显著的提升空间。我们的基准测试旨在指导和激励开发更具能力的视频理解模型，强调细粒度运动理解的重要性。项目页面：https://motion-bench.github.io。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>4. LLaVA-Mini：基于单一视觉令牌的高效图像与视频大型多模态模型</h3>
<p><strong>摘要：</strong>
随着GPT-4o等实时大型多模态模型（LMMs）的出现，高效LMMs引起了广泛关注。LMM框架通常将视觉输入编码为视觉令牌（连续表示），并将其与文本指令整合到大型语言模型（LLMs）的上下文中，其中大规模参数和大量上下文令牌（主要是视觉令牌）导致巨大的计算开销。以往关于高效LMMs的研究主要集中在用较小模型替换LLM骨干，而忽视了令牌数量的关键问题。本文介绍了LLaVA-Mini，一种具有最少视觉令牌的高效LMM。为了实现视觉令牌的高压缩比同时保留视觉信息，我们首先分析了LMMs如何理解视觉令牌，发现大多数视觉令牌仅在LLM骨干的早期层中起关键作用，它们主要将视觉信息融合到文本令牌中。基于这一发现，LLaVA-Mini引入了模态预融合，提前将视觉信息融合到文本令牌中，从而促进输入到LLM骨干的视觉令牌的极端压缩至单一令牌。LLaVA-Mini是一个统一的大型多模态模型，能够高效地支持图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中，实验表明LLaVA-Mini仅使用1个视觉令牌（而非576个）即可超越LLaVA-v1.5。效率分析显示，LLaVA-Mini可以减少77%的浮点运算（FLOPs），在40毫秒内提供低延迟响应，并在24GB内存的GPU硬件上处理超过10,000帧的视频。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>5. Sa2VA：将SAM2与LLaVA结合用于图像和视频的密集基础理解</h3>
<p><strong>摘要：</strong>
本文提出了Sa2VA，这是首个用于图像和视频密集基础理解的统一模型。与现有的多模态大语言模型不同，后者通常局限于特定模态和任务，而Sa2VA支持广泛的图像和视频任务，包括参考分割和对话，且仅需最小的一次性指令调优。Sa2VA结合了SAM-2（一个基础视频分割模型）和LLaVA（一个先进的视觉语言模型），并将文本、图像和视频统一到一个共享的LLM令牌空间中。通过使用LLM，Sa2VA生成指令令牌，指导SAM-2生成精确的掩码，从而实现对静态和动态视觉内容的基础多模态理解。此外，我们引入了Ref-SAV，这是一个包含复杂视频场景中超过72k对象表达式的自动标注数据集，旨在提升模型性能。我们还手动验证了Ref-SAV数据集中的2k视频对象，以在复杂环境中进行参考视频对象分割的基准测试。实验表明，Sa2VA在多个任务中达到了最先进的水平，特别是在参考视频对象分割方面，突显了其在复杂现实应用中的潜力。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>6. 扩散作为着色器：面向多功能视频生成控制的三维感知视频扩散</h3>
<p><strong>摘要：</strong>
扩散模型在从文本提示或图像生成高质量视频方面展示了令人印象深刻的性能。然而，对视频生成过程的精确控制，如相机操作或内容编辑，仍然是一个重大挑战。现有的受控视频生成方法通常仅限于单一控制类型，缺乏处理多样化控制需求的灵活性。在本文中，我们介绍了扩散作为着色器（DaS），这是一种在统一架构内支持多种视频控制任务的新方法。我们的关键见解是，实现多功能视频控制需要利用三维控制信号，因为视频本质上是动态三维内容的二维渲染。与仅限于二维控制信号的现有方法不同，DaS利用三维跟踪视频作为控制输入，使视频扩散过程本质上具有三维感知能力。这一创新使得DaS能够通过简单地操作三维跟踪视频来实现广泛的视频控制。使用三维跟踪视频的另一个优势是其能够有效地链接帧，显著增强生成视频的时间一致性。仅使用8个H800 GPU在不到10k视频上进行3天的微调，DaS在多种任务中展示了强大的控制能力，包括网格到视频生成、相机控制、运动转移和对象操作。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>7. PPTAgent：超越文本到幻灯片的演示文稿生成与评估</h3>
<p><strong>摘要：</strong>
从文档自动生成演示文稿是一项具有挑战性的任务，需要在内容质量、视觉设计和结构连贯性之间取得平衡。现有方法主要集中于单独改进和评估内容质量，往往忽视了视觉设计和结构连贯性，这限制了它们的实际应用性。为了解决这些局限性，我们提出了PPTAgent，它通过一种受人类工作流程启发的两阶段、基于编辑的方法，全面改进了演示文稿的生成。PPTAgent首先分析参考演示文稿以理解其结构模式和内容模式，然后通过代码操作起草大纲并生成幻灯片，以确保一致性和对齐。为了全面评估生成的演示文稿的质量，我们进一步引入了PPTEval，这是一个评估框架，从内容、设计和连贯性三个维度对演示文稿进行评估。实验表明，PPTAgent在所有三个维度上均显著优于传统的自动演示文稿生成方法。代码和数据可在https://github.com/icip-cas/PPTAgent获取。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>8. OpenOmni：基于大语言模型的零样本全模态对齐与实时自感知情感语音合成</h3>
<p><strong>摘要：</strong>
尽管主要在专有模型中实现，全模态学习在图像、文本和语音的理解与生成方面取得了显著进展。有限的全模态数据集以及与实时情感语音生成相关的固有挑战阻碍了开源领域的进展。为解决这些问题，我们提出了openomni，一种结合全模态对齐和语音生成的两阶段训练方法，以开发一种先进的全模态大语言模型。在对齐阶段，预训练的语音模型在文本-图像任务上进一步训练，以（近乎）零样本的方式从视觉泛化到语音，其性能优于在三模态数据集上训练的模型。在语音生成阶段，通过在语音任务和偏好学习上的训练，轻量级解码器实现了实时情感语音生成。实验表明，openomni在全模态、视觉-语言和语音-语言评估中持续改进，能够实现自然、情感丰富的对话和实时情感语音生成。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>9. 海豚：通过思考、实践和反馈实现闭环开放式自动研究</h3>
<p><strong>摘要：</strong>
由于人工智能（AI）的发展，科学研究范式正在经历深刻的变革。最近的研究表明，各种AI辅助的研究方法可以通过改进数据分析、加速计算和促进新想法的生成，大幅提高研究效率。为了进一步迈向终极目标（即自动化科学研究），本文提出了海豚（Dolphin），这是第一个闭环开放式自动研究框架，旨在进一步构建人类科学研究的全过程。海豚能够生成研究想法、执行实验，并从实验结果中获得反馈以生成更高质量的想法。具体而言，海豚首先基于相关论文生成新想法，这些论文按主题和任务属性进行排序。然后，代码会自动生成并通过异常回溯引导的局部代码结构进行调试。最后，海豚自动分析每个想法的结果，并将结果反馈到下一轮的想法生成中。在不同主题的基准数据集上进行了实验，结果表明海豚能够持续生成新想法并在循环中完成实验。我们强调，海豚在某些任务（如2D图像分类和3D点分类）中能够自动提出与最先进方法相媲美的方法。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>10. 文本分段及其奖励学习以改进语言模型中的RLHF</h3>
<p><strong>摘要：</strong>
基于人类反馈的强化学习（RLHF）已被广泛采用，以使语言模型（LM）与人类偏好保持一致。以往的RLHF研究通常采用赌博机（bandit）形式，虽然直观，但忽略了LM生成的序列性质，并且可能受到稀疏奖励问题的影响。尽管最近的研究提出了密集的令牌级RLHF，但将每个令牌视为一个动作可能过于细微，难以正确分配奖励。在本文中，我们通过训练和利用分段级奖励模型，旨在兼顾两者优势，该模型为每个跨越短序列令牌的语义完整文本段分配奖励。在奖励学习方面，我们的方法允许动态文本分割，并与标准序列偏好数据集兼容。为了有效进行基于RL的LM训练以对抗分段奖励，我们将经典的标量赌博机奖励归一化器推广为位置感知归一化函数，并对分段奖励进行插值以进一步密集化。通过这些设计，我们的方法在三个流行的RLHF基准测试中表现优异：AlpacaEval 2.0、Arena-Hard和MT-Bench。我们还进行了消融研究，以进一步证明我们方法的有效性。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>11. MoDec-GS：全局到局部运动分解与时间间隔调整的紧凑动态3D高斯泼溅</h3>
<p><strong>摘要：</strong>
3D高斯泼溅（3DGS）在场景表示和神经渲染方面取得了显著进展，尤其是在将其应用于动态场景方面进行了大量努力。尽管现有方法在渲染质量和速度上表现出色，但在存储需求和表示复杂现实世界运动方面仍面临挑战。为解决这些问题，我们提出了MoDecGS，一种内存高效的高斯泼溅框架，旨在在具有复杂运动的挑战性场景中重建新视图。我们引入了全局到局部运动分解（GLMD），以粗到细的方式有效捕捉动态运动。该方法利用全局规范支架（Global CS）和局部规范支架（Local CS），将静态支架表示扩展到动态视频重建。对于Global CS，我们提出了全局锚点变形（GAD），通过直接变形隐式支架属性（包括锚点位置、偏移和局部上下文特征）来高效表示沿复杂运动的全局动态。接下来，我们通过局部高斯变形（LGD）显式地精细调整局部运动。此外，我们引入了时间间隔调整（TIA），在训练过程中自动控制每个Local CS的时间覆盖范围，使MoDecGS能够基于指定的时间段数量找到最佳间隔分配。大量评估表明，MoDecGS在保持甚至提高渲染质量的同时，相较于现有动态3D高斯方法，模型大小平均减少了70%。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>12. 魔镜：基于视频扩散变换器的身份保持视频生成</h3>
<p><strong>摘要：</strong>
我们提出了魔镜（Magic Mirror）框架，用于生成具有电影级质量和动态运动的身份保持视频。尽管最近在视频扩散模型方面的进展在文本到视频生成方面展示了令人印象深刻的能力，但在生成自然运动的同时保持身份一致性仍然具有挑战性。先前的方法要么需要针对特定人物进行微调，要么难以在身份保持与运动多样性之间取得平衡。基于视频扩散变换器，我们的方法引入了三个关键组件：（1）双分支面部特征提取器，用于捕捉身份和结构特征；（2）轻量级跨模态适配器，结合条件自适应归一化以实现高效的身份集成；（3）结合合成身份对与视频数据的两阶段训练策略。大量实验表明，魔镜有效地平衡了身份一致性与自然运动，在多个指标上优于现有方法，同时仅需添加最少的参数。代码和模型将公开发布在：https://github.com/dvlab-research/MagicMirror/</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>13. 图感知同构注意力：Transformer中的自适应动态机制</h3>
<p><strong>摘要：</strong>
我们提出了一种通过将图感知关系推理整合到注意力机制中来修改Transformer架构的方法，融合了图神经网络和语言建模的概念。基于注意力机制与图论之间的内在联系，我们将Transformer的注意力机制重新表述为图操作，并提出了图感知同构注意力（Graph-Aware Isomorphic Attention）。该方法利用先进的图建模策略，包括图同构网络（Graph Isomorphism Networks, GIN）和主邻域聚合（Principal Neighborhood Aggregation, PNA），以丰富关系结构的表示。我们的方法能够捕捉复杂的依赖关系并在任务间泛化，这一点通过减少泛化差距和提高学习性能得到了验证。此外，我们扩展了图感知注意力的概念，引入了稀疏GIN注意力（Sparse GIN-Attention），这是一种采用稀疏GIN的微调方法。通过将注意力矩阵解释为稀疏邻接图，该技术以最小的计算开销增强了预训练基础模型的适应性，赋予它们图感知能力。与低秩适应（LoRA）等其他方法相比，稀疏GIN注意力微调在训练动态和泛化性能上表现更优。我们讨论了传统注意力机制中潜在的类图结构，为理解Transformer提供了新的视角。通过将Transformer演化为用于关系推理的分层GIN模型，这一视角为基础模型的开发提出了深远的影响，使得设计能够动态适应局部和全局依赖关系的架构成为可能。在生物信息学、材料科学、语言建模等领域的应用中，这种关系与序列数据建模的融合将有助于开发可解释且可泛化的建模策略。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>14. MagicFace：基于动作单元控制的高保真面部表情编辑</h3>
<p><strong>摘要：</strong>
我们通过控制同一人面部动作单元（AU）的相对变化来解决面部表情编辑问题。这使得我们能够以细粒度、连续且可解释的方式编辑特定人物的表情，同时保留其身份、姿态、背景和详细的面部属性。我们提出的模型名为MagicFace，其核心是一个基于AU变化的条件扩散模型和一个用于保持高一致性面部细节的ID编码器。具体来说，为了保留输入身份的面部细节，我们利用预训练的Stable-Diffusion模型的能力，并设计了一个ID编码器，通过自注意力机制融合外观特征。为了保持背景和姿态的一致性，我们引入了一个高效的属性控制器，通过显式地告知模型当前目标的背景和姿态。通过将AU变化注入去噪UNet，我们的模型能够使用各种AU组合为任意身份生成动画，与其他面部表情编辑工作相比，在高保真表情编辑方面取得了优异的结果。代码公开在https://github.com/weimengting/MagicFace。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h3>15. 面向文本引导图像到图像扩散模型的通用来源识别</h3>
<p><strong>摘要：</strong>
文本引导的图像到图像扩散模型在基于文本提示的图像转换方面表现出色，能够实现精确且富有创意的视觉修改。然而，这种强大的技术可能被滥用于传播错误信息、侵犯版权和逃避内容追踪。这促使我们提出了文本引导图像到图像扩散模型的来源识别任务（ID^2），旨在检索给定转换查询的原始图像。ID^2的一个直接解决方案是训练一个专门的深度嵌入模型，以提取和比较查询图像和参考图像的特征。然而，由于不同扩散模型生成的图像之间存在视觉差异，这种基于相似性的方法在从一个模型的图像训练并在另一个模型的图像上测试时失效，限制了其在实际应用中的有效性。为了解决这一挑战，我们贡献了第一个数据集和一种理论上保证的方法，两者都强调通用性。精心策划的数据集OriPID包含丰富的原始图像和引导提示，可用于训练和测试跨各种扩散模型的潜在识别模型。在方法部分，我们首先证明存在一种线性变换，可以最小化生成样本与其原始图像的预训练变分自编码器（VAE）嵌入之间的距离。随后，证明了这种简单的线性变换可以推广到不同的扩散模型中。实验结果表明，所提出的方法实现了令人满意的泛化性能，显著超越了基于相似性的方法（+31.6% mAP），即使是那些具有泛化设计的方法。</p>
<p><strong>论文链接：</strong> <a href=""></a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="stats/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="stats/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="HF-day-paper-deepseek/audio/2025-01-08_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>