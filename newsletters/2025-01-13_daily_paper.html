<h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-01-13 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：5</li>
<li>热门领域：LLM</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. VideoRAG：基于视频语料库的检索增强生成</h3>
<p><strong>原文标题：</strong> VideoRAG: Retrieval-Augmented Generation over Video Corpus</p>
<p><strong>摘要：</strong>
检索增强生成（Retrieval-Augmented Generation, RAG）是一种强大的策略，通过检索与查询相关的外部知识并将其整合到生成过程中，来解决基础模型生成事实错误输出的问题。然而，现有的RAG方法主要集中在文本信息上，最近的一些进展开始考虑图像，但它们大多忽略了视频这一丰富的多模态知识来源，视频能够比其他任何模态更有效地表示事件、过程和上下文细节。尽管最近的一些研究探索了在响应生成过程中整合视频的方法，但它们要么预定义了与查询相关的视频而没有根据查询进行检索，要么将视频转换为文本描述而没有利用其多模态的丰富性。为了解决这些问题，我们提出了VideoRAG，这是一种新颖的框架，不仅根据查询动态检索相关视频，还在输出生成中利用视频的视觉和文本信息。此外，为了实现这一点，我们的方法围绕最近的大型视频语言模型（Large Video Language Models, LVLMs）的进展展开，这些模型能够直接处理视频内容以进行检索，并将检索到的视频与查询无缝整合。我们通过实验验证了VideoRAG的有效性，展示了其优于相关基线方法的表现。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.05874">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.05874">arXiv</a></p>
<hr />
<h3>2. 通过自进化批评者实现可扩展监督</h3>
<p><strong>原文标题：</strong> Enabling Scalable Oversight via Self-Evolving Critic</p>
<p><strong>摘要：</strong>
尽管大型语言模型（LLMs）表现出色，但其发展面临一个关键挑战：在人类评估困难或LLMs超越人类的任务中提供有效反馈。尽管越来越多的人对使用LLMs进行批评感兴趣，但当前方法仍依赖于人类注释或更强大的模型，这使得在没有外部监督的情况下增强批评能力的问题仍未解决。我们引入了SCRIT（自进化批评者），一个能够实现批评能力真正自进化的框架。从技术上讲，SCRIT通过训练合成数据来自我改进，这些数据由基于对比的自我批评者生成，该批评者使用参考解决方案进行逐步批评，并通过自我验证机制确保批评质量，该机制通过纠正结果来确保批评质量。使用最强大的LLMs之一Qwen2.5-72B-Instruct实现，SCRIT在批评纠正和错误识别基准上实现了高达10.3%的改进。我们的分析表明，SCRIT的性能随着数据和模型规模的增加而正向扩展，优于其他方法，并且其自我验证组件对其性能至关重要。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.05727">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.05727">arXiv</a></p>
<hr />
<h3>3. OmniManip：通过以对象为中心的交互原语作为空间约束实现通用机器人操作</h3>
<p><strong>原文标题：</strong> OmniManip: Towards General Robotic Manipulation via Object-Centric
  Interaction Primitives as Spatial Constraints</p>
<p><strong>摘要：</strong>
开发能够在非结构化环境中进行操作的通用机器人系统是一个重大挑战。尽管视觉-语言模型（VLM）在高层常识推理方面表现出色，但它们缺乏精确操作任务所需的细粒度三维空间理解能力。通过在机器人数据集上微调VLM以创建视觉-语言-动作模型（VLA）是一种潜在的解决方案，但这种方法受到高数据收集成本和泛化问题的阻碍。为了解决这些挑战，我们提出了一种新颖的以对象为中心的表示方法，该方法弥合了VLM的高层推理与操作所需的低层精度之间的差距。我们的关键见解是，对象的功能性可供性所定义的规范空间提供了一种结构化和语义上有意义的方式来描述交互原语，例如点和方向。这些原语充当桥梁，将VLM的常识推理转化为可操作的三维空间约束。在此背景下，我们引入了一种双闭环、开放词汇的机器人操作系统：一个闭环通过原语重采样、交互渲染和VLM检查进行高层规划，另一个闭环通过6D姿态跟踪进行低层执行。这种设计确保了无需VLM微调的鲁棒实时控制。大量实验证明了该方法在多种机器人操作任务中的强大零样本泛化能力，突显了该方法在自动化大规模仿真数据生成方面的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.03841">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.03841">arXiv</a></p>
<hr />
<h3>4. LlamaV-o1：重新思考大语言模型中的逐步视觉推理</h3>
<p><strong>原文标题：</strong> LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</p>
<p><strong>摘要：</strong>
推理是解决复杂多步骤问题的基本能力，尤其是在需要逐步理解的视觉场景中。现有方法缺乏一个全面的框架来评估视觉推理，并且没有强调逐步解决问题的能力。为此，我们提出了一个全面的框架，通过三个关键贡献来推进大语言模型（LLMs）中的逐步视觉推理。首先，我们引入了一个专门设计用于评估多步骤推理任务的视觉推理基准。该基准提供了多样化的挑战，涵盖从复杂视觉感知到科学推理的八个不同类别，总计超过4,000个推理步骤，从而能够全面评估LLMs在多步骤中执行准确且可解释的视觉推理的能力。其次，我们提出了一种新的指标，该指标在单个步骤的粒度上评估视觉推理的质量，强调正确性和逻辑一致性。与传统的任务结束准确性指标相比，所提出的指标能够更深入地洞察推理性能。第三，我们提出了一种新的多模态视觉推理模型，名为LlamaV-o1，该模型通过多步骤课程学习方法进行训练，任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1专为多步骤推理设计，并通过结构化的训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并且在闭源专有模型中也表现良好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分为67.3，绝对增益为3.8%，同时在推理扩展过程中速度快了5倍。我们的基准、模型和代码均已公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.06186">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.06186">arXiv</a></p>
<hr />
<h3>5. 多智能体微调：通过多样化推理链实现自我改进</h3>
<p><strong>原文标题：</strong> Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains</p>
<p><strong>摘要：</strong>
近年来，大型语言模型（LLMs）取得了显著的性能，但其根本上受限于基础训练数据。为了在训练数据之外改进模型，最近的研究探索了如何利用LLMs生成合成数据以实现自主的自我改进。然而，连续的自我改进步骤可能会达到收益递减的临界点。在本研究中，我们提出了一种互补的自我改进方法，即对语言模型的多智能体社会进行微调。一组语言模型，均从相同的基础模型出发，通过模型间的多智能体交互生成的数据进行独立更新，从而实现各自的专门化。通过在独立的数据集上训练每个模型，我们展示了这种方法如何实现模型间的专门化和模型集的多样化。因此，我们的整体系统能够保留多样化的推理链，并在比单智能体自我改进方法更多的微调轮次中实现自主改进。我们在广泛的推理任务中定量展示了该方法的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2501.05707">HuggingFace</a> | <a href="https://arxiv.org/abs/2501.05707">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="audio/2025-01-13_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>