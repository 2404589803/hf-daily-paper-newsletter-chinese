[
    {
        "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document\n  Understanding",
        "id": "2403.12895",
        "summary": "Structure information is critical for understanding the semantics of\ntext-rich images, such as documents, tables, and charts. Existing Multimodal\nLarge Language Models (MLLMs) for Visual Document Understanding are equipped\nwith text recognition ability but lack general structure understanding\nabilities for text-rich document images. In this work, we emphasize the\nimportance of structure information in Visual Document Understanding and\npropose the Unified Structure Learning to boost the performance of MLLMs. Our\nUnified Structure Learning comprises structure-aware parsing tasks and\nmulti-grained text localization tasks across 5 domains: document, webpage,\ntable, chart, and natural image. To better encode structure information, we\ndesign a simple and effective vision-to-text module H-Reducer, which can not\nonly maintain the layout information but also reduce the length of visual\nfeatures by merging horizontal adjacent patches through convolution, enabling\nthe LLM to understand high-resolution images more efficiently. Furthermore, by\nconstructing structure-aware text sequences and multi-grained pairs of texts\nand bounding boxes for publicly available text-rich images, we build a\ncomprehensive training set DocStruct4M to support structure learning. Finally,\nwe construct a small but high-quality reasoning tuning dataset DocReason25K to\ntrigger the detailed explanation ability in the document domain. Our model\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document\nunderstanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM\nby more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are\npublicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5."
    },
    {
        "title": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
        "id": "2403.12706",
        "summary": "We present AnimateDiff-Lightning for lightning-fast video generation. Our\nmodel uses progressive adversarial diffusion distillation to achieve new\nstate-of-the-art in few-step video generation. We discuss our modifications to\nadapt it for the video modality. Furthermore, we propose to simultaneously\ndistill the probability flow of multiple base diffusion models, resulting in a\nsingle distilled motion module with broader style compatibility. We are pleased\nto release our distilled AnimateDiff-Lightning model for the community's use."
    },
    {
        "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
        "id": "2403.12173",
        "summary": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications."
    },
    {
        "title": "GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation",
        "id": "2403.12365",
        "summary": "Creating 4D fields of Gaussian Splatting from images or videos is a\nchallenging task due to its under-constrained nature. While the optimization\ncan draw photometric reference from the input videos or be regulated by\ngenerative models, directly supervising Gaussian motions remains underexplored.\nIn this paper, we introduce a novel concept, Gaussian flow, which connects the\ndynamics of 3D Gaussians and pixel velocities between consecutive frames. The\nGaussian flow can be efficiently obtained by splatting Gaussian dynamics into\nthe image space. This differentiable process enables direct dynamic supervision\nfrom optical flow. Our method significantly benefits 4D dynamic content\ngeneration and 4D novel view synthesis with Gaussian Splatting, especially for\ncontents with rich motions that are hard to be handled by existing methods. The\ncommon color drifting issue that happens in 4D generation is also resolved with\nimproved Guassian dynamics. Superior visual quality on extensive experiments\ndemonstrates our method's effectiveness. Quantitative and qualitative\nevaluations show that our method achieves state-of-the-art results on both\ntasks of 4D generation and 4D novel view synthesis. Project page:\nhttps://zerg-overmind.github.io/GaussianFlow.github.io/"
    },
    {
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression",
        "id": "2403.12968",
        "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x."
    },
    {
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "id": "2403.12596",
        "summary": "Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\nchen2023pali3, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\nliu2023deplot. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by hsieh2023distilling.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt chen2023program, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V."
    },
    {
        "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with\n  Cross-Attention Transformers",
        "id": "2403.12943",
        "summary": "While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io"
    },
    {
        "title": "ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware\n  Diffusion Guidance",
        "id": "2403.12409",
        "summary": "Generating high-quality 3D assets from a given image is highly desirable in\nvarious applications such as AR/VR. Recent advances in single-image 3D\ngeneration explore feed-forward models that learn to infer the 3D model of an\nobject without optimization. Though promising results have been achieved in\nsingle object generation, these methods often struggle to model complex 3D\nassets that inherently contain multiple objects. In this work, we present\nComboVerse, a 3D generation framework that produces high-quality 3D assets with\ncomplex compositions by learning to combine multiple models. 1) We first\nperform an in-depth analysis of this ``multi-object gap'' from both model and\ndata perspectives. 2) Next, with reconstructed 3D models of different objects,\nwe seek to adjust their sizes, rotation angles, and locations to create a 3D\nasset that matches the given image. 3) To automate this process, we apply\nspatially-aware score distillation sampling (SSDS) from pretrained diffusion\nmodels to guide the positioning of objects. Our proposed framework emphasizes\nspatial alignment of objects, compared with standard score distillation\nsampling, and thus achieves more accurate results. Extensive experiments\nvalidate ComboVerse achieves clear improvements over existing methods in\ngenerating compositional 3D assets."
    },
    {
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for\n  Large Language Models",
        "id": "2403.12881",
        "summary": "Open-sourced Large Language Models (LLMs) have achieved great success in\nvarious NLP tasks, however, they are still far inferior to API-based models\nwhen acting as agents. How to integrate agent ability into general LLMs becomes\na crucial and urgent problem. This paper first delivers three key observations:\n(1) the current agent training corpus is entangled with both formats following\nand agent reasoning, which significantly shifts from the distribution of its\npre-training data; (2) LLMs exhibit different learning speeds on the\ncapabilities required by agent tasks; and (3) current approaches have\nside-effects when improving agent abilities by introducing hallucinations.\nBased on the above findings, we propose Agent-FLAN to effectively Fine-tune\nLANguage models for Agents. Through careful decomposition and redesign of the\ntraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by\n3.5\\% across various agent evaluation datasets. With comprehensively\nconstructed negative samples, Agent-FLAN greatly alleviates the hallucination\nissues based on our established evaluation benchmark. Besides, it consistently\nimproves the agent capability of LLMs when scaling model sizes while slightly\nenhancing the general capability of LLMs. The code will be available at\nhttps://github.com/InternLM/Agent-FLAN."
    },
    {
        "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution\n  Image Synthesis",
        "id": "2403.12963",
        "summary": "In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale."
    },
    {
        "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
        "id": "2403.12957",
        "summary": "In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed (sim7\nseconds), effectively striking a balance between quality and efficiency."
    },
    {
        "title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation",
        "id": "2403.12962",
        "summary": "The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods."
    },
    {
        "title": "TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation",
        "id": "2403.12906",
        "summary": "Texturing 3D humans with semantic UV maps remains a challenge due to the\ndifficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D\nadvancements in supervising multi-view renderings using large text-to-image\n(T2I) models, issues persist with generation speed, text consistency, and\ntexture quality, resulting in data scarcity among existing datasets. We present\nTexDreamer, the first zero-shot multimodal high-fidelity 3D human texture\ngeneration model. Utilizing an efficient texture adaptation finetuning\nstrategy, we adapt large T2I model to a semantic UV structure while preserving\nits original generalization capability. Leveraging a novel feature translator\nmodule, the trained model is capable of generating high-fidelity 3D human\ntextures from either text or image within seconds. Furthermore, we introduce\nArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024)\n3D human texture dataset which contains 50k high-fidelity textures with text\ndescriptions."
    }
]