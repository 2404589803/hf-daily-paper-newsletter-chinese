[
    {
        "title": "Stealing Part of a Production Language Model",
        "id": "2403.06634",
        "summary": "We introduce the first model-stealing attack that extracts precise,\nnontrivial information from black-box production language models like OpenAI's\nChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding\nprojection layer (up to symmetries) of a transformer model, given typical API\naccess. For under \\20 USD, our attack extracts the entire projection matrix of\nOpenAI's Ada and Babbage language models. We thereby confirm, for the first\ntime, that these black-box models have a hidden dimension of 1024 and 2048,\nrespectively. We also recover the exact hidden dimension size of the\ngpt-3.5-turbo model, and estimate it would cost under 2,000 in queries to\nrecover the entire projection matrix. We conclude with potential defenses and\nmitigations, and discuss the implications of possible future work that could\nextend our attack."
    },
    {
        "title": "Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a\n  Single GPU",
        "id": "2403.06504",
        "summary": "Recent advances in large language models have brought immense value to the\nworld, with their superior capabilities stemming from the massive number of\nparameters they utilize. However, even the GPUs with the highest memory\ncapacities, currently peaking at 80GB, are far from sufficient to accommodate\nthese vast parameters and their associated optimizer states when conducting\nstochastic gradient descent-based optimization. One approach to hosting such\nhuge models is to aggregate device memory from many GPUs. However, this\napproach introduces prohibitive costs for most academic researchers, who always\nhave a limited budget for many high-end GPU servers. In this paper, we focus on\nhuge model fine-tuning on a single, even low-end, GPU in a commodity server,\nwhich is accessible to most AI researchers. In such a scenario, the\nstate-of-the-art work ZeRO-Infinity suffers from two severe issues when running\nin a commodity server: 1) low GPU utilization due to inefficient swapping, and\n2) limited trainable model size due to CPU memory capacity. The underlying\nreason is that ZeRO-Infinity is optimized for running on high-end GPU servers.\nTo this end, we present Fuyou, a low-cost training framework that enables\nefficient 100B huge model fine-tuning on a low-end server with a low-end GPU\nand limited CPU memory capacity. The key idea is to add the SSD-CPU\ncommunication as an optimization dimension and thus carefully co-optimize\ncomputation and data swapping from a systematic approach to maximize GPU\nutilization. The experimental results show that 1) Fuyou is able to fine-tune\n175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while\nZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model,\nFuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves\n45 TFLOPS."
    },
    {
        "title": "V3D: Video Diffusion Models are Effective 3D Generators",
        "id": "2403.06738",
        "summary": "Automatic 3D generation has recently attracted widespread attention. Recent\nmethods have greatly accelerated the generation speed, but usually produce\nless-detailed objects due to limited model capacity or 3D data. Motivated by\nrecent advancements in video diffusion models, we introduce V3D, which\nleverages the world simulation capacity of pre-trained video diffusion models\nto facilitate 3D generation. To fully unleash the potential of video diffusion\nto perceive the 3D world, we further introduce geometrical consistency prior\nand extend the video diffusion model to a multi-view consistent 3D generator.\nBenefiting from this, the state-of-the-art video diffusion model could be\nfine-tuned to generate 360degree orbit frames surrounding an object given a\nsingle image. With our tailored reconstruction pipelines, we can generate\nhigh-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method\ncan be extended to scene-level novel view synthesis, achieving precise control\nover the camera path with sparse input views. Extensive experiments demonstrate\nthe superior performance of the proposed approach, especially in terms of\ngeneration quality and multi-view consistency. Our code is available at\nhttps://github.com/heheyas/V3D"
    },
    {
        "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models",
        "id": "2403.06764",
        "summary": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV."
    },
    {
        "title": "VideoMamba: State Space Model for Efficient Video Understanding",
        "id": "2403.06977",
        "summary": "Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba."
    },
    {
        "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models",
        "id": "2403.06098",
        "summary": "The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, as well as other text-to-video diffusion models,\nhighly relies on the prompts, and there is no publicly available dataset\nfeaturing a study of text-to-video prompts. In this paper, we introduce\nVidProM, the first large-scale dataset comprising 1.67 million unique\ntext-to-video prompts from real users. Additionally, the dataset includes 6.69\nmillion videos generated by four state-of-the-art diffusion models and some\nrelated data. We initially demonstrate the curation of this large-scale\ndataset, which is a time-consuming and costly process. Subsequently, we show\nhow the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery\ndataset for image generation. Based on the analysis of these prompts, we\nidentify the necessity for a new prompt dataset specifically designed for\ntext-to-video generation and gain insights into the preferences of real users\nwhen creating videos. Our large-scale and diverse dataset also inspires many\nexciting new research areas. For instance, to develop better, more efficient,\nand safer text-to-video diffusion models, we suggest exploring text-to-video\nprompt engineering, efficient video generation, and video copy detection for\ndiffusion models. We make the collected dataset VidProM publicly available at\nGitHub and Hugging Face under the CC-BY- NC 4.0 License."
    },
    {
        "title": "Multistep Consistency Models",
        "id": "2403.06807",
        "summary": "Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas we show that a infty-step\nconsistency model is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation. We also show that\nour method scales to a text-to-image diffusion model, generating samples that\nare very close to the quality of the original model."
    },
    {
        "title": "Algorithmic progress in language models",
        "id": "2403.05812",
        "summary": "We investigate the rate at which algorithms for pre-training language models\nhave improved since the advent of deep learning. Using a dataset of over 200\nlanguage model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we\nfind that the compute required to reach a set performance threshold has halved\napproximately every 8 months, with a 95% confidence interval of around 5 to 14\nmonths, substantially faster than hardware gains per Moore's Law. We estimate\naugmented scaling laws, which enable us to quantify algorithmic progress and\ndetermine the relative contributions of scaling models versus innovations in\ntraining algorithms. Despite the rapid pace of algorithmic progress and the\ndevelopment of new architectures such as the transformer, our analysis reveals\nthat the increase in compute made an even larger contribution to overall\nperformance improvements over this time period. Though limited by noisy\nbenchmark data, our analysis quantifies the rapid progress in language\nmodeling, shedding light on the relative contributions from compute and\nalgorithms."
    },
    {
        "title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes\n  for One-shot Subject-Driven Generation",
        "id": "2403.06775",
        "summary": "Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain)."
    }
]