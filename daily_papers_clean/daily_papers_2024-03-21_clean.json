[
    {
        "title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework",
        "id": "2403.13248",
        "summary": "Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents."
    },
    {
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "id": "2403.13372",
        "summary": "Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks."
    },
    {
        "title": "Evolutionary Optimization of Model Merging Recipes",
        "id": "2403.13187",
        "summary": "We present a novel application of evolutionary algorithms to automate the\ncreation of powerful foundation models. While model merging has emerged as a\npromising approach for LLM development due to its cost-effectiveness, it\ncurrently relies on human intuition and domain knowledge, limiting its\npotential. Here, we propose an evolutionary approach that overcomes this\nlimitation by automatically discovering effective combinations of diverse\nopen-source models, harnessing their collective intelligence without requiring\nextensive additional training data or compute. Our approach operates in both\nparameter space and data flow space, allowing for optimization beyond just the\nweights of the individual models. This approach even facilitates cross-domain\nmerging, generating models like a Japanese LLM with Math reasoning\ncapabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art\nperformance on a variety of established Japanese LLM benchmarks, even\nsurpassing models with significantly more parameters, despite not being\nexplicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM\ngenerated through our approach demonstrates its effectiveness in describing\nJapanese culture-specific content, outperforming previous Japanese VLMs. This\nwork not only contributes new state-of-the-art models back to the open-source\ncommunity, but also introduces a new paradigm for automated model composition,\npaving the way for exploring alternative, efficient approaches to foundation\nmodel development."
    },
    {
        "title": "SceneScript: Reconstructing Scenes With An Autoregressive Structured\n  Language Model",
        "id": "2403.13064",
        "summary": "We introduce SceneScript, a method that directly produces full scene models\nas a sequence of structured language commands using an autoregressive,\ntoken-based approach. Our proposed scene representation is inspired by recent\nsuccesses in transformers & LLMs, and departs from more traditional methods\nwhich commonly describe scenes as meshes, voxel grids, point clouds or radiance\nfields. Our method infers the set of structured language commands directly from\nencoded visual data using a scene language encoder-decoder architecture. To\ntrain SceneScript, we generate and release a large-scale synthetic dataset\ncalled Aria Synthetic Environments consisting of 100k high-quality in-door\nscenes, with photorealistic and ground-truth annotated renders of egocentric\nscene walkthroughs. Our method gives state-of-the art results in architectural\nlayout estimation, and competitive results in 3D object detection. Lastly, we\nexplore an advantage for SceneScript, which is the ability to readily adapt to\nnew commands via simple additions to the structured language, which we\nillustrate for tasks such as coarse 3D object part reconstruction."
    },
    {
        "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS",
        "id": "2403.13806",
        "summary": "Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS."
    },
    {
        "title": "When Do We Not Need Larger Vision Models?",
        "id": "2403.13043",
        "summary": "Scaling up the size of vision models has been the de facto standard to obtain\nmore powerful visual representations. In this work, we discuss the point beyond\nwhich larger vision models are not necessary. First, we demonstrate the power\nof Scaling on Scales (S^2), whereby a pre-trained and frozen smaller vision\nmodel (e.g., ViT-B or ViT-L), run over multiple image scales, can outperform\nlarger models (e.g., ViT-H or ViT-G) on classification, segmentation, depth\nestimation, Multimodal LLM (MLLM) benchmarks, and robotic manipulation.\nNotably, S^2 achieves state-of-the-art performance in detailed understanding\nof MLLM on the V* benchmark, surpassing models such as GPT-4V. We examine the\nconditions under which S^2 is a preferred scaling approach compared to\nscaling on model size. While larger models have the advantage of better\ngeneralization on hard examples, we show that features of larger vision models\ncan be well approximated by those of multi-scale smaller models. This suggests\nmost, if not all, of the representations learned by current large pre-trained\nmodels can also be obtained from multi-scale smaller models. Our results show\nthat a multi-scale smaller model has comparable learning capacity to a larger\nmodel, and pre-training smaller models with S^2 can match or even exceed the\nadvantage of larger models. We release a Python package that can apply S^2 on\nany vision model with one line of code:\nhttps://github.com/bfshi/scaling_on_scales."
    },
    {
        "title": "IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models",
        "id": "2403.13535",
        "summary": "Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images."
    },
    {
        "title": "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models",
        "id": "2403.13447",
        "summary": "Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, e.g., LLaVA, transforms visual features into\ntext-like tokens using a static vision-language mapper, thereby enabling\nstatic LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the static tuning\nstrategy~The static tuning refers to the trained model with static\nparameters. that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA."
    },
    {
        "title": "DepthFM: Fast Monocular Depth Estimation with Flow Matching",
        "id": "2403.13788",
        "summary": "Monocular depth estimation is crucial for numerous downstream vision tasks\nand applications. Current discriminative approaches to this problem are limited\ndue to blurry artifacts, while state-of-the-art generative methods suffer from\nslow sampling due to their SDE nature. Rather than starting from noise, we seek\na direct mapping from input image to depth map. We observe that this can be\neffectively framed using flow matching, since its straight trajectories through\nsolution space offer efficiency and high quality. Our study demonstrates that a\npre-trained image diffusion model can serve as an adequate prior for a flow\nmatching depth model, allowing efficient training on only synthetic data to\ngeneralize to real images. We find that an auxiliary surface normals loss\nfurther improves the depth estimates. Due to the generative nature of our\napproach, our model reliably predicts the confidence of its depth estimates. On\nstandard benchmarks of complex natural scenes, our lightweight approach\nexhibits state-of-the-art performance at favorable low computational cost\ndespite only being trained on little synthetic data."
    },
    {
        "title": "Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos",
        "id": "2403.13044",
        "summary": "We propose a generative model that, given a coarsely edited image,\nsynthesizes a photorealistic output that follows the prescribed layout. Our\nmethod transfers fine details from the original image and preserves the\nidentity of its parts. Yet, it adapts it to the lighting and context defined by\nthe new layout. Our key insight is that videos are a powerful source of\nsupervision for this task: objects and camera motions provide many observations\nof how the world changes with viewpoint, lighting, and physical interactions.\nWe construct an image dataset in which each sample is a pair of source and\ntarget frames extracted from the same video at randomly chosen time intervals.\nWe warp the source frame toward the target using two motion models that mimic\nthe expected test-time user edits. We supervise our model to translate the\nwarped image into the ground truth, starting from a pretrained diffusion model.\nOur model design explicitly enables fine detail transfer from the source frame\nto the generated image, while closely following the user-specified layout. We\nshow that by using simple segmentations and coarse 2D manipulations, we can\nsynthesize a photorealistic edit faithful to the user's input while addressing\nsecond-order effects like harmonizing the lighting and physical interactions\nbetween edited objects."
    },
    {
        "title": "RewardBench: Evaluating Reward Models for Language Modeling",
        "id": "2403.13787",
        "summary": "Reward models (RMs) are at the crux of successful RLHF to align pretrained\nmodels to human preferences, yet there has been relatively little study that\nfocuses on evaluation of those reward models. Evaluating reward models presents\nan opportunity to understand the opaque technologies used for alignment of\nlanguage models and which values are embedded in them. To date, very few\ndescriptors of capabilities, training methods, or open-source reward models\nexist. In this paper, we present RewardBench, a benchmark dataset and code-base\nfor evaluation, to enhance scientific understanding of reward models. The\nRewardBench dataset is a collection of prompt-win-lose trios spanning chat,\nreasoning, and safety, to benchmark how reward models perform on challenging,\nstructured and out-of-distribution queries. We created specific comparison\ndatasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect\nfacts) why one answer should be preferred to another. On the RewardBench\nleaderboard, we evaluate reward models trained with a variety of methods, such\nas the direct MLE training of classifiers and the implicit reward modeling of\nDirect Preference Optimization (DPO), and on a spectrum of datasets. We present\nmany findings on propensity for refusals, reasoning limitations, and\ninstruction following shortcomings of various reward models towards a better\nunderstanding of the RLHF process."
    },
    {
        "title": "Reverse Training to Nurse the Reversal Curse",
        "id": "2403.13799",
        "summary": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue."
    },
    {
        "title": "Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific\n  Adaptation",
        "id": "2403.13745",
        "summary": "Video outpainting is a challenging task, aiming at generating video content\noutside the viewport of the input video while maintaining inter-frame and\nintra-frame consistency. Existing methods fall short in either generation\nquality or flexibility. We introduce MOTIA Mastering Video Outpainting Through\nInput-Specific Adaptation, a diffusion-based pipeline that leverages both the\nintrinsic data-specific patterns of the source video and the image/video\ngenerative prior for effective outpainting. MOTIA comprises two main phases:\ninput-specific adaptation and pattern-aware outpainting. The input-specific\nadaptation phase involves conducting efficient and effective pseudo outpainting\nlearning on the single-shot source video. This process encourages the model to\nidentify and learn patterns within the source video, as well as bridging the\ngap between standard generative processes and outpainting. The subsequent\nphase, pattern-aware outpainting, is dedicated to the generalization of these\nlearned patterns to generate outpainting outcomes. Additional strategies\nincluding spatial-aware insertion and noise travel are proposed to better\nleverage the diffusion model's generative prior and the acquired video patterns\nfrom source videos. Extensive evaluations underscore MOTIA's superiority,\noutperforming existing state-of-the-art methods in widely recognized\nbenchmarks. Notably, these advancements are achieved without necessitating\nextensive, task-specific tuning."
    },
    {
        "title": "ZigMa: Zigzag Mamba Diffusion Model",
        "id": "2403.13802",
        "summary": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ 1024times 1024 and UCF101, MultiModal-CelebA-HQ, and MS COCO\n256times 256. Code will be released at https://taohu.me/zigma/"
    },
    {
        "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
        "id": "2403.13501",
        "summary": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time."
    },
    {
        "title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image",
        "id": "2403.13524",
        "summary": "3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU."
    },
    {
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "id": "2401.13923",
        "summary": "Language Models (LMs) have greatly influenced diverse domains. However, their\ninherent limitation in comprehending 3D molecular structures has considerably\nconstrained their potential in the biomolecular domain. To bridge this gap, we\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\nencoder's representation space and the LM's input space. Moreover, to enhance\n3D-MoLM's ability of cross-modal molecular understanding and instruction\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\nand LM. It significantly surpasses existing baselines on downstream tasks,\nincluding molecule-text retrieval, molecule captioning, and more challenging\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties."
    },
    {
        "title": "Evaluating Frontier Models for Dangerous Capabilities",
        "id": "2403.13793",
        "summary": "To understand the risks posed by a new AI system, we must understand what it\ncan and cannot do. Building on prior work, we introduce a programme of new\n\"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our\nevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;\n(3) self-proliferation; and (4) self-reasoning. We do not find evidence of\nstrong dangerous capabilities in the models we evaluated, but we flag early\nwarning signs. Our goal is to help advance a rigorous science of dangerous\ncapability evaluation, in preparation for future models."
    }
]