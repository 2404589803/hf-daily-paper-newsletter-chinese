[
    {
        "title": "LLM Agent Operating System",
        "id": "2403.16971",
        "summary": "The integration and deployment of large language model (LLM)-based\nintelligent agents have been fraught with challenges that compromise their\nefficiency and efficacy. Among these issues are sub-optimal scheduling and\nresource allocation of agent requests over the LLM, the difficulties in\nmaintaining context during interactions between agent and LLM, and the\ncomplexities inherent in integrating heterogeneous agents with different\ncapabilities and specializations. The rapid increase of agent quantity and\ncomplexity further exacerbates these issues, often leading to bottlenecks and\nsub-optimal utilization of resources. Inspired by these challenges, this paper\npresents AIOS, an LLM agent operating system, which embeds large language model\ninto operating systems (OS). Specifically, AIOS is designed to optimize\nresource allocation, facilitate context switch across agents, enable concurrent\nexecution of agents, provide tool service for agents, and maintain access\ncontrol for agents. We present the architecture of such an operating system,\noutline the core challenges it aims to resolve, and provide the basic design\nand implementation of the AIOS. Our experiments on concurrent execution of\nmultiple agents demonstrate the reliability and efficiency of our AIOS modules.\nThrough this, we aim to not only improve the performance and efficiency of LLM\nagents but also to pioneer for better development and deployment of the AIOS\necosystem in the future. The project is open-source at\nhttps://github.com/agiresearch/AIOS."
    },
    {
        "title": "FlashFace: Human Image Personalization with High-fidelity Identity\n  Preservation",
        "id": "2403.17008",
        "summary": "This work presents FlashFace, a practical tool with which users can easily\npersonalize their own photos on the fly by providing one or a few reference\nface images and a text prompt. Our approach is distinguishable from existing\nhuman photo customization methods by higher-fidelity identity preservation and\nbetter instruction following, benefiting from two subtle designs. First, we\nencode the face identity into a series of feature maps instead of one image\ntoken as in prior arts, allowing the model to retain more details of the\nreference faces (e.g., scars, tattoos, and face shape ). Second, we introduce a\ndisentangled integration strategy to balance the text and image guidance during\nthe text-to-image generation process, alleviating the conflict between the\nreference faces and the text prompts (e.g., personalizing an adult into a\n\"child\" or an \"elder\"). Extensive experimental results demonstrate the\neffectiveness of our method on various applications, including human image\npersonalization, face swapping under language prompts, making virtual\ncharacters into real people, etc. Project Page:\nhttps://jshilong.github.io/flashface-page."
    },
    {
        "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image\n  Generation",
        "id": "2403.16990",
        "summary": "Text-to-image diffusion models have an unprecedented ability to generate\ndiverse and high-quality images. However, they often struggle to faithfully\ncapture the intended semantics of complex input prompts that include multiple\nsubjects. Recently, numerous layout-to-image extensions have been introduced to\nimprove user control, aiming to localize subjects represented by specific\ntokens. Yet, these methods often produce semantically inaccurate images,\nespecially when dealing with multiple semantically or visually similar\nsubjects. In this work, we study and analyze the causes of these limitations.\nOur exploration reveals that the primary issue stems from inadvertent semantic\nleakage between subjects in the denoising process. This leakage is attributed\nto the diffusion model's attention layers, which tend to blend the visual\nfeatures of different subjects. To address these issues, we introduce Bounded\nAttention, a training-free method for bounding the information flow in the\nsampling process. Bounded Attention prevents detrimental leakage among subjects\nand enables guiding the generation to promote each subject's individuality,\neven with complex multi-subject conditioning. Through extensive\nexperimentation, we demonstrate that our method empowers the generation of\nmultiple subjects that better align with given prompts and layouts."
    },
    {
        "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient\n  LLMs Under Compression",
        "id": "2403.15447",
        "summary": "Compressing high-capability Large Language Models (LLMs) has emerged as a\nfavored strategy for resource-efficient inferences. While state-of-the-art\n(SoTA) compression methods boast impressive advancements in preserving benign\ntask performance, the potential risks of compression in terms of safety and\ntrustworthiness have been largely neglected. This study conducts the first,\nthorough evaluation of three (3) leading LLMs using five (5) SoTA compression\ntechniques across eight (8) trustworthiness dimensions. Our experiments\nhighlight the intricate interplay between compression and trustworthiness,\nrevealing some interesting patterns. We find that quantization is currently a\nmore effective approach than pruning in achieving efficiency and\ntrustworthiness simultaneously. For instance, a 4-bit quantized model retains\nthe trustworthiness of its original counterpart, but model pruning\nsignificantly degrades trustworthiness, even at 50% sparsity. Moreover,\nemploying quantization within a moderate bit range could unexpectedly improve\ncertain trustworthiness dimensions such as ethics and fairness. Conversely,\nextreme quantization to very low bit levels (3 bits) tends to significantly\nreduce trustworthiness. This increased risk cannot be uncovered by looking at\nbenign performance alone, in turn, mandating comprehensive trustworthiness\nevaluation in practice. These findings culminate in practical recommendations\nfor simultaneously achieving high utility, efficiency, and trustworthiness in\nLLMs. Models and code are available at https://decoding-comp-trust.github.io/."
    },
    {
        "title": "TRIP: Temporal Residual Learning with Image Noise Prior for\n  Image-to-Video Diffusion Models",
        "id": "2403.17005",
        "summary": "Recent advances in text-to-video generation have demonstrated the utility of\npowerful diffusion models. Nevertheless, the problem is not trivial when\nshaping diffusion models to animate static image (i.e., image-to-video\ngeneration). The difficulty originates from the aspect that the diffusion\nprocess of subsequent animated frames should not only preserve the faithful\nalignment with the given image but also pursue temporal coherence among\nadjacent frames. To alleviate this, we present TRIP, a new recipe of\nimage-to-video diffusion paradigm that pivots on image noise prior derived from\nstatic image to jointly trigger inter-frame relational reasoning and ease the\ncoherent temporal modeling via temporal residual learning. Technically, the\nimage noise prior is first attained through one-step backward diffusion process\nbased on both static image and noised video latent codes. Next, TRIP executes a\nresidual-like dual-path scheme for noise prediction: 1) a shortcut path that\ndirectly takes image noise prior as the reference noise of each frame to\namplify the alignment between the first frame and subsequent frames; 2) a\nresidual path that employs 3D-UNet over noised video and static image latent\ncodes to enable inter-frame relational reasoning, thereby easing the learning\nof the residual noise for each frame. Furthermore, both reference and residual\nnoise of each frame are dynamically merged via attention mechanism for final\nvideo generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT\ndatasets demonstrate the effectiveness of our TRIP for image-to-video\ngeneration. Please see our project page at https://trip-i2v.github.io/TRIP/."
    },
    {
        "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
        "id": "2403.15484",
        "summary": "We introduce RakutenAI-7B, a suite of Japanese-oriented large language models\nthat achieve the best performance on the Japanese LM Harness benchmarks among\nthe open 7B models. Along with the foundation model, we release instruction-\nand chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat\nrespectively, under the Apache 2.0 license."
    },
    {
        "title": "VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation",
        "id": "2403.17001",
        "summary": "Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io."
    },
    {
        "title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions",
        "id": "2403.16627",
        "summary": "Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation."
    }
]