[
    {
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "id": "2403.07508",
        "summary": "The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets."
    },
    {
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
        "id": "2403.07816",
        "summary": "We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff."
    },
    {
        "title": "Chronos: Learning the Language of Time Series",
        "id": "2403.07815",
        "summary": "We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines."
    },
    {
        "title": "Synth^2: Boosting Visual-Language Models with Synthetic Captions and\n  Image Embeddings",
        "id": "2403.07750",
        "summary": "The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). We\npropose a novel approach that leverages the strengths of Large Language Models\n(LLMs) and image generation models to create synthetic image-text pairs for\nefficient and effective VLM training. Our method employs pretraining a\ntext-to-image model to synthesize image embeddings starting from captions\ngenerated by an LLM. These synthetic pairs are then used to train a VLM.\nExtensive experiments demonstrate that the VLM trained with synthetic data\nexhibits comparable performance on image captioning, while requiring a fraction\nof the data used by models trained solely on human-annotated data. In\nparticular, we outperform the baseline by 17% through augmentation with a\nsynthetic dataset. Furthermore, we show that synthesizing in the image\nembedding space is 25% faster than in the pixel space. This research introduces\na promising technique for generating large-scale, customizable image datasets,\nleading to enhanced VLM performance and wider applicability across various\ndomains, all with improved data efficiency and resource utilization."
    },
    {
        "title": "Motion Mamba: Efficient and Long Sequence Motion Generation with\n  Hierarchical and Bidirectional Selective SSM",
        "id": "2403.07487",
        "summary": "Human motion generation stands as a significant pursuit in generative\ncomputer vision, while achieving long-sequence and efficient motion generation\nremains challenging. Recent advancements in state space models (SSMs), notably\nMamba, have showcased considerable promise in long sequence modeling with an\nefficient hardware-aware design, which appears to be a promising direction to\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\ngeneration faces hurdles since the lack of a specialized design architecture to\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\nsimple and efficient approach that presents the pioneering motion generation\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\n(HTM) block to process temporal data by ensemble varying numbers of isolated\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\nblock to bidirectionally process latent poses, to enhance accurate motion\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\ncompared to the previous best diffusion-based method, which demonstrates strong\ncapabilities of high-quality long sequence motion modeling and real-time human\nmotion generation. See project website\nhttps://steve-zeyu-zhang.github.io/MotionMamba/"
    },
    {
        "title": "DragAnything: Motion Control for Anything using Entity Representation",
        "id": "2403.07420",
        "summary": "We introduce DragAnything, which utilizes a entity representation to achieve\nmotion control for any object in controllable video generation. Comparison to\nexisting motion control methods, DragAnything offers several advantages.\nFirstly, trajectory-based is more userfriendly for interaction, when acquiring\nother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\nneed to draw a line (trajectory) during interaction. Secondly, our entity\nrepresentation serves as an open-domain embedding capable of representing any\nobject, enabling the control of motion for diverse entities, including\nbackground. Lastly, our entity representation allows simultaneous and distinct\nmotion control for multiple objects. Extensive experiments demonstrate that our\nDragAnything achieves state-of-the-art performance for FVD, FID, and User\nStudy, particularly in terms of object motion control, where our method\nsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting."
    },
    {
        "title": "FAX: Scalable and Differentiable Federated Primitives in JAX",
        "id": "2403.07128",
        "summary": "We present FAX, a JAX-based library designed to support large-scale\ndistributed and federated computations in both data center and cross-device\napplications. FAX leverages JAX's sharding mechanisms to enable native\ntargeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX\nembeds building blocks for federated computations as primitives in JAX. This\nenables three key benefits. First, FAX computations can be translated to XLA\nHLO. Second, FAX provides a full implementation of federated automatic\ndifferentiation, greatly simplifying the expression of federated computations.\nLast, FAX computations can be interpreted out to existing production\ncross-device federated compute systems. We show that FAX provides an easily\nprogrammable, performant, and scalable framework for federated computations in\nthe data center. FAX is available at\nhttps://github.com/google-research/google-research/tree/master/fax ."
    },
    {
        "title": "Learning Generalizable Feature Fields for Mobile Manipulation",
        "id": "2403.07563",
        "summary": "An open problem in mobile manipulation is how to represent objects and scenes\nin a unified manner, so that robots can use it both for navigating in the\nenvironment and manipulating objects. The latter requires capturing intricate\ngeometry while understanding fine-grained semantics, whereas the former\ninvolves capturing the complexity inherit to an expansive physical scale. In\nthis work, we present GeFF (Generalizable Feature Fields), a scene-level\ngeneralizable neural feature field that acts as a unified representation for\nboth navigation and manipulation that performs in real-time. To do so, we treat\ngenerative novel view synthesis as a pre-training task, and then align the\nresulting rich scene priors with natural language via CLIP feature\ndistillation. We demonstrate the effectiveness of this approach by deploying\nGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\nability to generalize to open-set objects as well as running time, when\nperforming open-vocabulary mobile manipulation in dynamic scenes."
    }
]