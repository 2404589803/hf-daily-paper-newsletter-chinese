[
    {
        "title": "Advancing LLM Reasoning Generalists with Preference Trees",
        "id": "2404.02078",
        "summary": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model."
    },
    {
        "title": "Octopus v2: On-device language model for super agent",
        "id": "2404.01744",
        "summary": "Language models have shown effectiveness in a variety of software\napplications, particularly in tasks related to automatic workflow. These models\npossess the crucial ability to call functions, which is essential in creating\nAI agents. Despite the high performance of large-scale language models in cloud\nenvironments, they are often associated with concerns over privacy and cost.\nCurrent on-device models for function calling face issues with latency and\naccuracy. Our research presents a new method that empowers an on-device model\nwith 2 billion parameters to surpass the performance of GPT-4 in both accuracy\nand latency, and decrease the context length by 95\\%. When compared to Llama-7B\nwith a RAG-based function calling mechanism, our method enhances latency by\n35-fold. This method reduces the latency to levels deemed suitable for\ndeployment across a variety of edge devices in production environments,\naligning with the performance requisites for real-world applications."
    },
    {
        "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact\n  Language Model",
        "id": "2404.01331",
        "summary": "We train a suite of multimodal foundation models (MMFM) using the popular\nLLaVA framework with the recently released Gemma family of large language\nmodels (LLMs). Of particular interest is the 2B parameter Gemma model, which\nprovides opportunities to construct capable small-scale MMFMs. In line with\nfindings from other papers in this space, we test the effect of ablating three\ndesign features: pretraining the connector, utilizing a more powerful image\nbackbone, and increasing the size of the language backbone. The resulting\nmodels, which we call LLaVA-Gemma, exhibit moderate performance on an array of\nevaluations, but fail to improve past the current comparably sized SOTA models.\nCloser analysis of performance shows mixed effects; skipping pretraining tends\nto reduce performance, larger vision models sometimes improve performance, and\nincreasing language model size has inconsistent effects. We publicly release\ntraining recipes, code and weights for our models for the LLaVA-Gemma models."
    },
    {
        "title": "Long-context LLMs Struggle with Long In-context Learning",
        "id": "2404.02060",
        "summary": "Large Language Models (LLMs) have made significant strides in handling long\nsequences exceeding 32K tokens. However, their performance evaluation has\nlargely been confined to metrics like perplexity and synthetic tasks, which may\nnot fully capture their abilities in more nuanced, real-world scenarios. This\nstudy introduces a specialized benchmark (LIConBench) focusing on long\nin-context learning within the realm of extreme-label classification. We\nmeticulously selected six datasets with a label range spanning 28 to 174\nclasses covering different input (few-shot demonstration) length from 2K to\n50K. Our benchmark requires LLMs to comprehend the entire input to recognize\nthe massive label spaces to make correct prediction. We evaluate 13\nlong-context LLMs on our benchmarks. We find that the long-context LLMs perform\nrelatively well under the token length of 20K and the performance benefits from\nutilizing the long context window. However, after the context window exceeds\n20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap\nin current LLM capabilities for processing and understanding long, context-rich\nsequences. Further analysis revealed a tendency among models to favor\npredictions for labels presented towards the end at the sequence. Their ability\nto reason over multiple pieces in the long sequence is yet to be improved. Our\nstudy reveals that long context understanding and reasoning is still a\nchallenging task for the existing LLMs. We believe LIConBench could serve as a\nmore realistic evaluation for the future long context LLMs."
    },
    {
        "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion\n  Models",
        "id": "2404.01367",
        "summary": "We study the scaling properties of latent diffusion models (LDMs) with an\nemphasis on their sampling efficiency. While improved network architecture and\ninference algorithms have shown to effectively boost sampling efficiency of\ndiffusion models, the role of model size -- a critical determinant of sampling\nefficiency -- has not been thoroughly examined. Through empirical analysis of\nestablished text-to-image diffusion models, we conduct an in-depth\ninvestigation into how model size influences sampling efficiency across varying\nsampling steps. Our findings unveil a surprising trend: when operating under a\ngiven inference budget, smaller models frequently outperform their larger\nequivalents in generating high-quality results. Moreover, we extend our study\nto demonstrate the generalizability of the these findings by applying various\ndiffusion samplers, exploring diverse downstream tasks, evaluating\npost-distilled models, as well as comparing performance relative to training\ncompute. These findings open up new pathways for the development of LDM scaling\nstrategies which can be employed to enhance generative capabilities within\nlimited inference budgets."
    },
    {
        "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
        "id": "2404.02101",
        "summary": "Controllability plays a crucial role in video generation since it allows\nusers to create desired content. However, existing models largely overlooked\nthe precise control of camera pose that serves as a cinematic language to\nexpress deeper narrative nuances. To alleviate this issue, we introduce\nCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)\nmodels. After precisely parameterizing the camera trajectory, a plug-and-play\ncamera module is then trained on a T2V model, leaving others untouched.\nAdditionally, a comprehensive study on the effect of various datasets is also\nconducted, suggesting that videos with diverse camera distribution and similar\nappearances indeed enhance controllability and generalization. Experimental\nresults demonstrate the effectiveness of CameraCtrl in achieving precise and\ndomain-adaptive camera control, marking a step forward in the pursuit of\ndynamic and customized video storytelling from textual and camera pose inputs.\nOur project website is at: https://hehao13.github.io/projects-CameraCtrl/."
    },
    {
        "title": "Poro 34B and the Blessing of Multilinguality",
        "id": "2404.01856",
        "summary": "The pretraining of state-of-the-art large language models now requires\ntrillions of words of text, which is orders of magnitude more than available\nfor the vast majority of languages. While including text in more than one\nlanguage is an obvious way to acquire more pretraining data, multilinguality is\noften seen as a curse, and most model training efforts continue to focus\nnear-exclusively on individual large languages. We believe that multilinguality\ncan be a blessing and that it should be possible to substantially improve over\nthe capabilities of monolingual models for small languages through multilingual\ntraining. In this study, we introduce Poro 34B, a 34 billion parameter model\ntrained for 1 trillion tokens of Finnish, English, and programming languages,\nand demonstrate that a multilingual training approach can produce a model that\nnot only substantially advances over the capabilities of existing models for\nFinnish, but also excels in translation and is competitive in its class in\ngenerating English and programming languages. We release the model parameters,\nscripts, and data under open licenses at\nhttps://huggingface.co/LumiOpen/Poro-34B."
    },
    {
        "title": "HyperCLOVA X Technical Report",
        "id": "2404.01954",
        "summary": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored\nto the Korean language and culture, along with competitive capabilities in\nEnglish, math, and coding. HyperCLOVA X was trained on a balanced mix of\nKorean, English, and code data, followed by instruction-tuning with\nhigh-quality human-annotated datasets while abiding by strict safety guidelines\nreflecting our commitment to responsible AI. The model is evaluated across\nvarious benchmarks, including comprehensive reasoning, knowledge, commonsense,\nfactuality, coding, math, chatting, instruction-following, and harmlessness, in\nboth Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in\nKorean backed by a deep understanding of the language and cultural nuances.\nFurther analysis of the inherent bilingual nature and its extension to\nmultilingualism highlights the model's cross-lingual proficiency and strong\ngeneralization ability to untargeted languages, including machine translation\nbetween several language pairs and cross-lingual inference tasks. We believe\nthat HyperCLOVA X can provide helpful guidance for regions or countries in\ndeveloping their sovereign LLMs."
    },
    {
        "title": "Are large language models superhuman chemists?",
        "id": "2404.01475",
        "summary": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained. This is relevant for the chemical sciences, which face the\nproblem of small and diverse datasets that are frequently in the form of text.\nLLMs have shown promise in addressing these issues and are increasingly being\nharnessed to predict chemical properties, optimize reactions, and even design\nand conduct experiments autonomously. However, we still have only a very\nlimited systematic understanding of the chemical reasoning capabilities of\nLLMs, which would be required to improve models and mitigate potential harms.\nHere, we introduce \"ChemBench,\" an automated framework designed to rigorously\nevaluate the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of human chemists. We curated more than 7,000\nquestion-answer pairs for a wide array of subfields of the chemical sciences,\nevaluated leading open and closed-source LLMs, and found that the best models\noutperformed the best human chemists in our study on average. The models,\nhowever, struggle with some chemical reasoning tasks that are easy for human\nexperts and provide overconfident, misleading predictions, such as about\nchemicals' safety profiles. These findings underscore the dual reality that,\nalthough LLMs demonstrate remarkable proficiency in chemical tasks, further\nresearch is critical to enhancing their safety and utility in chemical\nsciences. Our findings also indicate a need for adaptations to chemistry\ncurricula and highlight the importance of continuing to develop evaluation\nframeworks to improve safe and useful LLMs."
    },
    {
        "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
        "id": "2404.02125",
        "summary": "We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images\ncapturing semantically similar objects. Given a collection of unlabeled\nInternet images, our goal is to associate the shared semantic parts from the\ninputs and aggregate the knowledge from 2D images to a shared 3D canonical\nspace. We introduce a general framework that tackles the task without assuming\nshape templates, poses, or any camera parameters. At its core is a canonical 3D\nrepresentation that encapsulates geometric and semantic information. The\nframework optimizes for the canonical representation together with the pose for\neach input image, and a per-image coordinate map that warps 2D pixel\ncoordinates to the 3D canonical frame to account for the shape matching. The\noptimization procedure fuses prior knowledge from a pre-trained image\ngenerative model and semantic information from input images. The former\nprovides strong knowledge guidance for this under-constraint task, while the\nlatter provides the necessary information to mitigate the training data bias\nfrom the pre-trained model. Our framework can be used for various tasks such as\ncorrespondence matching, pose estimation, and image editing, achieving strong\nresults on real-world image datasets under challenging illumination conditions\nand on in-the-wild online image collections."
    },
    {
        "title": "LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models",
        "id": "2404.01617",
        "summary": "We present LLM-ABR, the first system that utilizes the generative\ncapabilities of large language models (LLMs) to autonomously design adaptive\nbitrate (ABR) algorithms tailored for diverse network characteristics.\nOperating within a reinforcement learning framework, LLM-ABR empowers LLMs to\ndesign key components such as states and neural network architectures. We\nevaluate LLM-ABR across diverse network settings, including broadband,\nsatellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms."
    }
]