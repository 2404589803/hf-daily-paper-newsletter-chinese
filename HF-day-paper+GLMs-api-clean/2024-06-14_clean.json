[
    "1. \"Depth Anything V2\" - ID: 2406.09414\n2. \"An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels\" - ID: 2406.09415\n3. \"Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models\" - ID: 2406.09416\n4. \"Interpreting the Weight Space of Customized Diffusion Models\" - ID: 2406.09413\n5. \"OpenVLA: An Open-Source Vision-Language-Action Model\" - ID: 2406.09246\n6. \"Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning\" - ID: 2406.09170\n7. \"DiTFastAttn: Attention Compression for Diffusion Transformer Models\" - ID: 2406.08552\n8. \"MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding\" - ID: 2406.09411\n9. \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\" - ID: 2406.07522\n10. \"CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery\" - ID: 2406.08587\n11. \"Transformers meet Neural Algorithmic Reasoners\" - ID: 2406.09308\n12. \"Cognitively Inspired Energy-Based World Models\" - ID: 2406.08862\n13. \"EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts\" - ID: 2406.09162\n14. \"TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation\" - ID: 2406.08656\n15. \"mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus\" - ID: 2406.08707\n16. \"Explore the Limits of Omni-modal Pretraining at Scale\" - ID: 2406.09412\n17. \"Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs\" - ID: 2406.08657\n18. \"Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus\" - ID: 2406.08598\n19. \"Real3D: Scaling Up Large Reconstruction Models with Real-World Images\" - ID: 2406.08479\n20. \"Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation\" - ID: 2406.09305\n21. \"Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?\" - ID: 2406.07546\n22. \"LRM-Zero: Training Large Reconstruction Models with Synthesized Data\" - ID: 2406.09371\n23. \"CMC-Bench: Towards a New Paradigm of Visual Signal Compression\" - ID: 2406.09356\n24. \"MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding\" - ID: 2406.09297\n25. \"CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark\" - ID: 2406.05967\n26. \"Estimating the Hallucination Rate of Generative AI\" - ID: 2406.07457"
]