[
    {
        "paper": {
            "id": "2410.11623",
            "authors": [
                {
                    "_id": "6710b440eac1980726739204",
                    "user": {
                        "_id": "639003391c5a623727252735",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639003391c5a623727252735/fp_wZEhGzxGlyr64XF-x9.jpeg",
                        "isPro": false,
                        "fullname": "Sijie Cheng",
                        "user": "SijieCheng",
                        "type": "user"
                    },
                    "name": "Sijie Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:13.175Z",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac1980726739205",
                    "name": "Kechen Fang",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac1980726739206",
                    "name": "Yangyang Yu",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac1980726739207",
                    "name": "Sicheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac1980726739208",
                    "name": "Bohao Li",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac1980726739209",
                    "user": {
                        "_id": "6414061f996b2e426f23df48",
                        "avatarUrl": "/avatars/db0af49c5f017f7c455a076b8b2212b6.svg",
                        "isPro": false,
                        "fullname": "Ye Tian",
                        "user": "yetian",
                        "type": "user"
                    },
                    "name": "Ye Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:11.041Z",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac198072673920a",
                    "name": "Tingguang Li",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac198072673920b",
                    "name": "Lei Han",
                    "hidden": false
                },
                {
                    "_id": "6710b440eac198072673920c",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-15T14:08:53.000Z",
            "title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for\n  Embodied AI",
            "summary": "Recent advancements in Multi-modal Large Language Models (MLLMs) have opened\nnew avenues for applications in Embodied AI. Building on previous work,\nEgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating\negocentric video understanding capabilities. To bridge the gap between MLLMs\nand low-level control in Embodied AI, we design four key interrelated tasks:\nvideo question-answering, hierarchy planning, visual grounding and reward\nmodeling. To minimize manual annotation costs, we develop an automatic data\ngeneration pipeline based on the Ego4D dataset, leveraging the prior knowledge\nand multimodal capabilities of GPT-4o. Three human annotators then filter the\ngenerated data to ensure diversity and quality, resulting in the VidEgoThink\nbenchmark. We conduct extensive experiments with three types of models:\nAPI-based MLLMs, open-source image-based MLLMs, and open-source video-based\nMLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform\npoorly across all tasks related to egocentric video understanding. These\nfindings suggest that foundation models still require significant advancements\nto be effectively applied to first-person scenarios in Embodied AI. In\nconclusion, VidEgoThink reflects a research trend towards employing MLLMs for\negocentric vision, akin to human capabilities, enabling active observation and\ninteraction in the complex real-world environments.",
            "upvotes": 37,
            "discussionId": "6710b442eac19807267392d2"
        },
        "publishedAt": "2024-10-17T05:23:48.356Z",
        "title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11623.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639003391c5a623727252735/fp_wZEhGzxGlyr64XF-x9.jpeg",
            "fullname": "Sijie Cheng",
            "name": "SijieCheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12381",
            "authors": [
                {
                    "_id": "67107f8dde5ba28c43c8c3f9",
                    "user": {
                        "_id": "64c8d4ac120a85440bfad55d",
                        "avatarUrl": "/avatars/d65334288d69700250de51d0df344d9b.svg",
                        "isPro": false,
                        "fullname": "Fengji Zhang",
                        "user": "zfj1998",
                        "type": "user"
                    },
                    "name": "Fengji Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:37.248Z",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3fa",
                    "name": "Linquan Wu",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3fb",
                    "name": "Huiyu Bai",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3fc",
                    "name": "Guancheng Lin",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3fd",
                    "name": "Xiao Li",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3fe",
                    "name": "Xiao Yu",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c3ff",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c400",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "67107f8dde5ba28c43c8c401",
                    "name": "Jacky Keung",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T09:04:57.000Z",
            "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks",
            "summary": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.",
            "upvotes": 32,
            "discussionId": "67107f90de5ba28c43c8c510"
        },
        "publishedAt": "2024-10-17T01:49:58.303Z",
        "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12381.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d65334288d69700250de51d0df344d9b.svg",
            "fullname": "Fengji Zhang",
            "name": "zfj1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12787",
            "authors": [
                {
                    "_id": "6710899cb95dec8a173c7140",
                    "name": "Sicong Leng",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7141",
                    "name": "Yun Xing",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7142",
                    "user": {
                        "_id": "65b2529285b6c21448a10d65",
                        "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
                        "isPro": false,
                        "fullname": "Zesen Cheng",
                        "user": "ClownRat",
                        "type": "user"
                    },
                    "name": "Zesen Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:30.926Z",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7143",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7144",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7145",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7146",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7147",
                    "name": "Shijian Lu",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7148",
                    "name": "Chunyan Miao",
                    "hidden": false
                },
                {
                    "_id": "6710899cb95dec8a173c7149",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T17:59:02.000Z",
            "title": "The Curse of Multi-Modalities: Evaluating Hallucinations of Large\n  Multimodal Models across Language, Visual, and Audio",
            "summary": "Recent advancements in large multimodal models (LMMs) have significantly\nenhanced performance across diverse tasks, with ongoing efforts to further\nintegrate additional modalities such as video and audio. However, most existing\nLMMs remain vulnerable to hallucinations, the discrepancy between the factual\nmultimodal input and the generated textual output, which has limited their\napplicability in various real-world scenarios. This paper presents the first\nsystematic investigation of hallucinations in LMMs involving the three most\ncommon modalities: language, visual, and audio. Our study reveals two key\ncontributors to hallucinations: overreliance on unimodal priors and spurious\ninter-modality correlations. To address these challenges, we introduce the\nbenchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates\nhallucinations in LMMs, providing a detailed analysis of their underlying\nissues. Our findings highlight key vulnerabilities, including imbalances in\nmodality integration and biases from training data, underscoring the need for\nbalanced cross-modal learning and enhanced hallucination mitigation strategies.\nBased on our observations and findings, we suggest potential research\ndirections that could enhance the reliability of LMMs.",
            "upvotes": 24,
            "discussionId": "6710899eb95dec8a173c7200"
        },
        "publishedAt": "2024-10-17T02:21:58.907Z",
        "title": "The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12787.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg",
            "fullname": "Leng Sicong",
            "name": "Sicong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12409",
            "authors": [
                {
                    "_id": "67106ab8636030421b1ad24b",
                    "user": {
                        "_id": "62d65139667051e0a29bffe7",
                        "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
                        "isPro": false,
                        "fullname": "Jian Xie",
                        "user": "hsaest",
                        "type": "user"
                    },
                    "name": "Jian Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:25:59.002Z",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad24c",
                    "user": {
                        "_id": "6074ff65cb07b90d90c9e5ac",
                        "avatarUrl": "/avatars/770822e287821f8c6b284388f4145b93.svg",
                        "isPro": false,
                        "fullname": "Kexun Zhang",
                        "user": "kexunz",
                        "type": "user"
                    },
                    "name": "Kexun Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-17T01:39:05.772Z",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad24d",
                    "user": {
                        "_id": "606ed1884ffe81d1e03e81e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639375346654-606ed1884ffe81d1e03e81e5.png",
                        "isPro": false,
                        "fullname": "Jiangjie Chen",
                        "user": "jiangjiechen",
                        "type": "user"
                    },
                    "name": "Jiangjie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:25:56.929Z",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad24e",
                    "name": "Siyu Yuan",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad24f",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad250",
                    "name": "Yikai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad251",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "67106ab8636030421b1ad252",
                    "name": "Yanghua Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T09:44:38.000Z",
            "title": "Revealing the Barriers of Language Agents in Planning",
            "summary": "Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.",
            "upvotes": 19,
            "discussionId": "67106ab9636030421b1ad284"
        },
        "publishedAt": "2024-10-17T00:09:59.357Z",
        "title": "Revealing the Barriers of Language Agents in Planning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12409.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "fullname": "Jian Xie",
            "name": "hsaest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12613",
            "authors": [
                {
                    "_id": "6710757f66dbb164b08db625",
                    "name": "Yedi Hu",
                    "hidden": false
                },
                {
                    "_id": "6710757f66dbb164b08db626",
                    "name": "Yunzhi Yao",
                    "hidden": false
                },
                {
                    "_id": "6710757f66dbb164b08db627",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6710757f66dbb164b08db628",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "6710757f66dbb164b08db629",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T14:29:29.000Z",
            "title": "Exploring Model Kinship for Merging Large Language Models",
            "summary": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.",
            "upvotes": 17,
            "discussionId": "6710758066dbb164b08db672"
        },
        "publishedAt": "2024-10-17T00:55:42.831Z",
        "title": "Exploring Model Kinship for Merging Large Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/97LEUopx82A-BASM4N-Dm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12613.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10672",
            "authors": [
                {
                    "_id": "670e8861559ae9c01f9282cc",
                    "name": "Yahan Li",
                    "hidden": false
                },
                {
                    "_id": "670e8861559ae9c01f9282cd",
                    "user": {
                        "_id": "653b5cb0d994e992e2831539",
                        "avatarUrl": "/avatars/e6b917cb263f10a6d9c72ec539276428.svg",
                        "isPro": false,
                        "fullname": "xiatingyu",
                        "user": "xiatingyu",
                        "type": "user"
                    },
                    "name": "Tingyu Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-16T09:44:15.176Z",
                    "hidden": false
                },
                {
                    "_id": "670e8861559ae9c01f9282ce",
                    "name": "Yi Chang",
                    "hidden": false
                },
                {
                    "_id": "670e8861559ae9c01f9282cf",
                    "user": {
                        "_id": "670e57b3391f1a7021182bff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
                        "isPro": false,
                        "fullname": "Yuan Wu",
                        "user": "WhiteCatY",
                        "type": "user"
                    },
                    "name": "Yuan Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:26:54.978Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T16:15:57.000Z",
            "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
            "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}-norm \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.",
            "upvotes": 16,
            "discussionId": "670e8870559ae9c01f92867c"
        },
        "publishedAt": "2024-10-17T05:44:04.122Z",
        "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10672.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
            "fullname": "Yuan Wu",
            "name": "WhiteCatY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12628",
            "authors": [
                {
                    "_id": "671072e2b4ee492058f68981",
                    "name": "Zhiyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "671072e2b4ee492058f68982",
                    "name": "Hengrui Kang",
                    "hidden": false
                },
                {
                    "_id": "671072e2b4ee492058f68983",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "671072e2b4ee492058f68984",
                    "name": "Conghui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T14:50:47.000Z",
            "title": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse\n  Synthetic Data and Global-to-Local Adaptive Perception",
            "summary": "Document Layout Analysis is crucial for real-world document understanding\nsystems, but it encounters a challenging trade-off between speed and accuracy:\nmultimodal methods leveraging both text and visual features achieve higher\naccuracy but suffer from significant latency, whereas unimodal methods relying\nsolely on visual features offer faster processing speeds at the expense of\naccuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel\napproach that enhances accuracy while maintaining speed advantages through\ndocument-specific optimizations in both pre-training and model design. For\nrobust document pre-training, we introduce the Mesh-candidate BestFit\nalgorithm, which frames document synthesis as a two-dimensional bin packing\nproblem, generating the large-scale, diverse DocSynth-300K dataset.\nPre-training on the resulting DocSynth-300K dataset significantly improves\nfine-tuning performance across various document types. In terms of model\noptimization, we propose a Global-to-Local Controllable Receptive Module that\nis capable of better handling multi-scale variations of document elements.\nFurthermore, to validate performance across different document types, we\nintroduce a complex and challenging benchmark named DocStructBench. Extensive\nexperiments on downstream datasets demonstrate that DocLayout-YOLO excels in\nboth speed and accuracy. Code, data, and models are available at\nhttps://github.com/opendatalab/DocLayout-YOLO.",
            "upvotes": 15,
            "discussionId": "671072eab4ee492058f68cae"
        },
        "publishedAt": "2024-10-17T00:44:41.174Z",
        "title": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12628.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672388558183-noauth.jpeg",
            "fullname": "Bin Wang",
            "name": "wanderkid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12405",
            "authors": [
                {
                    "_id": "671076a0eac19807265e1f3f",
                    "user": {
                        "_id": "6530c7263976e5f4412ba737",
                        "avatarUrl": "/avatars/8b4d9d847a9e115c3da8cad629bd0a41.svg",
                        "isPro": false,
                        "fullname": "Jingming Zhuo",
                        "user": "JingmingZ",
                        "type": "user"
                    },
                    "name": "Jingming Zhuo",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-17T02:29:54.659Z",
                    "hidden": false
                },
                {
                    "_id": "671076a0eac19807265e1f40",
                    "user": {
                        "_id": "630716d11801ecc7d2595021",
                        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                        "isPro": false,
                        "fullname": "Songyang Zhang",
                        "user": "zsytony",
                        "type": "user"
                    },
                    "name": "Songyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:43.913Z",
                    "hidden": false
                },
                {
                    "_id": "671076a0eac19807265e1f41",
                    "name": "Xinyu Fang",
                    "hidden": false
                },
                {
                    "_id": "671076a0eac19807265e1f42",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:41.809Z",
                    "hidden": false
                },
                {
                    "_id": "671076a0eac19807265e1f43",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "671076a0eac19807265e1f44",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T09:38:13.000Z",
            "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
            "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .",
            "upvotes": 11,
            "discussionId": "671076a2eac19807265e1ff9"
        },
        "publishedAt": "2024-10-17T01:00:15.207Z",
        "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12405.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "fullname": "Songyang Zhang",
            "name": "zsytony",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08968",
            "authors": [
                {
                    "_id": "67110e21e15dea1149bdfc6b",
                    "name": "Jingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67110e21e15dea1149bdfc6c",
                    "name": "Ahmed Elgohary",
                    "hidden": false
                },
                {
                    "_id": "67110e21e15dea1149bdfc6d",
                    "name": "Ahmed Magooda",
                    "hidden": false
                },
                {
                    "_id": "67110e21e15dea1149bdfc6e",
                    "name": "Daniel Khashabi",
                    "hidden": false
                },
                {
                    "_id": "67110e21e15dea1149bdfc6f",
                    "name": "Benjamin Van Durme",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T16:38:01.000Z",
            "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse\n  Safety Requirements",
            "summary": "The current paradigm for safety alignment of large language models (LLMs)\nfollows a one-size-fits-all approach: the model refuses to interact with any\ncontent deemed unsafe by the model provider. This approach lacks flexibility in\nthe face of varying social norms across cultures and regions. In addition,\nusers may have diverse safety needs, making a model with static safety\nstandards too restrictive to be useful, as well as too costly to be re-aligned.\n  We propose Controllable Safety Alignment (CoSA), a framework designed to\nadapt models to diverse safety requirements without re-training. Instead of\naligning a fixed model, we align models to follow safety configs -- free-form\nnatural language descriptions of the desired safety behaviors -- that are\nprovided as part of the system prompt. To adjust model safety behavior,\nauthorized users only need to modify such safety configs at inference time. To\nenable that, we propose CoSAlign, a data-centric method for aligning LLMs to\neasily adapt to diverse safety configs. Furthermore, we devise a novel\ncontrollability evaluation protocol that considers both helpfulness and\nconfigured safety, summarizing them into CoSA-Score, and construct CoSApien, a\nhuman-authored benchmark that consists of real-world LLM use cases with diverse\nsafety requirements and corresponding evaluation prompts.\n  We show that CoSAlign leads to substantial gains of controllability over\nstrong baselines including in-context alignment. Our framework encourages\nbetter representation and adaptation to pluralistic human values in LLMs, and\nthereby increasing their practicality.",
            "upvotes": 9,
            "discussionId": "67110e22e15dea1149bdfcbf"
        },
        "publishedAt": "2024-10-17T11:46:21.484Z",
        "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08968.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
            "fullname": "Jack Zhang",
            "name": "jackzhang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.11817",
            "authors": [
                {
                    "_id": "67106d4b63f0a6c38b2633de",
                    "user": {
                        "_id": "63e8a1fc46574e63a2bf3e88",
                        "avatarUrl": "/avatars/b91f587920c9102cabeca77d72d33995.svg",
                        "isPro": false,
                        "fullname": "Luping Liu",
                        "user": "luping-liu",
                        "type": "user"
                    },
                    "name": "Luping Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:54.296Z",
                    "hidden": false
                },
                {
                    "_id": "67106d4b63f0a6c38b2633df",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "67106d4b63f0a6c38b2633e0",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "67106d4b63f0a6c38b2633e1",
                    "name": "Zehan Wang",
                    "hidden": false
                },
                {
                    "_id": "67106d4b63f0a6c38b2633e2",
                    "name": "Chongxuan Li",
                    "hidden": false
                },
                {
                    "_id": "67106d4b63f0a6c38b2633e3",
                    "name": "Dong Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-15T17:46:31.000Z",
            "title": "Improving Long-Text Alignment for Text-to-Image Diffusion Models",
            "summary": "The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning 512 times 512 Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-alpha and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.",
            "upvotes": 9,
            "discussionId": "67106d4e63f0a6c38b263509"
        },
        "publishedAt": "2024-10-17T00:20:12.844Z",
        "title": "Improving Long-Text Alignment for Text-to-Image Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11817.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b91f587920c9102cabeca77d72d33995.svg",
            "fullname": "Luping Liu",
            "name": "luping-liu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08584",
            "authors": [
                {
                    "_id": "670ca3f0977ff8095c05cb73",
                    "name": "Yefei He",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb74",
                    "name": "Feng Chen",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb75",
                    "name": "Jing Liu",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb76",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb77",
                    "name": "Hong Zhou",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb78",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:27:33.696Z",
                    "hidden": false
                },
                {
                    "_id": "670ca3f0977ff8095c05cb79",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T07:24:21.000Z",
            "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
            "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6times and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
            "upvotes": 8,
            "discussionId": "670ca3f1977ff8095c05cb90"
        },
        "publishedAt": "2024-10-17T03:40:12.490Z",
        "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08584.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.11081",
            "authors": [
                {
                    "_id": "670ff433d126c976117487b3",
                    "name": "Cheng Lu",
                    "hidden": false
                },
                {
                    "_id": "670ff433d126c976117487b4",
                    "name": "Yang Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T20:43:25.000Z",
            "title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
            "summary": "Consistency models (CMs) are a powerful class of diffusion-based generative\nmodels optimized for fast sampling. Most existing CMs are trained using\ndiscretized timesteps, which introduce additional hyperparameters and are prone\nto discretization errors. While continuous-time formulations can mitigate these\nissues, their success has been limited by training instability. To address\nthis, we propose a simplified theoretical framework that unifies previous\nparameterizations of diffusion models and CMs, identifying the root causes of\ninstability. Based on this analysis, we introduce key improvements in diffusion\nprocess parameterization, network architecture, and training objectives. These\nchanges enable us to train continuous-time CMs at an unprecedented scale,\nreaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm,\nusing only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on\nImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores\nwith the best existing diffusion models to within 10%.",
            "upvotes": 7,
            "discussionId": "670ff43bd126c97611748988"
        },
        "publishedAt": "2024-10-17T07:42:04.740Z",
        "title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11081.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677251053045-62fc758172a7ab50b4b89c8c.jpeg",
            "fullname": "Zhicheng Sun",
            "name": "feifeiobama",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07722",
            "authors": [
                {
                    "_id": "6710f99feea7a876d49d4088",
                    "name": "Thong Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6710f99feea7a876d49d4089",
                    "name": "Shubham Chatterjee",
                    "hidden": false
                },
                {
                    "_id": "6710f99feea7a876d49d408a",
                    "name": "Sean MacAvaney",
                    "hidden": false
                },
                {
                    "_id": "6710f99feea7a876d49d408b",
                    "name": "Iain Mackie",
                    "hidden": false
                },
                {
                    "_id": "6710f99feea7a876d49d408c",
                    "name": "Jeff Dalton",
                    "hidden": false
                },
                {
                    "_id": "6710f99feea7a876d49d408d",
                    "name": "Andrew Yates",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T08:41:34.000Z",
            "title": "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities",
            "summary": "Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained\ntransformers, which often split entities into nonsensical fragments. Splitting\nentities can reduce retrieval accuracy and limits the model's ability to\nincorporate up-to-date world knowledge not included in the training data. In\nthis work, we enhance the LSR vocabulary with Wikipedia concepts and entities,\nenabling the model to resolve ambiguities more effectively and stay current\nwith evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)\nhead, which leverages existing entity embeddings and an entity retrieval\ncomponent that identifies entities relevant to a query or document. We use the\nDyVo head to generate entity weights, which are then merged with word piece\nweights to create joint representations for efficient indexing and retrieval\nusing an inverted index. In experiments across three entity-rich document\nranking datasets, the resulting DyVo model substantially outperforms\nstate-of-the-art baselines.",
            "upvotes": 6,
            "discussionId": "6710f9a1eea7a876d49d4102"
        },
        "publishedAt": "2024-10-17T10:24:44.464Z",
        "title": "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07722.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599574940380-5efef195ff69163f6f59e647.jpeg",
            "fullname": "Andrew Yates",
            "name": "andrewyates",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09870",
            "authors": [
                {
                    "_id": "670df14b4968ab91d81a72e4",
                    "name": "Yein Park",
                    "hidden": false
                },
                {
                    "_id": "670df14b4968ab91d81a72e5",
                    "name": "Chanwoong Yoon",
                    "hidden": false
                },
                {
                    "_id": "670df14b4968ab91d81a72e6",
                    "name": "Jungwoo Park",
                    "hidden": false
                },
                {
                    "_id": "670df14b4968ab91d81a72e7",
                    "name": "Donghyeon Lee",
                    "hidden": false
                },
                {
                    "_id": "670df14b4968ab91d81a72e8",
                    "user": {
                        "_id": "64587be872b60ae7a3817858",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
                        "isPro": false,
                        "fullname": "Minbyul Jeong",
                        "user": "Minbyul",
                        "type": "user"
                    },
                    "name": "Minbyul Jeong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:26:59.904Z",
                    "hidden": false
                },
                {
                    "_id": "670df14b4968ab91d81a72e9",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-13T15:08:49.000Z",
            "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains",
            "summary": "Large language models (LLMs) have significantly impacted many aspects of our\nlives. However, assessing and ensuring their chronological knowledge remains\nchallenging. Existing approaches fall short in addressing the accumulative\nnature of knowledge, often relying on a single time stamp. To overcome this, we\nintroduce ChroKnowBench, a benchmark dataset designed to evaluate\nchronologically accumulated knowledge across three key aspects: multiple\ndomains, time dependency, temporal state. Our benchmark distinguishes between\nknowledge that evolves (e.g., scientific discoveries, amended laws) and\nknowledge that remain constant (e.g., mathematical truths, commonsense facts).\nBuilding on this benchmark, we present ChroKnowledge (Chronological\nCategorization of Knowledge), a novel sampling-based framework for evaluating\nand updating LLMs' non-parametric chronological knowledge. Our evaluation\nshows: (1) The ability of eliciting temporal knowledge varies depending on the\ndata format that model was trained on. (2) LLMs partially recall knowledge or\nshow a cut-off at temporal boundaries rather than recalling all aspects of\nknowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting\nto elicit chronological knowledge by traversing step-by-step through the\nsurrounding time spans. We observe that our framework successfully updates the\noverall knowledge across the entire timeline in both the biomedical domain\n(+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in\nrefining temporal knowledge. This non-parametric approach also enables\nknowledge updates not only in open-source models but also in proprietary LLMs,\nensuring comprehensive applicability across model types. We perform a\ncomprehensive analysis based on temporal characteristics of ChroKnowPrompt and\nvalidate the potential of various models to elicit intrinsic temporal knowledge\nthrough our method.",
            "upvotes": 4,
            "discussionId": "670df1504968ab91d81a7554"
        },
        "publishedAt": "2024-10-17T11:56:11.435Z",
        "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09870.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
            "fullname": "Minbyul Jeong",
            "name": "Minbyul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12109",
            "authors": [
                {
                    "_id": "67107aea2b203d5fcbbe5b5c",
                    "user": {
                        "_id": "65c0b5babf441a1953586df7",
                        "avatarUrl": "/avatars/b767ab08dc096e0aeac29acfcb26d39e.svg",
                        "isPro": false,
                        "fullname": "Arushi Goel",
                        "user": "goarushi27",
                        "type": "user"
                    },
                    "name": "Arushi Goel",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-17T02:48:12.423Z",
                    "hidden": false
                },
                {
                    "_id": "67107aea2b203d5fcbbe5b5d",
                    "name": "Karan Sapra",
                    "hidden": false
                },
                {
                    "_id": "67107aea2b203d5fcbbe5b5e",
                    "name": "Matthieu Le",
                    "hidden": false
                },
                {
                    "_id": "67107aea2b203d5fcbbe5b5f",
                    "name": "Rafael Valle",
                    "hidden": false
                },
                {
                    "_id": "67107aea2b203d5fcbbe5b60",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "67107aea2b203d5fcbbe5b61",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-15T23:16:28.000Z",
            "title": "OMCAT: Omni Context Aware Transformer",
            "summary": "Large Language Models (LLMs) have made significant strides in text generation\nand comprehension, with recent advancements extending into multimodal LLMs that\nintegrate visual and audio inputs. However, these models continue to struggle\nwith fine-grained, cross-modal temporal understanding, particularly when\ncorrelating events across audio and video streams. We address these challenges\nwith two key contributions: a new dataset and model, called OCTAV and OMCAT\nrespectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset\ndesigned to capture event transitions across audio and video. Second, OMCAT\n(Omni Context Aware Transformer) is a powerful model that leverages RoTE\n(Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal\ngrounding and computational efficiency in time-anchored tasks. Through a robust\nthree-stage training pipeline-feature alignment, instruction tuning, and\nOCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our\nmodel demonstrates state-of-the-art performance on Audio-Visual Question\nAnswering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in\ntemporal reasoning and cross-modal alignment, as validated through\ncomprehensive experiments and ablation studies. Our dataset and code will be\nmade publicly available. The link to our demo page is https://om-cat.github.io.",
            "upvotes": 4,
            "discussionId": "67107aec2b203d5fcbbe5bd1"
        },
        "publishedAt": "2024-10-17T01:18:30.867Z",
        "title": "OMCAT: Omni Context Aware Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12109.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12391",
            "authors": [
                {
                    "_id": "6710bd1963f0a6c38b3f9c53",
                    "user": {
                        "_id": "63ca9515145e6c9716fbf376",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca9515145e6c9716fbf376/BXt9rJ5nIisijzFVLymai.png",
                        "isPro": false,
                        "fullname": "Niels Horn",
                        "user": "nilq",
                        "type": "user"
                    },
                    "name": "Niels Horn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:23:56.379Z",
                    "hidden": false
                },
                {
                    "_id": "6710bd1963f0a6c38b3f9c54",
                    "name": "Desmond Elliott",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T09:18:39.000Z",
            "title": "Tracking Universal Features Through Fine-Tuning and Model Merging",
            "summary": "We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.",
            "upvotes": 3,
            "discussionId": "6710bd1a63f0a6c38b3f9ca9"
        },
        "publishedAt": "2024-10-17T10:10:35.292Z",
        "title": "Tracking Universal Features Through Fine-Tuning and Model Merging",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ca9515145e6c9716fbf376/IMMxhFHvjBYWgUwwZFRtm.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ca9515145e6c9716fbf376/OaXMHO-dr1kGGQnIEQcsn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12391.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca9515145e6c9716fbf376/BXt9rJ5nIisijzFVLymai.png",
            "fullname": "Niels Horn",
            "name": "nilq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12491",
            "authors": [
                {
                    "_id": "6710e1e2786f6ba453d1787f",
                    "name": "Jared Joselowitz",
                    "hidden": false
                },
                {
                    "_id": "6710e1e2786f6ba453d17880",
                    "name": "Arjun Jagota",
                    "hidden": false
                },
                {
                    "_id": "6710e1e2786f6ba453d17881",
                    "user": {
                        "_id": "6186fef1b1085ab638324e7f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
                        "isPro": false,
                        "fullname": "Satya",
                        "user": "skrishna",
                        "type": "user"
                    },
                    "name": "Satyapriya Krishna",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:23:44.325Z",
                    "hidden": false
                },
                {
                    "_id": "6710e1e2786f6ba453d17882",
                    "name": "Sonali Parbhoo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T12:14:25.000Z",
            "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL",
            "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
            "upvotes": 3,
            "discussionId": "6710e1e5786f6ba453d17953"
        },
        "publishedAt": "2024-10-17T08:37:56.165Z",
        "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12491.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
            "fullname": "Satya",
            "name": "skrishna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.11878",
            "authors": [
                {
                    "_id": "6710b99b3ff472e2be83ba05",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "6710b99b3ff472e2be83ba06",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T14:49:58.000Z",
            "title": "Neural Metamorphosis",
            "summary": "This paper introduces a new learning paradigm termed Neural Metamorphosis\n(NeuMeta), which aims to build self-morphable neural networks. Contrary to\ncrafting separate models for different architectures or sizes, NeuMeta directly\nlearns the continuous weight manifold of neural networks. Once trained, we can\nsample weights for any-sized network directly from the manifold, even for\npreviously unseen configurations, without retraining. To achieve this ambitious\ngoal, NeuMeta trains neural implicit functions as hypernetworks. They accept\ncoordinates within the model space as input, and generate corresponding weight\nvalues on the manifold. In other words, the implicit function is learned in a\nway, that the predicted weights is well-performed across various models sizes.\nIn training those models, we notice that, the final performance closely relates\non smoothness of the learned manifold. In pursuit of enhancing this smoothness,\nwe employ two strategies. First, we permute weight matrices to achieve\nintra-model smoothness, by solving the Shortest Hamiltonian Path problem.\nBesides, we add a noise on the input coordinates when training the implicit\nfunction, ensuring models with various sizes shows consistent outputs. As such,\nNeuMeta shows promising results in synthesizing parameters for various network\nconfigurations. Our extensive tests in image classification, semantic\nsegmentation, and image generation reveal that NeuMeta sustains full-size\nperformance even at a 75% compression rate.",
            "upvotes": 3,
            "discussionId": "6710b99f3ff472e2be83bb30"
        },
        "publishedAt": "2024-10-17T08:23:01.176Z",
        "title": "Neural Metamorphosis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11878.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "fullname": "Xingyi Yang",
            "name": "adamdad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12722",
            "authors": [
                {
                    "_id": "6710cf9452f304c3e265e2ab",
                    "name": "João Matos",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2ac",
                    "user": {
                        "_id": "63600b93d9e4214b9a67807c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63600b93d9e4214b9a67807c/BovBE1oiZPdU8Td9I2YIn.jpeg",
                        "isPro": false,
                        "fullname": "Shan Chen",
                        "user": "shanchen",
                        "type": "user"
                    },
                    "name": "Shan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:23:46.339Z",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2ad",
                    "name": "Siena Placino",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2ae",
                    "name": "Yingya Li",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2af",
                    "name": "Juan Carlos Climent Pardo",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b0",
                    "name": "Daphna Idan",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b1",
                    "name": "Takeshi Tohyama",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b2",
                    "name": "David Restrepo",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b3",
                    "name": "Luis F. Nakayama",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b4",
                    "name": "Jose M. M. Pascual-Leone",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b5",
                    "name": "Guergana Savova",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b6",
                    "name": "Hugo Aerts",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b7",
                    "name": "Leo A. Celi",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b8",
                    "name": "A. Ian Wong",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2b9",
                    "name": "Danielle S. Bitterman",
                    "hidden": false
                },
                {
                    "_id": "6710cf9452f304c3e265e2ba",
                    "name": "Jack Gallifant",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T16:31:24.000Z",
            "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for\n  multimodal language models evaluation",
            "summary": "Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.",
            "upvotes": 3,
            "discussionId": "6710cf9552f304c3e265e2e9"
        },
        "publishedAt": "2024-10-17T07:19:55.659Z",
        "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12722.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63600b93d9e4214b9a67807c/BovBE1oiZPdU8Td9I2YIn.jpeg",
            "fullname": "Shan Chen",
            "name": "shanchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09724",
            "authors": [
                {
                    "_id": "67107fcbf1279fe0dfcfbe96",
                    "name": "Jixuan Leng",
                    "hidden": false
                },
                {
                    "_id": "67107fcbf1279fe0dfcfbe97",
                    "user": {
                        "_id": "647b8885aba7062fe5c32000",
                        "avatarUrl": "/avatars/128ceae78490110ae41202851e84d58e.svg",
                        "isPro": false,
                        "fullname": "Banghua Zhu",
                        "user": "banghua",
                        "type": "user"
                    },
                    "name": "Chengsong Huang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-17T03:09:09.055Z",
                    "hidden": false
                },
                {
                    "_id": "67107fcbf1279fe0dfcfbe98",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "67107fcbf1279fe0dfcfbe99",
                    "user": {
                        "_id": "64efbf39b3610349e84db417",
                        "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
                        "isPro": false,
                        "fullname": "Jiaxin Huang",
                        "user": "teapot123",
                        "type": "user"
                    },
                    "name": "Jiaxin Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-17T11:24:34.435Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-13T04:48:40.000Z",
            "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
            "summary": "Language model calibration refers to the alignment between the confidence of\nthe model and the actual performance of its responses. While previous studies\npoint out the overconfidence phenomenon in Large Language Models (LLMs) and\nshow that LLMs trained with Reinforcement Learning from Human Feedback (RLHF)\nare overconfident with a more sharpened output probability, in this study, we\nreveal that RLHF tends to lead models to express verbalized overconfidence in\ntheir own responses. We investigate the underlying cause of this overconfidence\nand demonstrate that reward models used for Proximal Policy Optimization (PPO)\nexhibit inherent biases towards high-confidence scores regardless of the actual\nquality of responses. Building upon this insight, we propose two PPO variants:\nPPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated\nReward Calculation. PPO-M integrates explicit confidence scores in reward model\ntraining, which calibrates reward models to better capture the alignment\nbetween response quality and verbalized confidence. PPO-C adjusts the reward\nscore during PPO based on the difference between the current reward and the\nmoving average of past rewards. Both PPO-M and PPO-C can be seamlessly\nintegrated into the current PPO pipeline and do not require additional golden\nlabels. We evaluate our methods on both Llama3-8B and Mistral-7B across six\ndiverse datasets including multiple-choice and open-ended generation.\nExperiment results demonstrate that both of our methods can reduce calibration\nerror and maintain performance comparable to standard PPO. We further show that\nthey do not compromise model capabilities in open-ended conversation settings.",
            "upvotes": 1,
            "discussionId": "67107fd5f1279fe0dfcfc17e"
        },
        "publishedAt": "2024-10-17T14:29:03.668Z",
        "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09724.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
            "fullname": "Jiaxin Huang",
            "name": "teapot123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12490",
            "authors": [
                {
                    "_id": "6710bfb58bbf77ae7996d0e9",
                    "name": "Yongxin Zhu",
                    "hidden": false
                },
                {
                    "_id": "6710bfb58bbf77ae7996d0ea",
                    "name": "Bocheng Li",
                    "hidden": false
                },
                {
                    "_id": "6710bfb58bbf77ae7996d0eb",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6710bfb58bbf77ae7996d0ec",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "6710bfb58bbf77ae7996d0ed",
                    "name": "Linli Xu",
                    "hidden": false
                },
                {
                    "_id": "6710bfb58bbf77ae7996d0ee",
                    "name": "Lidong Bing",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T12:13:17.000Z",
            "title": "Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective",
            "summary": "Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling. Experimental results show that image autoregressive\nmodeling with our tokenizer (DiGIT) benefits both image understanding and image\ngeneration with the next token prediction principle, which is inherently\nstraightforward for GPT models but challenging for other generative models.\nRemarkably, for the first time, a GPT-style autoregressive model for images\noutperforms LDMs, which also exhibits substantial improvement akin to GPT when\nscaling up model size. Our findings underscore the potential of an optimized\nlatent space and the integration of discrete tokenization in advancing the\ncapabilities of image generative models. The code is available at\nhttps://github.com/DAMO-NLP-SG/DiGIT.",
            "upvotes": 1,
            "discussionId": "6710bfb78bbf77ae7996d192"
        },
        "publishedAt": "2024-10-17T13:58:54.421Z",
        "title": "Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/U44ztFayeNBYgHHtx01Xv.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/Rn_nOKmLzkVaGU698PRR9.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/AaNyxdprbOmffHoG3fqRA.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/YFpJDQnJbuKne7sXM9yvx.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/h-5ivD-BLjDWE_GMZYxSw.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/Gf2ssIVzVVKIZbVUXEeXu.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/637c7d8a88699fba70e1e1ff/GEnmRWeJ-ZMBLauCJ-50B.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12490.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
            "fullname": "yongxinzhu",
            "name": "zyx123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]