[
  {
    "paper": {
      "id": "2501.09732",
      "authors": [
        {
          "_id": "6789e1dfaa9f64e4af498482",
          "name": "Nanye Ma",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498483",
          "name": "Shangyuan Tong",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498484",
          "name": "Haolin Jia",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498485",
          "name": "Hexiang Hu",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498486",
          "name": "Yu-Chuan Su",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498487",
          "name": "Mingda Zhang",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498488",
          "name": "Xuan Yang",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af498489",
          "name": "Yandong Li",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af49848a",
          "name": "Tommi Jaakkola",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af49848b",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "6789e1dfaa9f64e4af49848c",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T18:30:37.000Z",
      "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps",
      "summary": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.",
      "upvotes": 9,
      "discussionId": "6789e1e4aa9f64e4af498679"
    },
    "publishedAt": "2025-01-16T23:52:15.279Z",
    "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09751",
      "authors": [
        {
          "_id": "6789f776766dd160379b89fb",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b89fc",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b89fd",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b89fe",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b89ff",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b8a00",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b8a01",
          "name": "Jiang Yong",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b8a02",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b8a03",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6789f776766dd160379b8a04",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T18:58:06.000Z",
      "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through\n  Thinking",
      "summary": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, utility, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, repetitive, and\nunoriginal outputs. To address these issues, we propose OmniThink, a machine\nwriting framework that emulates the human-like process of iterative expansion\nand reflection. The core idea behind OmniThink is to simulate the cognitive\nbehavior of learners as they progressively deepen their knowledge of the\ntopics. Experimental results demonstrate that OmniThink improves the knowledge\ndensity of generated articles without compromising metrics such as coherence\nand depth. Human evaluations and expert feedback further highlight the\npotential of OmniThink to address real-world challenges in the generation of\nlong-form articles.",
      "upvotes": 7,
      "discussionId": "6789f777766dd160379b8a39"
    },
    "publishedAt": "2025-01-17T01:24:01.494Z",
    "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dbaa6f5760d1530d7580d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
      "fullname": "Simeon Emanuilov",
      "name": "s-emanuilov",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09484",
      "authors": [
        {
          "_id": "6789f2795a84f1087bc9274a",
          "name": "Zhaocheng Liu",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc9274b",
          "name": "Quan Tu",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc9274c",
          "name": "Wen Ye",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc9274d",
          "name": "Yu Xiao",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc9274e",
          "name": "Zhishou Zhang",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc9274f",
          "name": "Hengfu Cui",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc92750",
          "name": "Yalun Zhu",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc92751",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc92752",
          "name": "Shizheng Li",
          "hidden": false
        },
        {
          "_id": "6789f2795a84f1087bc92753",
          "name": "Jian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T11:41:14.000Z",
      "title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient\n  Simulators",
      "summary": "Online medical consultation (OMC) restricts doctors to gathering patient\ninformation solely through inquiries, making the already complex sequential\ndecision-making process of diagnosis even more challenging. Recently, the rapid\nadvancement of large language models has demonstrated a significant potential\nto transform OMC. However, most studies have primarily focused on improving\ndiagnostic accuracy under conditions of relatively sufficient information,\nwhile paying limited attention to the \"inquiry\" phase of the consultation\nprocess. This lack of focus has left the relationship between \"inquiry\" and\n\"diagnosis\" insufficiently explored. In this paper, we first extract real\npatient interaction strategies from authentic doctor-patient conversations and\nuse these strategies to guide the training of a patient simulator that closely\nmirrors real-world behavior. By inputting medical records into our patient\nsimulator to simulate patient responses, we conduct extensive experiments to\nexplore the relationship between \"inquiry\" and \"diagnosis\" in the consultation\nprocess. Experimental results demonstrate that inquiry and diagnosis adhere to\nthe Liebig's law: poor inquiry quality limits the effectiveness of diagnosis,\nregardless of diagnostic capability, and vice versa. Furthermore, the\nexperiments reveal significant differences in the inquiry performance of\nvarious models. To investigate this phenomenon, we categorize the inquiry\nprocess into four types: (1) chief complaint inquiry; (2) specification of\nknown symptoms; (3) inquiry about accompanying symptoms; and (4) gathering\nfamily or medical history. We analyze the distribution of inquiries across the\nfour types for different models to explore the reasons behind their significant\nperformance differences. We plan to open-source the weights and related code of\nour patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.",
      "upvotes": 5,
      "discussionId": "6789f27a5a84f1087bc9279e"
    },
    "publishedAt": "2025-01-17T01:05:31.701Z",
    "title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60ec4a33375c1280fb422704",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633664276374-60ec4a33375c1280fb422704.png",
      "fullname": "Wang Yulong",
      "name": "wangyulong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09755",
      "authors": [
        {
          "_id": "6789e275810f471d6aa3d2fa",
          "name": "Philippe Hansen-Estruch",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d2fb",
          "name": "David Yan",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d2fc",
          "name": "Ching-Yao Chung",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d2fd",
          "name": "Orr Zohar",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d2fe",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d2ff",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d300",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d301",
          "name": "Sriram Vishwanath",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d302",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "6789e275810f471d6aa3d303",
          "name": "Xinlei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T18:59:04.000Z",
      "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and\n  Generation",
      "summary": "Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.",
      "upvotes": 4,
      "discussionId": "6789e27a810f471d6aa3d4e2"
    },
    "publishedAt": "2025-01-16T23:54:42.823Z",
    "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.08617",
      "authors": [
        {
          "_id": "6789d842f16a4dec461a2040",
          "name": "Kaiqu Liang",
          "hidden": false
        },
        {
          "_id": "6789d842f16a4dec461a2041",
          "name": "Haimin Hu",
          "hidden": false
        },
        {
          "_id": "6789d842f16a4dec461a2042",
          "name": "Ryan Liu",
          "hidden": false
        },
        {
          "_id": "6789d842f16a4dec461a2043",
          "name": "Thomas L. Griffiths",
          "hidden": false
        },
        {
          "_id": "6789d842f16a4dec461a2044",
          "name": "Jaime Fernández Fisac",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-15T06:33:15.000Z",
      "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
      "summary": "Generative AI systems like foundation models (FMs) must align well with human\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\nperformance using human judgments, existing RLHF pipelines predominantly rely\non immediate feedback, which can fail to accurately reflect the downstream\nimpact of an interaction on users' utility. We demonstrate that feedback based\non evaluators' foresight estimates of downstream consequences systematically\ninduces Goodhart's Law dynamics, incentivizing misaligned behaviors like\nsycophancy and deception and ultimately degrading user outcomes. To alleviate\nthis, we propose decoupling evaluation from prediction by refocusing RLHF on\nhindsight feedback. Our theoretical analysis reveals that conditioning\nevaluator feedback on downstream observations mitigates misalignment and\nimproves expected human utility, even when these observations are simulated by\nthe AI system itself. To leverage this insight in a practical alignment\nalgorithm, we introduce Reinforcement Learning from Hindsight Simulation\n(RLHS), which first simulates plausible consequences and then elicits feedback\nto assess what behaviors were genuinely beneficial in hindsight. We apply RLHS\nto two widely-employed online and offline preference optimization methods --\nProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --\nand show empirically that misalignment is significantly reduced with both\nmethods. Through an online human user study, we show that RLHS consistently\noutperforms RLHF in helping users achieve their goals and earns higher\nsatisfaction ratings, despite being trained solely with simulated hindsight\nfeedback. These results underscore the importance of focusing on long-term\nconsequences, even simulated ones, to mitigate misalignment in RLHF.",
      "upvotes": 4,
      "discussionId": "6789d844f16a4dec461a20dc"
    },
    "publishedAt": "2025-01-16T23:16:25.298Z",
    "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650237b5d1b0b0db4f29ae8a/TdAsH0rQE1qFCphiura5C.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650237b5d1b0b0db4f29ae8a",
      "avatarUrl": "/avatars/8f92cf8f3f1ddb45c2c58c4a59ce4633.svg",
      "fullname": "KAIQU LIANG",
      "name": "kaiquliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09756",
      "authors": [
        {
          "_id": "6789e9b78a27185f5084533a",
          "name": "Sumit Chaturvedi",
          "hidden": false
        },
        {
          "_id": "6789e9b78a27185f5084533b",
          "name": "Mengwei Ren",
          "hidden": false
        },
        {
          "_id": "6789e9b78a27185f5084533c",
          "name": "Yannick Hold-Geoffroy",
          "hidden": false
        },
        {
          "_id": "6789e9b78a27185f5084533d",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "6789e9b78a27185f5084533e",
          "name": "Julie Dorsey",
          "hidden": false
        },
        {
          "_id": "6789e9b78a27185f5084533f",
          "name": "Zhixin Shu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T18:59:48.000Z",
      "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to\n  Re-render Synthetic Faces",
      "summary": "We introduce SynthLight, a diffusion model for portrait relighting. Our\napproach frames image relighting as a re-rendering problem, where pixels are\ntransformed in response to changes in environmental lighting conditions. Using\na physically-based rendering engine, we synthesize a dataset to simulate this\nlighting-conditioned transformation with 3D head assets under varying lighting.\nWe propose two training and inference strategies to bridge the gap between the\nsynthetic and real image domains: (1) multi-task training that takes advantage\nof real human portraits without lighting labels; (2) an inference time\ndiffusion sampling procedure based on classifier-free guidance that leverages\nthe input portrait to better preserve details. Our method generalizes to\ndiverse real photographs and produces realistic illumination effects, including\nspecular highlights and cast shadows, while preserving the subject's identity.\nOur quantitative experiments on Light Stage data demonstrate results comparable\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\nimages showcase rich and unprecedented illumination effects. Project Page:\nhttps://vrroom.github.io/synthlight/",
      "upvotes": 3,
      "discussionId": "6789e9bd8a27185f50845514"
    },
    "publishedAt": "2025-01-17T00:25:22.582Z",
    "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09747",
      "authors": [
        {
          "_id": "6789e918e1b3fda757de947f",
          "name": "Karl Pertsch",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9480",
          "name": "Kyle Stachowicz",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9481",
          "name": "Brian Ichter",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9482",
          "name": "Danny Driess",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9483",
          "name": "Suraj Nair",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9484",
          "name": "Quan Vuong",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9485",
          "name": "Oier Mees",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9486",
          "name": "Chelsea Finn",
          "hidden": false
        },
        {
          "_id": "6789e918e1b3fda757de9487",
          "name": "Sergey Levine",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T18:57:04.000Z",
      "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
      "summary": "Autoregressive sequence models, such as Transformer-based vision-language\naction (VLA) policies, can be tremendously effective for capturing complex and\ngeneralizable robotic behaviors. However, such models require us to choose a\ntokenization of our continuous action signals, which determines how the\ndiscrete symbols predicted by the model map to continuous robot actions. We\nfind that current approaches for robot action tokenization, based on simple\nper-dimension, per-timestep binning schemes, typically perform poorly when\nlearning dexterous skills from high-frequency robot data. To address this\nchallenge, we propose a new compression-based tokenization scheme for robot\nactions, based on the discrete cosine transform. Our tokenization approach,\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\nautoregressive VLAs for highly dexterous and high-frequency tasks where\nstandard discretization methods fail completely. Based on FAST, we release\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\naction sequences, with diverse action spaces and control frequencies. Finally,\nwe show that, when combined with the pi0 VLA, our method can scale to training\non 10k hours of robot data and match the performance of diffusion VLAs, while\nreducing training time by up to 5x.",
      "upvotes": 2,
      "discussionId": "6789e91ae1b3fda757de94d3"
    },
    "publishedAt": "2025-01-17T00:22:48.005Z",
    "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09686",
      "authors": [
        {
          "_id": "6789e04652739943c940af38",
          "name": "Fengli Xu",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af39",
          "name": "Qianyue Hao",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3a",
          "name": "Zefang Zong",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3b",
          "name": "Jingwei Wang",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3c",
          "name": "Yunke Zhang",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3d",
          "name": "Jingyi Wang",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3e",
          "name": "Xiaochong Lan",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af3f",
          "name": "Jiahui Gong",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af40",
          "name": "Tianjian Ouyang",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af41",
          "name": "Fanjin Meng",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af42",
          "name": "Chenyang Shao",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af43",
          "name": "Yuwei Yan",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af44",
          "name": "Qinglong Yang",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af45",
          "name": "Yiwen Song",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af46",
          "name": "Sijian Ren",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af47",
          "name": "Xinyuan Hu",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af48",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af49",
          "name": "Jie Feng",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af4a",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "6789e04652739943c940af4b",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T17:37:58.000Z",
      "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models",
      "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
      "upvotes": 2,
      "discussionId": "6789e04752739943c940afa5"
    },
    "publishedAt": "2025-01-16T23:45:20.697Z",
    "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09686.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09503",
      "authors": [
        {
          "_id": "6789e8178a14e7d6ea183dd3",
          "name": "Junjie He",
          "hidden": false
        },
        {
          "_id": "6789e8178a14e7d6ea183dd4",
          "name": "Yuxiang Tuo",
          "hidden": false
        },
        {
          "_id": "6789e8178a14e7d6ea183dd5",
          "user": {
            "_id": "63b66df8889aa6707f167f5d",
            "avatarUrl": "/avatars/62248a70698a95f74ce7267ca42cacef.svg",
            "isPro": false,
            "fullname": "ashui",
            "user": "ashui",
            "type": "user"
          },
          "name": "Binghui Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-17T05:18:19.329Z",
          "hidden": false
        },
        {
          "_id": "6789e8178a14e7d6ea183dd6",
          "name": "Chongyang Zhong",
          "hidden": false
        },
        {
          "_id": "6789e8178a14e7d6ea183dd7",
          "name": "Yifeng Geng",
          "hidden": false
        },
        {
          "_id": "6789e8178a14e7d6ea183dd8",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T12:28:39.000Z",
      "title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in\n  Text-to-Image Generation",
      "summary": "Recently, large-scale generative models have demonstrated outstanding\ntext-to-image generation capabilities. However, generating high-fidelity\npersonalized images with specific subjects still presents challenges,\nespecially in cases involving multiple subjects. In this paper, we propose\nAnyStory, a unified approach for personalized subject generation. AnyStory not\nonly achieves high-fidelity personalization for single subjects, but also for\nmultiple subjects, without sacrificing subject fidelity. Specifically, AnyStory\nmodels the subject personalization problem in an \"encode-then-route\" manner. In\nthe encoding step, AnyStory utilizes a universal and powerful image encoder,\ni.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve\nhigh-fidelity encoding of subject features. In the routing step, AnyStory\nutilizes a decoupled instance-aware subject router to accurately perceive and\npredict the potential location of the corresponding subject in the latent\nspace, and guide the injection of subject conditions. Detailed experimental\nresults demonstrate the excellent performance of our method in retaining\nsubject details, aligning text descriptions, and personalizing for multiple\nsubjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .",
      "upvotes": 1,
      "discussionId": "6789e81b8a14e7d6ea183f16"
    },
    "publishedAt": "2025-01-17T00:18:47.834Z",
    "title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09503.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09433",
      "authors": [
        {
          "_id": "6789e73d7a7ce2b07045dd3e",
          "name": "Hwan Heo",
          "hidden": false
        },
        {
          "_id": "6789e73d7a7ce2b07045dd3f",
          "name": "Jangyeong Kim",
          "hidden": false
        },
        {
          "_id": "6789e73d7a7ce2b07045dd40",
          "name": "Seongyeong Lee",
          "hidden": false
        },
        {
          "_id": "6789e73d7a7ce2b07045dd41",
          "name": "Jeong A Wi",
          "hidden": false
        },
        {
          "_id": "6789e73d7a7ce2b07045dd42",
          "name": "Junyoung Choi",
          "hidden": false
        },
        {
          "_id": "6789e73d7a7ce2b07045dd43",
          "name": "Sangjun Ahn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T10:03:15.000Z",
      "title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation",
      "summary": "The synthesis of high-quality 3D assets from textual or visual inputs has\nbecome a central objective in modern generative modeling. Despite the\nproliferation of 3D generation algorithms, they frequently grapple with\nchallenges such as multi-view inconsistency, slow generation times, low\nfidelity, and surface reconstruction problems. While some studies have\naddressed some of these issues, a comprehensive solution remains elusive. In\nthis paper, we introduce CaPa, a carve-and-paint framework that\ngenerates high-fidelity 3D assets efficiently. CaPa employs a two-stage\nprocess, decoupling geometry generation from texture synthesis. Initially, a 3D\nlatent diffusion model generates geometry guided by multi-view inputs, ensuring\nstructural consistency across perspectives. Subsequently, leveraging a novel,\nmodel-agnostic Spatially Decoupled Attention, the framework synthesizes\nhigh-resolution textures (up to 4K) for a given geometry. Furthermore, we\npropose a 3D-aware occlusion inpainting algorithm that fills untextured\nregions, resulting in cohesive results across the entire model. This pipeline\ngenerates high-quality 3D assets in less than 30 seconds, providing\nready-to-use outputs for commercial applications. Experimental results\ndemonstrate that CaPa excels in both texture fidelity and geometric stability,\nestablishing a new standard for practical, scalable 3D asset generation.",
      "upvotes": 1,
      "discussionId": "6789e7437a7ce2b07045df3b"
    },
    "publishedAt": "2025-01-17T00:14:48.500Z",
    "title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5683
    },
    "isAuthorParticipating": false
  }
]