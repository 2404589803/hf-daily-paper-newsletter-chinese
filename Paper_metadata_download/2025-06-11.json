[
  {
    "paper": {
      "id": "2506.06751",
      "authors": [
        {
          "_id": "6848fecf42e4f9106973f315",
          "name": "Mikhail Salnikov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f316",
          "name": "Dmitrii Korzh",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f317",
          "name": "Ivan Lazichny",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f318",
          "name": "Elvir Karimov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f319",
          "name": "Artyom Iudin",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31a",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31b",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31c",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31d",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31e",
          "name": "Elena Tutubalina",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T10:45:17.000Z",
      "submittedOnDailyAt": "2025-06-11T02:53:12.616Z",
      "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
      "submittedOnDailyBy": {
        "_id": "62bd6c6baaf1480f1aa2222e",
        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
        "isPro": false,
        "fullname": "Mikhail Salnikov",
        "user": "msalnikov",
        "type": "user"
      },
      "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
      "upvotes": 19,
      "discussionId": "6848fed042e4f9106973f31f",
      "projectPage": "https://airi-institute.github.io/geopolitical_llm_bias",
      "githubRepo": "https://github.com/AIRI-Institute/geopolitical_llm_bias",
      "ai_summary": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.",
      "ai_keywords": [
        "LLMs",
        "geopolitical biases",
        "historical events",
        "national narratives",
        "debiasing prompts"
      ]
    },
    "publishedAt": "2025-06-07T06:45:17.000Z",
    "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
    "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bd6c6baaf1480f1aa2222e",
      "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
      "fullname": "Mikhail Salnikov",
      "name": "msalnikov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07927",
      "authors": [
        {
          "_id": "684794003ec10bdd8ab4de11",
          "name": "Jiayi Sheng",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de12",
          "name": "Luna Lyu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de13",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de14",
          "name": "Tony Xia",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de15",
          "name": "Alex Gu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de16",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de17",
          "name": "Pan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
      ],
      "publishedAt": "2025-06-09T16:43:38.000Z",
      "submittedOnDailyAt": "2025-06-11T04:15:25.994Z",
      "title": "Solving Inequality Proofs with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "60f5f68fa7fd83d025749234",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
        "isPro": true,
        "fullname": "Pan Lu",
        "user": "lupantech",
        "type": "user"
      },
      "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
      "upvotes": 7,
      "discussionId": "684794013ec10bdd8ab4de18",
      "ai_summary": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.",
      "ai_keywords": [
        "LLMs",
        "IneqMath",
        "bound estimation",
        "relation prediction",
        "theorem-guided reasoning",
        "self-refinement"
      ]
    },
    "publishedAt": "2025-06-09T12:43:38.000Z",
    "title": "Solving Inequality Proofs with Large Language Models",
    "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09040",
      "authors": [
        {
          "_id": "6848fff842e4f9106973f321",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f322",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f323",
          "name": "Yikun Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f324",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f325",
          "name": "Kaicheng Yu",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f326",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f327",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:57:50.000Z",
      "submittedOnDailyAt": "2025-06-11T05:55:58.221Z",
      "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
      "upvotes": 6,
      "discussionId": "6848fff842e4f9106973f328",
      "ai_summary": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.",
      "ai_keywords": [
        "autoregressive supervision",
        "large vision-language models (LVLMs)",
        "visual modality",
        "image captions",
        "autoregressive image generation",
        "multimodal learning",
        "semantic representation",
        "discrete semantic tokens",
        "multimodal understanding benchmarks",
        "LLaVA-1.5"
      ]
    },
    "publishedAt": "2025-06-10T13:57:50.000Z",
    "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
    "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08009",
      "authors": [
        {
          "_id": "68485e5b4fe3b60e21b258bd",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258be",
          "name": "Zhengqi Li",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258bf",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c0",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c1",
          "name": "Eli Shechtman",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
      ],
      "publishedAt": "2025-06-09T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-11T04:34:32.742Z",
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
      "submittedOnDailyBy": {
        "_id": "67492ee82ad3cfc108a41bbb",
        "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
        "isPro": false,
        "fullname": "Guande He",
        "user": "gdhe17",
        "type": "user"
      },
      "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
      "upvotes": 5,
      "discussionId": "68485e5b4fe3b60e21b258c2",
      "projectPage": "https://self-forcing.github.io/",
      "githubRepo": "https://github.com/guandeh17/Self-Forcing",
      "ai_summary": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.",
      "ai_keywords": [
        "Self Forcing",
        "autoregressive video diffusion models",
        "exposure bias",
        "denoising",
        "key-value (KV) caching",
        "autoregressive rollout",
        "holistic loss",
        "few-step diffusion model",
        "stochastic gradient truncation",
        "rolling KV cache mechanism",
        "video extrapolation"
      ]
    },
    "publishedAt": "2025-06-09T13:59:55.000Z",
    "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
    "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67492ee82ad3cfc108a41bbb",
      "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
      "fullname": "Guande He",
      "name": "gdhe17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08002",
      "authors": [
        {
          "_id": "6848f8c242e4f9106973f2f6",
          "name": "Aadarsh Sahoo",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f7",
          "name": "Vansh Tibrewal",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f8",
          "name": "Georgia Gkioxari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
      ],
      "publishedAt": "2025-06-09T17:59:37.000Z",
      "submittedOnDailyAt": "2025-06-11T02:05:41.608Z",
      "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
      "submittedOnDailyBy": {
        "_id": "638e5fc6485360fbdfeb1301",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
        "isPro": false,
        "fullname": "Aadarsh Sahoo",
        "user": "aadarsh99",
        "type": "user"
      },
      "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
      "upvotes": 5,
      "discussionId": "6848f8c242e4f9106973f2f9",
      "projectPage": "https://glab-caltech.github.io/kyvo/",
      "githubRepo": "https://github.com/AadSah/kyvo",
      "ai_summary": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.",
      "ai_keywords": [
        "autoregressive models",
        "LLM framework",
        "data representation",
        "modality-specific objectives",
        "3D rendering",
        "3D recognition",
        "instruction-following",
        "question-answering",
        "3D datasets",
        "quantized shape encodings"
      ]
    },
    "publishedAt": "2025-06-09T13:59:37.000Z",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e5fc6485360fbdfeb1301",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
      "fullname": "Aadarsh Sahoo",
      "name": "aadarsh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05167",
      "authors": [
        {
          "_id": "68468cb23ec10bdd8ab4db5b",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:11.745Z",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5c",
          "name": "Jinsu Kim",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5d",
          "name": "Dohyeon Lee",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5e",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:43:49.000Z",
      "submittedOnDailyAt": "2025-06-11T00:47:14.627Z",
      "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
      "upvotes": 5,
      "discussionId": "68468cb23ec10bdd8ab4db5f",
      "githubRepo": "https://github.com/ldilab/ECoRAG",
      "ai_summary": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "context compression",
        "evidentiality",
        "LLM",
        "Open-Domain Question Answering (ODQA)"
      ]
    },
    "publishedAt": "2025-06-05T11:43:49.000Z",
    "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08887",
      "authors": [
        {
          "_id": "6848ec1542e4f9106973f2ac",
          "name": "Leqi Shen",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ad",
          "name": "Guoqiang Gong",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ae",
          "name": "Tianxiang Hao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2af",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b0",
          "name": "Yifeng Zhang",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b1",
          "name": "Pengzhang Liu",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b2",
          "name": "Sicheng Zhao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b3",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b4",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:16:40.000Z",
      "submittedOnDailyAt": "2025-06-11T01:17:30.703Z",
      "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
      "submittedOnDailyBy": {
        "_id": "6364b81b3e248b1e28a68b26",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
        "isPro": false,
        "fullname": "LeqiShen",
        "user": "lunar677",
        "type": "user"
      },
      "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
      "upvotes": 4,
      "discussionId": "6848ec1642e4f9106973f2b5",
      "githubRepo": "https://github.com/LunarShen/DsicoVLA",
      "ai_summary": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.",
      "ai_keywords": [
        "parameter-efficient adaptation",
        "image-text pretraining model",
        "CLIP",
        "video-text retrieval",
        "vision",
        "language",
        "alignment",
        "Image-Video Features Fusion",
        "pseudo image captions",
        "Image-to-Video Alignment Distillation",
        "MSRVTT",
        "R@1"
      ]
    },
    "publishedAt": "2025-06-10T11:16:40.000Z",
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
    "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6364b81b3e248b1e28a68b26",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
      "fullname": "LeqiShen",
      "name": "lunar677",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07177",
      "authors": [
        {
          "_id": "6849036342e4f9106973f32a",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32b",
          "name": "Taekyung Ki",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32c",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32d",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32e",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32f",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f330",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
      ],
      "publishedAt": "2025-06-08T14:54:41.000Z",
      "submittedOnDailyAt": "2025-06-11T02:49:49.936Z",
      "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
      "upvotes": 4,
      "discussionId": "6849036342e4f9106973f331",
      "ai_summary": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.",
      "ai_keywords": [
        "diffusion models",
        "frame-level signals",
        "keyframes",
        "style reference images",
        "sketches",
        "depth maps",
        "latent processing",
        "latent optimization",
        "globally coherent video generation",
        "video models",
        "keyframe guidance",
        "stylization",
        "looping"
      ]
    },
    "publishedAt": "2025-06-08T10:54:41.000Z",
    "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
    "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07177.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04614",
      "authors": [
        {
          "_id": "684921e342e4f9106973f3e7",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e8",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e9",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ea",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3eb",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ec",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ed",
          "name": "Yutong Kou",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ee",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f0",
          "name": "Xiaoshan Yang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f1",
          "name": "Weiming Dong",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f2",
          "name": "Changsheng Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
      ],
      "publishedAt": "2025-06-05T04:12:36.000Z",
      "submittedOnDailyAt": "2025-06-11T04:59:22.770Z",
      "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
      "upvotes": 4,
      "discussionId": "684921e442e4f9106973f3f3",
      "githubRepo": "https://github.com/X-PLUG/MobileAgent",
      "ai_summary": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Suggestion-aware Gradient Relative Policy Optimization",
        "pre-operative critic mechanism",
        "reasoning-bootstrapping",
        "GUI automation",
        "GUI-Critic-R1",
        "GUI-Critic-Test",
        "GUI-Critic-Train"
      ]
    },
    "publishedAt": "2025-06-05T00:12:36.000Z",
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07932",
      "authors": [
        {
          "_id": "68487f6342e4f9106973f17a",
          "name": "Rishit Dagli",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17b",
          "name": "Yushi Guan",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17c",
          "name": "Sankeerth Durvasula",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17d",
          "name": "Mohammadreza Mofayezi",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17e",
          "name": "Nandita Vijaykumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T16:52:10.000Z",
      "submittedOnDailyAt": "2025-06-11T02:25:06.388Z",
      "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
      "submittedOnDailyBy": {
        "_id": "60796959c59d9e1697fa2324",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
        "isPro": false,
        "fullname": "Rishit Dagli",
        "user": "rishitdagli",
        "type": "user"
      },
      "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
      "upvotes": 1,
      "discussionId": "68487f6442e4f9106973f17f",
      "projectPage": "https://squeeze3d.github.io/",
      "ai_summary": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.",
      "ai_keywords": [
        "pre-trained 3D generative models",
        "latent spaces",
        "encode",
        "latent code",
        "mapping networks",
        "radiance fields",
        "synthetic data",
        "compression ratios",
        "visual quality",
        "compression latency",
        "decompression latency"
      ]
    },
    "publishedAt": "2025-06-09T12:52:10.000Z",
    "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
    "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60796959c59d9e1697fa2324",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
      "fullname": "Rishit Dagli",
      "name": "rishitdagli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07047",
      "authors": [
        {
          "_id": "68492bf142e4f9106973f411",
          "name": "Yu Xuejun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f412",
          "name": "Jianyuan Zhong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f413",
          "name": "Zijin Feng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f414",
          "name": "Pengyi Zhai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f415",
          "name": "Roozbeh Yousefzadeh",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f416",
          "name": "Wei Chong Ng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f417",
          "name": "Haoxiong Liu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f418",
          "name": "Ziyi Shou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f419",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41a",
          "name": "Yudong Zhou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41b",
          "name": "Claudia Beth Ong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41c",
          "name": "Austen Jeremy Sugiarto",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41d",
          "name": "Yaoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41e",
          "name": "Wai Ming Tai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41f",
          "name": "Huan Cao",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f420",
          "name": "Dongcai Lu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f421",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f422",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f423",
          "name": "Shen Xin",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f424",
          "name": "Zhenguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T09:04:14.000Z",
      "submittedOnDailyAt": "2025-06-11T05:45:24.452Z",
      "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
      "upvotes": 1,
      "discussionId": "68492bf142e4f9106973f425",
      "githubRepo": "https://github.com/Huawei-AI4Math/Mathesis"
    },
    "publishedAt": "2025-06-08T05:04:14.000Z",
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05928",
      "authors": [
        {
          "_id": "6847b3393ec10bdd8ab4df20",
          "user": {
            "_id": "65ea90741b0d7e029a3a1fb0",
            "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
            "isPro": false,
            "fullname": "cj",
            "user": "cajie",
            "type": "user"
          },
          "name": "Jie Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:50.209Z",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df21",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df22",
          "name": "Hongyang He",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df23",
          "name": "Rolan Yan",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df24",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df25",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df26",
          "name": "Dongping Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df27",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df28",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:54:19.000Z",
      "submittedOnDailyAt": "2025-06-11T01:28:46.209Z",
      "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "65ea90741b0d7e029a3a1fb0",
        "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
        "isPro": false,
        "fullname": "cj",
        "user": "cajie",
        "type": "user"
      },
      "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
      "upvotes": 1,
      "discussionId": "6847b3393ec10bdd8ab4df29",
      "githubRepo": "https://github.com/DCDmllm/MoA",
      "ai_summary": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "Mixture-of-Experts",
        "parameter-efficient fine-tuning",
        "Large Language Model",
        "homogeneous",
        "representation collapse",
        "expert load imbalance",
        "heterogeneous",
        "Mixture-of-Adapters",
        "soft MoA",
        "sparse MoA"
      ]
    },
    "publishedAt": "2025-06-06T05:54:19.000Z",
    "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
    "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ea90741b0d7e029a3a1fb0",
      "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
      "fullname": "cj",
      "name": "cajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05700",
      "authors": [
        {
          "_id": "6848de6e42e4f9106973f273",
          "user": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "name": "Yan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T01:39:59.328Z",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f274",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f275",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f276",
          "name": "Jeff Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T03:02:52.000Z",
      "submittedOnDailyAt": "2025-06-11T00:15:04.904Z",
      "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
      "upvotes": 1,
      "discussionId": "6848de6e42e4f9106973f277",
      "ai_summary": "RKEFino1, a regulation-aware LLM fine-tuned with financial domain knowledge, effectively handles compliance-critical tasks including QA and numerical NER.",
      "ai_keywords": [
        "Large language models",
        "financial reasoning",
        "Digital Regulatory Reporting",
        "regulation knowledge-enhanced",
        "fine-tuning",
        "domain knowledge",
        "XBRL",
        "CDM",
        "MOF",
        "QA tasks",
        "knowledge-based reasoning",
        "mathematical reasoning",
        "Numerical NER",
        "financial entities"
      ]
    },
    "publishedAt": "2025-06-05T23:02:52.000Z",
    "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
    "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]