[
  {
    "paper": {
      "id": "2502.14739",
      "authors": [
        {
          "_id": "67b7efc26348a1df80a8ae53",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae54",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae55",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae56",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae57",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae58",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae59",
          "name": "Kang Zhu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5a",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5b",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5c",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5d",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5e",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5f",
          "name": "Kaixing Deng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae60",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae61",
          "name": "Shian Jia",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae62",
          "name": "Sichao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae63",
          "name": "Yiyan Liao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae64",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae65",
          "name": "Qinrui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae66",
          "name": "Sirun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae67",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae68",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae69",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6a",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6c",
          "name": "Qiyao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6d",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6e",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6f",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae70",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae71",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae72",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae73",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae74",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae75",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae76",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae77",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae78",
          "name": "Yifan Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae79",
          "name": "Chengtuo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7a",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7b",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7c",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7d",
          "name": "Yun Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7e",
          "name": "Yaoru Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7f",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae80",
          "name": "Zhaoqun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae81",
          "name": "Tianhao Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae82",
          "name": "Chengdong Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae83",
          "name": "Hongquan Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae84",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae85",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae86",
          "name": "Zifan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae87",
          "name": "Qige Qi",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae88",
          "name": "Shi Qiu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8a",
          "name": "Yizhou Tan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8c",
          "name": "Chenqing Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8d",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8e",
          "name": "Yiya Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae90",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae91",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae92",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae93",
          "name": "Yuanhao Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae94",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae95",
          "name": "Chun Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae96",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae97",
          "name": "Xiyue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae98",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae99",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9a",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9b",
          "name": "Xiangyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9c",
          "name": "Chenghua Zhong",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9d",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9e",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9f",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea0",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea1",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea2",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea3",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea4",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea6",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea7",
          "name": "Shi Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea8",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea9",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaa",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeab",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeac",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aead",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeae",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaf",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb1",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:05:58.000Z",
      "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
      "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
      "upvotes": 53,
      "discussionId": "67b7efc66348a1df80a8afc8"
    },
    "publishedAt": "2025-02-20T22:15:33.133Z",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14786",
      "authors": [
        {
          "_id": "67b7ed0d58f6b70b18dda7b4",
          "name": "Michael Tschannen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b5",
          "name": "Alexey Gritsenko",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b6",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b7",
          "name": "Muhammad Ferjad Naeem",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b8",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b9",
          "name": "Nikhil Parthasarathy",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7ba",
          "name": "Talfan Evans",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bb",
          "name": "Lucas Beyer",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bc",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bd",
          "name": "Basil Mustafa",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7be",
          "name": "Olivier Hénaff",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bf",
          "name": "Jeremiah Harmsen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c0",
          "name": "Andreas Steiner",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c1",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:08:29.000Z",
      "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features",
      "summary": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).",
      "upvotes": 32,
      "discussionId": "67b7ed0e58f6b70b18dda7f4"
    },
    "publishedAt": "2025-02-20T22:33:22.039Z",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14382",
      "authors": [
        {
          "_id": "67b7ed3e58f6b70b18ddb4bc",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bd",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4be",
          "name": "Chengkun Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bf",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c0",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c1",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c2",
          "name": "Jiarong Xing",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c3",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c4",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:18:53.000Z",
      "title": "S*: Test Time Scaling for Code Generation",
      "summary": "Increasing test-time compute for LLMs shows promise across domains but\nremains underexplored in code generation, despite extensive study in math. In\nthis paper, we propose S*, the first hybrid test-time scaling framework that\nsubstantially improves the coverage and selection accuracy of generated code.\nS* extends the existing parallel scaling paradigm with sequential scaling to\npush performance boundaries. It further leverages a novel selection mechanism\nthat adaptively generates distinguishing inputs for pairwise comparison,\ncombined with execution-grounded information to robustly identify correct\nsolutions. We evaluate across 12 Large Language Models and Large Reasoning\nModel and show: (1) S* consistently improves performance across model families\nand sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables\nnon-reasoning models to surpass reasoning models - GPT-4o-mini with S*\noutperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts\nstate-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S*\nachieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be\navailable under https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 23,
      "discussionId": "67b7ed3f58f6b70b18ddb510"
    },
    "publishedAt": "2025-02-20T22:04:42.635Z",
    "title": "S*: Test Time Scaling for Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14258",
      "authors": [
        {
          "_id": "67b7fa96c3f48f8b3fc632fe",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc632ff",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63300",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63301",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63302",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T04:52:05.000Z",
      "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall\n  Time-specific Information",
      "summary": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads primarily responsible for\nprocessing temporal knowledge through circuit analysis. We confirm that these\nheads are present across multiple models, though their specific locations may\nvary, and their responses differ depending on the type of knowledge and its\ncorresponding years. Disabling these heads degrades the model's ability to\nrecall time-specific knowledge while maintaining its general capabilities\nwithout compromising time-invariant and question-answering performances.\nMoreover, the heads are activated not only numeric conditions (\"In 2004\") but\nalso textual aliases (\"In the year ...\"), indicating that they encode a\ntemporal dimension beyond simple numerical representation. Furthermore, we\nexpand the potential of our findings by demonstrating how temporal knowledge\ncan be edited by adjusting the values of these heads.",
      "upvotes": 16,
      "discussionId": "67b7fa9ac3f48f8b3fc63452"
    },
    "publishedAt": "2025-02-20T23:02:42.672Z",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14834",
      "authors": [
        {
          "_id": "67b7f3c4d00e69f10cff219e",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff219f",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a0",
          "name": "Daniel Zhang-Li",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a1",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a2",
          "name": "Jifan Yu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a3",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a4",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a5",
          "name": "Huiqin Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a8",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:47:36.000Z",
      "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models",
      "summary": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V",
      "upvotes": 14,
      "discussionId": "67b7f3c7d00e69f10cff2258"
    },
    "publishedAt": "2025-02-20T22:39:21.551Z",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14282",
      "authors": [
        {
          "_id": "67b7f5587f4d732dc469270e",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc469270f",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692710",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692711",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692712",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692713",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692714",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692715",
          "name": "Chunfeng Yuan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692716",
          "name": "Changsheng Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692717",
          "name": "Weiming Hu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692718",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T05:41:55.000Z",
      "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC",
      "summary": "In the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code will be publicly\navailable.",
      "upvotes": 10,
      "discussionId": "67b7f55b7f4d732dc46927c1"
    },
    "publishedAt": "2025-02-20T22:39:48.180Z",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14768",
      "authors": [
        {
          "_id": "67b7f08c357c2729ac20a81b",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81c",
          "name": "Zitian Gao",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81d",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81e",
          "name": "Haoming Luo",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81f",
          "name": "Yuqian Hong",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a820",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a821",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a822",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a823",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a824",
          "name": "Chong Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:49:26.000Z",
      "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning",
      "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
      "upvotes": 10,
      "discussionId": "67b7f08e357c2729ac20a88f"
    },
    "publishedAt": "2025-02-20T22:19:05.902Z",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14372",
      "authors": [
        {
          "_id": "67b81870cc6b0136b3d84254",
          "user": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "isPro": false,
            "fullname": "Austin He",
            "user": "basil2115",
            "type": "user"
          },
          "name": "Austin Yubo He",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
          "hidden": false
        },
        {
          "_id": "67b81870cc6b0136b3d84255",
          "name": "Zi-Wen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:05:34.000Z",
      "title": "Discovering highly efficient low-weight quantum error-correcting codes\n  with reinforcement learning",
      "summary": "The realization of scalable fault-tolerant quantum computing is expected to\nhinge on quantum error-correcting codes. In the quest for more efficient\nquantum fault tolerance, a critical code parameter is the weight of\nmeasurements that extract information about errors to enable error correction:\nas higher measurement weights require higher implementation costs and introduce\nmore errors, it is important in code design to optimize measurement weight.\nThis underlies the surging interest in quantum low-density parity-check (qLDPC)\ncodes, the study of which has primarily focused on the asymptotic\n(large-code-limit) properties. In this work, we introduce a versatile and\ncomputationally efficient approach to stabilizer code weight reduction based on\nreinforcement learning (RL), which produces new low-weight codes that\nsubstantially outperform the state of the art in practically relevant parameter\nregimes, extending significantly beyond previously accessible small distances.\nFor example, our approach demonstrates savings in physical qubit overhead\ncompared to existing results by 1 to 2 orders of magnitude for weight 6 codes\nand brings the overhead into a feasible range for near-future experiments. We\nalso investigate the interplay between code parameters using our RL framework,\noffering new insights into the potential efficiency and power of practically\nviable coding strategies. Overall, our results demonstrate how RL can\neffectively advance the crucial yet challenging problem of quantum code\ndiscovery and thereby facilitate a faster path to the practical implementation\nof fault-tolerant quantum technologies.",
      "upvotes": 5,
      "discussionId": "67b81873cc6b0136b3d8430a"
    },
    "publishedAt": "2025-02-21T01:11:34.971Z",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6530a78069751712276d60ed",
      "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
      "fullname": "Austin He",
      "name": "basil2115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14844",
      "authors": [
        {
          "_id": "67b7f5ee8b3dff28b749be78",
          "name": "Rameen Abdal",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be79",
          "name": "Or Patashnik",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7a",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7b",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7c",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7e",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7f",
          "name": "Kfir Aberman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:53:39.000Z",
      "title": "Dynamic Concepts Personalization from Single Videos",
      "summary": "Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.",
      "upvotes": 5,
      "discussionId": "67b7f5f18b3dff28b749bf45"
    },
    "publishedAt": "2025-02-20T22:41:47.210Z",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14846",
      "authors": [
        {
          "_id": "67b7f4f1b15c19d57189fc5e",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc5f",
          "name": "Ajay Patel",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc60",
          "name": "Matt Deitke",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc61",
          "name": "Tanmay Gupta",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc62",
          "name": "Luca Weihs",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc63",
          "name": "Andrew Head",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc64",
          "name": "Mark Yatskar",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc65",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc66",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc67",
          "name": "Aniruddha Kembhavi",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc68",
          "name": "Christopher Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:55:30.000Z",
      "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
      "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
      "upvotes": 4,
      "discussionId": "67b7f4f2b15c19d57189fc95"
    },
    "publishedAt": "2025-02-20T22:38:36.406Z",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14499",
      "authors": [
        {
          "_id": "67b7ee1dfedfe971271dcca0",
          "user": {
            "_id": "6114c9fae7a2566ae7d1a1a7",
            "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
            "isPro": false,
            "fullname": "Deepak Nathani",
            "user": "dnathani",
            "type": "user"
          },
          "name": "Deepak Nathani",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca1",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca2",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca3",
          "name": "Nikolay Bashlykov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca4",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca5",
          "name": "Vincent Moens",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca6",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca7",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca8",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca9",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaa",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccab",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccac",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccad",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccae",
          "name": "Yoram Bachrach",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaf",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccb0",
          "user": {
            "_id": "633e94793a17ab61de8e2b9c",
            "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
            "isPro": false,
            "fullname": "Roberta Raileanu",
            "user": "rraileanu",
            "type": "user"
          },
          "name": "Roberta Raileanu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:28:23.000Z",
      "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
      "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents.",
      "upvotes": 4,
      "discussionId": "67b7ee1ffedfe971271dcd3a"
    },
    "publishedAt": "2025-02-20T22:08:38.225Z",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14669",
      "authors": [
        {
          "_id": "67b7eeddaf9f1b1bd95b878b",
          "name": "Alan Dao",
          "hidden": false
        },
        {
          "_id": "67b7eeddaf9f1b1bd95b878c",
          "name": "Dinh Bach Vu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T16:05:18.000Z",
      "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
      "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
      "upvotes": 3,
      "discussionId": "67b7eeddaf9f1b1bd95b87c8"
    },
    "publishedAt": "2025-02-20T22:11:45.130Z",
    "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14377",
      "authors": [
        {
          "_id": "67b7f350357c2729ac216494",
          "name": "Ke Cao",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216495",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216496",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216497",
          "name": "Jiasong Feng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216498",
          "name": "Zhanjie Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216499",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649a",
          "name": "Shanyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649b",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649c",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649d",
          "name": "Yuhui Yin",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649e",
          "name": "Jie Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:10:05.000Z",
      "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
      "summary": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta. More examples are available\nat https://relactrl.github.io/RelaCtrl/.",
      "upvotes": 2,
      "discussionId": "67b7f354357c2729ac216582"
    },
    "publishedAt": "2025-02-20T22:30:51.542Z",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6158
    },
    "isAuthorParticipating": false
  }
]