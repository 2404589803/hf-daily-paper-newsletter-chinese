[
  {
    "paper": {
      "id": "2510.19338",
      "authors": [
        {
          "_id": "68f98d4fb9b2e4ae046737dc",
          "name": "Ling Team",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737dd",
          "name": "Bin Han",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737de",
          "name": "Caizhi Tang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737df",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e0",
          "name": "Donghao Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e1",
          "name": "Fan Yuan",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e2",
          "name": "Feng Zhu",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e3",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e4",
          "name": "Jingyu Hu",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e5",
          "name": "Longfei Li",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e6",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e7",
          "name": "Mingyang Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e8",
          "name": "Peijie Jiang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e9",
          "name": "Peng Jiao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ea",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737eb",
          "name": "Qingyuan Yang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ec",
          "name": "Wenbo Shen",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ed",
          "name": "Xinxing Yang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ee",
          "name": "Yalin Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ef",
          "name": "Yankun Ren",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f0",
          "name": "Yao Zhao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f1",
          "name": "Yibo Cao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f2",
          "name": "Yixuan Sun",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f3",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f4",
          "name": "Yuchen Fang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f5",
          "name": "Zibin Lin",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f6",
          "name": "Zixuan Cheng",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f7",
          "name": "Jun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:59:38.000Z",
      "submittedOnDailyAt": "2025-10-23T00:35:16.243Z",
      "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
      "upvotes": 51,
      "discussionId": "68f98d50b9b2e4ae046737f8",
      "ai_summary": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.",
      "ai_keywords": [
        "linear attention",
        "softmax attention",
        "hybrid architecture",
        "FP8 operator library",
        "reinforcement learning",
        "SOTA performance"
      ],
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-22T03:59:38.000Z",
    "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
    "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19338.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18927",
      "authors": [
        {
          "_id": "68f9a187b9b2e4ae04673870",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673871",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673872",
          "name": "Yang Nan",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673873",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673874",
          "name": "Junrui Shen",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673875",
          "name": "Wenxiang Chen",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673876",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673877",
          "name": "Jixuan Huang",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673878",
          "name": "Zhihao Zhang",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673879",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387a",
          "name": "Xun Deng",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387b",
          "name": "Zhikai Lei",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387c",
          "name": "Miao Zheng",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387d",
          "name": "Guoteng Wang",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387e",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae0467387f",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673880",
          "name": "Rui Zheng",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673881",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673882",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673883",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "68f9a187b9b2e4ae04673884",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T12:55:04.000Z",
      "submittedOnDailyAt": "2025-10-23T02:07:16.010Z",
      "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
      "submittedOnDailyBy": {
        "_id": "653a6e5cae155b92bae77b74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
        "isPro": false,
        "fullname": "Zhiheng Xi",
        "user": "WooooDyy",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.",
      "upvotes": 49,
      "discussionId": "68f9a187b9b2e4ae04673885",
      "projectPage": "https://github.com/WooooDyy/BAPO",
      "githubRepo": "https://github.com/WooooDyy/BAPO",
      "ai_summary": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "off-policy settings",
        "policy entropy",
        "policy gradient",
        "Entropy-Clip Rule",
        "PPO-like objectives",
        "gradient explosions",
        "sample replay",
        "partial rollout",
        "AIME 2024",
        "AIME 2025",
        "SkyWork-OR1-7B",
        "o3-mini",
        "Gemini-2.5-Flash-Thinking"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-10-21T08:55:04.000Z",
    "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
    "summary": "Reinforcement learning (RL) has recently become the core paradigm for\naligning and strengthening large language models (LLMs). Yet, applying RL in\noff-policy settings--where stale data from past policies are used for\ntraining--improves sample efficiency, but remains challenging: policy entropy\ndeclines sharply, optimization often becomes unstable and may even collapse.\nThrough theoretical and empirical analysis, we identify two key insights: (i)\nan imbalance in optimization, where negative-advantage samples dominate the\npolicy gradient, suppressing useful behaviors and risking gradient explosions;\nand (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping\nmechanism in PPO-like objectives systematically blocks entropy-increasing\nupdates, thereby driving the policy toward over-exploitation at the expense of\nexploration. Building on these insights, we propose BAlanced Policy\nOptimization with Adaptive Clipping (BAPO), a simple yet effective method that\ndynamically adjusts clipping bounds to adaptively re-balance positive and\nnegative contributions, preserve entropy, and stabilize RL optimization. Across\ndiverse off-policy scenarios--including sample replay and partial rollout--BAPO\nachieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025\nbenchmarks, our 7B BAPO model surpasses open-source counterparts such as\nSkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art\nresults among models of the same scale but also outperforms leading proprietary\nsystems like o3-mini and Gemini-2.5-Flash-Thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a6e5cae155b92bae77b74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
      "fullname": "Zhiheng Xi",
      "name": "WooooDyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19363",
      "authors": [
        {
          "_id": "68f9884db9b2e4ae0467374e",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae0467374f",
          "name": "Gaokai Zhang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673750",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673751",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673752",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673753",
          "name": "Dongyao Chen",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673754",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T08:35:28.000Z",
      "submittedOnDailyAt": "2025-10-23T00:18:13.806Z",
      "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
      "submittedOnDailyBy": {
        "_id": "62b0009c72043b05d29492b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
        "isPro": false,
        "fullname": "Li Lyna Zhang",
        "user": "lynazhang",
        "type": "user"
      },
      "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
      "upvotes": 29,
      "discussionId": "68f9884db9b2e4ae04673755",
      "ai_summary": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "LoongRL",
        "KeyChain",
        "multi-hop QA",
        "long-context reasoning",
        "plan-retrieve-reason-recheck",
        "RL rollout",
        "Qwen2.5-7B",
        "Qwen2.5-14B",
        "o3-mini",
        "DeepSeek-R1",
        "needle-in-a-haystack stress tests"
      ],
      "organization": {
        "_id": "68151d0f51add3813f3f7d1b",
        "name": "MicrosoftResearch",
        "fullname": "Microsoft Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
      }
    },
    "publishedAt": "2025-10-22T04:35:28.000Z",
    "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
    "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "organization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19430",
      "authors": [
        {
          "_id": "68f99e80b9b2e4ae04673853",
          "name": "GigaBrain Team",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673854",
          "name": "Angen Ye",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673855",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673856",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673857",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673858",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673859",
          "name": "Haoyun Li",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385a",
          "name": "Jie Li",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385b",
          "name": "Jiagang Zhu",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385c",
          "name": "Lv Feng",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385d",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385e",
          "name": "Qiuping Deng",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467385f",
          "name": "Runqi Ouyang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673860",
          "name": "Wenkang Qin",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673861",
          "name": "Xinze Chen",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673862",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673863",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673864",
          "name": "Yifan Li",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673865",
          "name": "Yilong Li",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673866",
          "name": "Yiran Ding",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673867",
          "name": "Yuan Xu",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673868",
          "name": "Yun Ye",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae04673869",
          "name": "Yukun Zhou",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467386a",
          "name": "Zhehao Dong",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467386b",
          "name": "Zhenan Wang",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467386c",
          "name": "Zhichao Liu",
          "hidden": false
        },
        {
          "_id": "68f99e80b9b2e4ae0467386d",
          "name": "Zheng Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T09:57:13.000Z",
      "submittedOnDailyAt": "2025-10-23T03:11:44.473Z",
      "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
      "submittedOnDailyBy": {
        "_id": "6426616ea5ec4a5cbc535634",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
        "isPro": false,
        "fullname": "JeffWang",
        "user": "Jeff-Wang",
        "type": "user"
      },
      "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
      "upvotes": 22,
      "discussionId": "68f99e81b9b2e4ae0467386e",
      "projectPage": "https://gigabrain0.github.io/",
      "githubRepo": "https://github.com/open-gigaai/giga-brain-0",
      "ai_summary": "GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.",
      "ai_keywords": [
        "VLA models",
        "world model-generated data",
        "video generation",
        "real2real transfer",
        "human transfer",
        "view transfer",
        "sim2real transfer",
        "RGBD input modeling",
        "Chain-of-Thought supervision",
        "spatial geometry",
        "object states",
        "long-horizon dependencies",
        "dexterous manipulation",
        "mobile manipulation"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "68d6587936e2de9610d9f5f0",
        "name": "open-gigaai",
        "fullname": "GigaAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
      }
    },
    "publishedAt": "2025-10-22T05:57:13.000Z",
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19430.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6426616ea5ec4a5cbc535634",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
      "fullname": "JeffWang",
      "name": "Jeff-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "68d6587936e2de9610d9f5f0",
      "name": "open-gigaai",
      "fullname": "GigaAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19488",
      "authors": [
        {
          "_id": "68f98a91b9b2e4ae0467376d",
          "name": "Dunjie Lu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467376e",
          "name": "Yiheng Xu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467376f",
          "name": "Junli Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673770",
          "name": "Haoyuan Wu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673771",
          "name": "Xinyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673772",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673773",
          "name": "Junlin Yang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673774",
          "name": "Hongjin Su",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673775",
          "name": "Jixuan Chen",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673776",
          "name": "Junda Chen",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673777",
          "name": "Yuchen Mao",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673778",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673779",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467377a",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467377b",
          "name": "Tao Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T11:25:48.000Z",
      "submittedOnDailyAt": "2025-10-23T00:23:34.212Z",
      "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
      "upvotes": 14,
      "discussionId": "68f98a91b9b2e4ae0467377c",
      "projectPage": "https://videoagenttrek.github.io/",
      "ai_summary": "VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.",
      "ai_keywords": [
        "inverse dynamics module",
        "video grounding model",
        "action-content recognizer",
        "continued pretraining",
        "supervised fine-tuning",
        "OSWorld-Verified",
        "AgentNetBench"
      ]
    },
    "publishedAt": "2025-10-22T07:25:48.000Z",
    "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
    "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19488.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19336",
      "authors": [
        {
          "_id": "68f98a97b9b2e4ae0467377e",
          "name": "Kai Shi",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae0467377f",
          "name": "Jun Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673780",
          "name": "Ni Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673781",
          "name": "Binqiang Pan",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673782",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673783",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673784",
          "name": "Zhenyu Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673785",
          "name": "Tianhuang Su",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673786",
          "name": "Haonan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:57:59.000Z",
      "submittedOnDailyAt": "2025-10-23T00:27:06.757Z",
      "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
      "submittedOnDailyBy": {
        "_id": "66e24afddce93c7249b418c0",
        "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
        "isPro": false,
        "fullname": "OPPO AI Center",
        "user": "AIGCer-OPPO",
        "type": "user"
      },
      "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
      "upvotes": 14,
      "discussionId": "68f98a97b9b2e4ae04673787",
      "ai_summary": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "multitask supervised fine-tuning",
        "DaMo",
        "Data Mixture Optimizer",
        "PhoneAgentBench",
        "BFCL-v3",
        "MME-Reasoning",
        "MME-Perception",
        "OCRBench"
      ],
      "organization": {
        "_id": "67177eecd0fad5b4ccc09461",
        "name": "OPPOer",
        "fullname": "OPPO",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
      }
    },
    "publishedAt": "2025-10-22T03:57:59.000Z",
    "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
    "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e24afddce93c7249b418c0",
      "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
      "fullname": "OPPO AI Center",
      "name": "AIGCer-OPPO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "67177eecd0fad5b4ccc09461",
      "name": "OPPOer",
      "fullname": "OPPO",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19808",
      "authors": [
        {
          "_id": "68f988deb9b2e4ae0467375c",
          "name": "Yusu Qian",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375d",
          "name": "Eli Bocek-Rivele",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375e",
          "name": "Liangchen Song",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375f",
          "name": "Jialing Tong",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673760",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673761",
          "name": "Jiasen Lu",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673762",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673763",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T17:43:15.000Z",
      "submittedOnDailyAt": "2025-10-23T00:16:18.374Z",
      "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
      "upvotes": 9,
      "discussionId": "68f988deb9b2e4ae04673764",
      "githubRepo": "https://github.com/apple/pico-banana-400k",
      "ai_summary": "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.",
      "ai_keywords": [
        "multimodal models",
        "text-guided image editing",
        "GPT-4o",
        "Nano-Banana",
        "OpenImages",
        "fine-grained image editing taxonomy",
        "MLLM-based quality scoring",
        "multi-turn editing",
        "preference subset",
        "instruction rewriting",
        "instruction summarization"
      ],
      "githubStars": 39,
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-10-22T13:43:15.000Z",
    "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
    "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19307",
      "authors": [
        {
          "_id": "68f9929eb9b2e4ae04673817",
          "name": "Byung-Kwan Lee",
          "hidden": false
        },
        {
          "_id": "68f9929eb9b2e4ae04673818",
          "name": "Ryo Hachiuma",
          "hidden": false
        },
        {
          "_id": "68f9929eb9b2e4ae04673819",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68f9929eb9b2e4ae0467381a",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68f9929eb9b2e4ae0467381b",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/ur9tbO9Ie-KPIHBbzRIQ5.mp4"
      ],
      "publishedAt": "2025-10-22T07:12:14.000Z",
      "submittedOnDailyAt": "2025-10-23T01:01:49.909Z",
      "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
      "upvotes": 8,
      "discussionId": "68f9929fb9b2e4ae0467381c",
      "projectPage": "https://byungkwanlee.github.io/RIL-page/",
      "ai_summary": "A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.",
      "ai_keywords": [
        "Unified Reinforcement and Imitation Learning (RIL)",
        "reinforcement learning",
        "adversarial imitation learning",
        "LLM-based discriminator",
        "vision-language models (VLMs)",
        "text generation",
        "generative capabilities",
        "vision-language benchmarks"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-22T03:12:14.000Z",
    "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/ur9tbO9Ie-KPIHBbzRIQ5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 60
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16844",
      "authors": [
        {
          "_id": "68f86f6b7669bcaeecce0e59",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5a",
          "name": "Yuyao Zhang",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5b",
          "name": "Yimeng Xu",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5c",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5d",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5e",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T14:05:35.000Z",
      "submittedOnDailyAt": "2025-10-23T00:50:26.504Z",
      "title": "FinSight: Towards Real-World Financial Deep Research",
      "submittedOnDailyBy": {
        "_id": "6695f14df0ffd8e3a379ad61",
        "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
        "isPro": false,
        "fullname": "Jiajie Jin",
        "user": "jinjiajie",
        "type": "user"
      },
      "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
      "upvotes": 6,
      "discussionId": "68f86f6b7669bcaeecce0e5f",
      "ai_summary": "FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.",
      "ai_keywords": [
        "multi agent framework",
        "Code Agent with Variable Memory (CAVM)",
        "programmable variable space",
        "Iterative Vision-Enhanced Mechanism",
        "Chain-of-Analysis",
        "citation-aware",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-10-19T10:05:35.000Z",
    "title": "FinSight: Towards Real-World Financial Deep Research",
    "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6695f14df0ffd8e3a379ad61",
      "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
      "fullname": "Jiajie Jin",
      "name": "jinjiajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17932",
      "authors": [
        {
          "_id": "68f867b07669bcaeecce0e2d",
          "name": "Jiahao Tang",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e2e",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e2f",
          "name": "Lijian Wu",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e30",
          "name": "Yifei Tao",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e31",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e32",
          "name": "Yang Wan",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e33",
          "name": "Jingru Tan",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e34",
          "name": "Min Zeng",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e35",
          "name": "Min Li",
          "hidden": false
        },
        {
          "_id": "68f867b07669bcaeecce0e36",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T15:11:56.000Z",
      "submittedOnDailyAt": "2025-10-23T04:00:16.699Z",
      "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "647d7eb9770c299e56f5b39b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
        "isPro": false,
        "fullname": "Henry Hengyuan Zhao",
        "user": "hhenryz",
        "type": "user"
      },
      "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.",
      "upvotes": 5,
      "discussionId": "68f867b17669bcaeecce0e37",
      "projectPage": "https://csu-jpg.github.io/Chart2Code.github.io/",
      "githubRepo": "https://github.com/CSU-JPG/Chart2Code",
      "ai_summary": "Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.",
      "ai_keywords": [
        "multimodal models",
        "LMMs",
        "Chart2Code",
        "Chart Reproduction",
        "Chart Editing",
        "Long-Table to Chart Generation",
        "hierarchical benchmark",
        "code correctness",
        "visual fidelity",
        "GPT-5",
        "Qwen2.5-VL",
        "InternVL3/3.5",
        "MiMo-VL",
        "Seed-1.6-VL"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-10-20T11:11:56.000Z",
    "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
    "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Henry Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19817",
      "authors": [
        {
          "_id": "68f98861b9b2e4ae04673757",
          "name": "Jake Poznanski",
          "hidden": false
        },
        {
          "_id": "68f98861b9b2e4ae04673758",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "68f98861b9b2e4ae04673759",
          "name": "Kyle Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T17:53:02.000Z",
      "submittedOnDailyAt": "2025-10-23T00:14:13.036Z",
      "title": "olmOCR 2: Unit Test Rewards for Document OCR",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
      "upvotes": 4,
      "discussionId": "68f98861b9b2e4ae0467375a",
      "projectPage": "https://olmocr.allen.ai/",
      "ai_summary": "olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.",
      "ai_keywords": [
        "vision language model",
        "reinforcement learning",
        "verifiable rewards",
        "synthetic documents",
        "ground-truth HTML",
        "test cases",
        "olmOCR-Bench",
        "math formula conversion",
        "table parsing",
        "multi-column layouts"
      ],
      "organization": {
        "_id": "5e70f3648ce3c604d78fe132",
        "name": "allenai",
        "fullname": "Ai2",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
      }
    },
    "publishedAt": "2025-10-22T13:53:02.000Z",
    "title": "olmOCR 2: Unit Test Rewards for Document OCR",
    "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19817.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "organization": {
      "_id": "5e70f3648ce3c604d78fe132",
      "name": "allenai",
      "fullname": "Ai2",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19386",
      "authors": [
        {
          "_id": "68f98cceb9b2e4ae046737c3",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c4",
          "name": "Qiqiang Lin",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c5",
          "name": "Zheng Wu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c6",
          "name": "Xiaoyun Mo",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c8",
          "name": "Yin Zhao",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c9",
          "name": "Xiangmou Qu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737ca",
          "name": "Jiamu Zhou",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cb",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cc",
          "name": "Congmin Zheng",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cd",
          "name": "Yuanyi Song",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737ce",
          "name": "Hongjiang Chen",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cf",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d0",
          "name": "Jihong Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d1",
          "name": "Jiaxin Yin",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d2",
          "name": "Jingwei Yu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d3",
          "name": "Junwei Liao",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d4",
          "name": "Qiuying Peng",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d5",
          "name": "Xingyu Lou",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d6",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d7",
          "name": "Weiwen Liu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d8",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d9",
          "name": "Weinan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T09:02:48.000Z",
      "submittedOnDailyAt": "2025-10-23T00:33:11.605Z",
      "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
      "upvotes": 4,
      "discussionId": "68f98cceb9b2e4ae046737da",
      "githubRepo": "https://github.com/MadeAgents/mobile-use",
      "ai_summary": "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.",
      "ai_keywords": [
        "step-wise reinforcement learning",
        "self-evolving training",
        "multi-agent framework",
        "personalized user intent recognition",
        "proactive engagement",
        "AndroidWorld",
        "AndroidLab"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-10-22T05:02:48.000Z",
    "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
    "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19386.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19286",
      "authors": [
        {
          "_id": "68f98627b9b2e4ae04673747",
          "name": "Reza Esfandiarpoor",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae04673748",
          "name": "Vishwas Suryanarayanan",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae04673749",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae0467374a",
          "name": "Vishal Chowdhary",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae0467374b",
          "name": "Anthony Aue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T06:42:01.000Z",
      "submittedOnDailyAt": "2025-10-23T00:05:22.073Z",
      "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
      "submittedOnDailyBy": {
        "_id": "63316bb618711776b465b788",
        "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
        "isPro": false,
        "fullname": "Reza Esfandiarpoor",
        "user": "rezaesfandiarpoor",
        "type": "user"
      },
      "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
      "upvotes": 4,
      "discussionId": "68f98628b9b2e4ae0467374c",
      "githubRepo": "https://github.com/Reza-esfandiarpoor/the-mcp-company",
      "ai_summary": "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.",
      "ai_keywords": [
        "Model Context Protocol",
        "Large Language Models",
        "tool-calling agents",
        "REST APIs",
        "ground-truth tools",
        "tool retrieval",
        "GPT-5",
        "reasoning models",
        "retrieval models"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-10-22T02:42:01.000Z",
    "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
    "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316bb618711776b465b788",
      "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
      "fullname": "Reza Esfandiarpoor",
      "name": "rezaesfandiarpoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19028",
      "authors": [
        {
          "_id": "68f9ad08b9b2e4ae046738d2",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d3",
          "name": "Junyeong Park",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d4",
          "name": "Juhyun Oh",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d5",
          "name": "Kiwoong Park",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d6",
          "name": "Seyoung Song",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d7",
          "name": "A. Seza Dogruoz",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d8",
          "name": "Najoung Kim",
          "hidden": false
        },
        {
          "_id": "68f9ad08b9b2e4ae046738d9",
          "name": "Alice Oh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T19:12:47.000Z",
      "submittedOnDailyAt": "2025-10-23T02:52:18.993Z",
      "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "As large language models (LLMs) are increasingly used in human-AI\ninteractions, their social reasoning capabilities in interpersonal contexts are\ncritical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,\nsourced from movie scripts. The task involves evaluating models' social\nreasoning capability to infer the interpersonal relationships (e.g., friends,\nsisters, lovers) between speakers in each dialogue. Each dialogue is annotated\nwith probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by\nnative (or equivalent) Korean and English speakers from Korea and the U.S.\nEvaluating nine models on our task, current proprietary LLMs achieve around\n75-80% on the English dataset, whereas their performance on Korean drops to\n58-69%. More strikingly, models select Unlikely relationships in 10-25% of\ntheir responses. Furthermore, we find that thinking models and chain-of-thought\nprompting, effective for general reasoning, provide minimal benefits for social\nreasoning and occasionally amplify social biases. Our findings reveal\nsignificant limitations in current LLMs' social reasoning capabilities,\nhighlighting the need for efforts to develop socially-aware language models.",
      "upvotes": 4,
      "discussionId": "68f9ad08b9b2e4ae046738da",
      "githubRepo": "https://github.com/rladmstn1714/SCRIPTS",
      "ai_summary": "Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.",
      "ai_keywords": [
        "large language models",
        "social reasoning",
        "interpersonal relationships",
        "SCRIPTS",
        "dialogue dataset",
        "probabilistic relational labels",
        "thinking models",
        "chain-of-thought prompting",
        "socially-aware language models"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-21T15:12:47.000Z",
    "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues",
    "summary": "As large language models (LLMs) are increasingly used in human-AI\ninteractions, their social reasoning capabilities in interpersonal contexts are\ncritical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,\nsourced from movie scripts. The task involves evaluating models' social\nreasoning capability to infer the interpersonal relationships (e.g., friends,\nsisters, lovers) between speakers in each dialogue. Each dialogue is annotated\nwith probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by\nnative (or equivalent) Korean and English speakers from Korea and the U.S.\nEvaluating nine models on our task, current proprietary LLMs achieve around\n75-80% on the English dataset, whereas their performance on Korean drops to\n58-69%. More strikingly, models select Unlikely relationships in 10-25% of\ntheir responses. Furthermore, we find that thinking models and chain-of-thought\nprompting, effective for general reasoning, provide minimal benefits for social\nreasoning and occasionally amplify social biases. Our findings reveal\nsignificant limitations in current LLMs' social reasoning capabilities,\nhighlighting the need for efforts to develop socially-aware language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19028.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15731",
      "authors": [
        {
          "_id": "68f89d937669bcaeecce0f2f",
          "user": {
            "_id": "643986457245b3b7f453aec7",
            "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
            "isPro": false,
            "fullname": "Maximo Rulli",
            "user": "maximorulli",
            "type": "user"
          },
          "name": "Maximo Eduardo Rulli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-22T15:42:15.171Z",
          "hidden": false
        },
        {
          "_id": "68f89d937669bcaeecce0f30",
          "name": "Simone Petruzzi",
          "hidden": false
        },
        {
          "_id": "68f89d937669bcaeecce0f31",
          "name": "Edoardo Michielon",
          "hidden": false
        },
        {
          "_id": "68f89d937669bcaeecce0f32",
          "name": "Fabrizio Silvestri",
          "hidden": false
        },
        {
          "_id": "68f89d937669bcaeecce0f33",
          "name": "Simone Scardapane",
          "hidden": false
        },
        {
          "_id": "68f89d937669bcaeecce0f34",
          "name": "Alessio Devoto",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/KoRDpre1sVGD2TXW-IYqT.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/h22QYQOvEuKU3GG9mWLFP.png",
        "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/OxnCchZHPfFKbTsue4RMy.png"
      ],
      "publishedAt": "2025-10-17T15:23:58.000Z",
      "submittedOnDailyAt": "2025-10-23T04:07:43.805Z",
      "title": "Attention Sinks in Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "643986457245b3b7f453aec7",
        "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
        "isPro": false,
        "fullname": "Maximo Rulli",
        "user": "maximorulli",
        "type": "user"
      },
      "summary": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising\nalternative to traditional Autoregressive Models (ARMs). DLMs employ\ntransformer encoders with bidirectional attention, enabling parallel token\ngeneration while maintaining competitive performance. Although their efficiency\nand effectiveness have been extensively studied, the internal mechanisms that\ngovern DLMs remain largely unexplored. In this work, we conduct an empirical\nanalysis of DLM attention patterns, focusing on the attention sinking\nphenomenon, an effect previously observed in various transformer-based\narchitectures. Our findings reveal that DLMs also exhibit attention sinks, but\nwith distinct characteristics. First, unlike in ARMs, the sink positions in\nDLMs tend to shift throughout the generation process, displaying a dynamic\nbehaviour. Second, while ARMs are highly sensitive to the removal of attention\nsinks, DLMs remain robust: masking sinks leads to only a minor degradation in\nperformance. These results provide new insights into the inner workings of\ndiffusion-based language models and highlight fundamental differences in how\nthey allocate and utilize attention compared to autoregressive models.",
      "upvotes": 4,
      "discussionId": "68f89d947669bcaeecce0f35",
      "ai_summary": "Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).",
      "ai_keywords": [
        "Masked Diffusion Language Models",
        "DLMs",
        "Autoregressive Models",
        "ARMs",
        "transformer encoders",
        "bidirectional attention",
        "parallel token generation",
        "attention sinking",
        "attention patterns",
        "dynamic behaviour",
        "performance degradation"
      ]
    },
    "publishedAt": "2025-10-17T11:23:58.000Z",
    "title": "Attention Sinks in Diffusion Language Models",
    "summary": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising\nalternative to traditional Autoregressive Models (ARMs). DLMs employ\ntransformer encoders with bidirectional attention, enabling parallel token\ngeneration while maintaining competitive performance. Although their efficiency\nand effectiveness have been extensively studied, the internal mechanisms that\ngovern DLMs remain largely unexplored. In this work, we conduct an empirical\nanalysis of DLM attention patterns, focusing on the attention sinking\nphenomenon, an effect previously observed in various transformer-based\narchitectures. Our findings reveal that DLMs also exhibit attention sinks, but\nwith distinct characteristics. First, unlike in ARMs, the sink positions in\nDLMs tend to shift throughout the generation process, displaying a dynamic\nbehaviour. Second, while ARMs are highly sensitive to the removal of attention\nsinks, DLMs remain robust: masking sinks leads to only a minor degradation in\nperformance. These results provide new insights into the inner workings of\ndiffusion-based language models and highlight fundamental differences in how\nthey allocate and utilize attention compared to autoregressive models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/KoRDpre1sVGD2TXW-IYqT.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/h22QYQOvEuKU3GG9mWLFP.png",
      "https://cdn-uploads.huggingface.co/production/uploads/643986457245b3b7f453aec7/OxnCchZHPfFKbTsue4RMy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15731.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643986457245b3b7f453aec7",
      "avatarUrl": "/avatars/37b3fb9a67de7674c898788999a12265.svg",
      "fullname": "Maximo Rulli",
      "name": "maximorulli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.19457",
      "authors": [
        {
          "_id": "68f98c94b9b2e4ae046737a2",
          "name": "Kailin Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a3",
          "name": "Ning Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a4",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a5",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a6",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a7",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a8",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a9",
          "name": "Qingqing Liu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737aa",
          "name": "Xianhao Wang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ab",
          "name": "Yifan Jia",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ac",
          "name": "Hongbo Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ad",
          "name": "Yaocong Hu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ae",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737af",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737b0",
          "name": "Yuntao Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T10:41:57.000Z",
      "submittedOnDailyAt": "2025-10-23T00:36:11.231Z",
      "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "65745569839aa08899ea5d27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
        "isPro": false,
        "fullname": "kailinjiang",
        "user": "kailinjiang",
        "type": "user"
      },
      "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
      "upvotes": 3,
      "discussionId": "68f98c94b9b2e4ae046737b1",
      "projectPage": "https://mined-lmm.github.io/"
    },
    "publishedAt": "2025-10-22T06:41:57.000Z",
    "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65745569839aa08899ea5d27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
      "fullname": "kailinjiang",
      "name": "kailinjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19316",
      "authors": [
        {
          "_id": "68f98c87b9b2e4ae04673796",
          "name": "Kailin Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673797",
          "name": "Hongbo Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673798",
          "name": "Ning Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673799",
          "name": "Zhi Gao",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379a",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379b",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379c",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379d",
          "name": "Yuntao Du",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379e",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379f",
          "name": "Qing Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:26:55.000Z",
      "submittedOnDailyAt": "2025-10-23T00:34:23.319Z",
      "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
      "submittedOnDailyBy": {
        "_id": "65745569839aa08899ea5d27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
        "isPro": false,
        "fullname": "kailinjiang",
        "user": "kailinjiang",
        "type": "user"
      },
      "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
      "upvotes": 3,
      "discussionId": "68f98c87b9b2e4ae046737a0",
      "projectPage": "https://kore-lmm.github.io/",
      "ai_summary": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.",
      "ai_keywords": [
        "knowledge adaptation",
        "knowledge retention",
        "catastrophic forgetting",
        "KORE",
        "KnOwledge-oRientEd augmentations",
        "covariance matrix",
        "linear layer activations",
        "adapter",
        "fine-tuning direction",
        "LMMs",
        "LLaVA-v1.5-7B",
        "LLaVA-v1.5-13B",
        "Qwen2.5-VL-7B"
      ]
    },
    "publishedAt": "2025-10-22T03:26:55.000Z",
    "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
    "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19316.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65745569839aa08899ea5d27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
      "fullname": "kailinjiang",
      "name": "kailinjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18313",
      "authors": [
        {
          "_id": "68f98bbdb9b2e4ae04673789",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378a",
          "name": "Zhuang Ma",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378b",
          "name": "Dalong Du",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378c",
          "name": "Baorui Peng",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378d",
          "name": "Zhujin Liang",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378e",
          "name": "Zhenqiang Liu",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378f",
          "name": "Chao Ma",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673790",
          "name": "Yueming Jin",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673791",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673792",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673793",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
      ],
      "publishedAt": "2025-10-21T05:49:01.000Z",
      "submittedOnDailyAt": "2025-10-23T00:33:35.049Z",
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "submittedOnDailyBy": {
        "_id": "66480d4cea529a27ecaaee66",
        "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
        "isPro": false,
        "fullname": "Bohan Li",
        "user": "Arlolo0",
        "type": "user"
      },
      "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
      "upvotes": 3,
      "discussionId": "68f98bbdb9b2e4ae04673794",
      "projectPage": "https://arlo0o.github.io/OmniNWM/",
      "githubRepo": "https://github.com/Ma-Zhuang/OmniNWM",
      "ai_summary": "OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.",
      "ai_keywords": [
        "panoramic navigation world model",
        "OmniNWM",
        "panoramic videos",
        "RGB",
        "semantics",
        "metric depth",
        "3D occupancy",
        "auto-regressive generation",
        "normalized panoramic Plucker ray-map",
        "rule-based dense rewards",
        "driving compliance",
        "safety",
        "occupancy-grounded rewards"
      ],
      "githubStars": 34
    },
    "publishedAt": "2025-10-21T01:49:01.000Z",
    "title": "OmniNWM: Omniscient Driving Navigation World Models",
    "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18313.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66480d4cea529a27ecaaee66",
      "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
      "fullname": "Bohan Li",
      "name": "Arlolo0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18909",
      "authors": [
        {
          "_id": "68f9b1efb9b2e4ae0467390d",
          "name": "Hongyi He",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae0467390e",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae0467390f",
          "name": "Zhenghao Lin",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673910",
          "name": "Mingni Tang",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673911",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673912",
          "name": "Jintao Wang",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673913",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673914",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "68f9b1efb9b2e4ae04673915",
          "name": "Yeyun Gong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T03:37:31.000Z",
      "submittedOnDailyAt": "2025-10-23T03:11:38.991Z",
      "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection",
      "submittedOnDailyBy": {
        "_id": "63fb6e281b4b1bd4e7ffc5be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
        "isPro": false,
        "fullname": "Xiao Liu",
        "user": "lx865712528",
        "type": "user"
      },
      "summary": "High-quality pre-training data is crutial for large language models, where\nquality captures factual reliability and semantic value, and diversity ensures\nbroad coverage and distributional heterogeneity. Existing approaches typically\nrely on single or multiple-dimensional score-based selection. However, directly\nselecting top-scored data often degrades performance, and sampling from a\nbroader range is required to recover results. The above non-monotonicity\nbetween dataset scores and downstream benchmark results reveals a fundamental\nbias: score-based methods collapse correlated dimensions, causing top-scored\ndata to appear high-quality while systematically overlooking diversity. We\nargue that ensuring diversity requires decomposing correlated metrics into\northogonal feature dimensions, from which the top-scored data can be directly\nselected. Therefore, we proposed the Orthogonal Diversity-Aware Selection\n(ODiS) algorithm, which preserves both quality and diversity during data\nselection. First, ODiS evaluates data from multiple dimensions, covering\nlanguage quality, knowledge quality, and comprehension difficulty. The\nmulti-dimensional scores are then decorrelated via Principal Component Analysis\n(PCA), yielding orthogonal evaluation dimensions. For each dimension, a\nRoberta-based scorer is trained to regress the data onto PCA-projected scores,\nenabling scalable inference on large corpora. Finally, ODiS constructs the\ntraining dataset by selecting top-scored data within each orthogonal dimension,\nthereby ensuring both quality and diversity. Empirical results show that\nODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming\northogonality between dimensions. More importantly, models trained with\nODiS-selected data significantly outperform other baselines on downstream\nbenchmarks, highlighting the necessity of orthogonal, diversity-aware data\nselection for LLMs.",
      "upvotes": 3,
      "discussionId": "68f9b1efb9b2e4ae04673916",
      "ai_summary": "The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.",
      "ai_keywords": [
        "Orthogonal Diversity-Aware Selection",
        "ODiS",
        "Principal Component Analysis",
        "PCA",
        "Roberta-based scorer",
        "language quality",
        "knowledge quality",
        "comprehension difficulty",
        "large language models",
        "LLMs"
      ]
    },
    "publishedAt": "2025-10-20T23:37:31.000Z",
    "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection",
    "summary": "High-quality pre-training data is crutial for large language models, where\nquality captures factual reliability and semantic value, and diversity ensures\nbroad coverage and distributional heterogeneity. Existing approaches typically\nrely on single or multiple-dimensional score-based selection. However, directly\nselecting top-scored data often degrades performance, and sampling from a\nbroader range is required to recover results. The above non-monotonicity\nbetween dataset scores and downstream benchmark results reveals a fundamental\nbias: score-based methods collapse correlated dimensions, causing top-scored\ndata to appear high-quality while systematically overlooking diversity. We\nargue that ensuring diversity requires decomposing correlated metrics into\northogonal feature dimensions, from which the top-scored data can be directly\nselected. Therefore, we proposed the Orthogonal Diversity-Aware Selection\n(ODiS) algorithm, which preserves both quality and diversity during data\nselection. First, ODiS evaluates data from multiple dimensions, covering\nlanguage quality, knowledge quality, and comprehension difficulty. The\nmulti-dimensional scores are then decorrelated via Principal Component Analysis\n(PCA), yielding orthogonal evaluation dimensions. For each dimension, a\nRoberta-based scorer is trained to regress the data onto PCA-projected scores,\nenabling scalable inference on large corpora. Finally, ODiS constructs the\ntraining dataset by selecting top-scored data within each orthogonal dimension,\nthereby ensuring both quality and diversity. Empirical results show that\nODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming\northogonality between dimensions. More importantly, models trained with\nODiS-selected data significantly outperform other baselines on downstream\nbenchmarks, highlighting the necessity of orthogonal, diversity-aware data\nselection for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fb6e281b4b1bd4e7ffc5be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
      "fullname": "Xiao Liu",
      "name": "lx865712528",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18940",
      "authors": [
        {
          "_id": "68f998bdb9b2e4ae0467382f",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68f998bdb9b2e4ae04673830",
          "name": "Yixian Shen",
          "hidden": false
        },
        {
          "_id": "68f998bdb9b2e4ae04673831",
          "name": "Congfeng Cao",
          "hidden": false
        },
        {
          "_id": "68f998bdb9b2e4ae04673832",
          "name": "Ekaterina Shutova",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b8e6adaf1addc58cfcc578/yBex6P35yrg7PiO8WMIsQ.png"
      ],
      "publishedAt": "2025-10-21T17:59:24.000Z",
      "submittedOnDailyAt": "2025-10-23T01:42:36.167Z",
      "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "62b8e6adaf1addc58cfcc578",
        "avatarUrl": "/avatars/3f2b190eea73bd2bf2b4ddec1014edf5.svg",
        "isPro": false,
        "fullname": "ZhiZhang",
        "user": "ZhiZhang",
        "type": "user"
      },
      "summary": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into\ntwo categories: addition-based and selective in-situ adaptation. The former,\nsuch as LoRA, introduce additional modules to adapt the model to downstream\ntasks, offering strong memory efficiency. However, their representational\ncapacity is often limited, making them less suitable for fine-grained\nadaptation. In contrast, the latter directly fine-tunes a carefully chosen\nsubset of the original model parameters, allowing for more precise and\neffective adaptation, but at the cost of significantly increased memory\nconsumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT\nmethod that enables fine-grained model finetuning while maintaining high memory\nefficiency. Our approach first identifies important parameters (i.e.,\nconnections within the network) as in selective adaptation, and then introduces\nbypass connections for these selected parameters. During finetuning, only the\nbypass connections are updated, leaving the original model parameters frozen.\nEmpirical results on 23+ tasks spanning both natural language generation and\nunderstanding demonstrate that NeuroAda achieves state-of-the-art performance\nwith as little as leq 0.02% trainable parameters, while reducing\nCUDA memory usage by up to 60%. We release our code here:\nhttps://github.com/FightingFighting/NeuroAda.git.",
      "upvotes": 2,
      "discussionId": "68f998bdb9b2e4ae04673833",
      "githubRepo": "https://github.com/\nFightingFighting/NeuroAda.git",
      "ai_summary": "NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.",
      "ai_keywords": [
        "parameter-efficient fine-tuning",
        "PEFT",
        "LoRA",
        "addition-based",
        "selective in-situ adaptation",
        "NeuroAda",
        "important parameters",
        "bypass connections",
        "natural language generation",
        "natural language understanding"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "6274e45cbe455dadd1063972",
        "name": "uva",
        "fullname": "University of Amsterdam",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651827662266-6273a78c3d70b36612a8bd9e.png"
      }
    },
    "publishedAt": "2025-10-21T13:59:24.000Z",
    "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning",
    "summary": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into\ntwo categories: addition-based and selective in-situ adaptation. The former,\nsuch as LoRA, introduce additional modules to adapt the model to downstream\ntasks, offering strong memory efficiency. However, their representational\ncapacity is often limited, making them less suitable for fine-grained\nadaptation. In contrast, the latter directly fine-tunes a carefully chosen\nsubset of the original model parameters, allowing for more precise and\neffective adaptation, but at the cost of significantly increased memory\nconsumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT\nmethod that enables fine-grained model finetuning while maintaining high memory\nefficiency. Our approach first identifies important parameters (i.e.,\nconnections within the network) as in selective adaptation, and then introduces\nbypass connections for these selected parameters. During finetuning, only the\nbypass connections are updated, leaving the original model parameters frozen.\nEmpirical results on 23+ tasks spanning both natural language generation and\nunderstanding demonstrate that NeuroAda achieves state-of-the-art performance\nwith as little as leq 0.02% trainable parameters, while reducing\nCUDA memory usage by up to 60%. We release our code here:\nhttps://github.com/FightingFighting/NeuroAda.git.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b8e6adaf1addc58cfcc578/yBex6P35yrg7PiO8WMIsQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18940.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b8e6adaf1addc58cfcc578",
      "avatarUrl": "/avatars/3f2b190eea73bd2bf2b4ddec1014edf5.svg",
      "fullname": "ZhiZhang",
      "name": "ZhiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "6274e45cbe455dadd1063972",
      "name": "uva",
      "fullname": "University of Amsterdam",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1651827662266-6273a78c3d70b36612a8bd9e.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18428",
      "authors": [
        {
          "_id": "68f998c9b9b2e4ae04673835",
          "name": "Minwei Kong",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673836",
          "name": "Ao Qu",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673837",
          "name": "Xiaotong Guo",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673838",
          "name": "Wenbin Ouyang",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673839",
          "name": "Chonghe Jiang",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383a",
          "name": "Han Zheng",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383b",
          "name": "Yining Ma",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383c",
          "name": "Dingyi Zhuang",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383d",
          "name": "Yuhan Tang",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383e",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae0467383f",
          "name": "Hai Wang",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673840",
          "name": "Cathy Wu",
          "hidden": false
        },
        {
          "_id": "68f998c9b9b2e4ae04673841",
          "name": "Jinhua Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T09:03:26.000Z",
      "submittedOnDailyAt": "2025-10-23T01:25:54.240Z",
      "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library",
      "submittedOnDailyBy": {
        "_id": "6556223bb3bb6c3a41809f7e",
        "avatarUrl": "/avatars/ea7640aa9ae4fe35a2e245b3b6f8c0ff.svg",
        "isPro": true,
        "fullname": "Ao Qu",
        "user": "quao627",
        "type": "user"
      },
      "summary": "Optimization modeling enables critical decisions across industries but\nremains difficult to automate: informal language must be mapped to precise\nmathematical formulations and executable solver code. Prior LLM approaches\neither rely on brittle prompting or costly retraining with limited\ngeneralization. We present AlphaOPT, a self-improving experience library that\nenables an LLM to learn from limited demonstrations (even answers alone,\nwithout gold-standard programs) and solver feedback - without annotated\nreasoning traces or parameter updates. AlphaOPT operates in a continual\ntwo-phase cycle: (i) a Library Learning phase that reflects on failed attempts,\nextracting solver-verified, structured insights as {taxonomy, condition,\nexplanation, example}; and (ii) a Library Evolution phase that diagnoses\nretrieval misalignments and refines the applicability conditions of stored\ninsights, improving transfer across tasks. This design (1) learns efficiently\nfrom limited demonstrations without curated rationales, (2) expands continually\nwithout costly retraining by updating the library rather than model weights,\nand (3) makes knowledge explicit and interpretable for human inspection and\nintervention. Experiments show that AlphaOPT steadily improves with more data\n(65% to 72% from 100 to 300 training items) and surpasses the strongest\nbaseline by 7.7% on the out-of-distribution OptiBench dataset when trained only\non answers. Code and data are available at:\nhttps://github.com/Minw913/AlphaOPT.",
      "upvotes": 2,
      "discussionId": "68f998cab9b2e4ae04673842",
      "ai_summary": "AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.",
      "ai_keywords": [
        "LLM",
        "AlphaOPT",
        "Library Learning",
        "Library Evolution",
        "solver-verified",
        "structured insights",
        "taxonomy",
        "condition",
        "explanation",
        "example",
        "retrieval misalignments",
        "applicability conditions",
        "transfer across tasks",
        "OptiBench dataset"
      ]
    },
    "publishedAt": "2025-10-21T05:03:26.000Z",
    "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library",
    "summary": "Optimization modeling enables critical decisions across industries but\nremains difficult to automate: informal language must be mapped to precise\nmathematical formulations and executable solver code. Prior LLM approaches\neither rely on brittle prompting or costly retraining with limited\ngeneralization. We present AlphaOPT, a self-improving experience library that\nenables an LLM to learn from limited demonstrations (even answers alone,\nwithout gold-standard programs) and solver feedback - without annotated\nreasoning traces or parameter updates. AlphaOPT operates in a continual\ntwo-phase cycle: (i) a Library Learning phase that reflects on failed attempts,\nextracting solver-verified, structured insights as {taxonomy, condition,\nexplanation, example}; and (ii) a Library Evolution phase that diagnoses\nretrieval misalignments and refines the applicability conditions of stored\ninsights, improving transfer across tasks. This design (1) learns efficiently\nfrom limited demonstrations without curated rationales, (2) expands continually\nwithout costly retraining by updating the library rather than model weights,\nand (3) makes knowledge explicit and interpretable for human inspection and\nintervention. Experiments show that AlphaOPT steadily improves with more data\n(65% to 72% from 100 to 300 training items) and surpasses the strongest\nbaseline by 7.7% on the out-of-distribution OptiBench dataset when trained only\non answers. Code and data are available at:\nhttps://github.com/Minw913/AlphaOPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6556223bb3bb6c3a41809f7e",
      "avatarUrl": "/avatars/ea7640aa9ae4fe35a2e245b3b6f8c0ff.svg",
      "fullname": "Ao Qu",
      "name": "quao627",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19753",
      "authors": [
        {
          "_id": "68f9b3f2b9b2e4ae04673918",
          "name": "Qilin Ye",
          "hidden": false
        },
        {
          "_id": "68f9b3f2b9b2e4ae04673919",
          "name": "Deqing Fu",
          "hidden": false
        },
        {
          "_id": "68f9b3f2b9b2e4ae0467391a",
          "name": "Robin Jia",
          "hidden": false
        },
        {
          "_id": "68f9b3f2b9b2e4ae0467391b",
          "name": "Vatsal Sharan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T16:43:32.000Z",
      "submittedOnDailyAt": "2025-10-23T03:21:04.789Z",
      "title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
      "submittedOnDailyBy": {
        "_id": "63c8454e46421a2efe82709d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
        "isPro": true,
        "fullname": "Deqing Fu",
        "user": "deqing",
        "type": "user"
      },
      "summary": "Transformers often fail to learn generalizable algorithms, instead relying on\nbrittle heuristics. Using graph connectivity as a testbed, we explain this\nphenomenon both theoretically and empirically. We consider a simplified\nTransformer architecture, the disentangled Transformer, and prove that an\nL-layer model has capacity to solve for graphs with diameters up to exactly\n3^L, implementing an algorithm equivalent to computing powers of the\nadjacency matrix. We analyze the training-dynamics, and show that the learned\nstrategy hinges on whether most training instances are within this model\ncapacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a\ncorrect algorithmic solution while beyond-capacity graphs drive the learning of\na simple heuristic based on node degrees. Finally, we empirically demonstrate\nthat restricting training data within a model's capacity leads to both standard\nand disentangled transformers learning the exact algorithm rather than the\ndegree-based heuristic.",
      "upvotes": 1,
      "discussionId": "68f9b3f3b9b2e4ae0467391c",
      "ai_summary": "Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.",
      "ai_keywords": [
        "Transformers",
        "disentangled Transformer",
        "graph connectivity",
        "adjacency matrix",
        "training-dynamics",
        "model capacity",
        "graph diameter",
        "algorithmic solution",
        "degree-based heuristic"
      ]
    },
    "publishedAt": "2025-10-22T12:43:32.000Z",
    "title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
    "summary": "Transformers often fail to learn generalizable algorithms, instead relying on\nbrittle heuristics. Using graph connectivity as a testbed, we explain this\nphenomenon both theoretically and empirically. We consider a simplified\nTransformer architecture, the disentangled Transformer, and prove that an\nL-layer model has capacity to solve for graphs with diameters up to exactly\n3^L, implementing an algorithm equivalent to computing powers of the\nadjacency matrix. We analyze the training-dynamics, and show that the learned\nstrategy hinges on whether most training instances are within this model\ncapacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a\ncorrect algorithmic solution while beyond-capacity graphs drive the learning of\na simple heuristic based on node degrees. Finally, we empirically demonstrate\nthat restricting training data within a model's capacity leads to both standard\nand disentangled transformers learning the exact algorithm rather than the\ndegree-based heuristic.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19753.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c8454e46421a2efe82709d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
      "fullname": "Deqing Fu",
      "name": "deqing",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19492",
      "authors": [
        {
          "_id": "68f9ab5bb9b2e4ae046738bb",
          "name": "Ryuto Koike",
          "hidden": false
        },
        {
          "_id": "68f9ab5bb9b2e4ae046738bc",
          "name": "Liam Dugan",
          "hidden": false
        },
        {
          "_id": "68f9ab5bb9b2e4ae046738bd",
          "name": "Masahiro Kaneko",
          "hidden": false
        },
        {
          "_id": "68f9ab5bb9b2e4ae046738be",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "68f9ab5bb9b2e4ae046738bf",
          "name": "Naoaki Okazaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T11:39:01.000Z",
      "submittedOnDailyAt": "2025-10-23T02:45:24.799Z",
      "title": "Machine Text Detectors are Membership Inference Attacks",
      "submittedOnDailyBy": {
        "_id": "6509e1e01b3694179dee256e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
        "isPro": false,
        "fullname": "Liam Dugan",
        "user": "liamdugan",
        "type": "user"
      },
      "summary": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks.",
      "upvotes": 1,
      "discussionId": "68f9ab5bb9b2e4ae046738c0",
      "githubRepo": "https://github.com/ryuryukke/mint",
      "ai_summary": "Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.",
      "ai_keywords": [
        "membership inference attacks",
        "machine-generated text detection",
        "language model",
        "probability distribution",
        "transferability",
        "Binoculars",
        "MINT"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-10-22T07:39:01.000Z",
    "title": "Machine Text Detectors are Membership Inference Attacks",
    "summary": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6509e1e01b3694179dee256e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
      "fullname": "Liam Dugan",
      "name": "liamdugan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18941",
      "authors": [
        {
          "_id": "68f972efb9b2e4ae04673706",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae04673707",
          "name": "Jaehun Jung",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae04673708",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae04673709",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370a",
          "name": "Ellie Evans",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370b",
          "name": "Jiaqi Zeng",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370c",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370d",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370e",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68f972efb9b2e4ae0467370f",
          "name": "Yi Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T17:59:44.000Z",
      "submittedOnDailyAt": "2025-10-23T02:05:26.971Z",
      "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Evaluating progress in large language models (LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluating LLMs in processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduce ProfBench: a set of over 7000\nresponse-criterion pairs as evaluated by human-experts with professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordable LLM-Judges to evaluate ProfBench rubrics, by\nmitigating self-enhancement bias and reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal that ProfBench poses significant challenges even for\nstate-of-the-art LLMs, with top-performing models like GPT-5-high achieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role that extended thinking plays in addressing complex,\nprofessional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBench and Code:\nhttps://github.com/NVlabs/ProfBench",
      "upvotes": 1,
      "discussionId": "68f972efb9b2e4ae04673710",
      "ai_summary": "ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "ProfBench",
        "response-criterion pairs",
        "human-experts",
        "LLM-Judges",
        "self-enhancement bias",
        "GPT-5-high",
        "extended thinking",
        "professional-domain tasks"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-21T13:59:44.000Z",
    "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
    "summary": "Evaluating progress in large language models (LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluating LLMs in processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduce ProfBench: a set of over 7000\nresponse-criterion pairs as evaluated by human-experts with professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordable LLM-Judges to evaluate ProfBench rubrics, by\nmitigating self-enhancement bias and reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal that ProfBench poses significant challenges even for\nstate-of-the-art LLMs, with top-performing models like GPT-5-high achieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role that extended thinking plays in addressing complex,\nprofessional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBench and Code:\nhttps://github.com/NVlabs/ProfBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18917",
      "authors": [
        {
          "_id": "68f980bcb9b2e4ae04673722",
          "name": "Mandip Goswami",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T06:53:14.000Z",
      "submittedOnDailyAt": "2025-10-23T01:32:06.704Z",
      "title": "RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling",
      "submittedOnDailyBy": {
        "_id": "67d43aeb6ffb8add49ea6712",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
        "isPro": false,
        "fullname": "Mandip Goswami",
        "user": "mandipgoswami",
        "type": "user"
      },
      "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies.",
      "upvotes": 1,
      "discussionId": "68f980bcb9b2e4ae04673723",
      "projectPage": "https://doi.org/10.5281/zenodo.17387402",
      "githubRepo": "https://github.com/mandip42/rirmega",
      "ai_summary": "RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.",
      "ai_keywords": [
        "Hugging Face Datasets",
        "Random Forest",
        "time features",
        "spectral features",
        "RT60",
        "room impulse responses"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-21T02:53:14.000Z",
    "title": "RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling",
    "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18917.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d43aeb6ffb8add49ea6712",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg",
      "fullname": "Mandip Goswami",
      "name": "mandipgoswami",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]