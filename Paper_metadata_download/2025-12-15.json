[
  {
    "paper": {
      "id": "2512.11558",
      "authors": [
        {
          "_id": "693f7554f516c693246811d3",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d4",
          "name": "Jiaming Zhang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d5",
          "name": "Junjie Zhao",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d6",
          "name": "Ziyi Zeng",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d7",
          "name": "Yanchao Li",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d8",
          "name": "Jingyi Liang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811d9",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811da",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811db",
          "name": "Jiajun You",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811dc",
          "name": "Shuzhi Deng",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811dd",
          "name": "Tongfei Wang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811de",
          "name": "Wanting Chen",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811df",
          "name": "Chunxiu Hao",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e0",
          "name": "Ruiqi Xie",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e1",
          "name": "Zhenwei Wen",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e2",
          "name": "Xiangyi Feng",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e3",
          "name": "Zou Ting",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e4",
          "name": "Jin Zou Lin",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e5",
          "name": "Jianquan Li",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e6",
          "name": "Guangjun Yu",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e7",
          "name": "Liangyi Chen",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e8",
          "name": "Junwen Wang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811e9",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "693f7554f516c693246811ea",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-12T13:42:57.000Z",
      "submittedOnDailyAt": "2025-12-15T00:14:07.445Z",
      "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
      "submittedOnDailyBy": {
        "_id": "64f1a34f2c5c8b767916447e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg",
        "isPro": false,
        "fullname": "Zhenyang Cai",
        "user": "Eric3200",
        "type": "user"
      },
      "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
      "upvotes": 31,
      "discussionId": "693f7555f516c693246811eb",
      "ai_summary": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.",
      "ai_keywords": [
        "multimodal large language models",
        "dentalGPT",
        "domain knowledge injection",
        "reinforcement learning",
        "annotated multimodal dataset",
        "dental images",
        "visual understanding",
        "multimodal complex reasoning",
        "intraoral benchmarks",
        "panoramic benchmarks",
        "medical VQA benchmarks",
        "disease classification",
        "dental VQA",
        "parameters"
      ]
    },
    "publishedAt": "2025-12-12T08:42:57.000Z",
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11558.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f1a34f2c5c8b767916447e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg",
      "fullname": "Zhenyang Cai",
      "name": "Eric3200",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11799",
      "authors": [
        {
          "_id": "693f741df516c693246811ad",
          "name": "Ye Fang",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811ae",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811af",
          "name": "Valentin Deschaintre",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b0",
          "name": "Duygu Ceylan",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b1",
          "name": "Iliyan Georgiev",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b2",
          "name": "Chun-Hao Paul Huang",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b3",
          "name": "Yiwei Hu",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b4",
          "name": "Xuelin Chen",
          "hidden": false
        },
        {
          "_id": "693f741df516c693246811b5",
          "name": "Tuanfeng Yang Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IKbwcaT_GNej00AHtNUA1.mp4"
      ],
      "publishedAt": "2025-12-12T18:59:54.000Z",
      "submittedOnDailyAt": "2025-12-15T00:06:37.373Z",
      "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
      "upvotes": 20,
      "discussionId": "693f741ef516c693246811b6",
      "projectPage": "https://aleafy.github.io/vrgbx/",
      "githubRepo": "https://github.com/Aleafy/V-RGBX",
      "githubRepoAddedBy": "user",
      "ai_summary": "V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.",
      "ai_keywords": [
        "video inverse rendering",
        "intrinsic channels",
        "photorealistic video synthesis",
        "keyframe-based video editing",
        "interleaved conditioning mechanism",
        "object appearance editing",
        "scene-level relighting"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-12-12T13:59:54.000Z",
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IKbwcaT_GNej00AHtNUA1.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11799.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 183
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08269",
      "authors": [
        {
          "_id": "693bc3359874a2a5e4ffb3e3",
          "user": {
            "_id": "664df2176bc1025819f81caf",
            "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg",
            "isPro": false,
            "fullname": "taewoongkang",
            "user": "Keh0t0",
            "type": "user"
          },
          "name": "Taewoong Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-12T09:14:18.431Z",
          "hidden": false
        },
        {
          "_id": "693bc3359874a2a5e4ffb3e4",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "693bc3359874a2a5e4ffb3e5",
          "name": "Dohyeon Kim",
          "hidden": false
        },
        {
          "_id": "693bc3359874a2a5e4ffb3e6",
          "name": "Minho Park",
          "hidden": false
        },
        {
          "_id": "693bc3359874a2a5e4ffb3e7",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "693bc3359874a2a5e4ffb3e8",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"
      ],
      "publishedAt": "2025-12-09T05:53:39.000Z",
      "submittedOnDailyAt": "2025-12-15T02:29:24.566Z",
      "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
      "submittedOnDailyBy": {
        "_id": "664df2176bc1025819f81caf",
        "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg",
        "isPro": false,
        "fullname": "taewoongkang",
        "user": "Keh0t0",
        "type": "user"
      },
      "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
      "upvotes": 20,
      "discussionId": "693bc3369874a2a5e4ffb3e9",
      "projectPage": "https://keh0t0.github.io/EgoX/",
      "githubRepo": "https://github.com/KEH0T0/EgoX",
      "githubRepoAddedBy": "user",
      "ai_summary": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.",
      "ai_keywords": [
        "video diffusion models",
        "LoRA adaptation",
        "unified conditioning",
        "width and channel wise concatenation",
        "geometry-guided self-attention",
        "egocentric videos",
        "exocentric videos"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-12-09T00:53:39.000Z",
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08269.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664df2176bc1025819f81caf",
      "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg",
      "fullname": "taewoongkang",
      "name": "Keh0t0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.11749",
      "authors": [
        {
          "_id": "693f754ef516c693246811c3",
          "name": "Minglei Shi",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c4",
          "name": "Haolin Wang",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c5",
          "name": "Borui Zhang",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c6",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c7",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c8",
          "name": "Ziyang Yuan",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811c9",
          "name": "Xiaoshi Wu",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811ca",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811cb",
          "name": "Huan Yang",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811cc",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811cd",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811ce",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811cf",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "693f754ef516c693246811d0",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"
      ],
      "publishedAt": "2025-12-12T17:45:03.000Z",
      "submittedOnDailyAt": "2025-12-15T00:19:12.400Z",
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "upvotes": 19,
      "discussionId": "693f754ff516c693246811d1",
      "githubRepo": "https://github.com/KlingTeam/SVG-T2I",
      "githubRepoAddedBy": "user",
      "ai_summary": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.",
      "ai_keywords": [
        "Visual Foundation Model",
        "VFM",
        "SVG",
        "Self-supervised representations for Visual Generation",
        "SVG-T2I",
        "text-to-image diffusion",
        "GenEval",
        "DPG-Bench",
        "autoencoder",
        "generation model"
      ],
      "githubStars": 14,
      "organization": {
        "_id": "662c559b322afcbae51b3c8b",
        "name": "KlingTeam",
        "fullname": "Kling Team",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
      }
    },
    "publishedAt": "2025-12-12T12:45:03.000Z",
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11749.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 183
    },
    "organization": {
      "_id": "662c559b322afcbae51b3c8b",
      "name": "KlingTeam",
      "fullname": "Kling Team",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11253",
      "authors": [
        {
          "_id": "693f75c3f516c693246811ed",
          "name": "Zhiyuan Li",
          "hidden": false
        },
        {
          "_id": "693f75c3f516c693246811ee",
          "name": "Chi-Man Pun",
          "hidden": false
        },
        {
          "_id": "693f75c3f516c693246811ef",
          "name": "Chen Fang",
          "hidden": false
        },
        {
          "_id": "693f75c3f516c693246811f0",
          "name": "Jue Wang",
          "hidden": false
        },
        {
          "_id": "693f75c3f516c693246811f1",
          "name": "Xiaodong Cun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Dqr8Qb2QmiTS3fnJWMGgC.mp4"
      ],
      "publishedAt": "2025-12-12T03:24:40.000Z",
      "submittedOnDailyAt": "2025-12-15T00:14:55.766Z",
      "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
      "upvotes": 8,
      "discussionId": "693f75c3f516c693246811f2",
      "githubRepo": "https://github.com/GVCLab/PersonaLive",
      "githubRepoAddedBy": "user",
      "ai_summary": "PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming.",
      "ai_keywords": [
        "diffusion-based framework",
        "implicit facial representations",
        "3D implicit keypoints",
        "expressive image-level motion control",
        "appearance distillation",
        "autoregressive micro-chunk streaming",
        "sliding training strategy",
        "historical keyframe mechanism"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-12-11T22:24:40.000Z",
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Dqr8Qb2QmiTS3fnJWMGgC.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 183
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.10411",
      "authors": [
        {
          "_id": "693bca119874a2a5e4ffb3fb",
          "name": "Yijiong Yu",
          "hidden": false
        },
        {
          "_id": "693bca119874a2a5e4ffb3fc",
          "name": "Jiale Liu",
          "hidden": false
        },
        {
          "_id": "693bca119874a2a5e4ffb3fd",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "693bca119874a2a5e4ffb3fe",
          "name": "Huazheng Wang",
          "hidden": false
        },
        {
          "_id": "693bca119874a2a5e4ffb3ff",
          "name": "Ji Pei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6374c494958cd71fa7ea0a9d/uufd1O2gmYQsB47R_HezI.png"
      ],
      "publishedAt": "2025-12-11T08:21:24.000Z",
      "submittedOnDailyAt": "2025-12-15T03:40:32.426Z",
      "title": "Sliding Window Attention Adaptation",
      "submittedOnDailyBy": {
        "_id": "6374c494958cd71fa7ea0a9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/b2SjfvbjYqPCW38LzkzWl.jpeg",
        "isPro": false,
        "fullname": "yuyijiong",
        "user": "yuyijiong",
        "type": "user"
      },
      "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
      "upvotes": 8,
      "discussionId": "693bca119874a2a5e4ffb400",
      "ai_summary": "Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.",
      "ai_keywords": [
        "self-attention mechanism",
        "Transformer-based Large Language Models (LLMs)",
        "sliding window attention (SWA)",
        "full attention (FA)",
        "Sliding Window Attention Adaptation (SWAA)",
        "prefilling",
        "sink tokens",
        "chain-of-thought (CoT)"
      ],
      "organization": {
        "_id": "6897df91ad3033f4085e432c",
        "name": "OregonStateUniversity",
        "fullname": "Oregon State University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6897df118dbb78d2e8837335/ssdqjm2xjvu285uuDBZbd.png"
      }
    },
    "publishedAt": "2025-12-11T03:21:24.000Z",
    "title": "Sliding Window Attention Adaptation",
    "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6374c494958cd71fa7ea0a9d/uufd1O2gmYQsB47R_HezI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6374c494958cd71fa7ea0a9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/b2SjfvbjYqPCW38LzkzWl.jpeg",
      "fullname": "yuyijiong",
      "name": "yuyijiong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 48
    },
    "organization": {
      "_id": "6897df91ad3033f4085e432c",
      "name": "OregonStateUniversity",
      "fullname": "Oregon State University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6897df118dbb78d2e8837335/ssdqjm2xjvu285uuDBZbd.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11792",
      "authors": [
        {
          "_id": "693f78a8f516c69324681216",
          "name": "Yang Fei",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c69324681217",
          "name": "George Stoica",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c69324681218",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c69324681219",
          "name": "Qifeng Chen",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c6932468121a",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c6932468121b",
          "name": "Xiaojuan Wang",
          "hidden": false
        },
        {
          "_id": "693f78a8f516c6932468121c",
          "name": "Benlin Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648ffe95669191ccb6772a2e/vHOdQzZdsRcbaNkkCf9b7.mp4"
      ],
      "publishedAt": "2025-12-12T18:56:35.000Z",
      "submittedOnDailyAt": "2025-12-15T00:52:13.247Z",
      "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
      "submittedOnDailyBy": {
        "_id": "648ffe95669191ccb6772a2e",
        "avatarUrl": "/avatars/025cac7dc64ea9ef2074754c92086baa.svg",
        "isPro": false,
        "fullname": "Yang Fei",
        "user": "sunfly",
        "type": "user"
      },
      "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
      "upvotes": 5,
      "discussionId": "693f78a8f516c6932468121d",
      "projectPage": "https://sam2videox.github.io/",
      "ai_summary": "SAM2VideoX improves realistic motion generation in video models by integrating structure-preserving priors from an autoregressive model into a bidirectional diffusion model with novel feature fusion and local alignment techniques.",
      "ai_keywords": [
        "diffusion models",
        "autoregressive video tracking model",
        "bidirectional video diffusion model",
        "bidirectional feature fusion module",
        "Local Gram Flow loss",
        "VBench",
        "FVD",
        "human preference",
        "REPA",
        "LoRA-finetuning"
      ]
    },
    "publishedAt": "2025-12-12T13:56:35.000Z",
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648ffe95669191ccb6772a2e/vHOdQzZdsRcbaNkkCf9b7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11792.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ffe95669191ccb6772a2e",
      "avatarUrl": "/avatars/025cac7dc64ea9ef2074754c92086baa.svg",
      "fullname": "Yang Fei",
      "name": "sunfly",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11464",
      "authors": [
        {
          "_id": "693f7a62f516c6932468121f",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681220",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681221",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681222",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681223",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681224",
          "name": "Weifeng Chen",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681225",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681226",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681227",
          "name": "Junzhe Sun",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681228",
          "name": "Zhipeng Fan",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c69324681229",
          "name": "Ali Thabet",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c6932468122a",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "693f7a62f516c6932468122b",
          "name": "Chu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/AXHUZPYF38Zi3K8XUNXs3.qt"
      ],
      "publishedAt": "2025-12-12T11:07:11.000Z",
      "submittedOnDailyAt": "2025-12-15T00:34:31.893Z",
      "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
      "upvotes": 5,
      "discussionId": "693f7a62f516c6932468122c",
      "ai_summary": "MetaCanvas leverages multimodal large language models as latent-space planners to enhance precise and structured image and video generation, outperforming global-conditioning methods.",
      "ai_keywords": [
        "multimodal large language models",
        "diffusion models",
        "latent spaces",
        "text-to-image generation",
        "text/image-to-video generation",
        "image/video editing",
        "in-context video generation"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2025-12-12T06:07:11.000Z",
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/AXHUZPYF38Zi3K8XUNXs3.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 183
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06818",
      "authors": [
        {
          "_id": "693fb648f516c693246812ae",
          "name": "Jan Held",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812af",
          "name": "Sanghyun Son",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b0",
          "name": "Renaud Vandeghen",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b1",
          "name": "Daniel Rebain",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b2",
          "name": "Matheus Gadelha",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b3",
          "name": "Yi Zhou",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b4",
          "name": "Anthony Cioppa",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b5",
          "name": "Ming C. Lin",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b6",
          "name": "Marc Van Droogenbroeck",
          "hidden": false
        },
        {
          "_id": "693fb648f516c693246812b7",
          "name": "Andrea Tagliasacchi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-07T12:31:04.000Z",
      "submittedOnDailyAt": "2025-12-15T04:51:16.693Z",
      "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
      "submittedOnDailyBy": {
        "_id": "6675bb599be4300672be16f0",
        "avatarUrl": "/avatars/0bca4a89515a197ed624b670f704002e.svg",
        "isPro": false,
        "fullname": "Jan Held",
        "user": "janheld14",
        "type": "user"
      },
      "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
      "upvotes": 3,
      "discussionId": "693fb648f516c693246812b8",
      "projectPage": "https://meshsplatting.github.io/",
      "githubRepo": "https://github.com/meshsplatting/mesh-splatting",
      "githubRepoAddedBy": "user",
      "ai_summary": "MeshSplatting, a mesh-based reconstruction method, enhances novel view synthesis by optimizing geometry and appearance through differentiable rendering, improving quality and efficiency over existing techniques.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "MeshSplatting",
        "differentiable rendering",
        "restricted Delaunay triangulation",
        "surface consistency",
        "PSNR",
        "MiLo",
        "Mip-NeRF360"
      ]
    },
    "publishedAt": "2025-12-07T07:31:04.000Z",
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6675bb599be4300672be16f0",
      "avatarUrl": "/avatars/0bca4a89515a197ed624b670f704002e.svg",
      "fullname": "Jan Held",
      "name": "janheld14",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11393",
      "authors": [
        {
          "_id": "693f7501f516c693246811b8",
          "name": "Zhifan Zhu",
          "hidden": false
        },
        {
          "_id": "693f7501f516c693246811b9",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "693f7501f516c693246811ba",
          "name": "Yoichi Sato",
          "hidden": false
        },
        {
          "_id": "693f7501f516c693246811bb",
          "name": "Dima Damen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fzfyGG7vXBE03zXNs6-iw.mp4"
      ],
      "publishedAt": "2025-12-12T09:07:21.000Z",
      "submittedOnDailyAt": "2025-12-15T00:10:34.183Z",
      "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
      "upvotes": 1,
      "discussionId": "693f7501f516c693246811bc",
      "projectPage": "https://zhifanzhu.github.io/ego-nbody/",
      "ai_summary": "A model learns to parallelize tasks from a single egocentric video by addressing spatial and object conflicts, achieving improved action coverage and reduced collisions.",
      "ai_keywords": [
        "N-Body Problem",
        "Vision-Language Model (VLM)",
        "3D environment",
        "object usage",
        "temporal dependencies",
        "EPIC-Kitchens",
        "HD-EPIC",
        "action coverage",
        "collision rates",
        "object conflicts",
        "causal conflicts"
      ]
    },
    "publishedAt": "2025-12-12T04:07:21.000Z",
    "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fzfyGG7vXBE03zXNs6-iw.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 183
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.10715",
      "authors": [
        {
          "_id": "693d7929f516c69324680d80",
          "name": "Matias Cosarinsky",
          "hidden": false
        },
        {
          "_id": "693d7929f516c69324680d81",
          "name": "Nicolas Gaggion",
          "hidden": false
        },
        {
          "_id": "693d7929f516c69324680d82",
          "name": "Rodrigo Echeveste",
          "hidden": false
        },
        {
          "_id": "693d7929f516c69324680d83",
          "name": "Enzo Ferrante",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-11T14:50:23.000Z",
      "submittedOnDailyAt": "2025-12-15T05:12:18.501Z",
      "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
      "submittedOnDailyBy": {
        "_id": "6735088b22d14a01ae17501f",
        "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg",
        "isPro": false,
        "fullname": "Matias Cosarinsky",
        "user": "mcosarinsky",
        "type": "user"
      },
      "summary": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
      "upvotes": 1,
      "discussionId": "693d792af516c69324680d84",
      "ai_summary": "Uncertainty estimation in landmark-based segmentation of chest X-rays using hybrid neural network architectures improves reliability and robustness in clinical deployment.",
      "ai_keywords": [
        "hybrid neural network architectures",
        "image convolutional encoders",
        "graph-based generative decoders",
        "variational latent space",
        "latent uncertainty",
        "predictive uncertainty",
        "controlled corruption experiments",
        "out-of-distribution detection",
        "CheXmask dataset",
        "CheXmask-U",
        "anatomical landmark-based segmentation"
      ]
    },
    "publishedAt": "2025-12-11T09:50:23.000Z",
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "summary": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6735088b22d14a01ae17501f",
      "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg",
      "fullname": "Matias Cosarinsky",
      "name": "mcosarinsky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06951",
      "authors": [
        {
          "_id": "6937909419d912300c34a265",
          "user": {
            "_id": "61e188e4ad060b40ab27de05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e188e4ad060b40ab27de05/74Nz_9y-RyhRR3yvjpqAr.jpeg",
            "isPro": true,
            "fullname": "Ilia Larchenko",
            "user": "IliaLarchenko",
            "type": "user"
          },
          "name": "Ilia Larchenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-09T11:47:02.189Z",
          "hidden": false
        },
        {
          "_id": "6937909419d912300c34a266",
          "name": "Gleb Zarin",
          "hidden": false
        },
        {
          "_id": "6937909419d912300c34a267",
          "name": "Akash Karnatak",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61e188e4ad060b40ab27de05/HvOPzfZ7OPPxC9Im7gn5U.qt"
      ],
      "publishedAt": "2025-12-07T18:08:45.000Z",
      "submittedOnDailyAt": "2025-12-15T00:16:35.456Z",
      "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
      "submittedOnDailyBy": {
        "_id": "61e188e4ad060b40ab27de05",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e188e4ad060b40ab27de05/74Nz_9y-RyhRR3yvjpqAr.jpeg",
        "isPro": true,
        "fullname": "Ilia Larchenko",
        "user": "IliaLarchenko",
        "type": "user"
      },
      "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
      "upvotes": 1,
      "discussionId": "6937909519d912300c34a268",
      "projectPage": "https://behavior.stanford.edu/challenge/",
      "githubRepo": "https://github.com/IliaLarchenko/behavior-1k-solution",
      "githubRepoAddedBy": "user",
      "ai_summary": "A vision-action policy using correlated noise for flow matching and learnable mixed-layer attention wins the 2025 BEHAVIOR Challenge with high performance across diverse household tasks.",
      "ai_keywords": [
        "flow matching",
        "correlated noise",
        "Pi0.5 architecture",
        "correlation-aware inpainting",
        "learnable mixed-layer attention",
        "System 2 stage tracking",
        "multi-sample flow matching",
        "action compression",
        "q-score"
      ],
      "githubStars": 104
    },
    "publishedAt": "2025-12-07T13:08:45.000Z",
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e188e4ad060b40ab27de05/HvOPzfZ7OPPxC9Im7gn5U.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e188e4ad060b40ab27de05",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e188e4ad060b40ab27de05/74Nz_9y-RyhRR3yvjpqAr.jpeg",
      "fullname": "Ilia Larchenko",
      "name": "IliaLarchenko",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 49
    },
    "isAuthorParticipating": true
  }
]