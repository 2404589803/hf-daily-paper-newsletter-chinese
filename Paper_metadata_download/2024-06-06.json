[
    {
        "paper": {
            "id": "2406.02539",
            "authors": [
                {
                    "_id": "665fc9ffb99c631f4f55d34b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
                        "isPro": false,
                        "fullname": "Allen Sun",
                        "user": "Allen8",
                        "type": "user"
                    },
                    "name": "Hai-Long Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-05T07:30:37.957Z",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d34c",
                    "user": {
                        "avatarUrl": "/avatars/134325256b9073a96c6d7733a84d2630.svg",
                        "isPro": false,
                        "fullname": "Da-Wei Zhou",
                        "user": "zhoudw",
                        "type": "user"
                    },
                    "name": "Da-Wei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:26:57.211Z",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d34d",
                    "user": {
                        "avatarUrl": "/avatars/afa6e30b42a6205d2a9de6b51ce4efaa.svg",
                        "isPro": false,
                        "fullname": "liyang",
                        "user": "liyang",
                        "type": "user"
                    },
                    "name": "Yang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-06T09:27:32.961Z",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d34e",
                    "name": "Shiyin Lu",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d34f",
                    "name": "Chao Yi",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d350",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d351",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d352",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d353",
                    "user": {
                        "avatarUrl": "/avatars/e497ba5f41a2587837b4a6118d9367bb.svg",
                        "isPro": false,
                        "fullname": "Kaifu Zhang",
                        "user": "zhangkaifu314",
                        "type": "user"
                    },
                    "name": "Kaifu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:27:52.743Z",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d354",
                    "name": "De-Chuan Zhan",
                    "hidden": false
                },
                {
                    "_id": "665fc9ffb99c631f4f55d355",
                    "name": "Han-Jia Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-04T17:56:28.000Z",
            "title": "Parrot: Multilingual Visual Instruction Tuning",
            "summary": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable.",
            "upvotes": 23
        },
        "publishedAt": "2024-06-06T02:01:45.367Z",
        "title": "Parrot: Multilingual Visual Instruction Tuning",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02539.png",
        "numComments": 1
    },
    {
        "paper": {
            "id": "2406.01014",
            "authors": [
                {
                    "_id": "666113b372f344f198df601c",
                    "user": {
                        "avatarUrl": "/avatars/c83dbd3e10e88db97c2a86092bad5917.svg",
                        "isPro": false,
                        "fullname": "Junyang Wang",
                        "user": "junyangwang0410",
                        "type": "user"
                    },
                    "name": "Junyang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:16:21.069Z",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df601d",
                    "user": {
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-06T06:55:55.255Z",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df601e",
                    "name": "Haitao Jia",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df601f",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df6020",
                    "user": {
                        "avatarUrl": "/avatars/48adf00c3b653df02628f80511639e19.svg",
                        "isPro": false,
                        "fullname": "Ming",
                        "user": "MingYan123",
                        "type": "user"
                    },
                    "name": "Ming Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:14:41.428Z",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df6021",
                    "user": {
                        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
                        "isPro": false,
                        "fullname": "Weizhou Shen",
                        "user": "shenwzh3",
                        "type": "user"
                    },
                    "name": "Weizhou Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:14:54.788Z",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df6022",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df6023",
                    "user": {
                        "avatarUrl": "/avatars/229fb72180529141515d1df797b33709.svg",
                        "isPro": false,
                        "fullname": "Fei Huang",
                        "user": "hzhwcmhf",
                        "type": "user"
                    },
                    "name": "Fei Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:13:19.531Z",
                    "hidden": false
                },
                {
                    "_id": "666113b372f344f198df6024",
                    "name": "Jitao Sang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-03T05:50:00.000Z",
            "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective\n  Navigation via Multi-Agent Collaboration",
            "summary": "Mobile device operation tasks are increasingly becoming a popular multi-modal\nAI application scenario. Current Multi-modal Large Language Models (MLLMs),\nconstrained by their training data, lack the capability to function effectively\nas operation assistants. Instead, MLLM-based agents, which enhance capabilities\nthrough tool invocation, are gradually being applied to this scenario. However,\nthe two major navigation challenges in mobile device operation tasks, task\nprogress navigation and focus content navigation, are significantly complicated\nunder the single-agent architecture of existing work. This is due to the overly\nlong token sequences and the interleaved text-image data format, which limit\nperformance. To address these navigation challenges effectively, we propose\nMobile-Agent-v2, a multi-agent architecture for mobile device operation\nassistance. The architecture comprises three agents: planning agent, decision\nagent, and reflection agent. The planning agent generates task progress, making\nthe navigation of history operations more efficient. To retain focus content,\nwe design a memory unit that updates with task progress. Additionally, to\ncorrect erroneous operations, the reflection agent observes the outcomes of\neach operation and handles any mistakes accordingly. Experimental results\nindicate that Mobile-Agent-v2 achieves over a 30% improvement in task\ncompletion compared to the single-agent architecture of Mobile-Agent. The code\nis open-sourced at https://github.com/X-PLUG/MobileAgent.",
            "upvotes": 20
        },
        "publishedAt": "2024-06-06T04:06:27.283Z",
        "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.01014.png",
        "numComments": 1
    },
    {
        "paper": {
            "id": "2406.02657",
            "authors": [
                {
                    "_id": "666120834e5af73c6a4ea91d",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677826870505-62d0f7faad741b94f5d13744.jpeg",
                        "isPro": false,
                        "fullname": "Namgyu Ho",
                        "user": "itsnamgyu",
                        "type": "user"
                    },
                    "name": "Namgyu Ho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:13:42.601Z",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea91e",
                    "user": {
                        "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
                        "isPro": false,
                        "fullname": "Sangmin Bae",
                        "user": "raymin0223",
                        "type": "user"
                    },
                    "name": "Sangmin Bae",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:13:59.532Z",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea91f",
                    "user": {
                        "avatarUrl": "/avatars/a797b72363e06cd2222165493bf7e39c.svg",
                        "isPro": false,
                        "fullname": "Taehyeon Kim",
                        "user": "Kthyeon",
                        "type": "user"
                    },
                    "name": "Taehyeon Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:14:45.383Z",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea920",
                    "name": "Hyunjik Jo",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea921",
                    "user": {
                        "avatarUrl": "/avatars/9be000990e11c4c73a4217416bcffd34.svg",
                        "isPro": false,
                        "fullname": "Yireun Kim",
                        "user": "yireun",
                        "type": "user"
                    },
                    "name": "Yireun Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:20:39.322Z",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea922",
                    "user": {
                        "avatarUrl": "/avatars/4afb3cc5b940b24bd2794d07f694b9ef.svg",
                        "isPro": false,
                        "fullname": "Tal Schuster",
                        "user": "tals",
                        "type": "user"
                    },
                    "name": "Tal Schuster",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:20:45.770Z",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea923",
                    "name": "Adam Fisch",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea924",
                    "name": "James Thorne",
                    "hidden": false
                },
                {
                    "_id": "666120834e5af73c6a4ea925",
                    "name": "Se-Young Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-04T17:45:26.000Z",
            "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
            "summary": "This paper presents the Block Transformer architecture which adopts\nhierarchical global-to-local modeling to autoregressive transformers to\nmitigate the inference bottlenecks of self-attention. To apply self-attention,\nthe key-value (KV) cache of all previous sequences must be retrieved from\nmemory at every decoding step. Thereby, this KV cache IO becomes a significant\nbottleneck in batch inference. We notice that these costs stem from applying\nself-attention on the global context, therefore we isolate the expensive\nbottlenecks of global modeling to lower layers and apply fast local modeling in\nupper layers. To mitigate the remaining costs in the lower layers, we aggregate\ninput tokens into fixed size blocks and then apply self-attention at this\ncoarse level. Context information is aggregated into a single embedding to\nenable upper layers to decode the next block of tokens, without global\nattention. Free of global attention bottlenecks, the upper layers can fully\nutilize the compute hardware to maximize inference throughput. By leveraging\nglobal and local modules, the Block Transformer architecture demonstrates\n10-20x gains in inference throughput compared to vanilla transformers with\nequivalent perplexity. Our work introduces a new approach to optimize language\nmodel inference through novel application of global-to-local modeling. Code is\navailable at https://github.com/itsnamgyu/block-transformer.",
            "upvotes": 14
        },
        "publishedAt": "2024-06-06T02:35:49.034Z",
        "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02657.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.03184",
            "authors": [
                {
                    "_id": "666119207427717d286d8ece",
                    "name": "Hao Wen",
                    "hidden": false
                },
                {
                    "_id": "666119207427717d286d8ecf",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                        "isPro": false,
                        "fullname": "zehuan-huang",
                        "user": "huanngzh",
                        "type": "user"
                    },
                    "name": "Zehuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:25:33.462Z",
                    "hidden": false
                },
                {
                    "_id": "666119207427717d286d8ed0",
                    "user": {
                        "avatarUrl": "/avatars/a42092119777d65e60b12eb5ba0e45f1.svg",
                        "isPro": false,
                        "fullname": "Yaohui Wang",
                        "user": "YaohuiW",
                        "type": "user"
                    },
                    "name": "Yaohui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:25:22.841Z",
                    "hidden": false
                },
                {
                    "_id": "666119207427717d286d8ed1",
                    "user": {
                        "avatarUrl": "/avatars/73ec521ab5ba84cc7908c52c0acef6ef.svg",
                        "isPro": false,
                        "fullname": "Xinyuan Chen",
                        "user": "AriaChen",
                        "type": "user"
                    },
                    "name": "Xinyuan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T07:24:50.672Z",
                    "hidden": false
                },
                {
                    "_id": "666119207427717d286d8ed2",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "666119207427717d286d8ed3",
                    "name": "Lu Sheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T12:15:22.000Z",
            "title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion",
            "summary": "Existing single image-to-3D creation methods typically involve a two-stage\nprocess, first generating multi-view images, and then using these images for 3D\nreconstruction. However, training these two stages separately leads to\nsignificant data bias in the inference phase, thus affecting the quality of\nreconstructed results. We introduce a unified 3D generation framework, named\nOuroboros3D, which integrates diffusion-based multi-view image generation and\n3D reconstruction into a recursive diffusion process. In our framework, these\ntwo modules are jointly trained through a self-conditioning mechanism, allowing\nthem to adapt to each other's characteristics for robust inference. During the\nmulti-view denoising process, the multi-view diffusion model uses the 3D-aware\nmaps rendered by the reconstruction module at the previous timestep as\nadditional conditions. The recursive diffusion framework with 3D-aware feedback\nunites the entire process and improves geometric consistency.Experiments show\nthat our framework outperforms separation of these two stages and existing\nmethods that combine them at the inference phase. Project page:\nhttps://costwen.github.io/Ouroboros3D/",
            "upvotes": 14
        },
        "publishedAt": "2024-06-06T02:04:18.671Z",
        "title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.03184.png",
        "numComments": 2
    },
    {
        "paper": {
            "id": "2406.03215",
            "authors": [
                {
                    "_id": "66613546754f90a8582cfbfa",
                    "user": {
                        "avatarUrl": "/avatars/fcbae1dc7134e625f2d79f767b34e142.svg",
                        "isPro": false,
                        "fullname": "Haoran Cheng",
                        "user": "chrxx98",
                        "type": "user"
                    },
                    "name": "Haoran Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:05:26.459Z",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfbfb",
                    "user": {
                        "avatarUrl": "/avatars/76cb9044866017749c626001da1762e8.svg",
                        "isPro": false,
                        "fullname": "PengLiang",
                        "user": "PengLiang002",
                        "type": "user"
                    },
                    "name": "Liang Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-06T10:53:31.404Z",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfbfc",
                    "user": {
                        "avatarUrl": "/avatars/8c944d27f934760fecd31942464ea9de.svg",
                        "isPro": false,
                        "fullname": "Linxuan Xia",
                        "user": "Charlo7te",
                        "type": "user"
                    },
                    "name": "Linxuan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:05:57.120Z",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfbfd",
                    "user": {
                        "avatarUrl": "/avatars/cb238ed8549150f88207d9206787d083.svg",
                        "isPro": false,
                        "fullname": "Yuepeng Hu",
                        "user": "HifiHu",
                        "type": "user"
                    },
                    "name": "Yuepeng Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:06:04.579Z",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfbfe",
                    "name": "Hengjia Li",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfbff",
                    "user": {
                        "avatarUrl": "/avatars/a1d0480fa94f59ed5db3a1bba09bd660.svg",
                        "isPro": false,
                        "fullname": "lu",
                        "user": "qinglin3",
                        "type": "user"
                    },
                    "name": "Qinglin Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:07:00.863Z",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfc00",
                    "name": "Xiaofei He",
                    "hidden": false
                },
                {
                    "_id": "66613546754f90a8582cfc01",
                    "name": "Boxi Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T12:53:28.000Z",
            "title": "Searching Priors Makes Text-to-Video Synthesis Better",
            "summary": "Significant advancements in video diffusion models have brought substantial\nprogress to the field of text-to-video (T2V) synthesis. However, existing T2V\nsynthesis model struggle to accurately generate complex motion dynamics,\nleading to a reduction in video realism. One possible solution is to collect\nmassive data and train the model on it, but this would be extremely expensive.\nTo alleviate this problem, in this paper, we reformulate the typical T2V\ngeneration process as a search-based generation pipeline. Instead of scaling up\nthe model training, we employ existing videos as the motion prior database.\nSpecifically, we divide T2V generation process into two steps: (i) For a given\nprompt input, we search existing text-video datasets to find videos with text\nlabels that closely match the prompt motions. We propose a tailored search\nalgorithm that emphasizes object motion features. (ii) Retrieved videos are\nprocessed and distilled into motion priors to fine-tune a pre-trained base T2V\nmodel, followed by generating desired videos using input prompt. By utilizing\nthe priors gleaned from the searched videos, we enhance the realism of the\ngenerated videos' motion. All operations can be finished on a single NVIDIA RTX\n4090 GPU. We validate our method against state-of-the-art T2V models across\ndiverse prompt inputs. The code will be public.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-06T04:04:26.160Z",
        "title": "Searching Priors Makes Text-to-Video Synthesis Better",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.03215.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.02897",
            "authors": [
                {
                    "_id": "6661279f3902e37d2d90bc5c",
                    "user": {
                        "avatarUrl": "/avatars/673bd6d1a5c6d58073b5feeea9c7c5e1.svg",
                        "isPro": false,
                        "fullname": "Trung Dang",
                        "user": "trungdang",
                        "type": "user"
                    },
                    "name": "Trung Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-06T15:01:05.958Z",
                    "hidden": false
                },
                {
                    "_id": "6661279f3902e37d2d90bc5d",
                    "user": {
                        "avatarUrl": "/avatars/19dc5be2c790b7be73301a0beb9df94a.svg",
                        "isPro": false,
                        "fullname": "David Aponte",
                        "user": "bambamcito",
                        "type": "user"
                    },
                    "name": "David Aponte",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:46:25.079Z",
                    "hidden": false
                },
                {
                    "_id": "6661279f3902e37d2d90bc5e",
                    "name": "Dung Tran",
                    "hidden": false
                },
                {
                    "_id": "6661279f3902e37d2d90bc5f",
                    "user": {
                        "avatarUrl": "/avatars/f3dd763d8e092f68cbc33469f148b337.svg",
                        "isPro": false,
                        "fullname": "Kazuhito Koishida",
                        "user": "koishida",
                        "type": "user"
                    },
                    "name": "Kazuhito Koishida",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-06T03:06:08.399Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T03:36:11.000Z",
            "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive\n  Modeling of Audio Discrete Codes",
            "summary": "Prior works have demonstrated zero-shot text-to-speech by using a generative\nlanguage model on audio tokens obtained via a neural audio codec. It is still\nchallenging, however, to adapt them to low-latency scenarios. In this paper, we\npresent LiveSpeech - a fully autoregressive language model-based approach for\nzero-shot text-to-speech, enabling low-latency streaming of the output audio.\nTo allow multiple token prediction within a single decoding step, we propose\n(1) using adaptive codebook loss weights that consider codebook contribution in\neach frame and focus on hard instances, and (2) grouping codebooks and\nprocessing groups in parallel. Experiments show our proposed models achieve\ncompetitive results to state-of-the-art baselines in terms of content accuracy,\nspeaker similarity, audio quality, and inference speed while being suitable for\nlow-latency streaming applications.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-06T03:06:08.419Z",
        "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02897.png",
        "numComments": 1
    },
    {
        "paper": {
            "id": "2406.03344",
            "authors": [
                {
                    "_id": "66612ef00dbf4ebf86c08924",
                    "user": {
                        "avatarUrl": "/avatars/f683bd957502162bad10039c28c8c244.svg",
                        "isPro": false,
                        "fullname": "Mehmet Hamza Erol",
                        "user": "mhamzaerol",
                        "type": "user"
                    },
                    "name": "Mehmet Hamza Erol",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:47:35.852Z",
                    "hidden": false
                },
                {
                    "_id": "66612ef00dbf4ebf86c08925",
                    "user": {
                        "avatarUrl": "/avatars/bb06fc9a47cb78ca9b4e9e7da5cc6b58.svg",
                        "isPro": false,
                        "fullname": "Arda Senocak",
                        "user": "ardasnck",
                        "type": "user"
                    },
                    "name": "Arda Senocak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:47:43.691Z",
                    "hidden": false
                },
                {
                    "_id": "66612ef00dbf4ebf86c08926",
                    "name": "Jiu Feng",
                    "hidden": false
                },
                {
                    "_id": "66612ef00dbf4ebf86c08927",
                    "user": {
                        "avatarUrl": "/avatars/56fbf5604dc4082b0c494bd49f345c71.svg",
                        "isPro": false,
                        "fullname": "Joon Son Chung",
                        "user": "joonson",
                        "type": "user"
                    },
                    "name": "Joon Son Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T08:48:07.919Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T15:00:59.000Z",
            "title": "Audio Mamba: Bidirectional State Space Model for Audio Representation\n  Learning",
            "summary": "Transformers have rapidly become the preferred choice for audio\nclassification, surpassing methods based on CNNs. However, Audio Spectrogram\nTransformers (ASTs) exhibit quadratic scaling due to self-attention. The\nremoval of this quadratic self-attention cost presents an appealing direction.\nRecently, state space models (SSMs), such as Mamba, have demonstrated potential\nin language and vision tasks in this regard. In this study, we explore whether\nreliance on self-attention is necessary for audio classification tasks. By\nintroducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based\nmodel for audio classification, we aim to address this question. We evaluate\nAuM on various audio datasets - comprising six different benchmarks - where it\nachieves comparable or better performance compared to well-established AST\nmodel.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-06T03:37:21.059Z",
        "title": "Audio Mamba: Bidirectional State Space Model for Audio Representation Learning",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.03344.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.02900",
            "authors": [
                {
                    "_id": "66612cac8585c09f28721245",
                    "name": "Rafael Rafailov",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f28721246",
                    "user": {
                        "avatarUrl": "/avatars/1bb03e24954d67b709827fe7af7a0634.svg",
                        "isPro": false,
                        "fullname": "Yaswanth Chittepu",
                        "user": "yaswanthchittepu",
                        "type": "user"
                    },
                    "name": "Yaswanth Chittepu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-06T12:15:24.221Z",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f28721247",
                    "name": "Ryan Park",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f28721248",
                    "user": {
                        "avatarUrl": "/avatars/50ad7ffcc226c9ed7c6a78634a5fc8b4.svg",
                        "isPro": false,
                        "fullname": "Harshit Sikchi",
                        "user": "hsikchi",
                        "type": "user"
                    },
                    "name": "Harshit Sikchi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-06T03:27:43.214Z",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f28721249",
                    "name": "Joey Hejna",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f2872124a",
                    "name": "Bradley Knox",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f2872124b",
                    "name": "Chelsea Finn",
                    "hidden": false
                },
                {
                    "_id": "66612cac8585c09f2872124c",
                    "name": "Scott Niekum",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T03:41:37.000Z",
            "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment\n  Algorithms",
            "summary": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the\nrecent success of Large Language Models (LLMs), however, it is often a complex\nand brittle process. In the classical RLHF framework, a reward model is first\ntrained to represent human preferences, which is in turn used by an online\nreinforcement learning (RL) algorithm to optimize the LLM. A prominent issue\nwith such methods is reward over-optimization or reward hacking,\nwhere performance as measured by the learned proxy reward model increases, but\ntrue quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs)\nlike Direct Preference Optimization have emerged as alternatives to the\nclassical RLHF pipeline by circumventing the reward modeling phase. However,\nalthough DAAs do not use a separate proxy reward model, they still commonly\ndeteriorate from over-optimization. While the so-called reward hacking\nphenomenon is not well-defined for DAAs, we still uncover similar trends: at\nhigher KL budgets, DAA algorithms exhibit similar degradation patterns to their\nclassic RLHF counterparts. In particular, we find that DAA methods deteriorate\nnot only across a wide range of KL budgets but also often before even a single\nepoch of the dataset is completed. Through extensive empirical experimentation,\nthis work formulates and formalizes the reward over-optimization or hacking\nproblem for DAAs and explores its consequences across objectives, training\nregimes, and model scales.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-06T03:27:43.242Z",
        "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02900.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.02884",
            "authors": [
                {
                    "_id": "666124877259d915ce0fd888",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "666124877259d915ce0fd889",
                    "user": {
                        "avatarUrl": "/avatars/00c3f71b9617c3a8c35212ef90527315.svg",
                        "isPro": false,
                        "fullname": "Yingmin Luo",
                        "user": "luoyingmin",
                        "type": "user"
                    },
                    "name": "Yingmin Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T09:27:36.813Z",
                    "hidden": false
                },
                {
                    "_id": "666124877259d915ce0fd88a",
                    "name": "Zhongang Qi",
                    "hidden": false
                },
                {
                    "_id": "666124877259d915ce0fd88b",
                    "name": "Yang Wu",
                    "hidden": false
                },
                {
                    "_id": "666124877259d915ce0fd88c",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "666124877259d915ce0fd88d",
                    "name": "Chang Wen Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T03:05:52.000Z",
            "title": "PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with\n  LLM",
            "summary": "Layout generation is the keystone in achieving automated graphic design,\nrequiring arranging the position and size of various multi-modal design\nelements in a visually pleasing and constraint-following manner. Previous\napproaches are either inefficient for large-scale applications or lack\nflexibility for varying design requirements. Our research introduces a unified\nframework for automated graphic layout generation, leveraging the multi-modal\nlarge language model (MLLM) to accommodate diverse design tasks. In contrast,\nour data-driven method employs structured text (JSON format) and visual\ninstruction tuning to generate layouts under specific visual and textual\nconstraints, including user-defined natural language specifications. We\nconducted extensive experiments and achieved state-of-the-art (SOTA)\nperformance on public multi-modal layout generation benchmarks, demonstrating\nthe effectiveness of our method. Moreover, recognizing existing datasets'\nlimitations in capturing the complexity of real-world graphic designs, we\npropose two new datasets for much more challenging tasks (user-constrained\ngeneration and complicated poster), further validating our model's utility in\nreal-life settings. Marking by its superior accessibility and adaptability,\nthis approach further automates large-scale graphic design tasks. The code and\ndatasets will be publicly available on\nhttps://github.com/posterllava/PosterLLaVA.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-06T02:52:57.994Z",
        "title": "PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with LLM",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02884.png",
        "numComments": 1
    },
    {
        "paper": {
            "id": "2406.02856",
            "authors": [
                {
                    "_id": "6661232202a5e5f4374e5492",
                    "name": "Yichuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6661232202a5e5f4374e5493",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6661232202a5e5f4374e5494",
                    "name": "Yu Yan",
                    "hidden": false
                },
                {
                    "_id": "6661232202a5e5f4374e5495",
                    "user": {
                        "avatarUrl": "/avatars/e5bbc245cef4ff5ca3f9e1ab28bb8b87.svg",
                        "isPro": false,
                        "fullname": "Xucheng Huang",
                        "user": "hxc123china",
                        "type": "user"
                    },
                    "name": "Xucheng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:18:29.757Z",
                    "hidden": false
                },
                {
                    "_id": "6661232202a5e5f4374e5496",
                    "name": "Ling Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T02:12:06.000Z",
            "title": "Xmodel-LM Technical Report",
            "summary": "We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on over 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-06T02:46:59.263Z",
        "title": "Xmodel-LM Technical Report",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02856.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.02886",
            "authors": [
                {
                    "_id": "66612600fb3dfd49f05b2215",
                    "user": {
                        "avatarUrl": "/avatars/fbbbc1347f8e423b2477e2506fdb43d9.svg",
                        "isPro": false,
                        "fullname": "Rongzhi Zhang",
                        "user": "Solute",
                        "type": "user"
                    },
                    "name": "Rongzhi Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-06T02:59:13.439Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b2216",
                    "user": {
                        "avatarUrl": "/avatars/cbc7840a6fb318dd37ce91347a7f96e4.svg",
                        "isPro": false,
                        "fullname": "Jiaming Shen",
                        "user": "jmshen",
                        "type": "user"
                    },
                    "name": "Jiaming Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:08:16.564Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b2217",
                    "user": {
                        "avatarUrl": "/avatars/39ce8999c296584ee0bdeb7848eee6d9.svg",
                        "isPro": false,
                        "fullname": "Tianqi Liu",
                        "user": "TianqiLiuAI",
                        "type": "user"
                    },
                    "name": "Tianqi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:08:45.814Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b2218",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665624822880-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Wang Haorui",
                        "user": "JerrySkywalker",
                        "type": "user"
                    },
                    "name": "Haorui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:08:58.413Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b2219",
                    "name": "Zhen Qin",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b221a",
                    "name": "Feng Han",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b221b",
                    "user": {
                        "avatarUrl": "/avatars/dc87416dcb2806a6e0d60bd636024978.svg",
                        "isPro": false,
                        "fullname": "liujialu",
                        "user": "Jialuliu",
                        "type": "user"
                    },
                    "name": "Jialu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:10:23.991Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b221c",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560f697e0a7720b6ae377bc/8b-p-lQ7KV_VygBJ6Vf3_.jpeg",
                        "isPro": false,
                        "fullname": "Simon Baumgartner",
                        "user": "sens3",
                        "type": "user"
                    },
                    "name": "Simon Baumgartner",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:10:32.982Z",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b221d",
                    "name": "Michael Bendersky",
                    "hidden": false
                },
                {
                    "_id": "66612600fb3dfd49f05b221e",
                    "name": "Chao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T03:08:25.000Z",
            "title": "PLaD: Preference-based Large Language Model Distillation with\n  Pseudo-Preference Pairs",
            "summary": "Large Language Models (LLMs) have exhibited impressive capabilities in\nvarious tasks, yet their vast parameter sizes restrict their applicability in\nresource-constrained settings. Knowledge distillation (KD) offers a viable\nsolution by transferring expertise from large teacher models to compact student\nmodels. However, traditional KD techniques face specific challenges when\napplied to LLMs, including restricted access to LLM outputs, significant\nteacher-student capacity gaps, and the inherited mis-calibration issue. In this\nwork, we present PLaD, a novel preference-based LLM distillation framework.\nPLaD exploits the teacher-student capacity discrepancy to generate\npseudo-preference pairs where teacher outputs are preferred over student\noutputs. Then, PLaD leverages a ranking loss to re-calibrate student's\nestimation of sequence likelihood, which steers the student's focus towards\nunderstanding the relative quality of outputs instead of simply imitating the\nteacher. PLaD bypasses the need for access to teacher LLM's internal states,\ntackles the student's expressivity limitations, and mitigates the student\nmis-calibration issue. Through extensive experiments on two sequence generation\ntasks and with various LLMs, we demonstrate the effectiveness of our proposed\nPLaD framework.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-06T02:59:13.459Z",
        "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02886.png",
        "numComments": 0
    },
    {
        "paper": {
            "id": "2406.02844",
            "authors": [
                {
                    "_id": "66612192754f90a85825e701",
                    "name": "Li Yang",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e702",
                    "name": "Anushya Subbiah",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e703",
                    "user": {
                        "avatarUrl": "/avatars/903d54804ef039137eca13b668ca8331.svg",
                        "isPro": false,
                        "fullname": "Hardik Patel",
                        "user": "hardikp",
                        "type": "user"
                    },
                    "name": "Hardik Patel",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-06T12:34:47.974Z",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e704",
                    "user": {
                        "avatarUrl": "/avatars/7ac0936f469e63cfe8ce15849e128a43.svg",
                        "isPro": false,
                        "fullname": "Judith Yue Li",
                        "user": "whereisdodo",
                        "type": "user"
                    },
                    "name": "Judith Yue Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:19:45.590Z",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e705",
                    "user": {
                        "avatarUrl": "/avatars/1cf4b573cf28b3649776f6057d0aaf8c.svg",
                        "isPro": false,
                        "fullname": "Song Yanwei",
                        "user": "songyw9",
                        "type": "user"
                    },
                    "name": "Yanwei Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-06T10:19:57.540Z",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e706",
                    "name": "Reza Mirghaderi",
                    "hidden": false
                },
                {
                    "_id": "66612192754f90a85825e707",
                    "name": "Vikram Aggarwal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-05T01:35:50.000Z",
            "title": "Item-Language Model for Conversational Recommendation",
            "summary": "Large-language Models (LLMs) have been extremely successful at tasks like\ncomplex dialogue understanding, reasoning and coding due to their emergent\nabilities. These emergent abilities have been extended with multi-modality to\ninclude image, audio, and video capabilities. Recommender systems, on the other\nhand, have been critical for information seeking and item discovery needs.\nRecently, there have been attempts to apply LLMs for recommendations. One\ndifficulty of current attempts is that the underlying LLM is usually not\ntrained on the recommender system data, which largely contains user interaction\nsignals and is often not publicly available. Another difficulty is user\ninteraction signals often have a different pattern from natural language text,\nand it is currently unclear if the LLM training setup can learn more\nnon-trivial knowledge from interaction signals compared with traditional\nrecommender system methods. Finally, it is difficult to train multiple LLMs for\ndifferent use-cases, and to retain the original language and reasoning\nabilities when learning from recommender system data. To address these three\nlimitations, we propose an Item-Language Model (ILM), which is composed of an\nitem encoder to produce text-aligned item representations that encode user\ninteraction signals, and a frozen LLM that can understand those item\nrepresentations with preserved pretrained knowledge. We conduct extensive\nexperiments which demonstrate both the importance of the language-alignment and\nof user interaction knowledge in the item encoder.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-06T02:40:19.255Z",
        "title": "Item-Language Model for Conversational Recommendation",
        "mediaUrl": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02844.png",
        "numComments": 0
    }
]