[
  {
    "paper": {
      "id": "2512.04677",
      "authors": [
        {
          "_id": "693251d36d1060ca587a2746",
          "name": "Yubo Huang",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a2747",
          "name": "Hailong Guo",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a2748",
          "name": "Fangtai Wu",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a2749",
          "name": "Shifeng Zhang",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274a",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274b",
          "name": "Qijun Gan",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274c",
          "name": "Lin Liu",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274d",
          "name": "Sirui Zhao",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274e",
          "name": "Enhong Chen",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a274f",
          "name": "Jiaming Liu",
          "hidden": false
        },
        {
          "_id": "693251d36d1060ca587a2750",
          "name": "Steven Hoi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T11:11:24.000Z",
      "submittedOnDailyAt": "2025-12-05T01:00:58.773Z",
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
      "upvotes": 57,
      "discussionId": "693251d46d1060ca587a2751",
      "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.",
      "ai_keywords": [
        "diffusion-based video generation",
        "sequential computation",
        "long-horizon inconsistency",
        "real-time",
        "streaming audio-driven avatar synthesis",
        "Timestep-forcing Pipeline Parallelism",
        "distributed inference paradigm",
        "pipeline parallelism",
        "denoising steps",
        "autoregressive bottleneck",
        "low-latency real-time streaming",
        "Rolling Sink Frame Mechanism",
        "sequence fidelity",
        "Self-Forcing Distribution Matching Distillation",
        "causal",
        "streamable adaptation",
        "visual quality",
        "end-to-end generation",
        "H800 GPUs"
      ]
    },
    "publishedAt": "2025-12-04T06:11:24.000Z",
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8841
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04987",
      "authors": [
        {
          "_id": "6932458a6d1060ca587a2618",
          "name": "Nex-AGI Team",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261a",
          "name": "Yuxuan Cai",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261b",
          "name": "Lu Chen",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261c",
          "name": "Qiaoling Chen",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261d",
          "name": "Yuyang Ding",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261e",
          "name": "Liwen Fan",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a261f",
          "name": "Wenjie Fu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2620",
          "name": "Yufei Gao",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2621",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2622",
          "name": "Pinxue Guo",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2623",
          "name": "Zhenhua Han",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2624",
          "name": "Zhengfu He",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2625",
          "name": "Hanglei Hu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2626",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2627",
          "name": "Shengjia Hua",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2628",
          "name": "Tianyu Huai",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2629",
          "name": "Baodai Huang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262a",
          "name": "Li Ji",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262b",
          "name": "Zhen Jiang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262c",
          "name": "Zhikai Lei",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262d",
          "name": "Bufan Li",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262e",
          "name": "Jiahang Lin",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a262f",
          "name": "Lizhi Lin",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2630",
          "name": "Jinxiu Liu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2631",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2632",
          "name": "Ziming Liu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2633",
          "name": "Yuchen Ni",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2634",
          "name": "Pengfang Qian",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2635",
          "name": "Yujiong Shen",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2636",
          "name": "Qingyun Shi",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2637",
          "name": "Wentao Shu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2638",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2639",
          "name": "Yiran Suo",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263a",
          "name": "Tian Tang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263b",
          "name": "Boyu Tian",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263c",
          "name": "Guoteng Wang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263d",
          "name": "Junzhe Wang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263e",
          "name": "Peixin Wang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a263f",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2640",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2641",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2642",
          "name": "Zhixiong Yang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2643",
          "name": "Tianchu Yao",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2644",
          "name": "Guangze Ye",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2645",
          "name": "Qianxi Yu",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2646",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2647",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2648",
          "name": "Yiqi Zhang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2649",
          "name": "Jiarong Zhao",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264a",
          "name": "Miao Zheng",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264b",
          "name": "Rui Zheng",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264c",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264d",
          "name": "Jiazheng Zhou",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264e",
          "name": "Maosen Zhou",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a264f",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2650",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2651",
          "name": "Yining Zheng",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2652",
          "name": "Xinchi Chen",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2653",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2654",
          "name": "Siyuan Feng",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2655",
          "name": "Qin Chen",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2656",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2657",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2658",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "6932458a6d1060ca587a2659",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T16:57:02.000Z",
      "submittedOnDailyAt": "2025-12-05T00:08:10.618Z",
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "upvotes": 48,
      "discussionId": "6932458a6d1060ca587a265a",
      "githubRepo": "https://github.com/nex-agi/Nex-N1",
      "ai_summary": "The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.",
      "ai_keywords": [
        "Large Language Models",
        "autonomous agents",
        "incentive-driven decision making",
        "policy learning",
        "interaction signals",
        "NexAU",
        "agent hierarchies",
        "NexA4A",
        "natural language",
        "NexGAP",
        "simulation-reality gap",
        "grounded trajectories synthesis",
        "SWE-bench",
        "tau2"
      ],
      "githubStars": 55,
      "organization": {
        "_id": "6907441c72f7d95376e910a5",
        "name": "nex-agi",
        "fullname": "Nex AGI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"
      }
    },
    "publishedAt": "2025-12-04T11:57:02.000Z",
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04987.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 177
    },
    "organization": {
      "_id": "6907441c72f7d95376e910a5",
      "name": "nex-agi",
      "fullname": "Nex AGI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04324",
      "authors": [
        {
          "_id": "693245c66d1060ca587a265c",
          "name": "Fangyu Lei",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a265d",
          "name": "Jinxiang Meng",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a265e",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a265f",
          "name": "Junjie Zhao",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2660",
          "name": "Yitong Zhang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2661",
          "name": "Jianwen Luo",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2662",
          "name": "Xin Zou",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2663",
          "name": "Ruiyi Yang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2664",
          "name": "Wenbo Shi",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2665",
          "name": "Yan Gao",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2666",
          "name": "Shizhu He",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2667",
          "name": "Zuo Wang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2668",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a2669",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a266a",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a266b",
          "name": "Jun Zhao",
          "hidden": false
        },
        {
          "_id": "693245c66d1060ca587a266c",
          "name": "Kang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-03T23:21:28.000Z",
      "submittedOnDailyAt": "2025-12-05T00:09:12.656Z",
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io",
      "upvotes": 42,
      "discussionId": "693245c66d1060ca587a266d",
      "projectPage": "https://da-comp.github.io/",
      "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.",
      "ai_keywords": [
        "data engineering",
        "data analysis",
        "DE tasks",
        "DA tasks",
        "SQL pipelines",
        "multi-metric evaluation",
        "LLM-judge",
        "hierarchical rubrics",
        "autonomous data agents"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-12-03T18:21:28.000Z",
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 177
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04678",
      "authors": [
        {
          "_id": "693248cc6d1060ca587a26fa",
          "name": "Yunhong Lu",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a26fb",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a26fc",
          "name": "Haobo Li",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a26fd",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a26fe",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a26ff",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2700",
          "name": "Jiapeng Zhu",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2701",
          "name": "Hengyuan Cao",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2702",
          "name": "Zhipeng Zhang",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2703",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2704",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "693248cc6d1060ca587a2705",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T11:12:13.000Z",
      "submittedOnDailyAt": "2025-12-05T00:25:31.113Z",
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "submittedOnDailyBy": {
        "_id": "63d4b843df01ef426a0f79fb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg",
        "isPro": false,
        "fullname": "Yanhong Zeng",
        "user": "zengyh1900",
        "type": "user"
      },
      "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
      "upvotes": 29,
      "discussionId": "693248cc6d1060ca587a2706",
      "projectPage": "https://reward-forcing.github.io/",
      "githubRepo": "https://github.com/JaydenLyh/Reward-Forcing",
      "ai_summary": "The paper introduces Reward Forcing, which enhances video generation by updating sink tokens with EMA-Sink and using Rewarded Distribution Matching Distillation to prioritize dynamic content.",
      "ai_keywords": [
        "video diffusion models",
        "sliding window attention",
        "sink tokens",
        "Reward Forcing",
        "EMA-Sink",
        "exponential moving average",
        "Rewarded Distribution Matching Distillation",
        "Re-DMD",
        "vision-language model",
        "motion dynamics",
        "data fidelity",
        "streaming video generation"
      ],
      "githubStars": 18
    },
    "publishedAt": "2025-12-04T06:12:13.000Z",
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04678.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63d4b843df01ef426a0f79fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg",
      "fullname": "Yanhong Zeng",
      "name": "zengyh1900",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05111",
      "authors": [
        {
          "_id": "693267756d1060ca587a2780",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2781",
          "name": "Xinyu Fang",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2782",
          "name": "Ziyu Liu",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2783",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2784",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2785",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2786",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2787",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2788",
          "name": "Jianze Liang",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a2789",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a278a",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a278b",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "693267756d1060ca587a278c",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T18:59:52.000Z",
      "submittedOnDailyAt": "2025-12-05T02:38:10.825Z",
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "submittedOnDailyBy": {
        "_id": "646cd947da8e99940b6e55cf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
        "isPro": false,
        "fullname": "Shengyuan Ding",
        "user": "ChrisDing1105",
        "type": "user"
      },
      "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.",
      "upvotes": 22,
      "discussionId": "693267766d1060ca587a278d",
      "projectPage": "https://github.com/InternLM/ARM-Thinker",
      "githubRepo": "https://github.com/InternLM/ARM-Thinker",
      "ai_summary": "ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks.",
      "ai_keywords": [
        "reward models",
        "vision-language systems",
        "hallucination",
        "visual grounding",
        "external tools",
        "image cropping",
        "doc page retrieval",
        "multi-stage reinforcement learning",
        "tool-calling decisions",
        "judgment accuracy",
        "agentic reward modeling",
        "ARMBench-VL",
        "fine-grained visual grounding",
        "multi-page document understanding",
        "instruction following",
        "multimodal math",
        "logical reasoning"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-12-04T13:59:52.000Z",
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646cd947da8e99940b6e55cf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
      "fullname": "Shengyuan Ding",
      "name": "ChrisDing1105",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02589",
      "authors": [
        {
          "_id": "693247096d1060ca587a2697",
          "name": "Junyi Hou",
          "hidden": false
        },
        {
          "_id": "693247096d1060ca587a2698",
          "name": "Andre Lin Huikai",
          "hidden": false
        },
        {
          "_id": "693247096d1060ca587a2699",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "693247096d1060ca587a269a",
          "name": "Yiwei Gong",
          "hidden": false
        },
        {
          "_id": "693247096d1060ca587a269b",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/V54jTqz6Z0NwKi4MazXwc.mp4"
      ],
      "publishedAt": "2025-12-02T10:00:37.000Z",
      "submittedOnDailyAt": "2025-12-05T00:32:20.451Z",
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "submittedOnDailyBy": {
        "_id": "641ac2207c21ab946bf036e8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
        "isPro": true,
        "fullname": "Nuo Chen",
        "user": "nuojohnchen",
        "type": "user"
      },
      "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
      "upvotes": 15,
      "discussionId": "693247096d1060ca587a269c",
      "projectPage": "https://www.paperdebugger.com/",
      "githubRepo": "https://github.com/PaperDebugger/PaperDebugger",
      "ai_summary": "PaperDebugger is an in-editor academic writing assistant that integrates large language models, enabling direct interaction within LaTeX editors for document state management, revision, and literature search.",
      "ai_keywords": [
        "large language models",
        "LaTeX editors",
        "Overleaf",
        "in-editor interaction",
        "multi-agent",
        "plugin-based",
        "bidirectional synchronization",
        "version control",
        "patching",
        "secure state management",
        "multi-agent scheduling",
        "Model Context Protocol",
        "literature search",
        "reference lookup",
        "document scoring",
        "revision pipelines",
        "localized edits",
        "structured reviews",
        "parallel agent execution",
        "diff-based updates"
      ],
      "githubStars": 124,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-12-02T05:00:37.000Z",
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/V54jTqz6Z0NwKi4MazXwc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ac2207c21ab946bf036e8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
      "fullname": "Nuo Chen",
      "name": "nuojohnchen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04926",
      "authors": [
        {
          "_id": "693241dd6d1060ca587a25fd",
          "name": "Yueming Pan",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a25fe",
          "name": "Ruoyu Feng",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a25ff",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a2600",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a2601",
          "name": "Wenfeng Lin",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a2602",
          "name": "Mingyu Guo",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a2603",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "693241dd6d1060ca587a2604",
          "name": "Nanning Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T15:57:27.000Z",
      "submittedOnDailyAt": "2025-12-05T00:25:00.105Z",
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "submittedOnDailyBy": {
        "_id": "644101cd1a80f6d83cae6ef1",
        "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg",
        "isPro": false,
        "fullname": "FishNotFish",
        "user": "RuoyuFeng",
        "type": "user"
      },
      "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
      "upvotes": 14,
      "discussionId": "693241dd6d1060ca587a2605",
      "projectPage": "https://yuemingpan.github.io/SFD.github.io/",
      "githubRepo": "https://github.com/yuemingPAN/SFD",
      "ai_summary": "Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.",
      "ai_keywords": [
        "Latent Diffusion Models",
        "SFD",
        "Semantic VAE",
        "noise schedules",
        "FID",
        "ImageNet",
        "ReDi",
        "VA-VAE"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "66a92d5a58cff488d93ab512",
        "name": "XianJiaotongUniversity",
        "fullname": "Xi'an Jiaotong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"
      }
    },
    "publishedAt": "2025-12-04T10:57:27.000Z",
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644101cd1a80f6d83cae6ef1",
      "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg",
      "fullname": "FishNotFish",
      "name": "RuoyuFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "66a92d5a58cff488d93ab512",
      "name": "XianJiaotongUniversity",
      "fullname": "Xi'an Jiaotong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05060",
      "authors": [
        {
          "_id": "693248aa6d1060ca587a26f0",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f1",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f2",
          "name": "Minghan Li",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f3",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f4",
          "name": "Xueqi Zhao",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f5",
          "name": "Zhongyuan Lai",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f6",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "693248aa6d1060ca587a26f7",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T18:15:27.000Z",
      "submittedOnDailyAt": "2025-12-05T00:24:03.406Z",
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "submittedOnDailyBy": {
        "_id": "6581a9e2e4bcbca0322e3608",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png",
        "isPro": false,
        "fullname": "Xianfeng Wu",
        "user": "Beckham808",
        "type": "user"
      },
      "summary": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
      "upvotes": 11,
      "discussionId": "693248aa6d1060ca587a26f8",
      "projectPage": "https://hustvl.github.io/4DLangVGGT/",
      "githubRepo": "https://github.com/hustvl/4DLangVGGT",
      "ai_summary": "A Transformer-based framework, 4DLangVGGT, enhances 4D scene understanding by integrating geometric perception and language alignment, achieving scalability and generalization across dynamic scenes.",
      "ai_keywords": [
        "4D language fields",
        "embodied AI",
        "augmented/virtual reality",
        "4D scene understanding",
        "semantic representations",
        "Gaussian splatting",
        "Transformer-based",
        "feed-forward unified framework",
        "4D Visual Geometry Transformer",
        "StreamVGGT",
        "Semantic Bridging Decoder",
        "SBD",
        "geometry-aware features",
        "language-aligned semantic space",
        "spatio-temporal geometric representations",
        "HyperNeRF",
        "Neu3D datasets"
      ],
      "githubStars": 14,
      "organization": {
        "_id": "63ea5d1ee7f148f46015ebb3",
        "name": "Huster",
        "fullname": "Huazhong University of Science and Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676303638801-63ea5a04e7f148f46015c770.jpeg"
      }
    },
    "publishedAt": "2025-12-04T13:15:27.000Z",
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "summary": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6581a9e2e4bcbca0322e3608",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png",
      "fullname": "Xianfeng Wu",
      "name": "Beckham808",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "63ea5d1ee7f148f46015ebb3",
      "name": "Huster",
      "fullname": "Huazhong University of Science and Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676303638801-63ea5a04e7f148f46015c770.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05112",
      "authors": [
        {
          "_id": "69324e4e6d1060ca587a271b",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a271c",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a271d",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a271e",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a271f",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2720",
          "name": "Jun He",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2721",
          "name": "Claire Guo",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2722",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2723",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2724",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2725",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "69324e4e6d1060ca587a2726",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6349214f8146350b3a4c5cdf/l2HR6iW1IVezjeanNRipg.png"
      ],
      "publishedAt": "2025-12-04T18:59:53.000Z",
      "submittedOnDailyAt": "2025-12-05T01:00:46.802Z",
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
      "upvotes": 8,
      "discussionId": "69324e4e6d1060ca587a2727",
      "githubRepo": "https://github.com/CaraJ7/DraCo",
      "ai_summary": "DraCo, a novel interleaved reasoning paradigm, enhances text-to-image generation by integrating both textual and visual content, generating low-resolution drafts, verifying semantic alignment, and refining images with super-resolution to address challenges in textual planning and rare attribute generation.",
      "ai_keywords": [
        "unified multimodal large language models",
        "MLLMs",
        "chain-of-thought",
        "CoT reasoning",
        "text-to-image generation",
        "Draft-as-CoT",
        "DraCo",
        "interleaved reasoning",
        "low-resolution draft image",
        "visual planning",
        "semantic misalignments",
        "super-resolution",
        "DraCo-240K",
        "classifier-free guidance",
        "CFG",
        "GenEval",
        "Imagine-Bench",
        "GenEval++"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-12-04T13:59:53.000Z",
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "summary": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6349214f8146350b3a4c5cdf/l2HR6iW1IVezjeanNRipg.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05112.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04797",
      "authors": [
        {
          "_id": "693247b66d1060ca587a269e",
          "name": "SIMA team",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a269f",
          "name": "Adrian Bolton",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a0",
          "name": "Alexander Lerchner",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a1",
          "name": "Alexandra Cordell",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a2",
          "name": "Alexandre Moufarek",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a3",
          "name": "Andrew Bolt",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a4",
          "name": "Andrew Lampinen",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a5",
          "name": "Anna Mitenkova",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a6",
          "name": "Arne Olav Hallingstad",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a7",
          "name": "Bojan Vujatovic",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a8",
          "name": "Bonnie Li",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26a9",
          "name": "Cong Lu",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26aa",
          "name": "Daan Wierstra",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ab",
          "name": "Daniel P. Sawyer",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ac",
          "name": "Daniel Slater",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ad",
          "name": "David Reichert",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ae",
          "name": "Davide Vercelli",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26af",
          "name": "Demis Hassabis",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b0",
          "name": "Drew A. Hudson",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b1",
          "name": "Duncan Williams",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b2",
          "name": "Ed Hirst",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b3",
          "name": "Fabio Pardo",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b4",
          "name": "Felix Hill",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b5",
          "name": "Frederic Besse",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b6",
          "name": "Hannah Openshaw",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b7",
          "name": "Harris Chan",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b8",
          "name": "Hubert Soyer",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26b9",
          "name": "Jane X. Wang",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ba",
          "name": "Jeff Clune",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26bb",
          "name": "John Agapiou",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26bc",
          "name": "John Reid",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26bd",
          "name": "Joseph Marino",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26be",
          "name": "Junkyung Kim",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26bf",
          "name": "Karol Gregor",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c0",
          "name": "Kaustubh Sridhar",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c1",
          "name": "Kay McKinney",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c2",
          "name": "Laura Kampis",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c3",
          "name": "Lei M. Zhang",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c4",
          "name": "Loic Matthey",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c5",
          "name": "Luyu Wang",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c6",
          "name": "Maria Abi Raad",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c7",
          "name": "Maria Loks-Thompson",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c8",
          "name": "Martin Engelcke",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26c9",
          "name": "Matija Kecman",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ca",
          "name": "Matthew Jackson",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26cb",
          "name": "Maxime Gazeau",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26cc",
          "name": "Ollie Purkiss",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26cd",
          "name": "Oscar Knagg",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26ce",
          "name": "Peter Stys",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26cf",
          "name": "Piermaria Mendolicchio",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d0",
          "name": "Raia Hadsell",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d1",
          "name": "Rosemary Ke",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d2",
          "name": "Ryan Faulkner",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d3",
          "name": "Sarah Chakera",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d4",
          "name": "Satinder Singh Baveja",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d5",
          "name": "Shane Legg",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d6",
          "name": "Sheleem Kashem",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d7",
          "name": "Tayfun Terzi",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d8",
          "name": "Thomas Keck",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26d9",
          "name": "Tim Harley",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26da",
          "name": "Tim Scholtes",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26db",
          "name": "Tyson Roberts",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26dc",
          "name": "Volodymyr Mnih",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26dd",
          "name": "Yulan Liu",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26de",
          "name": "Zhengdong Wang",
          "hidden": false
        },
        {
          "_id": "693247b66d1060ca587a26df",
          "name": "Zoubin Ghahramani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T13:46:11.000Z",
      "submittedOnDailyAt": "2025-12-05T00:17:26.017Z",
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "upvotes": 7,
      "discussionId": "693247b66d1060ca587a26e0",
      "ai_summary": "SIMA 2, built on a Gemini foundation model, interacts in 3D virtual worlds, reasons about goals, handles complex instructions, and autonomously learns new skills through open-ended self-improvement.",
      "ai_keywords": [
        "Gemini foundation model",
        "embodied agent",
        "high-level goals",
        "conversational abilities",
        "complex instructions",
        "language and images",
        "generalization",
        "open-ended self-improvement",
        "task generation",
        "reward provision",
        "autonomous learning"
      ],
      "organization": {
        "_id": "60f6cbb2852126bac698c89e",
        "name": "deepmind",
        "fullname": "Deepmind",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
      }
    },
    "publishedAt": "2025-12-04T08:46:11.000Z",
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 177
    },
    "organization": {
      "_id": "60f6cbb2852126bac698c89e",
      "name": "deepmind",
      "fullname": "Deepmind",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.03000",
      "authors": [
        {
          "_id": "693251cd6d1060ca587a2731",
          "name": "Kairun Wen",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2732",
          "name": "Yuzhi Huang",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2733",
          "name": "Runyu Chen",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2734",
          "name": "Hui Zheng",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2735",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2736",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2737",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2738",
          "name": "Wenyan Cong",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2739",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273a",
          "name": "Junbin Lu",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273b",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273c",
          "name": "Dilin Wang",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273d",
          "name": "Zhicheng Yan",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273e",
          "name": "Hongyu Xu",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a273f",
          "name": "Justin Theiss",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2740",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2741",
          "name": "Xinghao Ding",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2742",
          "name": "Rakesh Ranjan",
          "hidden": false
        },
        {
          "_id": "693251cd6d1060ca587a2743",
          "name": "Zhiwen Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/646a31bd3eb2bab0419a54ef/IcbnIqsR5PHuafs1Zc_TV.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/646a31bd3eb2bab0419a54ef/vb1tA3T0rLGTDWBja12nr.png"
      ],
      "publishedAt": "2025-12-02T18:24:27.000Z",
      "submittedOnDailyAt": "2025-12-05T01:14:07.509Z",
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "submittedOnDailyBy": {
        "_id": "646a31bd3eb2bab0419a54ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646a31bd3eb2bab0419a54ef/RD2GsZdewSZz7VUwWabUV.png",
        "isPro": true,
        "fullname": "Kairun Wen",
        "user": "kairunwen",
        "type": "user"
      },
      "summary": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "upvotes": 7,
      "discussionId": "693251ce6d1060ca587a2744",
      "projectPage": "https://dynamic-verse.github.io/",
      "githubRepo": "https://github.com/Dynamics-X/DynamicVerse",
      "ai_summary": "DynamicVerse is a framework that models dynamic real-world videos by integrating large vision, geometric, and multimodal models to produce a comprehensive 4D multimodal dataset, achieving superior performance in video depth estimation, camera pose estimation, and camera intrinsics estimation.",
      "ai_keywords": [
        "DynamicVerse",
        "multimodal 4D world modeling",
        "Bundle Adjustment",
        "global optimization",
        "video depth estimation",
        "camera pose estimation",
        "camera intrinsics estimation"
      ],
      "githubStars": 27,
      "organization": {
        "_id": "68c79bcac5692ecf2f2edd1e",
        "name": "Dynamics-X",
        "fullname": "Dynamics-X"
      }
    },
    "publishedAt": "2025-12-02T13:24:27.000Z",
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "summary": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/646a31bd3eb2bab0419a54ef/IcbnIqsR5PHuafs1Zc_TV.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/646a31bd3eb2bab0419a54ef/vb1tA3T0rLGTDWBja12nr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03000.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646a31bd3eb2bab0419a54ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646a31bd3eb2bab0419a54ef/RD2GsZdewSZz7VUwWabUV.png",
      "fullname": "Kairun Wen",
      "name": "kairunwen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "organization": {
      "_id": "68c79bcac5692ecf2f2edd1e",
      "name": "Dynamics-X",
      "fullname": "Dynamics-X"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05113",
      "authors": [
        {
          "_id": "693247cb6d1060ca587a26e2",
          "name": "Hao-Jen Chien",
          "hidden": false
        },
        {
          "_id": "693247cb6d1060ca587a26e3",
          "name": "Yi-Chuan Huang",
          "hidden": false
        },
        {
          "_id": "693247cb6d1060ca587a26e4",
          "name": "Chung-Ho Wu",
          "hidden": false
        },
        {
          "_id": "693247cb6d1060ca587a26e5",
          "name": "Wei-Lun Chao",
          "hidden": false
        },
        {
          "_id": "693247cb6d1060ca587a26e6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/LG1AxZYLaj5rZaLUGAA8i.jpeg"
      ],
      "publishedAt": "2025-12-04T18:59:53.000Z",
      "submittedOnDailyAt": "2025-12-05T00:20:49.312Z",
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
      "upvotes": 6,
      "discussionId": "693247cb6d1060ca587a26e7",
      "projectPage": "https://chien90190.github.io/splannequin/",
      "ai_summary": "Splannequin is a regularization technique that improves the visual quality of frozen 3D scenes synthesized from monocular videos by addressing artifacts in dynamic Gaussian splatting.",
      "ai_keywords": [
        "dynamic Gaussian splatting",
        "Gaussian primitives",
        "hidden states",
        "defective states",
        "temporal anchoring",
        "loss terms",
        "inference overhead"
      ]
    },
    "publishedAt": "2025-12-04T13:59:53.000Z",
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "summary": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/LG1AxZYLaj5rZaLUGAA8i.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04504",
      "authors": [
        {
          "_id": "693288bf6d1060ca587a27f9",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27fa",
          "name": "Bokai Yan",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27fb",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27fc",
          "name": "Hongzhou Zhu",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27fd",
          "name": "Jintao Zhang",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27fe",
          "name": "Shilong Liu",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a27ff",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "693288bf6d1060ca587a2800",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T06:24:04.000Z",
      "submittedOnDailyAt": "2025-12-05T04:56:07.987Z",
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64c269a52d73768f07ac266c",
        "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
        "isPro": false,
        "fullname": "Zhu Hongzhou",
        "user": "zhuhz22",
        "type": "user"
      },
      "summary": "Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at https://thu-ml.github.io/ultraimage.github.io/{https://thu-ml.github.io/ultraimage.github.io/}.",
      "upvotes": 6,
      "discussionId": "693288bf6d1060ca587a2801",
      "projectPage": "https://thu-ml.github.io/ultraimage.github.io/",
      "ai_summary": "UltraImage, a framework for high-fidelity image generation, addresses content repetition and quality degradation by correcting dominant frequency periodicity and using entropy-guided adaptive attention concentration, enabling high-resolution image generation without low-resolution guidance.",
      "ai_keywords": [
        "image diffusion transformers",
        "frequency-wise analysis",
        "positional embeddings",
        "dominant frequency correction",
        "entropy-guided adaptive attention concentration",
        "Qwen-Image",
        "Flux",
        "generation scenarios",
        "visual fidelity",
        "high-resolution image generation"
      ],
      "organization": {
        "_id": "640d3084536d9fe0f005cac3",
        "name": "thu-ml",
        "fullname": "Tsinghua Machine Learning Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
      }
    },
    "publishedAt": "2025-12-04T01:24:04.000Z",
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "summary": "Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at https://thu-ml.github.io/ultraimage.github.io/{https://thu-ml.github.io/ultraimage.github.io/}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c269a52d73768f07ac266c",
      "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
      "fullname": "Zhu Hongzhou",
      "name": "zhuhz22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "640d3084536d9fe0f005cac3",
      "name": "thu-ml",
      "fullname": "Tsinghua Machine Learning Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02631",
      "authors": [
        {
          "_id": "693247f36d1060ca587a26e9",
          "name": "Zhengcheng Wang",
          "hidden": false
        },
        {
          "_id": "693247f36d1060ca587a26ea",
          "name": "Zichuan Lin",
          "hidden": false
        },
        {
          "_id": "693247f36d1060ca587a26eb",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "693247f36d1060ca587a26ec",
          "name": "Haobo Fu",
          "hidden": false
        },
        {
          "_id": "693247f36d1060ca587a26ed",
          "name": "Deheng Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T10:40:46.000Z",
      "submittedOnDailyAt": "2025-12-05T00:20:45.040Z",
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "687da36e2eaea8261f1323d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hFH69bJGIDMikEYyClray.png",
        "isPro": false,
        "fullname": "zichuan lin",
        "user": "zichuan-lin",
        "type": "user"
      },
      "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
      "upvotes": 6,
      "discussionId": "693247f36d1060ca587a26ee",
      "githubRepo": "https://github.com/WzcTHU/SeeNav-Agent",
      "ai_summary": "A new VLN agent framework, SeeNav-Agent, improves navigation performance by reducing visual hallucinations and enhancing planning through dual-view visual prompts and step-level reinforcement fine-tuning with SRGPO.",
      "ai_keywords": [
        "VLN agent",
        "Large Vision-Language Models (LVLMs)",
        "perception hallucinations",
        "dual-view Visual Prompt (VP)",
        "step-level Reinforcement Fine-Tuning (RFT)",
        "Step Reward Group Policy Optimization (SRGPO)",
        "navigation success rate",
        "EmbodiedBench Navigation benchmark",
        "GPT-4.1",
        "Qwen2.5-VL-3B",
        "GRPO",
        "GiGPO"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-02T05:40:46.000Z",
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "687da36e2eaea8261f1323d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hFH69bJGIDMikEYyClray.png",
      "fullname": "zichuan lin",
      "name": "zichuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04356",
      "authors": [
        {
          "_id": "693242b86d1060ca587a2607",
          "name": "Kai-Po Chang",
          "hidden": false
        },
        {
          "_id": "693242b86d1060ca587a2608",
          "name": "Wei-Yuan Cheng",
          "hidden": false
        },
        {
          "_id": "693242b86d1060ca587a2609",
          "name": "Chi-Pin Huang",
          "hidden": false
        },
        {
          "_id": "693242b86d1060ca587a260a",
          "name": "Fu-En Yang",
          "hidden": false
        },
        {
          "_id": "693242b86d1060ca587a260b",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T01:05:16.000Z",
      "submittedOnDailyAt": "2025-12-05T03:50:09.487Z",
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "submittedOnDailyBy": {
        "_id": "65dad90a80bafdfb4bc94dc4",
        "avatarUrl": "/avatars/7131b82e11245744760e8a45ba914161.svg",
        "isPro": true,
        "fullname": "Kai-Po Chang",
        "user": "kaipochang0810",
        "type": "user"
      },
      "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
      "upvotes": 4,
      "discussionId": "693242b86d1060ca587a260c",
      "projectPage": "https://kpc0810.github.io/santa/",
      "ai_summary": "The SANTA framework addresses hallucinations in multimodal LLMs by using self-augmented contrastive alignment to enhance object and action faithfulness in video caption generation.",
      "ai_keywords": [
        "multimodal LLMs",
        "MLLMs",
        "descriptive captions",
        "factual inaccuracies",
        "hallucination issues",
        "self-augmented contrastive alignment",
        "visual object",
        "temporal action",
        "spurious correlations",
        "visual facts",
        "hallucinative self-augmentation",
        "contrasted negatives",
        "tracklet-phrase contrastive alignment",
        "regional objects",
        "relation-guided actions",
        "visual phrases",
        "temporal phrases",
        "hallucination examination benchmarks"
      ],
      "organization": {
        "_id": "673248e121823ee4ea594099",
        "name": "nationaltaiwan",
        "fullname": "",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
      }
    },
    "publishedAt": "2025-12-03T20:05:16.000Z",
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04356.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65dad90a80bafdfb4bc94dc4",
      "avatarUrl": "/avatars/7131b82e11245744760e8a45ba914161.svg",
      "fullname": "Kai-Po Chang",
      "name": "kaipochang0810",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "673248e121823ee4ea594099",
      "name": "nationaltaiwan",
      "fullname": "",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04220",
      "authors": [
        {
          "_id": "693246196d1060ca587a266f",
          "name": "Wenlong Deng",
          "hidden": false
        },
        {
          "_id": "693246196d1060ca587a2670",
          "name": "Yushu Li",
          "hidden": false
        },
        {
          "_id": "693246196d1060ca587a2671",
          "name": "Boying Gong",
          "hidden": false
        },
        {
          "_id": "693246196d1060ca587a2672",
          "name": "Yi Ren",
          "hidden": false
        },
        {
          "_id": "693246196d1060ca587a2673",
          "name": "Christos Thrampoulidis",
          "hidden": false
        },
        {
          "_id": "693246196d1060ca587a2674",
          "name": "Xiaoxiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-03T19:41:15.000Z",
      "submittedOnDailyAt": "2025-12-05T00:10:35.651Z",
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "upvotes": 4,
      "discussionId": "693246196d1060ca587a2675",
      "ai_summary": "Lazy Likelihood Displacement is identified as a critical issue in GRPO for tool-integrated reinforcement learning, causing training collapse; LLDS regularization addresses this problem, stabilizing training and improving performance.",
      "ai_keywords": [
        "Tool-integrated reinforcement learning",
        "GRPO",
        "Search-R1",
        "Lazy Likelihood Displacement",
        "LLD Death Spiral",
        "likelihood-preserving regularization",
        "LLDS",
        "Qwen2.5-3B",
        "Qwen2.5-7B",
        "open-domain QA",
        "multi-hop QA"
      ]
    },
    "publishedAt": "2025-12-03T14:41:15.000Z",
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 177
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05103",
      "authors": [
        {
          "_id": "6932468c6d1060ca587a2677",
          "name": "Xiaochuang Han",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2678",
          "name": "Youssef Emad",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2679",
          "name": "Melissa Hall",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267a",
          "name": "John Nguyen",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267b",
          "name": "Karthik Padthe",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267c",
          "name": "Liam Robbins",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267d",
          "name": "Amir Bar",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267e",
          "name": "Delong Chen",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a267f",
          "name": "Michal Drozdzal",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2680",
          "name": "Maha Elbayad",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2681",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2682",
          "name": "Shang-Wen Li",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2683",
          "name": "Sreya Dutta Roy",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2684",
          "name": "Jakob Verbeek",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2685",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2686",
          "name": "Marjan Ghazvininejad",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2687",
          "name": "Luke Zettlemoyer",
          "hidden": false
        },
        {
          "_id": "6932468c6d1060ca587a2688",
          "name": "Emily Dinan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T18:59:09.000Z",
      "submittedOnDailyAt": "2025-12-05T00:12:32.141Z",
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "upvotes": 3,
      "discussionId": "6932468d6d1060ca587a2689",
      "ai_summary": "A new video generation model, TV2TV, integrates text and video generation using a Mixture-of-Transformers to improve visual quality and controllability by leveraging language modeling for high-level reasoning.",
      "ai_keywords": [
        "video generation models",
        "omni video-text models",
        "TV2TV",
        "unified generative modeling framework",
        "language modeling",
        "video flow matching",
        "Mixture-of-Transformers",
        "text and video generation process",
        "next-token prediction",
        "next-frame prediction",
        "language modeling tower",
        "prompt alignment",
        "video game data",
        "natural videos",
        "vision-language models",
        "visual quality",
        "controllability",
        "textual reasoning"
      ],
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-12-04T13:59:09.000Z",
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "summary": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 177
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04746",
      "authors": [
        {
          "_id": "693245586d1060ca587a2612",
          "name": "Wenhua Cheng",
          "hidden": false
        },
        {
          "_id": "693245586d1060ca587a2613",
          "name": "Weiwei Zhang",
          "hidden": false
        },
        {
          "_id": "693245586d1060ca587a2614",
          "name": "Heng Guo",
          "hidden": false
        },
        {
          "_id": "693245586d1060ca587a2615",
          "name": "Haihao Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T12:35:10.000Z",
      "submittedOnDailyAt": "2025-12-05T00:08:00.865Z",
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "submittedOnDailyBy": {
        "_id": "60ac3318e3de7c7440abb850",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ac3318e3de7c7440abb850/DkMrPBr6Ew_dS_c9kog4e.jpeg",
        "isPro": false,
        "fullname": "Haihao Shen",
        "user": "Haihao",
        "type": "user"
      },
      "summary": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.",
      "upvotes": 3,
      "discussionId": "693245586d1060ca587a2616",
      "projectPage": "https://github.com/intel/auto-round",
      "ai_summary": "SignRoundV2, a post-training quantization framework, achieves competitive accuracy for Large Language Models at extremely low-bit quantization through layer-wise bit allocation and pre-tuning scale search.",
      "ai_keywords": [
        "post-training quantization",
        "SignRoundV2",
        "gradient information",
        "quantization-induced deviations",
        "layer-wise bit allocation",
        "pre-tuning",
        "Large Language Models",
        "low-bit quantization",
        "mixed-precision",
        "production-grade performance"
      ],
      "organization": {
        "_id": "6054ca445e96cd4dd1fc6d68",
        "name": "Intel",
        "fullname": "Intel",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1616186257611-60104afcc75e19ac1738fe70.png"
      }
    },
    "publishedAt": "2025-12-04T07:35:10.000Z",
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "summary": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04746.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ac3318e3de7c7440abb850",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ac3318e3de7c7440abb850/DkMrPBr6Ew_dS_c9kog4e.jpeg",
      "fullname": "Haihao Shen",
      "name": "Haihao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "6054ca445e96cd4dd1fc6d68",
      "name": "Intel",
      "fullname": "Intel",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1616186257611-60104afcc75e19ac1738fe70.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05106",
      "authors": [
        {
          "_id": "69324f7a6d1060ca587a2729",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "69324f7a6d1060ca587a272a",
          "name": "Charles Ochoa",
          "hidden": false
        },
        {
          "_id": "69324f7a6d1060ca587a272b",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "69324f7a6d1060ca587a272c",
          "name": "Vishal M. Patel",
          "hidden": false
        },
        {
          "_id": "69324f7a6d1060ca587a272d",
          "name": "Vitor Guizilini",
          "hidden": false
        },
        {
          "_id": "69324f7a6d1060ca587a272e",
          "name": "Rowan McAllister",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T18:59:18.000Z",
      "submittedOnDailyAt": "2025-12-05T00:50:56.814Z",
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion -PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. -PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, -PD produces controllable, spatially aligned results. When applied to the CARLA simulator, -PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.",
      "upvotes": 2,
      "discussionId": "69324f7b6d1060ca587a272f",
      "ai_summary": "Phase-Preserving Diffusion and Frequency-Selective Structured noise enable structure-aligned generation in diffusion models without altering architecture or introducing extra parameters, enhancing performance in tasks like re-rendering and simulation.",
      "ai_keywords": [
        "diffusion",
        "Gaussian noise",
        "Fourier coefficients",
        "geometric consistency",
        "Phase-Preserving Diffusion",
        "-PD",
        "Frequency-Selective Structured noise",
        "structural rigidity",
        "frequency-cutoff",
        "inference-time cost",
        "image-to-image generation",
        "video-to-video generation",
        "CARLA-to-Waymo planner"
      ]
    },
    "publishedAt": "2025-12-04T13:59:18.000Z",
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "summary": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion -PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. -PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, -PD produces controllable, spatially aligned results. When applied to the CARLA simulator, -PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8841
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05016",
      "authors": [
        {
          "_id": "693287186d1060ca587a27f2",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "693287186d1060ca587a27f3",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "693287186d1060ca587a27f4",
          "name": "Tinghan Yang",
          "hidden": false
        },
        {
          "_id": "693287186d1060ca587a27f5",
          "name": "Libiao Jin",
          "hidden": false
        },
        {
          "_id": "693287186d1060ca587a27f6",
          "name": "Siwei Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T17:27:32.000Z",
      "submittedOnDailyAt": "2025-12-05T05:03:15.365Z",
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "submittedOnDailyBy": {
        "_id": "6388a7e98a5dbe2f3dc61faa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
        "isPro": false,
        "fullname": "Qi Mao",
        "user": "HelenMao",
        "type": "user"
      },
      "summary": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
      "upvotes": 2,
      "discussionId": "693287186d1060ca587a27f7",
      "ai_summary": "GNVC-VD, a DiT-based generative neural video compression framework, integrates spatio-temporal latent compression and sequence-level generative refinement to improve perceptual quality and reduce flickering artifacts.",
      "ai_keywords": [
        "DiT-based",
        "generative neural video compression",
        "spatio-temporal latent compression",
        "sequence-level generative refinement",
        "unified codec",
        "perceptual codecs",
        "pre-trained image generative priors",
        "frame-wise nature",
        "temporal modeling",
        "perceptual flickering",
        "unified flow-matching latent refinement module",
        "video diffusion transformer",
        "sequence-level denoising",
        "intra-frame latents",
        "inter-frame latents",
        "Gaussian noise",
        "decoded spatio-temporal latents",
        "correction term",
        "diffusion prior",
        "compression-induced degradation",
        "conditioning adaptor",
        "intermediate DiT layers",
        "artifact removal",
        "temporal coherence",
        "bitrate constraints",
        "next-generation perceptual video compression"
      ],
      "organization": {
        "_id": "67dab498ed21a53369f5de73",
        "name": "CUC-MIPG",
        "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
      }
    },
    "publishedAt": "2025-12-04T12:27:32.000Z",
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "summary": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388a7e98a5dbe2f3dc61faa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
      "fullname": "Qi Mao",
      "name": "HelenMao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67dab498ed21a53369f5de73",
      "name": "CUC-MIPG",
      "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04515",
      "authors": [
        {
          "_id": "693260ff6d1060ca587a2775",
          "name": "Liuzhou Zhang",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a2776",
          "name": "Jiarui Ye",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a2777",
          "name": "Yuanlei Wang",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a2778",
          "name": "Ming Zhong",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a2779",
          "name": "Mingju Cao",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a277a",
          "name": "Wanke Xia",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a277b",
          "name": "Bowen Zeng",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a277c",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "693260ff6d1060ca587a277d",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/KzUVKfuQ0kK-ZMl04AhJ9.mp4"
      ],
      "publishedAt": "2025-12-04T06:53:01.000Z",
      "submittedOnDailyAt": "2025-12-05T02:06:35.395Z",
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
      "upvotes": 2,
      "discussionId": "693260ff6d1060ca587a277e",
      "projectPage": "https://aigeeksgroup.github.io/EgoLCD/",
      "githubRepo": "https://github.com/AIGeeksGroup/EgoLCD",
      "ai_summary": "EgoLCD addresses content drift in long egocentric video generation by integrating long-term sparse memory with attention-based short-term memory and structured narrative prompting, achieving state-of-the-art performance.",
      "ai_keywords": [
        "end-to-end framework",
        "long-term sparse KV Cache",
        "attention-based short-term memory",
        "LoRA",
        "Memory Regulation Loss",
        "Structured Narrative Prompting",
        "EgoVid-5M benchmark",
        "perceptual quality",
        "temporal consistency",
        "generative forgetting",
        "scalable world models",
        "embodied AI"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-12-04T01:53:01.000Z",
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/KzUVKfuQ0kK-ZMl04AhJ9.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04515.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05076",
      "authors": [
        {
          "_id": "693254586d1060ca587a2753",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2754",
          "name": "Qihang Zhang",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2755",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2756",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2757",
          "name": "Jan Ackermann",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2758",
          "name": "Zhengfei Kuang",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a2759",
          "name": "Yang Zheng",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a275a",
          "name": "Frano Raji",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a275b",
          "name": "Siyu Tang",
          "hidden": false
        },
        {
          "_id": "693254586d1060ca587a275c",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T18:40:52.000Z",
      "submittedOnDailyAt": "2025-12-05T01:11:43.016Z",
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
      "upvotes": 1,
      "discussionId": "693254586d1060ca587a275d",
      "ai_summary": "A 4D-controllable video diffusion framework decouples scene dynamics from camera pose, enabling precise manipulation of both and achieving high-quality generation across diverse timing patterns and camera trajectories.",
      "ai_keywords": [
        "video diffusion models",
        "scene dynamics",
        "camera pose",
        "4D positional encoding",
        "attention layer",
        "adaptive normalizations",
        "feature modulation",
        "4D control"
      ]
    },
    "publishedAt": "2025-12-04T13:40:52.000Z",
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "summary": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05076.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8841
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.05049",
      "authors": [
        {
          "_id": "693272316d1060ca587a278f",
          "name": "Yu-Chao Hsu",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2790",
          "name": "Jiun-Cheng Jiang",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2791",
          "name": "Chun-Hua Lin",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2792",
          "name": "Kuo-Chung Peng",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2793",
          "name": "Nan-Yow Chen",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2794",
          "name": "Samuel Yen-Chi Chen",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2795",
          "name": "En-Jui Kuo",
          "hidden": false
        },
        {
          "_id": "693272316d1060ca587a2796",
          "name": "Hsi-Sheng Goan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/68b93b3a6c86c127a199ad90/65uWTnQjRUrX5m9qJ0A4J.png"
      ],
      "publishedAt": "2025-12-04T18:03:23.000Z",
      "submittedOnDailyAt": "2025-12-05T03:22:21.298Z",
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "submittedOnDailyBy": {
        "_id": "68b93b3a6c86c127a199ad90",
        "avatarUrl": "/avatars/f370d99b240ce5b9e1bfdbd3130d9ee4.svg",
        "isPro": false,
        "fullname": "Jiun-Cheng Jiang",
        "user": "Jim137",
        "type": "user"
      },
      "summary": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
      "upvotes": 1,
      "discussionId": "693272316d1060ca587a2797",
      "ai_summary": "A quantum-inspired LSTM model with Data Re-Uploading Activation modules achieves superior predictive accuracy and parameter efficiency in sequential modeling tasks.",
      "ai_keywords": [
        "LSTM",
        "recurrent neural networks",
        "Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory",
        "QKAN-LSTM",
        "Data Re-Uploading Activation",
        "DARUAN",
        "quantum variational activation function",
        "QVAF",
        "Jiang-Huang-Chen-Goan Network",
        "JHCG Net",
        "latent KAN",
        "Hybrid QKAN",
        "HQKAN-LSTM"
      ]
    },
    "publishedAt": "2025-12-04T13:03:23.000Z",
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "summary": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/68b93b3a6c86c127a199ad90/65uWTnQjRUrX5m9qJ0A4J.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68b93b3a6c86c127a199ad90",
      "avatarUrl": "/avatars/f370d99b240ce5b9e1bfdbd3130d9ee4.svg",
      "fullname": "Jiun-Cheng Jiang",
      "name": "Jim137",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.03052",
      "authors": [
        {
          "_id": "693283706d1060ca587a27e2",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e3",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e4",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e5",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e6",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e7",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e8",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "693283706d1060ca587a27e9",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T03:22:19.000Z",
      "submittedOnDailyAt": "2025-12-05T04:32:42.836Z",
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.",
      "upvotes": 1,
      "discussionId": "693283706d1060ca587a27ea",
      "projectPage": "https://lattice3d.github.io/",
      "githubRepo": "https://github.com/Zeqiang-Lai/LATTICE",
      "ai_summary": "LATTICE, a new framework, uses VoxSet to generate high-fidelity 3D assets efficiently with a two-stage pipeline involving a rectified flow transformer.",
      "ai_keywords": [
        "VoxSet",
        "latent vectors",
        "voxel grid",
        "positional embeddings",
        "rectified flow transformer"
      ],
      "githubStars": 143
    },
    "publishedAt": "2025-11-23T22:22:19.000Z",
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "summary": "We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03052.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.04844",
      "authors": [
        {
          "_id": "693285f46d1060ca587a27ec",
          "name": "Atsuki Yamaguchi",
          "hidden": false
        },
        {
          "_id": "693285f46d1060ca587a27ed",
          "name": "Terufumi Morishita",
          "hidden": false
        },
        {
          "_id": "693285f46d1060ca587a27ee",
          "name": "Aline Villavicencio",
          "hidden": false
        },
        {
          "_id": "693285f46d1060ca587a27ef",
          "name": "Nikolaos Aletras",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-04T14:28:14.000Z",
      "submittedOnDailyAt": "2025-12-05T05:17:51.548Z",
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "submittedOnDailyBy": {
        "_id": "63998cdd438fba22d95f0826",
        "avatarUrl": "/avatars/20f04b6ad2d42361c2c7dfc540aa4694.svg",
        "isPro": false,
        "fullname": "Atsuki Yamaguchi",
        "user": "atsuki-yamaguchi",
        "type": "user"
      },
      "summary": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
      "upvotes": 0,
      "discussionId": "693285f46d1060ca587a27f0",
      "githubRepo": "https://github.com/gucci-j/ssu",
      "ai_summary": "Source-Shielded Updates (SSU) enables the adaptation of instruct LLMs to new languages using only unlabeled data, preserving source knowledge and achieving competitive target-language performance.",
      "ai_keywords": [
        "Source-Shielded Updates",
        "SSU",
        "parameter importance scoring",
        "column-wise freezing",
        "catastrophic forgetting",
        "instruct LLMs"
      ]
    },
    "publishedAt": "2025-12-04T09:28:14.000Z",
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "summary": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63998cdd438fba22d95f0826",
      "avatarUrl": "/avatars/20f04b6ad2d42361c2c7dfc540aa4694.svg",
      "fullname": "Atsuki Yamaguchi",
      "name": "atsuki-yamaguchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20233",
      "authors": [
        {
          "_id": "69324c136d1060ca587a2708",
          "name": "Chuyi Kong",
          "hidden": false
        },
        {
          "_id": "69324c136d1060ca587a2709",
          "name": "Gao Wei",
          "hidden": false
        },
        {
          "_id": "69324c136d1060ca587a270a",
          "name": "Jing Ma",
          "hidden": false
        },
        {
          "_id": "69324c136d1060ca587a270b",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "69324c136d1060ca587a270c",
          "name": "Yaxin Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T12:06:23.000Z",
      "submittedOnDailyAt": "2025-12-05T00:38:51.112Z",
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "upvotes": 0,
      "discussionId": "69324c136d1060ca587a270d",
      "ai_summary": "A new fact-checking paradigm, REFLEX, enhances verdict accuracy and explanation quality by leveraging internal model knowledge and adaptive activation signals in a role-play dialogue format.",
      "ai_keywords": [
        "reason-guided fact-checking",
        "latent explanations",
        "role-play dialogue",
        "verdict prediction",
        "explanation generation",
        "contrastive activation pairs",
        "steering vectors",
        "factual reasoning",
        "explanatory objectives"
      ]
    },
    "publishedAt": "2025-11-25T07:06:23.000Z",
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "summary": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]