[
  {
    "paper": {
      "id": "2602.05400",
      "authors": [
        {
          "_id": "698b396b1b2dc6b37d61b4be",
          "name": "Shaobo Wang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4bf",
          "name": "Xuan Ouyang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c0",
          "name": "Tianyi Xu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c1",
          "name": "Yuzheng Hu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c2",
          "name": "Jialin Liu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c3",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c4",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c5",
          "name": "Junhao Zheng",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c6",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c7",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c8",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c9",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T07:34:23.000Z",
      "submittedOnDailyAt": "2026-02-11T02:09:03.945Z",
      "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
      "submittedOnDailyBy": {
        "_id": "67e617d4470f96a302734e16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png",
        "isPro": false,
        "fullname": "Xuan Ouyang",
        "user": "YoungXuan",
        "type": "user"
      },
      "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "upvotes": 129,
      "discussionId": "698b396b1b2dc6b37d61b4ca",
      "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
      "ai_keywords": [
        "data selection",
        "optimizer-induced update space",
        "effective updates",
        "stable in-distribution proxy",
        "Ghost technique",
        "CountSketch",
        "Boltzmann sampling",
        "pre-training",
        "GPT-2",
        "Qwen3-8B-Base",
        "FineWeb",
        "FineWeb-Edu",
        "SciencePedia"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-02-05T02:34:23.000Z",
    "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
    "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e617d4470f96a302734e16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png",
      "fullname": "Xuan Ouyang",
      "name": "YoungXuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09856",
      "authors": [
        {
          "_id": "698bf5b66052d3bed9630aa7",
          "name": "Yuhao Zheng",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aa8",
          "name": "Li'an Zhong",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aa9",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aaa",
          "name": "Rui Dai",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aab",
          "name": "Kaikui Liu",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aac",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aad",
          "name": "Linyuan Lv",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aae",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aaf",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T14:56:19.000Z",
      "submittedOnDailyAt": "2026-02-11T01:02:42.385Z",
      "title": "Code2World: A GUI World Model via Renderable Code Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
      "upvotes": 106,
      "discussionId": "698bf5b66052d3bed9630ab0",
      "projectPage": "https://amap-ml.github.io/Code2World/",
      "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.",
      "ai_keywords": [
        "vision-language coder",
        "GUI World model",
        "action-conditioned prediction",
        "AndroidCode",
        "HTML generation",
        "visual-feedback revision mechanism",
        "SFT",
        "Render-Aware Reinforcement Learning",
        "visual semantic fidelity",
        "action consistency",
        "next UI prediction",
        "AndroidWorld navigation"
      ]
    },
    "publishedAt": "2026-02-10T09:56:19.000Z",
    "title": "Code2World: A GUI World Model via Renderable Code Generation",
    "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09082",
      "authors": [
        {
          "_id": "698bea506052d3bed96309cb",
          "name": "Veuns-Team",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309cd",
          "name": "Changlong Gao",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309ce",
          "name": "Zhangxuan Gu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309cf",
          "name": "Yulin Liu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d0",
          "name": "Xinyu Qiu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d1",
          "name": "Shuheng Shen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d2",
          "name": "Yue Wen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d3",
          "name": "Tianyu Xia",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d4",
          "name": "Zhenyu Xu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d5",
          "name": "Zhengwen Zeng",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d6",
          "name": "Beitong Zhou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d7",
          "name": "Xingran Zhou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d8",
          "name": "Weizhi Chen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d9",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309da",
          "name": "Jingya Dou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309db",
          "name": "Yichen Gong",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309dc",
          "name": "Yuan Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309dd",
          "name": "Zhenlin Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309de",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309df",
          "name": "Qian Li",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e0",
          "name": "Jinzhen Lin",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e1",
          "name": "Yuqi Zhou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e2",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e3",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e4",
          "name": "Zhenyu Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e5",
          "name": "Changhua Meng",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e6",
          "name": "Weiqiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:43:40.000Z",
      "submittedOnDailyAt": "2026-02-11T00:10:55.649Z",
      "title": "UI-Venus-1.5 Technical Report",
      "submittedOnDailyBy": {
        "_id": "60d2a2984956988b63753371",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg",
        "isPro": false,
        "fullname": "Zhangxuan Gu",
        "user": "zhangxgu",
        "type": "user"
      },
      "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
      "upvotes": 85,
      "discussionId": "698bea516052d3bed96309e7",
      "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/",
      "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5",
      "githubRepoAddedBy": "user",
      "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.",
      "ai_keywords": [
        "GUI agents",
        "Mid-Training stage",
        "Online Reinforcement Learning",
        "full-trajectory rollouts",
        "Model Merging",
        "dense variants",
        "mixture-of-experts variant"
      ],
      "githubStars": 648,
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2026-02-09T13:43:40.000Z",
    "title": "UI-Venus-1.5 Technical Report",
    "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60d2a2984956988b63753371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg",
      "fullname": "Zhangxuan Gu",
      "name": "zhangxgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08234",
      "authors": [
        {
          "_id": "698aba731b2dc6b37d61b0e4",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e5",
          "name": "Jianwen Chen",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e6",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e7",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e8",
          "name": "Kaide Zeng",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e9",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ea",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0eb",
          "name": "Yiyang Zhou",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ec",
          "name": "Xujiang Zhao",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ed",
          "name": "Haifeng Chen",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ee",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ef",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0f0",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"
      ],
      "publishedAt": "2026-02-09T03:17:17.000Z",
      "submittedOnDailyAt": "2026-02-11T00:14:23.139Z",
      "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
      "upvotes": 52,
      "discussionId": "698aba731b2dc6b37d61b0f1",
      "githubRepo": "https://github.com/aiming-lab/SkillRL",
      "githubRepoAddedBy": "user",
      "ai_summary": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.",
      "ai_keywords": [
        "large language model agents",
        "reinforcement learning",
        "skill discovery",
        "recursive evolution",
        "skill library",
        "SkillBank",
        "experience-based distillation",
        "adaptive retrieval strategy",
        "policy improvement",
        "token footprint"
      ],
      "githubStars": 43,
      "organization": {
        "_id": "669f9d1fec8789263c0e355a",
        "name": "UNC-ChapelHill",
        "fullname": "University of North Carolina at Chapel Hill",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
      }
    },
    "publishedAt": "2026-02-08T22:17:17.000Z",
    "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
    "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "669f9d1fec8789263c0e355a",
      "name": "UNC-ChapelHill",
      "fullname": "University of North Carolina at Chapel Hill",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09443",
      "authors": [
        {
          "_id": "698c019f6052d3bed9630b1c",
          "name": "Yun Luo",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1d",
          "name": "Futing Wang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1e",
          "name": "Qianjia Cheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1f",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b20",
          "name": "Haodi Lei",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b21",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b22",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b23",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b24",
          "name": "Yufeng Zhao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b25",
          "name": "Haiyuan Wan",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b26",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b27",
          "name": "Shenghe Zheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b28",
          "name": "Junchi Yao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b29",
          "name": "Qingyang Zhang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2a",
          "name": "Haonan He",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2b",
          "name": "Wenxuan Zeng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2c",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2d",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2e",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2f",
          "name": "Yizhuo Li",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b30",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b31",
          "name": "Rui Huang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b32",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b33",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b34",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b35",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b36",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b37",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b38",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b39",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b3a",
          "name": "Ganqu Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T06:28:08.000Z",
      "submittedOnDailyAt": "2026-02-11T04:00:48.041Z",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "submittedOnDailyBy": {
        "_id": "6086838b19137b3a6ba760e7",
        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
        "isPro": false,
        "fullname": "Jianhao Yan",
        "user": "Elliott",
        "type": "user"
      },
      "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
      "upvotes": 40,
      "discussionId": "698c019f6052d3bed9630b3b",
      "projectPage": "https://prime-rl.github.io/P1-VL",
      "githubRepo": "https://github.com/PRIME-RL/P1-VL",
      "githubRepoAddedBy": "user",
      "ai_summary": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.",
      "ai_keywords": [
        "vision-language models",
        "curriculum reinforcement learning",
        "agentic augmentation",
        "multimodal perception",
        "scientific reasoning",
        "physical consistency",
        "HiPhO benchmark",
        "P1-VL-235B-A22B",
        "Gemini-3-Pro"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "6747ee5decec679eafb90450",
        "name": "ShanghaiAiLab",
        "fullname": "shanghai ailab "
      }
    },
    "publishedAt": "2026-02-10T01:28:08.000Z",
    "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
    "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09443.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6086838b19137b3a6ba760e7",
      "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
      "fullname": "Jianhao Yan",
      "name": "Elliott",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6747ee5decec679eafb90450",
      "name": "ShanghaiAiLab",
      "fullname": "shanghai ailab "
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10063",
      "authors": [
        {
          "_id": "698bf4ef6052d3bed9630a96",
          "name": "Tianyi Jiang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a97",
          "name": "Arctanx An",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a98",
          "name": "Hengyi Feng",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a99",
          "name": "Naixin Zhai",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9a",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9b",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9c",
          "name": "Jiahui Liu",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9d",
          "name": "Hanwen Du",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9e",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9f",
          "name": "Zhi Yang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa0",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa1",
          "name": "Yuhua Li",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa2",
          "name": "Yongxin Ni",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa3",
          "name": "Huacan Wang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa4",
          "name": "Ronghao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:31:47.000Z",
      "submittedOnDailyAt": "2026-02-11T00:51:58.024Z",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "submittedOnDailyBy": {
        "_id": "64aa645404e7b379feccc490",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png",
        "isPro": false,
        "fullname": "Zhi Yang",
        "user": "yangzhi1",
        "type": "user"
      },
      "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.",
      "upvotes": 35,
      "discussionId": "698bf4f06052d3bed9630aa5",
      "githubRepo": "https://github.com/QuantaAlpha/chain-of-mindset",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.",
      "ai_keywords": [
        "Chain of Mindset",
        "CoM",
        "agentic framework",
        "step-level adaptive mindset orchestration",
        "Spatial mindset",
        "Convergent mindset",
        "Divergent mindset",
        "Algorithmic mindset",
        "Meta-Agent",
        "bidirectional Context Gate",
        "reasoning efficiency",
        "large language models"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "68b33ab6a9ed99140481cf44",
        "name": "QuantaAlpha",
        "fullname": "QuantaAlpha",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
      }
    },
    "publishedAt": "2026-02-10T13:31:47.000Z",
    "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aa645404e7b379feccc490",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png",
      "fullname": "Zhi Yang",
      "name": "yangzhi1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b33ab6a9ed99140481cf44",
      "name": "QuantaAlpha",
      "fullname": "QuantaAlpha",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08426",
      "authors": [
        {
          "_id": "698bedd06052d3bed9630a13",
          "name": "Xinghao Wang",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a14",
          "name": "Pengyu Wang",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a15",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a16",
          "name": "Fangxu Liu",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a17",
          "name": "Jason Chu",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a18",
          "name": "Kai Song",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a19",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T09:31:06.000Z",
      "submittedOnDailyAt": "2026-02-11T00:29:34.413Z",
      "title": "Prism: Spectral-Aware Block-Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6191f22d08a57f265f7f5266",
        "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg",
        "isPro": false,
        "fullname": "XinghaoWang",
        "user": "Singhoo",
        "type": "user"
      },
      "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
      "upvotes": 29,
      "discussionId": "698bedd06052d3bed9630a1a",
      "githubRepo": "https://github.com/xinghaow99/prism",
      "githubRepoAddedBy": "user",
      "ai_summary": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.",
      "ai_keywords": [
        "block-sparse attention",
        "long-context LLM",
        "pre-filling",
        "coarse-grained attention",
        "Rotary Positional Embeddings",
        "RoPE",
        "mean pooling",
        "low-pass filter",
        "destructive interference",
        "spectral-aware approach",
        "energy-based temperature calibration",
        "block selection",
        "attention mechanisms"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-02-09T04:31:06.000Z",
    "title": "Prism: Spectral-Aware Block-Sparse Attention",
    "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08426.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6191f22d08a57f265f7f5266",
      "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg",
      "fullname": "XinghaoWang",
      "name": "Singhoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10090",
      "authors": [
        {
          "_id": "698bf6fe6052d3bed9630ac0",
          "name": "Zhaoyang Wang",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac1",
          "name": "Canwen Xu",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac2",
          "name": "Boyi Liu",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac3",
          "name": "Yite Wang",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac4",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac5",
          "name": "Zhewei Yao",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac6",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac7",
          "name": "Yuxiong He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"
      ],
      "publishedAt": "2026-02-10T18:55:41.000Z",
      "submittedOnDailyAt": "2026-02-11T02:28:53.819Z",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "633122d3f242a8532b7a928d",
        "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg",
        "isPro": true,
        "fullname": "Chi",
        "user": "ChilleD",
        "type": "user"
      },
      "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "upvotes": 28,
      "discussionId": "698bf6ff6052d3bed9630ac8",
      "projectPage": "https://github.com/Snowflake-Labs/agent-world-model",
      "githubRepo": "https://github.com/Snowflake-Labs/agent-world-model",
      "githubRepoAddedBy": "user",
      "ai_summary": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.",
      "ai_keywords": [
        "large language model",
        "autonomous agents",
        "multi-turn interactions",
        "tool-use agents",
        "reinforcement learning",
        "synthetic environment generation",
        "code-driven environments",
        "database-backed state transitions",
        "out-of-distribution generalization"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "62cece4aa3a23014aca72499",
        "name": "Snowflake",
        "fullname": "Snowflake",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
      }
    },
    "publishedAt": "2026-02-10T13:55:41.000Z",
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10090.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633122d3f242a8532b7a928d",
      "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg",
      "fullname": "Chi",
      "name": "ChilleD",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62cece4aa3a23014aca72499",
      "name": "Snowflake",
      "fullname": "Snowflake",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07035",
      "authors": [
        {
          "_id": "698b43df6052d3bed963079f",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a0",
          "name": "Shaoxuan Xu",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a1",
          "name": "Zhongxiang Sun",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a2",
          "name": "Fengqi Zhu",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a3",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a4",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a5",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a6",
          "name": "Xiao Zhang",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a7",
          "name": "Jun Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T09:12:08.000Z",
      "submittedOnDailyAt": "2026-02-11T01:28:30.266Z",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "submittedOnDailyBy": {
        "_id": "6309bfdab8d7b3889319b588",
        "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
        "isPro": false,
        "fullname": "SunZX",
        "user": "Jeryi",
        "type": "user"
      },
      "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
      "upvotes": 22,
      "discussionId": "698b43df6052d3bed96307a8",
      "projectPage": "https://bubble65.github.io/dllm-searcher-pub/",
      "githubRepo": "https://github.com/bubble65/DLLM-Searcher",
      "githubRepoAddedBy": "user",
      "ai_summary": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.",
      "ai_keywords": [
        "Diffusion Large Language Models",
        "ReAct agent paradigm",
        "Latency Challenge",
        "Agentic Supervised Fine-Tuning",
        "Agentic Variance-Reduced Preference Optimization",
        "Parallel-Reasoning and Acting",
        "P-ReAct"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-02-03T04:12:08.000Z",
    "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
    "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07035.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6309bfdab8d7b3889319b588",
      "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
      "fullname": "SunZX",
      "name": "Jeryi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10104",
      "authors": [
        {
          "_id": "698bfbeb6052d3bed9630ae1",
          "name": "Yuxin Jiang",
          "hidden": false
        },
        {
          "_id": "698bfbeb6052d3bed9630ae2",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "698bfbeb6052d3bed9630ae3",
          "name": "Ivor W. Tsang",
          "hidden": false
        },
        {
          "_id": "698bfbeb6052d3bed9630ae4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:58:41.000Z",
      "submittedOnDailyAt": "2026-02-11T01:40:37.554Z",
      "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
      "upvotes": 18,
      "discussionId": "698bfbeb6052d3bed9630ae5",
      "ai_summary": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.",
      "ai_keywords": [
        "action-controllable world models",
        "latent action learning",
        "temporal feature differences",
        "self-supervised video encoder",
        "sequence-level control-effect alignment",
        "action-conditioned video world models",
        "zero-shot action transfer",
        "data-efficient adaptation"
      ]
    },
    "publishedAt": "2026-02-10T13:58:41.000Z",
    "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
    "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10104.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09084",
      "authors": [
        {
          "_id": "698be8996052d3bed96309ac",
          "name": "Ruijie Ye",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309ad",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309ae",
          "name": "Zhuoxin Liu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309af",
          "name": "Zihao Zhu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b0",
          "name": "Siyuan Yang",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b1",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b2",
          "name": "Tianfu Fu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b3",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b4",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b5",
          "name": "Jiacheng Zhu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b6",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b7",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b8",
          "name": "Zhengzhong Tu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:59:18.000Z",
      "submittedOnDailyAt": "2026-02-11T00:04:02.116Z",
      "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
      "upvotes": 17,
      "discussionId": "698be89a6052d3bed96309b9",
      "projectPage": "https://agent-banana.github.io/",
      "githubRepo": "https://github.com/taco-group/agent-banana",
      "githubRepoAddedBy": "user",
      "ai_summary": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.",
      "ai_keywords": [
        "agentic planner-executor framework",
        "context folding",
        "image layer decomposition",
        "multi-turn editing",
        "high-fidelity editing",
        "object-aware editing",
        "deliberative editing",
        "HDD-Bench",
        "dialogue-based benchmark",
        "native-resolution outputs",
        "long-horizon control",
        "stepwise targets"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "693049768605dfa68334b46d",
        "name": "TexasAMUniversity",
        "fullname": "Texas A&M University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"
      }
    },
    "publishedAt": "2026-02-09T13:59:18.000Z",
    "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
    "summary": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09084.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "693049768605dfa68334b46d",
      "name": "TexasAMUniversity",
      "fullname": "Texas A&M University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.04208",
      "authors": [
        {
          "_id": "69881f9dbeecc443208d245f",
          "name": "Hyeonbeom Choi",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2460",
          "user": {
            "_id": "6384ac60b2906edaf8342ca5",
            "avatarUrl": "/avatars/033d27d1ef43444300b8a5c97447000a.svg",
            "isPro": false,
            "fullname": "Daechul Ahn",
            "user": "Daechul",
            "type": "user"
          },
          "name": "Daechul Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:30:40.330Z",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2461",
          "name": "Youhan Lee",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2462",
          "name": "Taewook Kang",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2463",
          "name": "Seongwon Cho",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2464",
          "name": "Jonghyun Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T04:48:16.000Z",
      "submittedOnDailyAt": "2026-02-11T00:14:40.246Z",
      "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "6384ac60b2906edaf8342ca5",
        "avatarUrl": "/avatars/033d27d1ef43444300b8a5c97447000a.svg",
        "isPro": false,
        "fullname": "Daechul Ahn",
        "user": "Daechul",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
      "upvotes": 17,
      "discussionId": "69881f9dbeecc443208d2465",
      "projectPage": "https://dcahn12.github.io/projects/scale/",
      "ai_summary": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "test-time scaling",
        "active inference",
        "self-uncertainty",
        "visual perception",
        "action decoding",
        "perceptual ambiguity",
        "uncertainty-driven exploration",
        "single-pass inference",
        "adaptive execution"
      ],
      "organization": {
        "_id": "66d54dc8033492801db2bf5a",
        "name": "SeoulNatlUniv",
        "fullname": "Seoul National University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
      }
    },
    "publishedAt": "2026-02-03T23:48:16.000Z",
    "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04208.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6384ac60b2906edaf8342ca5",
      "avatarUrl": "/avatars/033d27d1ef43444300b8a5c97447000a.svg",
      "fullname": "Daechul Ahn",
      "name": "Daechul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66d54dc8033492801db2bf5a",
      "name": "SeoulNatlUniv",
      "fullname": "Seoul National University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.07022",
      "authors": [
        {
          "_id": "698c2ee36052d3bed9630c8e",
          "name": "Yucheng Zhou",
          "hidden": false
        },
        {
          "_id": "698c2ee36052d3bed9630c8f",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "698c2ee36052d3bed9630c90",
          "name": "Jianbing Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T07:48:04.000Z",
      "submittedOnDailyAt": "2026-02-11T04:59:56.431Z",
      "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
      "submittedOnDailyBy": {
        "_id": "636f37fa93d9a0c987e092fa",
        "avatarUrl": "/avatars/bdb9a8c2bd0fe1b7c8cfbd6bb2a4a395.svg",
        "isPro": false,
        "fullname": "Yucheng Zhou",
        "user": "YCZhou",
        "type": "user"
      },
      "summary": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
      "upvotes": 15,
      "discussionId": "698c2ee46052d3bed9630c91",
      "ai_summary": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "diffusion loss",
        "patch denoising optimization",
        "condition error",
        "condition distribution",
        "Optimal Transport (OT) theory",
        "Wasserstein Gradient Flow",
        "condition refinement",
        "conditional diffusion"
      ],
      "organization": {
        "_id": "66bf160955a1210c53a8dc1c",
        "name": "JordanLee7677888",
        "fullname": "University of Macau",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66bf1530a03b764ca9f80a54/WFe8vJJBZv9w3TFbIW7S3.png"
      }
    },
    "publishedAt": "2026-02-02T02:48:04.000Z",
    "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
    "summary": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636f37fa93d9a0c987e092fa",
      "avatarUrl": "/avatars/bdb9a8c2bd0fe1b7c8cfbd6bb2a4a395.svg",
      "fullname": "Yucheng Zhou",
      "name": "YCZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66bf160955a1210c53a8dc1c",
      "name": "JordanLee7677888",
      "fullname": "University of Macau",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66bf1530a03b764ca9f80a54/WFe8vJJBZv9w3TFbIW7S3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08847",
      "authors": [
        {
          "_id": "698c133d6052d3bed9630bf2",
          "name": "Lang Feng",
          "hidden": false
        },
        {
          "_id": "698c133d6052d3bed9630bf3",
          "name": "Longtao Zheng",
          "hidden": false
        },
        {
          "_id": "698c133d6052d3bed9630bf4",
          "name": "Shuo He",
          "hidden": false
        },
        {
          "_id": "698c133d6052d3bed9630bf5",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "698c133d6052d3bed9630bf6",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T16:13:39.000Z",
      "submittedOnDailyAt": "2026-02-11T03:10:03.516Z",
      "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
      "submittedOnDailyBy": {
        "_id": "66ba29dd59e8e7a957154c5f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
        "isPro": false,
        "fullname": "Lang Feng",
        "user": "langfeng01",
        "type": "user"
      },
      "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
      "upvotes": 10,
      "discussionId": "698c133d6052d3bed9630bf7",
      "githubRepo": "https://github.com/langfengQ/DrMAS",
      "githubRepoAddedBy": "user",
      "ai_summary": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.",
      "ai_keywords": [
        "multi-agent LLM systems",
        "reinforcement learning",
        "GRPO-style optimization",
        "gradient-norm instability",
        "agent-wise remedy",
        "advantage normalization",
        "Qwen2.5",
        "Qwen3 series"
      ],
      "githubStars": 39,
      "organization": {
        "_id": "6508b28cf36bb51c50faad98",
        "name": "NanyangTechnologicalUniversity",
        "fullname": "Nanyang Technological University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
      }
    },
    "publishedAt": "2026-02-09T11:13:39.000Z",
    "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
    "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08847.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ba29dd59e8e7a957154c5f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
      "fullname": "Lang Feng",
      "name": "langfeng01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508b28cf36bb51c50faad98",
      "name": "NanyangTechnologicalUniversity",
      "fullname": "Nanyang Technological University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06820",
      "authors": [
        {
          "_id": "69894bebbeecc443208d25cc",
          "name": "Dunwei Tu",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25cd",
          "name": "Hongyan Hao",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25ce",
          "user": {
            "_id": "65ced159d82f8d722c78e0cf",
            "avatarUrl": "/avatars/a7121348e4fe77055533f14cda7f90b8.svg",
            "isPro": false,
            "fullname": "Hansi Yang",
            "user": "animawang",
            "type": "user"
          },
          "name": "Hansi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:52.498Z",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25cf",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d0",
          "name": "Yi-Kai Zhang",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d1",
          "name": "Zhikang Xia",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d2",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d3",
          "name": "Yueqing Sun",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d4",
          "name": "Xingchen Liu",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d5",
          "name": "Furao Shen",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d6",
          "name": "Qi Gu",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d7",
          "name": "Hui Su",
          "hidden": false
        },
        {
          "_id": "69894bebbeecc443208d25d8",
          "name": "Xunliang Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T16:05:55.000Z",
      "submittedOnDailyAt": "2026-02-11T04:01:10.656Z",
      "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
      "submittedOnDailyBy": {
        "_id": "65ced159d82f8d722c78e0cf",
        "avatarUrl": "/avatars/a7121348e4fe77055533f14cda7f90b8.svg",
        "isPro": false,
        "fullname": "Hansi Yang",
        "user": "animawang",
        "type": "user"
      },
      "summary": "Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as ^2-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
      "upvotes": 10,
      "discussionId": "69894bebbeecc443208d25d9",
      "ai_summary": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.",
      "ai_keywords": [
        "generalist agents",
        "interactive environments",
        "self-exploration",
        "procedural testing",
        "tool dependency graph expansion",
        "executable action verification",
        "multi-turn tool-use benchmarks",
        "-Bench",
        "VitaBench",
        "model generalization"
      ],
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2026-02-06T11:05:55.000Z",
    "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
    "summary": "Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as ^2-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ced159d82f8d722c78e0cf",
      "avatarUrl": "/avatars/a7121348e4fe77055533f14cda7f90b8.svg",
      "fullname": "Hansi Yang",
      "name": "animawang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.10102",
      "authors": [
        {
          "_id": "698bf2046052d3bed9630a4d",
          "name": "Zhongwei Ren",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a4e",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a4f",
          "name": "Xiao Yu",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a50",
          "name": "Guixun Luo",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a51",
          "name": "Yao Zhao",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a52",
          "name": "Bingyi Kang",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a53",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "698bf2046052d3bed9630a54",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:58:19.000Z",
      "submittedOnDailyAt": "2026-02-11T00:41:09.836Z",
      "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
      "submittedOnDailyBy": {
        "_id": "64e1cabf12a5504dda7e4948",
        "avatarUrl": "/avatars/53851eddb4e1cae773f3e3607181094b.svg",
        "isPro": true,
        "fullname": "rzw",
        "user": "maverickrzw",
        "type": "user"
      },
      "summary": "Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
      "upvotes": 9,
      "discussionId": "698bf2046052d3bed9630a55",
      "projectPage": "https://maverickren.github.io/VideoWorld2.github.io/",
      "githubRepo": "https://github.com/ByteDance-Seed/VideoWorld/tree/main/VideoWorld2",
      "githubRepoAddedBy": "user",
      "ai_summary": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.",
      "ai_keywords": [
        "latent dynamics model",
        "video diffusion model",
        "action dynamics",
        "visual appearance",
        "latent codes",
        "autoregressive modeling",
        "task policies",
        "long-horizon reasoning",
        "Open-X dataset",
        "CALVIN"
      ],
      "githubStars": 667,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-02-10T13:58:19.000Z",
    "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
    "summary": "Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e1cabf12a5504dda7e4948",
      "avatarUrl": "/avatars/53851eddb4e1cae773f3e3607181094b.svg",
      "fullname": "rzw",
      "name": "maverickrzw",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09849",
      "authors": [
        {
          "_id": "698bf64c6052d3bed9630ab2",
          "name": "Yucheng Hu",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab3",
          "name": "Jianke Zhang",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab4",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab5",
          "name": "Yanjiang Guo",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab6",
          "name": "Xiaoyu Chen",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab7",
          "name": "Xinshu Sun",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab8",
          "name": "Kun Feng",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab9",
          "name": "Qingzhou Lu",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630aba",
          "name": "Sheng Chen",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abb",
          "name": "Yangang Zhang",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abc",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abd",
          "name": "Jianyu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T14:54:01.000Z",
      "submittedOnDailyAt": "2026-02-11T01:08:02.876Z",
      "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
      "submittedOnDailyBy": {
        "_id": "63044e025c70c21d0eaf08bc",
        "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
        "isPro": false,
        "fullname": "Wei Li",
        "user": "Wiley085",
        "type": "user"
      },
      "summary": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.",
      "upvotes": 6,
      "discussionId": "698bf64c6052d3bed9630abe",
      "projectPage": "https://cladernyjorn.github.io/BagelVLA.github.io/",
      "ai_summary": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "linguistic planning",
        "visual forecasting",
        "action generation",
        "pretrained unified understanding",
        "residual flow guidance",
        "denoising",
        "multi-stage reasoning"
      ]
    },
    "publishedAt": "2026-02-10T09:54:01.000Z",
    "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
    "summary": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044e025c70c21d0eaf08bc",
      "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
      "fullname": "Wei Li",
      "name": "Wiley085",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09823",
      "authors": [
        {
          "_id": "698bf2d76052d3bed9630a6f",
          "name": "Wenfu Wang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a70",
          "name": "Chenxing Li",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a71",
          "name": "Liqiang Zhang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a72",
          "name": "Yiyang Zhao",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a73",
          "name": "Yuxiang Zou",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a74",
          "name": "Hanzhao Li",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a75",
          "name": "Mingyu Cui",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a76",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a77",
          "name": "Kun Wei",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a78",
          "name": "Le Xu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a79",
          "name": "Zikang Huang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7a",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7b",
          "name": "Jiliang Hu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7c",
          "name": "Xiang He",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7d",
          "name": "Zeyu Xie",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7e",
          "name": "Jiawen Kang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a7f",
          "name": "Youjun Chen",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a80",
          "name": "Meng Yu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a81",
          "name": "Dong Yu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a82",
          "name": "Rilin Chen",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a83",
          "name": "Linlin Di",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a84",
          "name": "Shulin Feng",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a85",
          "name": "Na Hu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a86",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a87",
          "name": "Bang Wang",
          "hidden": false
        },
        {
          "_id": "698bf2d76052d3bed9630a88",
          "name": "Shan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T14:31:11.000Z",
      "submittedOnDailyAt": "2026-02-11T00:39:19.748Z",
      "title": "Covo-Audio Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.",
      "upvotes": 5,
      "discussionId": "698bf2d76052d3bed9630a89",
      "ai_summary": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques.",
      "ai_keywords": [
        "end-to-end LALM",
        "continuous audio inputs",
        "audio outputs",
        "large-scale curated pretraining",
        "targeted post-training",
        "speech-text modeling",
        "spoken dialogue",
        "speech understanding",
        "audio understanding",
        "full-duplex voice interaction",
        "speech-text comprehension",
        "semantic reasoning",
        "dialogue-oriented variant",
        "conversational abilities",
        "contextual reasoning",
        "instruction following",
        "empathetic responses",
        "full-duplex model",
        "intelligent-speaker decoupling strategy",
        "voice rendering",
        "text-to-speech"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2026-02-10T09:31:11.000Z",
    "title": "Covo-Audio Technical Report",
    "summary": "In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09823.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07153",
      "authors": [
        {
          "_id": "698abd921b2dc6b37d61b120",
          "user": {
            "_id": "683c642b02c1a474a867964e",
            "avatarUrl": "/avatars/63e44a9cf788ee7b3ad236407700ceca.svg",
            "isPro": false,
            "fullname": "Jinbiao Wei",
            "user": "mikeweii",
            "type": "user"
          },
          "name": "Jinbiao Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-02-10T19:44:23.938Z",
          "hidden": false
        },
        {
          "_id": "698abd921b2dc6b37d61b121",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "698abd921b2dc6b37d61b122",
          "name": "Kangqi Ni",
          "hidden": false
        },
        {
          "_id": "698abd921b2dc6b37d61b123",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T19:55:26.000Z",
      "submittedOnDailyAt": "2026-02-11T00:20:18.700Z",
      "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
      "submittedOnDailyBy": {
        "_id": "683c642b02c1a474a867964e",
        "avatarUrl": "/avatars/63e44a9cf788ee7b3ad236407700ceca.svg",
        "isPro": false,
        "fullname": "Jinbiao Wei",
        "user": "mikeweii",
        "type": "user"
      },
      "summary": "End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
      "upvotes": 5,
      "discussionId": "698abd921b2dc6b37d61b124",
      "ai_summary": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.",
      "ai_keywords": [
        "trajectory expansion",
        "GUI agents",
        "desktop environments",
        "seed demonstrations",
        "branch points",
        "state-grounded task variants",
        "trajectory generation",
        "verifier",
        "task completion",
        "step-level filtering",
        "denoising"
      ],
      "organization": {
        "_id": "6532df27d690f3012efde84c",
        "name": "yale-nlp",
        "fullname": "Yale NLP Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"
      }
    },
    "publishedAt": "2026-02-06T14:55:26.000Z",
    "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
    "summary": "End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07153.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "683c642b02c1a474a867964e",
      "avatarUrl": "/avatars/63e44a9cf788ee7b3ad236407700ceca.svg",
      "fullname": "Jinbiao Wei",
      "name": "mikeweii",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6532df27d690f3012efde84c",
      "name": "yale-nlp",
      "fullname": "Yale NLP Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.10116",
      "authors": [
        {
          "_id": "698bf1aa6052d3bed9630a2d",
          "name": "Hongchi Xia",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a2e",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a2f",
          "name": "Zhaoshuo Li",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a30",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a31",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a32",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a33",
          "name": "Yin Cui",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a34",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a35",
          "name": "Wei-Chiu Ma",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a36",
          "name": "Shenlong Wang",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a37",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "698bf1aa6052d3bed9630a38",
          "name": "Fangyin Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:59:55.000Z",
      "submittedOnDailyAt": "2026-02-11T00:45:34.378Z",
      "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
      "upvotes": 4,
      "discussionId": "698bf1ab6052d3bed9630a39",
      "projectPage": "https://nvlabs.github.io/sage",
      "ai_summary": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.",
      "ai_keywords": [
        "embodied agents",
        "simulation-ready environments",
        "scene-generation systems",
        "agentic framework",
        "layout generation",
        "object composition",
        "semantic plausibility",
        "visual realism",
        "physical stability",
        "iterative reasoning",
        "adaptive tool selection",
        "policy training",
        "embodied AI"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-02-10T13:59:55.000Z",
    "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
    "summary": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10098",
      "authors": [
        {
          "_id": "698bf4886052d3bed9630a8b",
          "name": "Jingwen Sun",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8c",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8d",
          "name": "Zekun Qi",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8e",
          "name": "Shaojie Ren",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8f",
          "name": "Zezhi Liu",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a90",
          "name": "Hanxin Zhu",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a91",
          "name": "Guangzhong Sun",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a92",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a93",
          "name": "Zhibo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:58:01.000Z",
      "submittedOnDailyAt": "2026-02-11T01:04:16.621Z",
      "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.",
      "upvotes": 4,
      "discussionId": "698bf4896052d3bed9630a94",
      "projectPage": "https://ginwind.github.io/VLA-JEPA/",
      "githubRepo": "https://github.com/ginwind/VLA-JEPA",
      "githubRepoAddedBy": "user",
      "ai_summary": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.",
      "ai_keywords": [
        "Vision-Language-Action",
        "JEPA",
        "latent-action objectives",
        "pixel variation",
        "action-relevant state transitions",
        "appearance bias",
        "nuisance motion",
        "information leakage",
        "target encoder",
        "student pathway",
        "latent representations",
        "future frames",
        "current observation",
        "latent space",
        "dynamics abstractions",
        "camera motion",
        "background changes",
        "JEPA pretraining",
        "action-head fine-tuning",
        "generalization",
        "robustness"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-02-10T13:58:01.000Z",
    "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
    "summary": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10098.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08382",
      "authors": [
        {
          "_id": "698c01df6052d3bed9630b55",
          "name": "Zhuoen Chen",
          "hidden": false
        },
        {
          "_id": "698c01df6052d3bed9630b56",
          "name": "Dongfang Li",
          "hidden": false
        },
        {
          "_id": "698c01df6052d3bed9630b57",
          "name": "Meishan Zhang",
          "hidden": false
        },
        {
          "_id": "698c01df6052d3bed9630b58",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "698c01df6052d3bed9630b59",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T08:33:11.000Z",
      "submittedOnDailyAt": "2026-02-11T05:17:58.176Z",
      "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "62e38a3ef3b208e2aecf2c84",
        "avatarUrl": "/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg",
        "isPro": false,
        "fullname": "Dongfang Li",
        "user": "crazyofapple",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
      "upvotes": 4,
      "discussionId": "698c01df6052d3bed9630b5a",
      "ai_summary": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.",
      "ai_keywords": [
        "large language models",
        "long-context processing",
        "retrieval-augmented generation",
        "chunk-wise compression",
        "selective memory recall",
        "learned compressor",
        "gating module",
        "reasoning module",
        "working memory",
        "end-to-end reinforcement learning",
        "multi-hop reasoning",
        "RULER-HQA",
        "MemAgent"
      ],
      "organization": {
        "_id": "629867d7f2bf8bd3e468706e",
        "name": "HIT-TMG",
        "fullname": "Lychee Team",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b7679a08e2452d18db9a9e/uk4QHGGYqcrEHmiweTFWy.png"
      }
    },
    "publishedAt": "2026-02-09T03:33:11.000Z",
    "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
    "summary": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e38a3ef3b208e2aecf2c84",
      "avatarUrl": "/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg",
      "fullname": "Dongfang Li",
      "name": "crazyofapple",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "629867d7f2bf8bd3e468706e",
      "name": "HIT-TMG",
      "fullname": "Lychee Team",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b7679a08e2452d18db9a9e/uk4QHGGYqcrEHmiweTFWy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09024",
      "authors": [
        {
          "_id": "698ab8ae1b2dc6b37d61b089",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08a",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08b",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08c",
          "name": "Xinyang Zhang",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08e",
          "name": "Liang-Chieh Chen",
          "hidden": false
        },
        {
          "_id": "698ab8ae1b2dc6b37d61b08f",
          "name": "Xi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:59:58.000Z",
      "submittedOnDailyAt": "2026-02-11T00:43:47.623Z",
      "title": "Autoregressive Image Generation with Masked Bit Modeling",
      "submittedOnDailyBy": {
        "_id": "648010b00b9d0f49849adb19",
        "avatarUrl": "/avatars/54419bb6e6ee788aba10cd64cd921204.svg",
        "isPro": false,
        "fullname": "Qihang Yu",
        "user": "yucornetto",
        "type": "user"
      },
      "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
      "upvotes": 3,
      "discussionId": "698ab8ae1b2dc6b37d61b090",
      "projectPage": "https://bar-gen.github.io/",
      "githubRepo": "https://github.com/amazon-far/BAR",
      "githubRepoAddedBy": "user",
      "ai_summary": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.",
      "ai_keywords": [
        "discrete tokenizers",
        "continuous pipelines",
        "latent space",
        "codebook size",
        "autoregressive transformer",
        "masked bit modeling",
        "Bit AutoRegressive modeling",
        "gFID",
        "ImageNet-256"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "68d5924a76c6b4bfe8b4ab60",
        "name": "Amazon-FAR",
        "fullname": "Amazon Frontier AI & Robotics",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68251d059667f9f347003874/YPxVjC8rCyexwo42cznvE.png"
      }
    },
    "publishedAt": "2026-02-09T13:59:58.000Z",
    "title": "Autoregressive Image Generation with Masked Bit Modeling",
    "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09024.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648010b00b9d0f49849adb19",
      "avatarUrl": "/avatars/54419bb6e6ee788aba10cd64cd921204.svg",
      "fullname": "Qihang Yu",
      "name": "yucornetto",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68d5924a76c6b4bfe8b4ab60",
      "name": "Amazon-FAR",
      "fullname": "Amazon Frontier AI & Robotics",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68251d059667f9f347003874/YPxVjC8rCyexwo42cznvE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08344",
      "authors": [
        {
          "_id": "698bf2606052d3bed9630a57",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a58",
          "name": "Jianing Wang",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a59",
          "name": "Deyang Kong",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5a",
          "name": "Xiangyu Xi",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5b",
          "name": "Jianfei Zhang",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5c",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5d",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5e",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a5f",
          "name": "Shikun Zhang",
          "hidden": false
        },
        {
          "_id": "698bf2606052d3bed9630a60",
          "name": "Wei Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T07:29:13.000Z",
      "submittedOnDailyAt": "2026-02-11T00:39:26.160Z",
      "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
      "submittedOnDailyBy": {
        "_id": "65a0aade5fafc248c2156e95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
        "isPro": false,
        "fullname": "DeyangKong",
        "user": "DeyangKong",
        "type": "user"
      },
      "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.",
      "upvotes": 3,
      "discussionId": "698bf2606052d3bed9630a61",
      "ai_summary": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.",
      "ai_keywords": [
        "parallel thinking",
        "large reasoning models",
        "Reinforcement Learning",
        "RLVR",
        "mutual information bottleneck",
        "outline-guided path exploration",
        "reasoning outlines",
        "iterative RL strategy",
        "aggregation phase",
        "path exploration stage"
      ]
    },
    "publishedAt": "2026-02-09T02:29:13.000Z",
    "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
    "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a0aade5fafc248c2156e95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
      "fullname": "DeyangKong",
      "name": "DeyangKong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09439",
      "authors": [
        {
          "_id": "698c12636052d3bed9630bec",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "698c12636052d3bed9630bed",
          "name": "Yitian Zhang",
          "hidden": false
        },
        {
          "_id": "698c12636052d3bed9630bee",
          "name": "Qihua Dong",
          "hidden": false
        },
        {
          "_id": "698c12636052d3bed9630bef",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T06:06:54.000Z",
      "submittedOnDailyAt": "2026-02-11T02:56:41.984Z",
      "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "62434e02ffb7778797651d50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654982689149-62434e02ffb7778797651d50.jpeg",
        "isPro": true,
        "fullname": "Xu Ma",
        "user": "ma-xu",
        "type": "user"
      },
      "summary": "High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
      "upvotes": 2,
      "discussionId": "698c12636052d3bed9630bf0",
      "projectPage": "https://huggingface.co/spaces/ma-xu/fine-t2i-explore",
      "ai_summary": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.",
      "ai_keywords": [
        "text-to-image",
        "fine-tuning",
        "diffusion models",
        "autoregressive models",
        "text-image alignment",
        "visual fidelity",
        "prompt quality",
        "dataset scaling"
      ],
      "organization": {
        "_id": "68aa797c53b0c4b2e53dc742",
        "name": "Northeastern",
        "fullname": "Northeastern University ",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68aa7916a2c0dfa90ecea840/rMbsVWRw5ypjq_Gls_X1O.png"
      }
    },
    "publishedAt": "2026-02-10T01:06:54.000Z",
    "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
    "summary": "High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62434e02ffb7778797651d50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654982689149-62434e02ffb7778797651d50.jpeg",
      "fullname": "Xu Ma",
      "name": "ma-xu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68aa797c53b0c4b2e53dc742",
      "name": "Northeastern",
      "fullname": "Northeastern University ",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68aa7916a2c0dfa90ecea840/rMbsVWRw5ypjq_Gls_X1O.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05435",
      "authors": [
        {
          "_id": "698c30536052d3bed9630c93",
          "name": "Donglin Yang",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c94",
          "name": "Yongxing Zhang",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c95",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c96",
          "name": "Liang Hou",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c97",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c98",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c99",
          "name": "Xiaojuan Qi",
          "hidden": false
        },
        {
          "_id": "698c30536052d3bed9630c9a",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66de6bd4174e9c6971dedb70/wGg_dE2ypVy7ZoMOHHUZr.png"
      ],
      "publishedAt": "2026-02-05T08:25:05.000Z",
      "submittedOnDailyAt": "2026-02-11T05:09:44.300Z",
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "submittedOnDailyBy": {
        "_id": "66de6bd4174e9c6971dedb70",
        "avatarUrl": "/avatars/40f35d8d87ecc543bc1debdb7790133f.svg",
        "isPro": false,
        "fullname": "Yang",
        "user": "linYD0718",
        "type": "user"
      },
      "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "upvotes": 2,
      "discussionId": "698c30536052d3bed9630c9b",
      "projectPage": "https://linydthu.github.io/StableVelocity/",
      "githubRepo": "https://github.com/linYDTHU/StableVelocity",
      "githubRepoAddedBy": "user",
      "ai_summary": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.",
      "ai_keywords": [
        "flow matching",
        "conditional velocities",
        "variance reduction",
        "Stable Velocity Matching",
        "Variance-Aware Representation Alignment",
        "Stable Velocity Sampling",
        "training efficiency",
        "sampling speed"
      ],
      "githubStars": 12
    },
    "publishedAt": "2026-02-05T03:25:05.000Z",
    "title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66de6bd4174e9c6971dedb70/wGg_dE2ypVy7ZoMOHHUZr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66de6bd4174e9c6971dedb70",
      "avatarUrl": "/avatars/40f35d8d87ecc543bc1debdb7790133f.svg",
      "fullname": "Yang",
      "name": "linYD0718",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08503",
      "authors": [
        {
          "_id": "698bb52d6052d3bed96308fd",
          "name": "Yi Ding",
          "hidden": false
        },
        {
          "_id": "698bb52d6052d3bed96308fe",
          "name": "Ziliang Qiu",
          "hidden": false
        },
        {
          "_id": "698bb52d6052d3bed96308ff",
          "name": "Bolian Li",
          "hidden": false
        },
        {
          "_id": "698bb52d6052d3bed9630900",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T10:55:13.000Z",
      "submittedOnDailyAt": "2026-02-11T04:01:57.995Z",
      "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.",
      "upvotes": 1,
      "discussionId": "698bb52e6052d3bed9630901",
      "projectPage": "https://dripnowhy.github.io/Octopus/",
      "ai_summary": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.",
      "ai_keywords": [
        "reinforcement learning",
        "vision-language models",
        "self-correction",
        "rollout augmentation",
        "response-masking strategy",
        "sample efficiency",
        "RL optimization",
        "controllable self-correction"
      ]
    },
    "publishedAt": "2026-02-09T05:55:13.000Z",
    "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
    "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08503.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07839",
      "authors": [
        {
          "_id": "698b1fc81b2dc6b37d61b42b",
          "name": "Jiaxi Liu",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b42c",
          "name": "Yanzuo Jiang",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b42d",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b42e",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b42f",
          "name": "Heng Chang",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b430",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b431",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "698b1fc81b2dc6b37d61b432",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-08T06:37:01.000Z",
      "submittedOnDailyAt": "2026-02-11T05:25:11.480Z",
      "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
      "submittedOnDailyBy": {
        "_id": "6363a1fa123a5d5cd4a800e2",
        "avatarUrl": "/avatars/a0961ca5463aae05de0b1574c0064fae.svg",
        "isPro": false,
        "fullname": "guibin zhang",
        "user": "greeky",
        "type": "user"
      },
      "summary": "Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.",
      "upvotes": 1,
      "discussionId": "698b1fc81b2dc6b37d61b433",
      "githubRepo": "https://github.com/EcthelionLiu/TodoEvolve",
      "githubRepoAddedBy": "user",
      "ai_summary": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.",
      "ai_keywords": [
        "meta-planning",
        "planning architectures",
        "PlanFactory",
        "Impedance-Guided Preference Optimization",
        "IGPO",
        "reinforcement learning",
        "task-specific planning",
        "modular design space"
      ],
      "githubStars": 4
    },
    "publishedAt": "2026-02-08T01:37:01.000Z",
    "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
    "summary": "Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07839.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6363a1fa123a5d5cd4a800e2",
      "avatarUrl": "/avatars/a0961ca5463aae05de0b1574c0064fae.svg",
      "fullname": "guibin zhang",
      "name": "greeky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07276",
      "authors": [
        {
          "_id": "698adc4e1b2dc6b37d61b29f",
          "name": "Pengrui Han",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a0",
          "name": "Xueqiang Xu",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a1",
          "name": "Keyang Xuan",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a2",
          "name": "Peiyang Song",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a3",
          "name": "Siru Ouyang",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a4",
          "name": "Runchu Tian",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a5",
          "name": "Yuqing Jiang",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a6",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a7",
          "name": "Pengcheng Jiang",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a8",
          "name": "Jiashuo Sun",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2a9",
          "name": "Junxia Cui",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2aa",
          "name": "Ming Zhong",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2ab",
          "name": "Ge Liu",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2ac",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "698adc4e1b2dc6b37d61b2ad",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-07T00:00:50.000Z",
      "submittedOnDailyAt": "2026-02-11T05:30:55.286Z",
      "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
      "submittedOnDailyBy": {
        "_id": "649c5cf5c1ae48cf4d7dda34",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg",
        "isPro": false,
        "fullname": "Peiyang Song",
        "user": "p-song1",
        "type": "user"
      },
      "summary": "Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
      "upvotes": 1,
      "discussionId": "698adc4e1b2dc6b37d61b2ae",
      "ai_summary": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.",
      "ai_keywords": [
        "activation steering",
        "large language models",
        "steering vectors",
        "semantic prior subspace",
        "linear combination",
        "inference-time adaptation",
        "data efficiency",
        "stability",
        "transparency"
      ],
      "organization": {
        "_id": "65448bef5b5d9185ba3202b9",
        "name": "UIUC-CS",
        "fullname": "University of Illinois at Urbana-Champaign",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
      }
    },
    "publishedAt": "2026-02-06T19:00:50.000Z",
    "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
    "summary": "Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649c5cf5c1ae48cf4d7dda34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg",
      "fullname": "Peiyang Song",
      "name": "p-song1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65448bef5b5d9185ba3202b9",
      "name": "UIUC-CS",
      "fullname": "University of Illinois at Urbana-Champaign",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02464",
      "authors": [
        {
          "_id": "698a0c131b2dc6b37d61adeb",
          "name": "Or Shafran",
          "hidden": false
        },
        {
          "_id": "698a0c131b2dc6b37d61adec",
          "name": "Shaked Ronen",
          "hidden": false
        },
        {
          "_id": "698a0c131b2dc6b37d61aded",
          "name": "Omri Fahn",
          "hidden": false
        },
        {
          "_id": "698a0c131b2dc6b37d61adee",
          "name": "Shauli Ravfogel",
          "hidden": false
        },
        {
          "_id": "698a0c131b2dc6b37d61adef",
          "name": "Atticus Geiger",
          "hidden": false
        },
        {
          "_id": "698a0c131b2dc6b37d61adf0",
          "name": "Mor Geva",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T18:49:05.000Z",
      "submittedOnDailyAt": "2026-02-11T05:12:22.421Z",
      "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
      "submittedOnDailyBy": {
        "_id": "67b738790d11464dcbc8adf5",
        "avatarUrl": "/avatars/f59109ff562d7b037be2f81bb11df270.svg",
        "isPro": false,
        "fullname": "Or Shafran",
        "user": "ordavids1",
        "type": "user"
      },
      "summary": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.",
      "upvotes": 1,
      "discussionId": "698a0c131b2dc6b37d61adf1",
      "githubRepo": "https://github.com/ordavid-s/decomposing-activations-local-geometry",
      "githubRepoAddedBy": "user",
      "ai_summary": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.",
      "ai_keywords": [
        "Mixture of Factor Analyzers",
        "activation space",
        "Gaussian regions",
        "local covariance structure",
        "concept discovery",
        "model control",
        "sparse autoencoders"
      ],
      "githubStars": 11
    },
    "publishedAt": "2026-02-02T13:49:05.000Z",
    "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
    "summary": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67b738790d11464dcbc8adf5",
      "avatarUrl": "/avatars/f59109ff562d7b037be2f81bb11df270.svg",
      "fullname": "Or Shafran",
      "name": "ordavids1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01725",
      "authors": [
        {
          "_id": "698c280b6052d3bed9630c6b",
          "name": "Yurun Chen",
          "hidden": false
        },
        {
          "_id": "698c280b6052d3bed9630c6c",
          "name": "Zeyi Liao",
          "hidden": false
        },
        {
          "_id": "698c280b6052d3bed9630c6d",
          "name": "Ping Yin",
          "hidden": false
        },
        {
          "_id": "698c280b6052d3bed9630c6e",
          "name": "Taotao Xie",
          "hidden": false
        },
        {
          "_id": "698c280b6052d3bed9630c6f",
          "name": "Keting Yin",
          "hidden": false
        },
        {
          "_id": "698c280b6052d3bed9630c70",
          "name": "Shengyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T07:04:06.000Z",
      "submittedOnDailyAt": "2026-02-11T04:29:21.661Z",
      "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
      "submittedOnDailyBy": {
        "_id": "6747285ed34bd6f05080ddda",
        "avatarUrl": "/avatars/ea63de7348aaabc8cff44e76207ba65c.svg",
        "isPro": false,
        "fullname": "Yurun Chen",
        "user": "yurun-chen",
        "type": "user"
      },
      "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
      "upvotes": 1,
      "discussionId": "698c280b6052d3bed9630c71",
      "githubRepo": "https://github.com/YurunChen/SafePred",
      "githubRepoAddedBy": "user",
      "ai_summary": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.",
      "ai_keywords": [
        "Computer-using Agents",
        "predictive guardrail approach",
        "risk-to-decision loop",
        "safety policies",
        "world model",
        "semantic representations",
        "risk prediction",
        "decision optimization",
        "step-level interventions",
        "task-level re-planning"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "6345aadf5efccdc07f1365a5",
        "name": "ZhejiangUniversity",
        "fullname": "Zhejiang University"
      }
    },
    "publishedAt": "2026-02-02T02:04:06.000Z",
    "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
    "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01725.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747285ed34bd6f05080ddda",
      "avatarUrl": "/avatars/ea63de7348aaabc8cff44e76207ba65c.svg",
      "fullname": "Yurun Chen",
      "name": "yurun-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21235",
      "authors": [
        {
          "_id": "6984eb314ad556f294b7ea1f",
          "user": {
            "_id": "6478fc1512ae749b62ebbbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
            "isPro": false,
            "fullname": "Alok Abhishek",
            "user": "alokabhishek",
            "type": "user"
          },
          "name": "Alok Abhishek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:52:35.456Z",
          "hidden": false
        },
        {
          "_id": "6984eb314ad556f294b7ea20",
          "name": "Tushar Bandopadhyay",
          "hidden": false
        },
        {
          "_id": "6984eb314ad556f294b7ea21",
          "name": "Lisa Erickson",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T03:54:25.000Z",
      "submittedOnDailyAt": "2026-02-11T00:42:12.084Z",
      "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6478fc1512ae749b62ebbbd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
        "isPro": false,
        "fullname": "Alok Abhishek",
        "user": "alokabhishek",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
      "upvotes": 1,
      "discussionId": "6984eb314ad556f294b7ea22",
      "ai_summary": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.",
      "ai_keywords": [
        "Social Harm Analysis via Risk Profiles",
        "multivariate random variable",
        "bias",
        "fairness",
        "ethics",
        "epistemic reliability",
        "union-of-failures aggregation",
        "cumulative log-risk",
        "Conditional Value at Risk",
        "tail exposure",
        "volatility",
        "worst-case behavior",
        "scalar benchmarks"
      ]
    },
    "publishedAt": "2026-01-28T22:54:25.000Z",
    "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
    "summary": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478fc1512ae749b62ebbbd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
      "fullname": "Alok Abhishek",
      "name": "alokabhishek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.04908",
      "authors": [
        {
          "_id": "69865a172d626112378ad57a",
          "name": "Chika Maduabuchi",
          "hidden": false
        },
        {
          "_id": "69865a172d626112378ad57b",
          "name": "Jindong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T00:05:21.000Z",
      "submittedOnDailyAt": "2026-02-11T00:16:25.401Z",
      "title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
      "submittedOnDailyBy": {
        "_id": "663d1442a1698765659fab3c",
        "avatarUrl": "/avatars/a2aa3454344213c49a91bda8d5d6bd18.svg",
        "isPro": false,
        "fullname": "Chika Maduabuchi",
        "user": "Chikap421",
        "type": "user"
      },
      "summary": "Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.",
      "upvotes": 0,
      "discussionId": "69865a182d626112378ad57c",
      "ai_summary": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.",
      "ai_keywords": [
        "continuous-time generative models",
        "diffusion models",
        "flow matching",
        "rectified flow",
        "velocity predictions",
        "temporal pair consistency",
        "variance reduction",
        "gradient variance",
        "probability paths",
        "trajectory coupling",
        "flow-matching objective"
      ]
    },
    "publishedAt": "2026-02-03T19:05:21.000Z",
    "title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
    "summary": "Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663d1442a1698765659fab3c",
      "avatarUrl": "/avatars/a2aa3454344213c49a91bda8d5d6bd18.svg",
      "fullname": "Chika Maduabuchi",
      "name": "Chikap421",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]