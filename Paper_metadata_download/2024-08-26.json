[
    {
        "paper": {
            "id": "2408.12637",
            "authors": [
                {
                    "_id": "66cc04c301fbc62ecda3f5c2",
                    "name": "Hugo Laurençon",
                    "hidden": false
                },
                {
                    "_id": "66cc04c301fbc62ecda3f5c3",
                    "name": "Andrés Marafioti",
                    "hidden": false
                },
                {
                    "_id": "66cc04c301fbc62ecda3f5c4",
                    "name": "Victor Sanh",
                    "hidden": false
                },
                {
                    "_id": "66cc04c301fbc62ecda3f5c5",
                    "name": "Léo Tronchon",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-22T17:47:24.000Z",
            "title": "Building and better understanding vision-language models: insights and\n  future directions",
            "summary": "The field of vision-language models (VLMs), which take images and texts as\ninputs and output texts, is rapidly evolving and has yet to reach consensus on\nseveral key aspects of the development pipeline, including data, architecture,\nand training methods. This paper can be seen as a tutorial for building a VLM.\nWe begin by providing a comprehensive overview of the current state-of-the-art\napproaches, highlighting the strengths and weaknesses of each, addressing the\nmajor challenges in the field, and suggesting promising research directions for\nunderexplored areas. We then walk through the practical steps to build\nIdefics3-8B, a powerful VLM that significantly outperforms its predecessor\nIdefics2-8B, while being trained efficiently, exclusively on open datasets, and\nusing a straightforward pipeline. These steps include the creation of Docmatix,\na dataset for improving document understanding capabilities, which is 240 times\nlarger than previously available datasets. We release the model along with the\ndatasets created for its training.",
            "upvotes": 26,
            "discussionId": "66cc04c401fbc62ecda3f604"
        },
        "publishedAt": "2024-08-26T03:00:48.374Z",
        "title": "Building and better understanding vision-language models: insights and future directions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12637.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635201569275-noauth.jpeg",
            "fullname": "Hugo Laurençon",
            "name": "HugoLaurencon",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.13252",
            "authors": [
                {
                    "_id": "66cbfc93888e99a28d4724c4",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724c5",
                    "name": "Jing Tan",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724c6",
                    "name": "Mengchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724c7",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724c8",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724c9",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724ca",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "66cbfc93888e99a28d4724cb",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T17:50:23.000Z",
            "title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation",
            "summary": "3D immersive scene generation is a challenging yet critical task in computer\nvision and graphics. A desired virtual 3D scene should 1) exhibit\nomnidirectional view consistency, and 2) allow for free exploration in complex\nscene hierarchies. Existing methods either rely on successive scene expansion\nvia inpainting or employ panorama representation to represent large FOV scene\nenvironments. However, the generated scene suffers from semantic drift during\nexpansion and is unable to handle occlusion among scene hierarchies. To tackle\nthese challenges, we introduce LayerPano3D, a novel framework for full-view,\nexplorable panoramic 3D scene generation from a single text prompt. Our key\ninsight is to decompose a reference 2D panorama into multiple layers at\ndifferent depth levels, where each layer reveals the unseen space from the\nreference views via diffusion prior. LayerPano3D comprises multiple dedicated\ndesigns: 1) we introduce a novel text-guided anchor view synthesis pipeline for\nhigh-quality, consistent panorama generation. 2) We pioneer the Layered 3D\nPanorama as underlying representation to manage complex scene hierarchies and\nlift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes\nwith unconstrained viewing paths. Extensive experiments demonstrate that our\nframework generates state-of-the-art 3D panoramic scene in both full view\nconsistency and immersive exploratory experience. We believe that LayerPano3D\nholds promise for advancing 3D panoramic scene creation with numerous\napplications.",
            "upvotes": 17,
            "discussionId": "66cbfc99888e99a28d472647"
        },
        "publishedAt": "2024-08-26T02:25:11.238Z",
        "title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13252.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.13257",
            "authors": [
                {
                    "_id": "66cbf6dab395ddc37b56e9c8",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9c9",
                    "user": {
                        "_id": "65e816bbcfd12cd15b052a0e",
                        "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
                        "isPro": false,
                        "fullname": "Huanyu_Zhang",
                        "user": "huanyu112",
                        "type": "user"
                    },
                    "name": "Huanyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-08-26T07:14:54.194Z",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9ca",
                    "name": "Haochen Tian",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9cb",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9cc",
                    "name": "Shuangqing Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9cd",
                    "name": "Junfei Wu",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9ce",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9cf",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9d0",
                    "name": "Qingsong Wen",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9d1",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9d2",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9d3",
                    "name": "Rong Jin",
                    "hidden": false
                },
                {
                    "_id": "66cbf6dab395ddc37b56e9d4",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T17:59:51.000Z",
            "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
            "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than 300K images from public datasets and the Internet,\nfiltering 13,366 high-quality images for annotation. This involves the\nefforts of professional 25 annotators and 7 experts in MLLMs, contributing\nto 29,429 question-answer pairs that cover 43 subtasks across 5\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving 28 prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n60% accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
            "upvotes": 15,
            "discussionId": "66cbf6deb395ddc37b56eae3"
        },
        "publishedAt": "2024-08-26T02:12:31.973Z",
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/gvBd8NYTUSEAbAMSfTrkd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/vzKoUfeQGP3SKB2JpFo_5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13257.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/8bde703a4124f3f6c07e8f0b6aaea036.svg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.13233",
            "authors": [
                {
                    "_id": "66cc05aed20991c2a015a6bc",
                    "name": "Yingyu Liang",
                    "hidden": false
                },
                {
                    "_id": "66cc05aed20991c2a015a6bd",
                    "name": "Zhizhou Sha",
                    "hidden": false
                },
                {
                    "_id": "66cc05aed20991c2a015a6be",
                    "name": "Zhenmei Shi",
                    "hidden": false
                },
                {
                    "_id": "66cc05aed20991c2a015a6bf",
                    "name": "Zhao Song",
                    "hidden": false
                },
                {
                    "_id": "66cc05aed20991c2a015a6c0",
                    "name": "Yufa Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T17:16:43.000Z",
            "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time",
            "summary": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time n^{1+o(1)}, where n is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results.",
            "upvotes": 10,
            "discussionId": "66cc05aed20991c2a015a6ed"
        },
        "publishedAt": "2024-08-26T03:13:28.024Z",
        "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13233.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/4c6611dabd492106ffb2e82fd680d983.svg",
            "fullname": "Zhizhou Sha",
            "name": "JamesSand",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.13239",
            "authors": [
                {
                    "_id": "66cbe22f8347e21e8a75a852",
                    "name": "Tao Wu",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a853",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a854",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a855",
                    "name": "Xianpan Zhou",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a856",
                    "name": "Guangcong Zheng",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a857",
                    "name": "Zhongang Qi",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a858",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "66cbe22f8347e21e8a75a859",
                    "name": "Xi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T17:26:06.000Z",
            "title": "CustomCrafter: Customized Video Generation with Preserving Motion and\n  Concept Composition Abilities",
            "summary": "Customized video generation aims to generate high-quality videos guided by\ntext prompts and subject's reference images. However, since it is only trained\non static images, the fine-tuning process of subject learning disrupts\nabilities of video diffusion models (VDMs) to combine concepts and generate\nmotions. To restore these abilities, some methods use additional video similar\nto the prompt to fine-tune or guide the model. This requires frequent changes\nof guiding videos and even re-tuning of the model when generating different\nmotions, which is very inconvenient for users. In this paper, we propose\nCustomCrafter, a novel framework that preserves the model's motion generation\nand conceptual combination abilities without additional video and fine-tuning\nto recovery. For preserving conceptual combination ability, we design a\nplug-and-play module to update few parameters in VDMs, enhancing the model's\nability to capture the appearance details and the ability of concept\ncombinations for new subjects. For motion generation, we observed that VDMs\ntend to restore the motion of video in the early stage of denoising, while\nfocusing on the recovery of subject details in the later stage. Therefore, we\npropose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our\nsubject learning modules, we reduce the impact of this module on motion\ngeneration in the early stage of denoising, preserving the ability to generate\nmotion of VDMs. In the later stage of denoising, we restore this module to\nrepair the appearance details of the specified subject, thereby ensuring the\nfidelity of the subject's appearance. Experimental results show that our method\nhas a significant improvement compared to previous methods.",
            "upvotes": 7,
            "discussionId": "66cbe2308347e21e8a75a8a4"
        },
        "publishedAt": "2024-08-26T00:32:25.779Z",
        "title": "CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13239.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.12885",
            "authors": [
                {
                    "_id": "66cbf66257c2405fdffb2896",
                    "name": "Wenshuo Peng",
                    "hidden": false
                },
                {
                    "_id": "66cbf66257c2405fdffb2897",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "66cbf66257c2405fdffb2898",
                    "name": "Sai Qian Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T07:37:26.000Z",
            "title": "T3M: Text Guided 3D Human Motion Synthesis from Speech",
            "summary": "Speech-driven 3D motion synthesis seeks to create lifelike animations based\non human speech, with potential uses in virtual reality, gaming, and the film\nproduction. Existing approaches reply solely on speech audio for motion\ngeneration, leading to inaccurate and inflexible synthesis results. To mitigate\nthis problem, we introduce a novel text-guided 3D human motion synthesis\nmethod, termed T3M. Unlike traditional approaches, T3M allows precise\ncontrol over motion synthesis via textual input, enhancing the degree of\ndiversity and user customization. The experiment results demonstrate that T3M\ncan greatly outperform the state-of-the-art methods in both quantitative\nmetrics and qualitative evaluations. We have publicly released our code at\nhttps://github.com/Gloria2tt/T3M.git{https://github.com/Gloria2tt/T3M.git}",
            "upvotes": 6,
            "discussionId": "66cbf66357c2405fdffb28f5"
        },
        "publishedAt": "2024-08-26T02:09:09.165Z",
        "title": "T3M: Text Guided 3D Human Motion Synthesis from Speech",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/_z_5izy8ThuFmAl-PGsUu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12885.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.12857",
            "authors": [
                {
                    "_id": "66cbe5ef50ce52b61b221506",
                    "name": "Kaizhao Liang",
                    "hidden": false
                },
                {
                    "_id": "66cbe5ef50ce52b61b221507",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "66cbe5ef50ce52b61b221508",
                    "name": "Lizhang Chen",
                    "hidden": false
                },
                {
                    "_id": "66cbe5ef50ce52b61b221509",
                    "name": "Qiang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T05:54:53.000Z",
            "title": "Memory-Efficient LLM Training with Online Subspace Descent",
            "summary": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the first convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines.",
            "upvotes": 3,
            "discussionId": "66cbe5f050ce52b61b221533"
        },
        "publishedAt": "2024-08-26T10:56:05.404Z",
        "title": "Memory-Efficient LLM Training with Online Subspace Descent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12857.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62140dcdcf7928035e8135ad/FTiirwS_L6IaLHmHwIo2g.png",
            "fullname": "Kaizhao Liang",
            "name": "kz919",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.12418",
            "authors": [
                {
                    "_id": "66caf58fe51ba30092ed6f6d",
                    "name": "Bastien van Delft",
                    "hidden": false
                },
                {
                    "_id": "66caf58fe51ba30092ed6f6e",
                    "user": {
                        "_id": "648097daf5be39206aeff790",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648097daf5be39206aeff790/BfDcQkmiCATncP6WuejpA.png",
                        "isPro": false,
                        "fullname": "Tommaso Martorella",
                        "user": "tommymarto",
                        "type": "user"
                    },
                    "name": "Tommaso Martorella",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-08-26T07:14:58.927Z",
                    "hidden": false
                },
                {
                    "_id": "66caf58fe51ba30092ed6f6f",
                    "name": "Alexandre Alahi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-22T14:12:20.000Z",
            "title": "CODE: Confident Ordinary Differential Editing",
            "summary": "Conditioning image generation facilitates seamless editing and the creation\nof photorealistic images. However, conditioning on noisy or Out-of-Distribution\n(OoD) images poses significant challenges, particularly in balancing fidelity\nto the input and realism of the output. We introduce Confident Ordinary\nDifferential Editing (CODE), a novel approach for image synthesis that\neffectively handles OoD guidance images. Utilizing a diffusion model as a\ngenerative prior, CODE enhances images through score-based updates along the\nprobability-flow Ordinary Differential Equation (ODE) trajectory. This method\nrequires no task-specific training, no handcrafted modules, and no assumptions\nregarding the corruptions affecting the conditioning image. Our method is\ncompatible with any diffusion model. Positioned at the intersection of\nconditional image generation and blind image restoration, CODE operates in a\nfully blind manner, relying solely on a pre-trained generative model. Our\nmethod introduces an alternative approach to blind restoration: instead of\ntargeting a specific ground truth image based on assumptions about the\nunderlying corruption, CODE aims to increase the likelihood of the input image\nwhile maintaining fidelity. This results in the most probable in-distribution\nimage around the input. Our contributions are twofold. First, CODE introduces a\nnovel editing method based on ODE, providing enhanced control, realism, and\nfidelity compared to its SDE-based counterpart. Second, we introduce a\nconfidence interval-based clipping method, which improves CODE's effectiveness\nby allowing it to disregard certain pixels or information, thus enhancing the\nrestoration process in a blind manner. Experimental results demonstrate CODE's\neffectiveness over existing methods, particularly in scenarios involving severe\ndegradation or OoD inputs.",
            "upvotes": 2,
            "discussionId": "66caf595e51ba30092ed7173"
        },
        "publishedAt": "2024-08-26T05:30:41.609Z",
        "title": "CODE: Confident Ordinary Differential Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12418.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648097daf5be39206aeff790/BfDcQkmiCATncP6WuejpA.png",
            "fullname": "Tommaso Martorella",
            "name": "tommymarto",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.12894",
            "authors": [
                {
                    "_id": "66cbf9d8d20991c2a0123ce9",
                    "name": "Yunji Seo",
                    "hidden": false
                },
                {
                    "_id": "66cbf9d8d20991c2a0123cea",
                    "name": "Young Sun Choi",
                    "hidden": false
                },
                {
                    "_id": "66cbf9d8d20991c2a0123ceb",
                    "name": "Hyun Seung Son",
                    "hidden": false
                },
                {
                    "_id": "66cbf9d8d20991c2a0123cec",
                    "name": "Youngjung Uh",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T07:56:25.000Z",
            "title": "FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting\n  for Customizable Rendering",
            "summary": "3D Gaussian Splatting (3DGS) achieves fast and high-quality renderings by\nusing numerous small Gaussians, which leads to significant memory consumption.\nThis reliance on a large number of Gaussians restricts the application of\n3DGS-based models on low-cost devices due to memory limitations. However,\nsimply reducing the number of Gaussians to accommodate devices with less memory\ncapacity leads to inferior quality compared to the quality that can be achieved\non high-end hardware. To address this lack of scalability, we propose\nintegrating a Flexible Level of Detail (FLoD) to 3DGS, to allow a scene to be\nrendered at varying levels of detail according to hardware capabilities. While\nexisting 3DGSs with LoD focus on detailed reconstruction, our method provides\nreconstructions using a small number of Gaussians for reduced memory\nrequirements, and a larger number of Gaussians for greater detail. Experiments\ndemonstrate our various rendering options with tradeoffs between rendering\nquality and memory usage, thereby allowing real-time rendering across different\nmemory constraints. Furthermore, we show that our method generalizes to\ndifferent 3DGS frameworks, indicating its potential for integration into future\nstate-of-the-art developments. Project page:\nhttps://3dgs-flod.github.io/flod.github.io/",
            "upvotes": 2,
            "discussionId": "66cbf9dbd20991c2a0123e3d"
        },
        "publishedAt": "2024-08-26T02:13:39.747Z",
        "title": "FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12894.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.13010",
            "authors": [
                {
                    "_id": "66cc7f5a251afec29b909fa6",
                    "name": "Chamith Mawela",
                    "hidden": false
                },
                {
                    "_id": "66cc7f5a251afec29b909fa7",
                    "name": "Chaouki Ben Issaid",
                    "hidden": false
                },
                {
                    "_id": "66cc7f5a251afec29b909fa8",
                    "name": "Mehdi Bennis",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-23T11:57:02.000Z",
            "title": "A Web-Based Solution for Federated Learning with LLM-Based Automation",
            "summary": "Federated Learning (FL) offers a promising approach for collaborative machine\nlearning across distributed devices. However, its adoption is hindered by the\ncomplexity of building reliable communication architectures and the need for\nexpertise in both machine learning and network programming. This paper presents\na comprehensive solution that simplifies the orchestration of FL tasks while\nintegrating intent-based automation. We develop a user-friendly web application\nsupporting the federated averaging (FedAvg) algorithm, enabling users to\nconfigure parameters through an intuitive interface. The backend solution\nefficiently manages communication between the parameter server and edge nodes.\nWe also implement model compression and scheduling algorithms to optimize FL\nperformance. Furthermore, we explore intent-based automation in FL using a\nfine-tuned Language Model (LLM) trained on a tailored dataset, allowing users\nto conduct FL tasks using high-level prompts. We observe that the LLM-based\nautomated solution achieves comparable test accuracy to the standard web-based\nsolution while reducing transferred bytes by up to 64% and CPU time by up to\n46% for FL tasks. Also, we leverage the neural architecture search (NAS) and\nhyperparameter optimization (HPO) using LLM to improve the performance. We\nobserve that by using this approach test accuracy can be improved by 10-20% for\nthe carried out FL tasks.",
            "upvotes": 1,
            "discussionId": "66cc7f5c251afec29b90a14c"
        },
        "publishedAt": "2024-08-26T11:43:02.942Z",
        "title": "A Web-Based Solution for Federated Learning with LLM-Based Automation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13010.png",
        "numComments": 0,
        "submittedBy": {
            "avatarUrl": "/avatars/1208629f14f010dbc2cd94f3c30f9baf.svg",
            "fullname": "JB D.",
            "name": "IAMJB",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10945",
            "authors": [
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b4",
                    "user": {
                        "_id": "654af6f173416a223f5eacf5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/FZvER_MtUz_BitTVf2QfV.jpeg",
                        "isPro": false,
                        "fullname": "Kazi Hasan Ibn Arif",
                        "user": "hasanar1f",
                        "type": "user"
                    },
                    "name": "Kazi Hasan Ibn Arif",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-08-26T07:15:19.294Z",
                    "hidden": false
                },
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b5",
                    "name": "JinYi Yoon",
                    "hidden": false
                },
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b6",
                    "name": "Dimitrios S. Nikolopoulos",
                    "hidden": false
                },
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b7",
                    "name": "Hans Vandierendonck",
                    "hidden": false
                },
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b8",
                    "name": "Deepu John",
                    "hidden": false
                },
                {
                    "_id": "66c80f9ade9f2c4dad1bd5b9",
                    "name": "Bo Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T15:34:27.000Z",
            "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments",
            "summary": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference.",
            "upvotes": 0,
            "discussionId": "66c80f9ade9f2c4dad1bd602"
        },
        "publishedAt": "2024-08-26T12:27:48.258Z",
        "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10945.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/FZvER_MtUz_BitTVf2QfV.jpeg",
            "fullname": "Kazi Hasan Ibn Arif",
            "name": "hasanar1f",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]