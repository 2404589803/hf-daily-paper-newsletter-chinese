[
    "{'paper': {'id': '2412.18525', 'authors': [{'_id': '67721383d565d51e49e7a90f', 'user': {'_id': '66702b9cd8101e70bd8f70ec', 'avatarUrl': '/avatars/2c20e4083ac314a4a42388b0ec4654e9.svg', 'isPro': False, 'fullname': 'sheny', 'user': 'axxkaya', 'type': 'user'}, 'name': 'Yang Shen', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-30T19:30:01.793Z', 'hidden': False}, {'_id': '67721383d565d51e49e7a910', 'name': 'Xiu-Shen Wei', 'hidden': False}, {'_id': '67721383d565d51e49e7a911', 'name': 'Yifan Sun', 'hidden': False}, {'_id': '67721383d565d51e49e7a912', 'name': 'Yuxin Song', 'hidden': False}, {'_id': '67721383d565d51e49e7a913', 'name': 'Tao Yuan', 'hidden': False}, {'_id': '67721383d565d51e49e7a914', 'name': 'Jian Jin', 'hidden': False}, {'_id': '67721383d565d51e49e7a915', 'name': 'Heyang Xu', 'hidden': False}, {'_id': '67721383d565d51e49e7a916', 'name': 'Yazhou Yao', 'hidden': False}, {'_id': '67721383d565d51e49e7a917', 'name': 'Errui Ding', 'hidden': False}], 'publishedAt': '2024-12-24T16:08:25.000Z', 'title': 'Explanatory Instructions: Towards Unified Vision Tasks Understanding and\\n  Zero-shot Generalization', 'summary': \"Computer Vision (CV) has yet to fully achieve the zero-shot task\\ngeneralization observed in Natural Language Processing (NLP), despite following\\nmany of the milestones established in NLP, such as large transformer models,\\nextensive pre-training, and the auto-regression paradigm, among others. In this\\npaper, we explore the idea that CV adopts discrete and terminological task\\ndefinitions (\\\\eg, ``image segmentation''), which may be a key barrier to\\nzero-shot task generalization. Our hypothesis is that without truly\\nunderstanding previously-seen tasks--due to these terminological\\ndefinitions--deep models struggle to generalize to novel tasks. To verify this,\\nwe introduce Explanatory Instructions, which provide an intuitive way to define\\nCV task objectives through detailed linguistic transformations from input\\nimages to outputs. We create a large-scale dataset comprising 12 million\\n``image input to explanatory instruction to output'' triplets, and train\\nan auto-regressive-based vision-language model (AR-based VLM) that takes both\\nimages and explanatory instructions as input. By learning to follow these\\ninstructions, the AR-based VLM achieves instruction-level zero-shot\\ncapabilities for previously-seen tasks and demonstrates strong zero-shot\\ngeneralization for unseen CV tasks. Code and dataset will be openly available\\non our GitHub repository.\", 'upvotes': 36, 'discussionId': '67721386d565d51e49e7a9b7'}, 'publishedAt': '2024-12-30T23:38:03.262Z', 'title': 'Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18525.png', 'numComments': 1, 'submittedBy': {'_id': '66702b9cd8101e70bd8f70ec', 'avatarUrl': '/avatars/2c20e4083ac314a4a42388b0ec4654e9.svg', 'fullname': 'sheny', 'name': 'axxkaya', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.20070', 'authors': [{'_id': '67735f9f9cc5d33bf6af3cef', 'name': 'Zhenyang Cai', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf0', 'name': 'Junying Chen', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf1', 'name': 'Rongsheng Wang', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf2', 'name': 'Weihong Wang', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf3', 'name': 'Yonglin Deng', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf4', 'name': 'Dingjie Song', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf5', 'name': 'Yize Chen', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf6', 'name': 'Zixu Zhang', 'hidden': False}, {'_id': '67735f9f9cc5d33bf6af3cf7', 'name': 'Benyou Wang', 'hidden': False}], 'publishedAt': '2024-12-28T07:50:00.000Z', 'title': 'On the Compositional Generalization of Multimodal LLMs for Medical\\n  Imaging', 'summary': 'Multimodal large language models (MLLMs) hold significant potential in the\\nmedical field, but their capabilities are often limited by insufficient data in\\ncertain medical domains, highlighting the need for understanding what kinds of\\nimages can be used by MLLMs for generalization. Current research suggests that\\nmulti-task training outperforms single-task as different tasks can benefit each\\nother, but they often overlook the internal relationships within these tasks,\\nproviding limited guidance on selecting datasets to enhance specific tasks. To\\nanalyze this phenomenon, we attempted to employ compositional generalization\\n(CG)-the ability of models to understand novel combinations by recombining\\nlearned elements-as a guiding framework. Since medical images can be precisely\\ndefined by Modality, Anatomical area, and Task, naturally providing an\\nenvironment for exploring CG. Therefore, we assembled 106 medical datasets to\\ncreate Med-MAT for comprehensive experiments. The experiments confirmed that\\nMLLMs can use CG to understand unseen medical images and identified CG as one\\nof the main drivers of the generalization observed in multi-task training.\\nAdditionally, further studies demonstrated that CG effectively supports\\ndatasets with limited data and delivers consistent performance across different\\nbackbones, highlighting its versatility and broad applicability. Med-MAT is\\npublicly available at https://github.com/FreedomIntelligence/Med-MAT.', 'upvotes': 27, 'discussionId': '67735fa09cc5d33bf6af3d85'}, 'publishedAt': '2024-12-30T22:29:50.947Z', 'title': 'On the Compositional Generalization of Multimodal LLMs for Medical Imaging', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20070.png', 'numComments': 2, 'submittedBy': {'_id': '64f1a34f2c5c8b767916447e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg', 'fullname': 'Zhenyang Cai', 'name': 'Eric3200', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.20422', 'authors': [{'_id': '67738c503be2064a656e7e70', 'name': 'Ohad Rahamim', 'hidden': False}, {'_id': '67738c503be2064a656e7e71', 'name': 'Ori Malca', 'hidden': False}, {'_id': '67738c503be2064a656e7e72', 'name': 'Dvir Samuel', 'hidden': False}, {'_id': '67738c503be2064a656e7e73', 'name': 'Gal Chechik', 'hidden': False}], 'publishedAt': '2024-12-29T10:12:01.000Z', 'title': 'Bringing Objects to Life: 4D generation from 3D objects', 'summary': 'Recent advancements in generative modeling now enable the creation of 4D\\ncontent (moving 3D objects) controlled with text prompts. 4D generation has\\nlarge potential in applications like virtual worlds, media, and gaming, but\\nexisting methods provide limited control over the appearance and geometry of\\ngenerated content. In this work, we introduce a method for animating\\nuser-provided 3D objects by conditioning on textual prompts to guide 4D\\ngeneration, enabling custom animations while maintaining the identity of the\\noriginal object. We first convert a 3D mesh into a ``static\" 4D Neural Radiance\\nField (NeRF) that preserves the visual attributes of the input object. Then, we\\nanimate the object using an Image-to-Video diffusion model driven by text. To\\nimprove motion realism, we introduce an incremental viewpoint selection\\nprotocol for sampling perspectives to promote lifelike movement and a masked\\nScore Distillation Sampling (SDS) loss, which leverages attention maps to focus\\noptimization on relevant regions. We evaluate our model in terms of temporal\\ncoherence, prompt adherence, and visual fidelity and find that our method\\noutperforms baselines that are based on other approaches, achieving up to\\nthreefold improvements in identity preservation measured using LPIPS scores,\\nand effectively balancing visual quality with dynamic content.', 'upvotes': 21, 'discussionId': '67738c513be2064a656e7ebd'}, 'publishedAt': '2024-12-31T01:20:42.246Z', 'title': 'Bringing Objects to Life: 4D generation from 3D objects', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63c59c3a6d132b995fedface/ew_rUs_DefvjsYIilOYOY.gif', 'https://cdn-uploads.huggingface.co/production/uploads/63c59c3a6d132b995fedface/34fPGLA17XCixLw3a_1xw.gif', 'https://cdn-uploads.huggingface.co/production/uploads/63c59c3a6d132b995fedface/6zKQea1jeti_MkXzliDxp.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20422.png', 'numComments': 1, 'submittedBy': {'_id': '63c59c3a6d132b995fedface', 'avatarUrl': '/avatars/4e18b19e477cb683ce1ba3ae6ab77d8e.svg', 'fullname': 'Ohad rahamim', 'name': 'ohad204', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.20993', 'authors': [{'_id': '6773560ad86f2a718772598b', 'name': 'Yichao Fu', 'hidden': False}, {'_id': '6773560ad86f2a718772598c', 'name': 'Junda Chen', 'hidden': False}, {'_id': '6773560ad86f2a718772598d', 'name': 'Siqi Zhu', 'hidden': False}, {'_id': '6773560ad86f2a718772598e', 'name': 'Zheyu Fu', 'hidden': False}, {'_id': '6773560ad86f2a718772598f', 'name': 'Zhongdongming Dai', 'hidden': False}, {'_id': '6773560ad86f2a7187725990', 'name': 'Aurick Qiao', 'hidden': False}, {'_id': '6773560ad86f2a7187725991', 'name': 'Hao Zhang', 'hidden': False}], 'publishedAt': '2024-12-30T14:57:53.000Z', 'title': 'Efficiently Serving LLM Reasoning Programs with Certaindex', 'summary': 'The rapid evolution of large language models (LLMs) has unlocked their\\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\\ncode generation, and legal analysis. Central to this progress are\\ninference-time reasoning algorithms, which refine outputs by exploring multiple\\nsolution paths, at the cost of increasing compute demands and response\\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\\nthese algorithms or the varying difficulty of queries, leading to inefficient\\nresource use and unmet latency targets.\\n  We present Dynasor, a system that optimizes inference-time compute for LLM\\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\\nrequests within reasoning queries and uses Certaindex, a proxy that measures\\nstatistical reasoning progress based on model certainty, to guide compute\\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\\nit allocates more compute to hard queries, reduces compute for simpler ones,\\nand terminates unpromising queries early, balancing accuracy, latency, and\\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\\nlatency SLOs in online serving.', 'upvotes': 18, 'discussionId': '6773560bd86f2a71877259f6'}, 'publishedAt': '2024-12-30T23:33:22.541Z', 'title': 'Efficiently Serving LLM Reasoning Programs with Certaindex', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20993.png', 'numComments': 1, 'submittedBy': {'_id': '62ba66296501b0ff15ba1075', 'avatarUrl': '/avatars/de468727cb1240fd4b1f24a19fb237a6.svg', 'fullname': 'Yichao Fu', 'name': 'Viol2000', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21037', 'authors': [{'_id': '67735d5d4a4e0c546461a90d', 'name': 'Chia-Yu Hung', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a90e', 'name': 'Navonil Majumder', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a90f', 'name': 'Zhifeng Kong', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a910', 'name': 'Ambuj Mehrish', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a911', 'name': 'Rafael Valle', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a912', 'name': 'Bryan Catanzaro', 'hidden': False}, {'_id': '67735d5d4a4e0c546461a913', 'name': 'Soujanya Poria', 'hidden': False}], 'publishedAt': '2024-12-30T16:02:44.000Z', 'title': 'TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\\n  Matching and Clap-Ranked Preference Optimization', 'summary': 'We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\\nlies in the difficulty of creating preference pairs, as TTA lacks structured\\nmechanisms like verifiable rewards or gold-standard answers available for Large\\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\\npreference data to enhance TTA alignment. We demonstrate that the audio\\npreference dataset generated using CRPO outperforms existing alternatives. With\\nthis framework, TangoFlux achieves state-of-the-art performance across both\\nobjective and subjective benchmarks. We open source all code and models to\\nsupport further research in TTA generation.', 'upvotes': 12, 'discussionId': '67735d5e4a4e0c546461a951'}, 'publishedAt': '2024-12-30T23:03:45.561Z', 'title': 'TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21037.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5510}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21079', 'authors': [{'_id': '67736096b272a4f186d161f9', 'name': 'Qingyan Bai', 'hidden': False}, {'_id': '67736096b272a4f186d161fa', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '67736096b272a4f186d161fb', 'name': 'Yinghao Xu', 'hidden': False}, {'_id': '67736096b272a4f186d161fc', 'name': 'Qiuyu Wang', 'hidden': False}, {'_id': '67736096b272a4f186d161fd', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '67736096b272a4f186d161fe', 'name': 'Ka Leong Cheng', 'hidden': False}, {'_id': '67736096b272a4f186d161ff', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '67736096b272a4f186d16200', 'name': 'Qifeng Chen', 'hidden': False}], 'publishedAt': '2024-12-30T16:56:44.000Z', 'title': 'Edicho: Consistent Image Editing in the Wild', 'summary': 'As a verified need, consistent editing across in-the-wild images remains a\\ntechnical challenge arising from various unmanageable factors, like object\\nposes, lighting conditions, and photography environments. Edicho steps in with\\na training-free solution based on diffusion models, featuring a fundamental\\ndesign principle of using explicit image correspondence to direct editing.\\nSpecifically, the key components include an attention manipulation module and a\\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\\nwhich take into account the pre-estimated correspondence. Such an\\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\\nediting under diverse settings. We will release the code to facilitate future\\nstudies.', 'upvotes': 12, 'discussionId': '6773609db272a4f186d16447'}, 'publishedAt': '2024-12-30T22:14:42.572Z', 'title': 'Edicho: Consistent Image Editing in the Wild', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21079.png', 'numComments': 1, 'submittedBy': {'_id': '64981bea09cea550852652af', 'avatarUrl': '/avatars/df528e9008972c8e5ae4d278e617476c.svg', 'fullname': 'Qiuyu Wang', 'name': 'qiuyuu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21140', 'authors': [{'_id': '6773c24cb272a4f186ec613c', 'user': {'_id': '652cedbdf120598322ae358a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/652cedbdf120598322ae358a/RrxrP0gtQus4SfNwfyAg_.jpeg', 'isPro': False, 'fullname': 'Mikhail', 'user': 'RefalMachine', 'type': 'user'}, 'name': 'Mikhail Tikhomirov', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-31T10:07:09.658Z', 'hidden': False}, {'_id': '6773c24cb272a4f186ec613d', 'name': 'Daniil Chernyshev', 'hidden': False}], 'publishedAt': '2024-12-30T18:15:45.000Z', 'title': 'Facilitating large language model Russian adaptation with Learned\\n  Embedding Propagation', 'summary': 'Rapid advancements of large language model (LLM) technologies led to the\\nintroduction of powerful open-source instruction-tuned LLMs that have the same\\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\\nWhile the emergence of such models accelerates the adoption of LLM technologies\\nin sensitive-information environments the authors of such models don not\\ndisclose the training data necessary for replication of the results thus making\\nthe achievements model-exclusive. Since those open-source models are also\\nmultilingual this in turn reduces the benefits of training a language specific\\nLLMs as improved inference computation efficiency becomes the only guaranteed\\nadvantage of such costly procedure. More cost-efficient options such as\\nvocabulary extension and subsequent continued pre-training are also inhibited\\nby the lack of access to high-quality instruction-tuning data since it is the\\nmajor factor behind the resulting LLM task-solving capabilities. To address the\\nlimitations and cut the costs of the language adaptation pipeline we propose\\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\\nlower training data size requirements due to minimal impact on existing LLM\\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\\nthat allows to skip the instruction-tuning step and instead implant the new\\nlanguage knowledge directly into any existing instruct-tuned variant. We\\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\\nshowing that LEP is competitive with traditional instruction-tuning methods,\\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\\nfurther improvements via self-calibration and continued tuning enhancing\\ntask-solving capabilities.', 'upvotes': 5, 'discussionId': '6773c24db272a4f186ec6190'}, 'publishedAt': '2024-12-31T05:22:56.879Z', 'title': 'Facilitating large language model Russian adaptation with Learned Embedding Propagation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21140.png', 'numComments': 1, 'submittedBy': {'_id': '652cedbdf120598322ae358a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/652cedbdf120598322ae358a/RrxrP0gtQus4SfNwfyAg_.jpeg', 'fullname': 'Mikhail', 'name': 'RefalMachine', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 31}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.21199', 'authors': [{'_id': '67738e22b8df35b9c86ace86', 'name': 'Zhaojian Yu', 'hidden': False}, {'_id': '67738e22b8df35b9c86ace87', 'name': 'Yilun Zhao', 'hidden': False}, {'_id': '67738e22b8df35b9c86ace88', 'name': 'Arman Cohan', 'hidden': False}, {'_id': '67738e22b8df35b9c86ace89', 'name': 'Xiao-Ping Zhang', 'hidden': False}], 'publishedAt': '2024-12-30T18:58:58.000Z', 'title': 'HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\\n  Self-invoking Code Generation', 'summary': \"We introduce self-invoking code generation, a new task designed to evaluate\\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\\ntask, models are presented with a base problem and a related, more complex\\nproblem. They must solve the base problem and then utilize its solution to\\naddress the more complex one. This work features three key contributions.\\nFirst, we propose a general recipe for generating more challenging versions of\\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\\nself-invoking code generation. Second, from the analysis of experimental\\nresults over twenty LLMs on our benchmarks, we have two important observations:\\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\\nand MBPP, but their performance declines on self-invoking tasks. For example,\\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\\n(ii) On self-invoking code generation task, the instruction-tuned models\\ndemonstrate only marginal improvements compared to the base models. Third, we\\ndisclose the types of failure modes that exist in our evaluation results. All\\nthese results underscore the need for further advancements in self-invoking\\ncode generation tasks and provide a new direction for future research on\\nenhancing LLMs' code reasoning capabilities.\", 'upvotes': 5, 'discussionId': '67738e24b8df35b9c86aced8'}, 'publishedAt': '2024-12-31T01:26:05.226Z', 'title': 'HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21199.png', 'numComments': 2, 'submittedBy': {'_id': '646f3443c261dc413383b8a4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/KTd5QS8HjUewP8P02inQj.png', 'fullname': 'Zhaojian Yu', 'name': 'zjy2001', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21139', 'authors': [{'_id': '67735961702f3c89046839f4', 'user': {'_id': '61568f37272f2d87a99ba884', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61568f37272f2d87a99ba884/lgvkl5f0rEyiQRVU5FE32.png', 'isPro': False, 'fullname': 'Jiayi Pan', 'user': 'Jiayi-Pan', 'type': 'user'}, 'name': 'Jiayi Pan', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-31T05:07:53.061Z', 'hidden': False}, {'_id': '67735961702f3c89046839f5', 'name': 'Xingyao Wang', 'hidden': False}, {'_id': '67735961702f3c89046839f6', 'name': 'Graham Neubig', 'hidden': False}, {'_id': '67735961702f3c89046839f7', 'name': 'Navdeep Jaitly', 'hidden': False}, {'_id': '67735961702f3c89046839f8', 'name': 'Heng Ji', 'hidden': False}, {'_id': '67735961702f3c89046839f9', 'user': {'_id': '6611e6e1188ff298b0dd0b79', 'avatarUrl': '/avatars/3a495283955ec9e06e1829c7eb2cd9a4.svg', 'isPro': False, 'fullname': 'Alane Suhr', 'user': 'alsuhr', 'type': 'user'}, 'name': 'Alane Suhr', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-31T02:39:30.019Z', 'hidden': False}, {'_id': '67735961702f3c89046839fa', 'name': 'Yizhe Zhang', 'hidden': False}], 'publishedAt': '2024-12-30T18:15:39.000Z', 'title': 'Training Software Engineering Agents and Verifiers with SWE-Gym', 'summary': 'We present SWE-Gym, the first environment for training real-world software\\nengineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task\\ninstances, each comprising a codebase with an executable runtime environment,\\nunit tests, and a task specified in natural language. We use SWE-Gym to train\\nlanguage model based SWE agents , achieving up to 19% absolute gains in resolve\\nrate on the popular SWE-Bench Verified and Lite test sets. We also experiment\\nwith inference-time scaling through verifiers trained on agent trajectories\\nsampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve\\n32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new\\nstate-of-the-art for open-weight SWE agents. To facilitate further research, we\\npublicly release SWE-Gym, models, and agent trajectories.', 'upvotes': 4, 'discussionId': '67735962702f3c8904683a22'}, 'publishedAt': '2024-12-30T23:56:15.734Z', 'title': 'Training Software Engineering Agents and Verifiers with SWE-Gym', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/61568f37272f2d87a99ba884/lscibgXrTDtXtAzDwsbCF.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21139.png', 'numComments': 1, 'submittedBy': {'_id': '61568f37272f2d87a99ba884', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61568f37272f2d87a99ba884/lgvkl5f0rEyiQRVU5FE32.png', 'fullname': 'Jiayi Pan', 'name': 'Jiayi-Pan', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.20005', 'authors': [{'_id': '677368fbe182b937d5860758', 'name': 'Yujie Luo', 'hidden': False}, {'_id': '677368fbe182b937d5860759', 'name': 'Xiangyuan Ru', 'hidden': False}, {'_id': '677368fbe182b937d586075a', 'name': 'Kangwei Liu', 'hidden': False}, {'_id': '677368fbe182b937d586075b', 'name': 'Lin Yuan', 'hidden': False}, {'_id': '677368fbe182b937d586075c', 'name': 'Mengshu Sun', 'hidden': False}, {'_id': '677368fbe182b937d586075d', 'name': 'Ningyu Zhang', 'hidden': False}, {'_id': '677368fbe182b937d586075e', 'name': 'Lei Liang', 'hidden': False}, {'_id': '677368fbe182b937d586075f', 'name': 'Zhiqiang Zhang', 'hidden': False}, {'_id': '677368fbe182b937d5860760', 'name': 'Jun Zhou', 'hidden': False}, {'_id': '677368fbe182b937d5860761', 'name': 'Lanning Wei', 'hidden': False}, {'_id': '677368fbe182b937d5860762', 'name': 'Da Zheng', 'hidden': False}, {'_id': '677368fbe182b937d5860763', 'name': 'Haofen Wang', 'hidden': False}, {'_id': '677368fbe182b937d5860764', 'name': 'Huajun Chen', 'hidden': False}], 'publishedAt': '2024-12-28T04:01:30.000Z', 'title': 'OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\\n  System', 'summary': \"We introduce OneKE, a dockerized schema-guided knowledge extraction system,\\nwhich can extract knowledge from the Web and raw PDF Books, and support various\\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\\nagents and a configure knowledge base. Different agents perform their\\nrespective roles, enabling support for various extraction scenarios. The\\nconfigure knowledge base facilitates schema configuration, error case debugging\\nand correction, further improving the performance. Empirical evaluations on\\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\\nelucidate its adaptability to diverse tasks across multiple domains,\\nhighlighting its potential for broad applications. We have open-sourced the\\nCode at https://github.com/zjunlp/OneKE and released a Video at\\nhttp://oneke.openkg.cn/demo.mp4.\", 'upvotes': 4, 'discussionId': '677368fce182b937d58607c6'}, 'publishedAt': '2024-12-30T22:51:09.824Z', 'title': 'OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1spJ3Z9MTeBY4AMTJUTX4.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20005.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21206', 'authors': [{'_id': '67735ba473932c3aa94fe0ac', 'name': 'Hyunsoo Cha', 'hidden': False}, {'_id': '67735ba473932c3aa94fe0ad', 'name': 'Inhee Lee', 'hidden': False}, {'_id': '67735ba473932c3aa94fe0ae', 'name': 'Hanbyul Joo', 'hidden': False}], 'publishedAt': '2024-12-30T18:59:58.000Z', 'title': 'PERSE: Personalized 3D Generative Avatars from A Single Portrait', 'summary': \"We present PERSE, a method for building an animatable personalized generative\\navatar from a reference portrait. Our avatar model enables facial attribute\\nediting in a continuous and disentangled latent space to control each facial\\nattribute, while preserving the individual's identity. To achieve this, our\\nmethod begins by synthesizing large-scale synthetic 2D video datasets, where\\neach video contains consistent changes in the facial expression and viewpoint,\\ncombined with a variation in a specific facial attribute from the original\\ninput. We propose a novel pipeline to produce high-quality, photorealistic 2D\\nvideos with facial attribute editing. Leveraging this synthetic attribute\\ndataset, we present a personalized avatar creation method based on the 3D\\nGaussian Splatting, learning a continuous and disentangled latent space for\\nintuitive facial attribute manipulation. To enforce smooth transitions in this\\nlatent space, we introduce a latent space regularization technique by using\\ninterpolated 2D faces as supervision. Compared to previous approaches, we\\ndemonstrate that PERSE generates high-quality avatars with interpolated\\nattributes while preserving identity of reference person.\", 'upvotes': 3, 'discussionId': '67735ba873932c3aa94fe15b'}, 'publishedAt': '2024-12-31T08:39:13.913Z', 'title': 'PERSE: Personalized 3D Generative Avatars from A Single Portrait', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62414e2b585605a4079c2f38/fvMRw70bYO-xBRQTYBFOO.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21206.png', 'numComments': 1, 'submittedBy': {'_id': '62414e2b585605a4079c2f38', 'avatarUrl': '/avatars/db1dc5dd2164b7ecbd789104329296bd.svg', 'fullname': 'Hyunsoo Cha', 'name': 'HyunsooCha', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.20631', 'authors': [{'_id': '6773f578fb77c86d80d4c9fc', 'name': 'Haoran Wei', 'hidden': False}, {'_id': '6773f578fb77c86d80d4c9fd', 'name': 'Youyang Yin', 'hidden': False}, {'_id': '6773f578fb77c86d80d4c9fe', 'name': 'Yumeng Li', 'hidden': False}, {'_id': '6773f578fb77c86d80d4c9ff', 'name': 'Jia Wang', 'hidden': False}, {'_id': '6773f578fb77c86d80d4ca00', 'name': 'Liang Zhao', 'hidden': False}, {'_id': '6773f578fb77c86d80d4ca01', 'name': 'Jianjian Sun', 'hidden': False}, {'_id': '6773f578fb77c86d80d4ca02', 'name': 'Zheng Ge', 'hidden': False}, {'_id': '6773f578fb77c86d80d4ca03', 'name': 'Xiangyu Zhang', 'hidden': False}], 'publishedAt': '2024-12-30T00:40:35.000Z', 'title': \"Slow Perception: Let's Perceive Geometric Figures Step-by-step\", 'summary': 'Recently, \"visual o1\" began to enter people\\'s vision, with expectations that\\nthis slow-thinking design can solve visual reasoning tasks, especially\\ngeometric math problems. However, the reality is that current LVLMs (Large\\nVision Language Models) can hardly even accurately copy a geometric figure, let\\nalone truly understand the complex inherent logic and spatial relationships\\nwithin geometric shapes. We believe accurate copying (strong perception) is the\\nfirst step to visual o1. Accordingly, we introduce the concept of \"slow\\nperception\" (SP), which guides the model to gradually perceive basic point-line\\ncombinations, as our humans, reconstruct complex geometric structures\\nprogressively. There are two-fold stages in SP: a) perception decomposition.\\nPerception is not instantaneous. In this stage, complex geometric figures are\\nbroken down into basic simple units to unify geometry representation. b)\\nperception flow, which acknowledges that accurately tracing a line is not an\\neasy task. This stage aims to avoid \"long visual jumps\" in regressing line\\nsegments by using a proposed \"perceptual ruler\" to trace each line\\nstroke-by-stroke. Surprisingly, such a human-like perception manner enjoys an\\ninference time scaling law -- the slower, the better. Researchers strive to\\nspeed up the model\\'s perception in the past, but we slow it down again,\\nallowing the model to read the image step-by-step and carefully.', 'upvotes': 2, 'discussionId': '6773f579fb77c86d80d4ca4d'}, 'publishedAt': '2024-12-31T08:45:57.560Z', 'title': \"Slow Perception: Let's Perceive Geometric Figures Step-by-step\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20631.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5510}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21187', 'authors': [{'_id': '6773f75a23a7829936cb36bd', 'name': 'Xingyu Chen', 'hidden': False}, {'_id': '6773f75a23a7829936cb36be', 'name': 'Jiahao Xu', 'hidden': False}, {'_id': '6773f75a23a7829936cb36bf', 'name': 'Tian Liang', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c0', 'name': 'Zhiwei He', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c1', 'name': 'Jianhui Pang', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c2', 'name': 'Dian Yu', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c3', 'name': 'Linfeng Song', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c4', 'name': 'Qiuzhi Liu', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c5', 'name': 'Mengfei Zhou', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c6', 'name': 'Zhuosheng Zhang', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c7', 'name': 'Rui Wang', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c8', 'name': 'Zhaopeng Tu', 'hidden': False}, {'_id': '6773f75a23a7829936cb36c9', 'name': 'Haitao Mi', 'hidden': False}, {'_id': '6773f75a23a7829936cb36ca', 'name': 'Dong Yu', 'hidden': False}], 'publishedAt': '2024-12-30T18:55:12.000Z', 'title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'summary': 'The remarkable performance of models like the OpenAI o1 can be attributed to\\ntheir ability to emulate human-like long-time thinking during inference. These\\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\\nstrategies to enhance problem-solving capabilities. However, a critical\\nquestion remains: How to intelligently and efficiently scale computational\\nresources during testing. This paper presents the first comprehensive study on\\nthe prevalent issue of overthinking in these models, where excessive\\ncomputational resources are allocated for simple problems with minimal benefit.\\nWe introduce novel efficiency metrics from both outcome and process\\nperspectives to evaluate the rational use of computational resources by o1-like\\nmodels. Using a self-training paradigm, we propose strategies to mitigate\\noverthinking, streamlining reasoning processes without compromising accuracy.\\nExperimental results show that our approach successfully reduces computational\\noverhead while preserving model performance across a range of testsets with\\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.', 'upvotes': 1, 'discussionId': '6773f75b23a7829936cb3729'}, 'publishedAt': '2024-12-31T08:54:02.196Z', 'title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21187.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5510}, 'isAuthorParticipating': False}"
]