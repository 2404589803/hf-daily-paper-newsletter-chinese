[
  {
    "paper": {
      "id": "2503.21776",
      "authors": [
        {
          "_id": "67e6090248742d6df75853ae",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853af",
          "name": "Kaixiong Gong",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b0",
          "name": "Bohao Li",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b1",
          "name": "Zonghao Guo",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b2",
          "name": "Yibing Wang",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b3",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b4",
          "name": "Benyou Wang",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b5",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T01:02:30.945Z",
      "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
      "upvotes": 40,
      "discussionId": "67e6090348742d6df75853de",
      "ai_keywords": [
        "rule-based reinforcement learning (RL)",
        "GRPO algorithm",
        "temporal modeling",
        "Video-R1-COT-165k",
        "Video-R1-260k",
        "SFT cold start",
        "VideoMMMU",
        "VSI-Bench",
        "MVBench",
        "TempCompass",
        "video spatial reasoning",
        "GPT-4o",
        "T-GRPO algorithm"
      ]
    },
    "publishedAt": "2025-03-27T13:59:51.000Z",
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21776.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21380",
      "authors": [
        {
          "_id": "67e5f4ad147ee85622ad0df1",
          "name": "Haoxiang Sun",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df2",
          "name": "Yingqian Min",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df3",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df4",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df5",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df6",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df7",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df8",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T11:20:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:15:16.769Z",
      "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "648e6a4567aa8ab0e0e4c30f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
        "isPro": false,
        "fullname": "Beichen Zhang",
        "user": "ToheartZhang",
        "type": "user"
      },
      "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
      "upvotes": 24,
      "discussionId": "67e5f4ae147ee85622ad0e27",
      "ai_keywords": [
        "DeepSeek-R1",
        "o3-mini"
      ]
    },
    "publishedAt": "2025-03-27T07:20:17.000Z",
    "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
    "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21380.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648e6a4567aa8ab0e0e4c30f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
      "fullname": "Beichen Zhang",
      "name": "ToheartZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21620",
      "authors": [
        {
          "_id": "67e606fb6c44ab0376a498a1",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a2",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a3",
          "name": "Yaxuan Guo",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a5",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a6",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a7",
          "name": "Guanjing Xiong",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a8",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T15:39:30.000Z",
      "submittedOnDailyAt": "2025-03-28T00:48:51.950Z",
      "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
      "upvotes": 22,
      "discussionId": "67e606fe6c44ab0376a49962",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "multimodal large language models (MLLMs)",
        "graphical user interface (GUI) action prediction tasks",
        "rule-based action reward",
        "Group Relative Policy Optimization (GRPO)",
        "in-domain (ID) tasks",
        "out-of-domain (OOD) tasks",
        "action type accuracy",
        "grounding accuracy",
        "supervised fine-tuning (SFT)",
        "GUI grounding benchmark ScreenSpot-Pro",
        "OS-Atlas-7B"
      ]
    },
    "publishedAt": "2025-03-27T11:39:30.000Z",
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21620.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21755",
      "authors": [
        {
          "_id": "67e60823284844fd3014f62b",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62c",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62d",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62e",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62f",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f630",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f631",
          "name": "Yuanhan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f632",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f633",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f634",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f635",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:53:44.602Z",
      "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
      "submittedOnDailyBy": {
        "_id": "60efe7fa0d920bc7805cada5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
        "isPro": false,
        "fullname": "Ziqi Huang",
        "user": "Ziqi",
        "type": "user"
      },
      "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
      "upvotes": 21,
      "discussionId": "67e60824284844fd3014f68e",
      "ai_keywords": [
        "VBench",
        "VBench-2.0",
        "visual generation",
        "per-frame aesthetics",
        "temporal consistency",
        "prompt adherence",
        "intrinsic faithfulness",
        "physical laws",
        "commonsense reasoning",
        "anatomical correctness",
        "compositional integrity",
        "AI-assisted filmmaking",
        "simulated world modeling",
        "VLMs",
        "LLMs",
        "anomaly detection"
      ]
    },
    "publishedAt": "2025-03-27T13:57:01.000Z",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
    "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60efe7fa0d920bc7805cada5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
      "fullname": "Ziqi Huang",
      "name": "Ziqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21749",
      "authors": [
        {
          "_id": "67e6041d9a97e46f3102f7cc",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cd",
          "name": "Qilong Wu",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7ce",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cf",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d1",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d2",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d3",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d4",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d5",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d6",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d7",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d8",
          "name": "Zhen Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:56:15.000Z",
      "submittedOnDailyAt": "2025-03-28T00:54:19.590Z",
      "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.",
      "upvotes": 14,
      "discussionId": "67e6041f9a97e46f3102f89b",
      "projectPage": "https://zhaoshitian.github.io/lexart/",
      "ai_keywords": [
        "Deepseek-R1",
        "LeX-Enhancer",
        "LeX-FLUX",
        "LeX-Lumina",
        "LeX-Bench",
        "Pairwise Normalized Edit Distance (PNED)"
      ]
    },
    "publishedAt": "2025-03-27T13:56:15.000Z",
    "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis",
    "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21749.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21758",
      "authors": [
        {
          "_id": "67e6092a40fb111ac9342c39",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3a",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3b",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3c",
          "name": "Ruoyi Du",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3d",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3e",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3f",
          "name": "Yiting Lu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c40",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c41",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c42",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c43",
          "name": "Xiangyang Zhu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c44",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c45",
          "name": "Will Beddow",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c46",
          "name": "Erwann Millon",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c47",
          "name": "Victor Perez",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c48",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c49",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4a",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4b",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4c",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4d",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4e",
          "name": "Chang Xu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4f",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:07.000Z",
      "submittedOnDailyAt": "2025-03-28T00:59:52.114Z",
      "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
      "submittedOnDailyBy": {
        "_id": "6285a9133ab6642179158944",
        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
        "isPro": false,
        "fullname": "Zhen Li",
        "user": "Paper99",
        "type": "user"
      },
      "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.",
      "upvotes": 11,
      "discussionId": "67e6092e40fb111ac9342d4e",
      "ai_keywords": [
        "Unified Next-DiT",
        "text and image tokens",
        "cross-modal interactions",
        "Unified Captioner (UniCap)",
        "text-to-image (T2I)",
        "multi-stage progressive training",
        "inference acceleration techniques",
        "academic benchmarks",
        "public text-to-image arenas"
      ]
    },
    "publishedAt": "2025-03-27T13:57:07.000Z",
    "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
    "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6285a9133ab6642179158944",
      "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
      "fullname": "Zhen Li",
      "name": "Paper99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21729",
      "authors": [
        {
          "_id": "67e610ee6e73232cf0903b73",
          "name": "Zhicheng Lee",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b74",
          "name": "Shulin Cao",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b75",
          "name": "Jinxin Liu",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b76",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b77",
          "name": "Weichuan Liu",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b78",
          "name": "Xiaoyin Che",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b79",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b7a",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
      ],
      "publishedAt": "2025-03-27T17:44:18.000Z",
      "submittedOnDailyAt": "2025-03-28T01:51:24.016Z",
      "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation",
      "submittedOnDailyBy": {
        "_id": "652542861e9db26e407aa1fc",
        "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
        "isPro": false,
        "fullname": "Lee Zhicheng",
        "user": "ZhiCheng0326",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
      "upvotes": 11,
      "discussionId": "67e610ee6e73232cf0903ba6",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "parametric knowledge",
        "retrieval capabilities",
        "overthinking",
        "robustness",
        "question answering (QA)",
        "ReaRAG",
        "factuality-enhanced reasoning model",
        "reasoning chain length",
        "LRM",
        "deliberate thinking",
        "predefined action space",
        "Search",
        "Finish",
        "RAG engine",
        "observation",
        "multi-hop QA",
        "reflective ability",
        "reasoning trajectory",
        "Retrieval-Augmented Generation (RAG)"
      ]
    },
    "publishedAt": "2025-03-27T13:44:18.000Z",
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652542861e9db26e407aa1fc",
      "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
      "fullname": "Lee Zhicheng",
      "name": "ZhiCheng0326",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21460",
      "authors": [
        {
          "_id": "67e609ee389245233f0d316f",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3170",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3171",
          "name": "Ye Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3172",
          "name": "Yusheng Zhao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3173",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3174",
          "name": "Yiyang Gu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3175",
          "name": "Bohan Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3176",
          "name": "Binqi Chen",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3177",
          "name": "Ziyue Qiao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3178",
          "name": "Qingqing Long",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3179",
          "name": "Rongcheng Tu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317a",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317b",
          "name": "Wei Ju",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317c",
          "name": "Zhiping Xiao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317d",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317e",
          "name": "Meng Xiao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317f",
          "name": "Chenwu Liu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3180",
          "name": "Jingyang Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3181",
          "name": "Shichang Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3182",
          "name": "Yiqiao Jin",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3183",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3184",
          "name": "Xian Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3185",
          "name": "Hanqing Zhao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3186",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3187",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3188",
          "name": "Ming Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
      ],
      "publishedAt": "2025-03-27T12:50:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:08:58.527Z",
      "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
      "upvotes": 9,
      "discussionId": "67e609ef389245233f0d31c0",
      "githubRepo": "https://github.com/luo-junyu/Awesome-Agent-Papers",
      "ai_keywords": [
        "Large Language Model (LLM) agents",
        "goal-driven behaviors",
        "dynamic adaptation capabilities",
        "artificial general intelligence",
        "methodology-centered taxonomy",
        "architectural foundations",
        "collaboration mechanisms",
        "evolutionary pathways",
        "agent design principles",
        "emergent behaviors",
        "complex environments",
        "unified architectural perspective",
        "evaluation methodologies",
        "practical challenges",
        "diverse application domains"
      ]
    },
    "publishedAt": "2025-03-27T08:50:17.000Z",
    "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
    "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21248",
      "authors": [
        {
          "_id": "67e63581e8a16f38741f4baa",
          "name": "Yujie Liu",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bab",
          "name": "Zonglin Yang",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bac",
          "name": "Tong Xie",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bad",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bae",
          "name": "Ben Gao",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4baf",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb0",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb1",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb2",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb3",
          "name": "Dongzhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T08:09:15.000Z",
      "submittedOnDailyAt": "2025-03-28T04:07:23.222Z",
      "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
      "upvotes": 8,
      "discussionId": "67e63582e8a16f38741f4c16",
      "ai_keywords": [
        "large language models (LLMs)",
        "scientific discovery",
        "inspiration retrieval",
        "hypothesis composition",
        "hypothesis ranking",
        "automated framework",
        "research questions",
        "background surveys",
        "inspirations",
        "hypotheses",
        "out-of-distribution task",
        "automated scientific discovery",
        "innovative hypotheses"
      ]
    },
    "publishedAt": "2025-03-27T04:09:15.000Z",
    "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
    "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21248.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21774",
      "authors": [
        {
          "_id": "67e6094c48742d6df7586714",
          "name": "Jianning Pei",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586715",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586716",
          "name": "Shuyang Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:46.000Z",
      "submittedOnDailyAt": "2025-03-28T00:58:59.244Z",
      "title": "Optimal Stepsize for Diffusion Sampling",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
      "upvotes": 7,
      "discussionId": "67e6095248742d6df75868db",
      "ai_keywords": [
        "diffusion models",
        "generation quality",
        "computational intensive sampling",
        "step discretization",
        "denoising directions",
        "Optimal Stepsize Distillation",
        "dynamic programming framework",
        "theoretical optimal schedules",
        "knowledge distillation",
        "reference trajectories",
        "stepsize optimization",
        "recursive error minimization",
        "global discretization bounds",
        "optimal substructure exploitation",
        "robustness",
        "ODE solvers",
        "noise schedules",
        "text-to-image generation",
        "GenEval"
      ]
    },
    "publishedAt": "2025-03-27T13:59:46.000Z",
    "title": "Optimal Stepsize for Diffusion Sampling",
    "summary": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21765",
      "authors": [
        {
          "_id": "67e60da88decdc7da4bf69a9",
          "name": "Minghui Lin",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69aa",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ab",
          "name": "Yishan Wang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ac",
          "user": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "isPro": false,
            "fullname": "Siteng Huang",
            "user": "huangsiteng",
            "type": "user"
          },
          "name": "Shu Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:47:05.703Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ad",
          "name": "Fengqi Dai",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ae",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69af",
          "name": "Cunxiang Wang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b0",
          "name": "Zhengrong Zuo",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b1",
          "name": "Nong Sang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b2",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b3",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:58:33.000Z",
      "submittedOnDailyAt": "2025-03-28T01:19:58.442Z",
      "title": "Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.",
      "upvotes": 7,
      "discussionId": "67e60da98decdc7da4bf6a28",
      "githubRepo": "https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation",
      "ai_keywords": [
        "diffusion models",
        "physical cognition",
        "motion representations",
        "generative systems",
        "real-world dynamic scenarios",
        "cognitive science",
        "schema perception",
        "passive cognition",
        "active cognition",
        "state-of-the-art methods",
        "classical paradigms",
        "benchmarks",
        "key challenges",
        "interpretable",
        "controllable",
        "physically consistent",
        "human-like physical comprehension"
      ]
    },
    "publishedAt": "2025-03-27T13:58:33.000Z",
    "title": "Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey",
    "summary": "Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21696",
      "authors": [
        {
          "_id": "67e64ff17fe72aad5c26ab2f",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab30",
          "name": "Mengna Wang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab31",
          "name": "Gangao Liu",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab32",
          "name": "Xu Huixin",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab33",
          "name": "Yiwei Jiang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab34",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab35",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab36",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab37",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab38",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab39",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3b",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
      ],
      "publishedAt": "2025-03-27T17:00:51.000Z",
      "submittedOnDailyAt": "2025-03-28T06:01:25.299Z",
      "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
      "submittedOnDailyBy": {
        "_id": "6485bd278d14bcd5cdbb7c8d",
        "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
        "isPro": false,
        "fullname": "Wenqi Zhang",
        "user": "zwq2018",
        "type": "user"
      },
      "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.",
      "upvotes": 7,
      "discussionId": "67e64ff37fe72aad5c26ac06",
      "ai_keywords": [
        "Embodied Reasoner",
        "Observation-Thought-Action trajectories",
        "imitation learning",
        "rejection sampling",
        "reflection tuning",
        "visual reasoning models",
        "long-horizon tasks"
      ]
    },
    "publishedAt": "2025-03-27T13:00:51.000Z",
    "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
    "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21696.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485bd278d14bcd5cdbb7c8d",
      "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
      "fullname": "Wenqi Zhang",
      "name": "zwq2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21144",
      "authors": [
        {
          "_id": "67e60e315f20b94fcd0d1f9b",
          "name": "Jinwei Qi",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9c",
          "name": "Chaonan Ji",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9d",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9e",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9f",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1fa0",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T04:18:53.000Z",
      "submittedOnDailyAt": "2025-03-28T01:20:25.562Z",
      "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.",
      "upvotes": 5,
      "discussionId": "67e60e325f20b94fcd0d1fff",
      "ai_keywords": [
        "hierarchical motion diffusion models",
        "explicit and implicit motion representations",
        "stylistic control",
        "synchronization between head and body movements",
        "face refinement",
        "continous generation",
        "upper-body portrait video",
        "interactive video-chat"
      ]
    },
    "publishedAt": "2025-03-27T00:18:53.000Z",
    "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model",
    "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21088",
      "authors": [
        {
          "_id": "67e602b6dabfd9d4bbf10849",
          "user": {
            "_id": "66f4bdbdc51768d9d4498d16",
            "avatarUrl": "/avatars/0f6ded5fd9cf4e6f0b180b5aa329ea33.svg",
            "isPro": false,
            "fullname": "Haoming Xu",
            "user": "HaomingXu",
            "type": "user"
          },
          "name": "Haoming Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:00:25.958Z",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084a",
          "name": "Shuxun Wang",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084b",
          "name": "Yanqiu Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084c",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084d",
          "name": "Ziyan Jiang",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084e",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084f",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10850",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10851",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T02:03:25.000Z",
      "submittedOnDailyAt": "2025-03-28T00:32:00.822Z",
      "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
      "upvotes": 4,
      "discussionId": "67e602badabfd9d4bbf10973",
      "githubRepo": "https://github.com/zjunlp/unlearn",
      "ai_keywords": [
        "Model Merging",
        "TIES-Merging",
        "large language models",
        "unlearning",
        "over-forgetting",
        "under-forgetting",
        "unlearned model",
        "performance trajectories",
        "loss dynamics",
        "weight perspectives",
        "MIA scores",
        "ROUGE-based metrics"
      ]
    },
    "publishedAt": "2025-03-26T22:03:25.000Z",
    "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
    "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20776",
      "authors": [
        {
          "_id": "67e6019af54a34f8a989d8eb",
          "name": "Shijie Zhou",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ec",
          "name": "Hui Ren",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ed",
          "name": "Yijia Weng",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ee",
          "name": "Shuwang Zhang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ef",
          "name": "Zhen Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f0",
          "name": "Dejia Xu",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f1",
          "name": "Zhiwen Fan",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f2",
          "name": "Suya You",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f3",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f4",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f5",
          "name": "Achuta Kadambi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:56:16.000Z",
      "submittedOnDailyAt": "2025-03-28T00:26:28.356Z",
      "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
      "upvotes": 4,
      "discussionId": "67e6019ff54a34f8a989d9d6",
      "ai_keywords": [
        "Feature4X",
        "4D feature field distillation",
        "Gaussian Splatting",
        "SAM2",
        "InternVideo2",
        "novel view segment anything",
        "geometric and appearance scene editing",
        "free-form VQA",
        "LLMs",
        "agentic AI applications",
        "scalable systems",
        "contextually aware",
        "spatiotemporally aware",
        "immersive dynamic 4D scene interaction"
      ]
    },
    "publishedAt": "2025-03-26T13:56:16.000Z",
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
    "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20822",
      "authors": [
        {
          "_id": "67e61017b116b3c559188a0f",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a10",
          "name": "Xingyu Ni",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a11",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a12",
          "name": "Feng Cheng",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a13",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a14",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a15",
          "name": "Bohan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T00:45:07.000Z",
      "submittedOnDailyAt": "2025-03-28T01:28:16.803Z",
      "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
      "upvotes": 3,
      "discussionId": "67e6101bb116b3c559188b67",
      "ai_keywords": [
        "video generation models",
        "synthetic videos",
        "computer graphics pipelines",
        "3D consistency",
        "transfering physical realism",
        "unwanted artifacts",
        "physical consistency",
        "physical fidelity",
        "video synthesis"
      ]
    },
    "publishedAt": "2025-03-25T20:45:07.000Z",
    "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "summary": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20822.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20853",
      "authors": [
        {
          "_id": "67e5ff24cce3913200f29387",
          "name": "Alexander Swerdlow",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29388",
          "name": "Mihir Prabhudesai",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29389",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938a",
          "name": "Deepak Pathak",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938b",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T00:17:26.527Z",
      "title": "Unified Multimodal Discrete Diffusion",
      "submittedOnDailyBy": {
        "_id": "6310ff7dd43c55e811f8772f",
        "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
        "isPro": false,
        "fullname": "Mihir Prabhudesai",
        "user": "mihirpd",
        "type": "user"
      },
      "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
      "upvotes": 2,
      "discussionId": "67e5ff28cce3913200f2951e",
      "projectPage": "https://unidisc.github.io/",
      "githubRepo": "https://github.com/alexanderswerdlow/unidisc",
      "ai_keywords": [
        "autoregressive (AR) approaches",
        "discrete diffusion models",
        "multimodal inpainting",
        "control over quality versus diversity",
        "Unity Multimodal Discrete Diffusion (UniDisc)",
        "scaling analysis",
        "generation quality",
        "inference-time compute",
        "controllability",
        "editability"
      ]
    },
    "publishedAt": "2025-03-26T13:59:51.000Z",
    "title": "Unified Multimodal Discrete Diffusion",
    "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff7dd43c55e811f8772f",
      "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
      "fullname": "Mihir Prabhudesai",
      "name": "mihirpd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20990",
      "authors": [
        {
          "_id": "67e626a9cb305c5a3ee11fac",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fad",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fae",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11faf",
          "name": "Shashidhar Reddy Javaji",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb0",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb1",
          "name": "Jimin Huang",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb2",
          "name": "Zining Zhu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb3",
          "name": "Qianqian Xie",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb4",
          "name": "Xiao-yang Liu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb5",
          "name": "Koduvayur Subbalakshmi",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb6",
          "name": "Meikang Qiu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb7",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb8",
          "name": "Jian-Yun Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T21:07:51.000Z",
      "submittedOnDailyAt": "2025-03-28T03:07:27.038Z",
      "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications",
      "submittedOnDailyBy": {
        "_id": "63a0c0803c8841cfe2cd1f15",
        "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
        "isPro": false,
        "fullname": "Xueqing Peng",
        "user": "Xueqing",
        "type": "user"
      },
      "summary": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce FinAudio, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the FinAudio benchmark.\nThen, we evaluate seven prevalent AudioLLMs on FinAudio. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
      "upvotes": 1,
      "discussionId": "67e626aacb305c5a3ee12027",
      "ai_keywords": [
        "Audio Large Language Models (AudioLLMs)",
        "automatic speech recognition (ASR)",
        "financial scenarios",
        "benchmark",
        "financial analysis",
        "investment decisions",
        "summarization",
        "ASR for short financial audio",
        "ASR for long financial audio",
        "summarization of long financial audio",
        "\\textsc{FinAudio}"
      ]
    },
    "publishedAt": "2025-03-26T17:07:51.000Z",
    "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications",
    "summary": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce FinAudio, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the FinAudio benchmark.\nThen, we evaluate seven prevalent AudioLLMs on FinAudio. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a0c0803c8841cfe2cd1f15",
      "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
      "fullname": "Xueqing Peng",
      "name": "Xueqing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20578",
      "authors": [
        {
          "_id": "67e5fd806e73232cf08af3bb",
          "user": {
            "_id": "659430138fec845e50a27558",
            "avatarUrl": "/avatars/f5de806f55c90ae303cd94af7c15005c.svg",
            "isPro": false,
            "fullname": "Alif Al Hasan",
            "user": "alifalhasan",
            "type": "user"
          },
          "name": "Alif Al Hasan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-28T04:39:45.135Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bc",
          "name": "Subarna Saha",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bd",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T01:38:09.589Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3be",
          "name": "Tarannum Shaila Zaman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T14:25:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:08:31.084Z",
      "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
      "upvotes": 1,
      "discussionId": "67e5fd816e73232cf08af3e2",
      "projectPage": "https://zenodo.org/records/15092886",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "generative LLMs",
        "LLPut",
        "LLaMA",
        "Qwen",
        "Qwen-Coder",
        "natural language",
        "empirical evaluation",
        "bug diagnosis"
      ]
    },
    "publishedAt": "2025-03-26T10:25:01.000Z",
    "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
    "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]