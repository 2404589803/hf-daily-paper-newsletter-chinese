[
  {
    "paper": {
      "id": "2509.17567",
      "authors": [
        {
          "_id": "68d2047d1ca7156988a8eca6",
          "name": "Yang Xiao",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8eca7",
          "user": {
            "_id": "66d01e4401f2a6b4cd93ad87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
            "isPro": false,
            "fullname": "Mohan Jiang",
            "user": "mhjiang0408",
            "type": "user"
          },
          "name": "Mohan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-23T02:42:46.715Z",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8eca8",
          "name": "Jie Sun",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8eca9",
          "name": "Keyu Li",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecaa",
          "name": "Jifan Lin",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecab",
          "name": "Yumin Zhuang",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecac",
          "name": "Ji Zeng",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecad",
          "name": "Shijie Xia",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecae",
          "name": "Qishuo Hua",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecaf",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb0",
          "name": "Xiaojie Cai",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb1",
          "name": "Tongyu Wang",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb2",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb3",
          "name": "Liming Liu",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb4",
          "name": "Xia Wu",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb5",
          "name": "Jinlong Hou",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb6",
          "name": "Yuan Cheng",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb7",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb8",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecb9",
          "name": "Dequan Wang",
          "hidden": false
        },
        {
          "_id": "68d2047d1ca7156988a8ecba",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6002c316698168af3bb9f4a6/bxJ1rPnNWVVybfIHpLjfo.png"
      ],
      "publishedAt": "2025-09-22T10:59:32.000Z",
      "submittedOnDailyAt": "2025-09-23T02:56:05.460Z",
      "title": "LIMI: Less is More for Agency",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.",
      "upvotes": 40,
      "discussionId": "68d2047e1ca7156988a8ecbb",
      "projectPage": "https://github.com/GAIR-NLP/LIMI",
      "githubRepo": "https://github.com/GAIR-NLP/LIMI",
      "ai_summary": "LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.",
      "ai_keywords": [
        "Agency",
        "autonomous agents",
        "self-directed engagement",
        "agentic intelligence",
        "LIMI",
        "Agency Efficiency Principle"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-09-22T06:59:32.000Z",
    "title": "LIMI: Less is More for Agency",
    "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6002c316698168af3bb9f4a6/bxJ1rPnNWVVybfIHpLjfo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17567.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.17627",
      "authors": [
        {
          "_id": "68d20ce31ca7156988a8ed2a",
          "name": "Jinshu Chen",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed2b",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed2c",
          "name": "Xu Bai",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed2d",
          "name": "Tianxiang Ma",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed2e",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed2f",
          "name": "Zhuowei Chen",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed30",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed31",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed32",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed33",
          "name": "Bingchuan Li",
          "hidden": false
        },
        {
          "_id": "68d20ce31ca7156988a8ed34",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/sVfS9qyD1e1SqO9753UK_.mp4"
      ],
      "publishedAt": "2025-09-22T11:35:55.000Z",
      "submittedOnDailyAt": "2025-09-23T01:35:03.582Z",
      "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
      "submittedOnDailyBy": {
        "_id": "6752cd83ffaeeb979db974ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
        "isPro": false,
        "fullname": "Xinghui Li",
        "user": "Crayon-Shinchan",
        "type": "user"
      },
      "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
      "upvotes": 37,
      "discussionId": "68d20ce41ca7156988a8ed35",
      "ai_summary": "OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.",
      "ai_keywords": [
        "diffusion models",
        "Mask-free Video Insertion",
        "data scarcity",
        "Condition-Specific Feature Injection",
        "Progressive Training",
        "Subject-Focused Loss",
        "Insertive Preference Optimization",
        "Context-Aware Rephraser",
        "InsertBench"
      ]
    },
    "publishedAt": "2025-09-22T07:35:55.000Z",
    "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
    "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/sVfS9qyD1e1SqO9753UK_.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17627.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6752cd83ffaeeb979db974ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
      "fullname": "Xinghui Li",
      "name": "Crayon-Shinchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.18091",
      "authors": [
        {
          "_id": "68d20b8a1ca7156988a8ece7",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ece8",
          "name": "Jiakai Tang",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ece9",
          "name": "Jiahua Wu",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecea",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8eceb",
          "name": "Yuxuan Zhu",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecec",
          "name": "Bingjun Chen",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8eced",
          "name": "Bangyang Hong",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecee",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecef",
          "name": "Cong Fu",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf0",
          "name": "Kangle Wu",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf1",
          "name": "Yabo Ni",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf2",
          "name": "Anxiang Zeng",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf3",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf4",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf5",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "68d20b8a1ca7156988a8ecf6",
          "name": "See-Kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T17:59:07.000Z",
      "submittedOnDailyAt": "2025-09-23T01:26:16.690Z",
      "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over +2% GMV/UU and a +2.90% increase in advertising\nrevenue.",
      "upvotes": 23,
      "discussionId": "68d20b8b1ca7156988a8ecf7",
      "ai_summary": "OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.",
      "ai_keywords": [
        "Transformer architectures",
        "Deep Learning Recommendation Models (DLRMs)",
        "context engineering",
        "multi-step reasoning",
        "structured context engineering",
        "block-wise latent reasoning",
        "progressive multi-task training"
      ]
    },
    "publishedAt": "2025-09-22T13:59:07.000Z",
    "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
    "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over +2% GMV/UU and a +2.90% increase in advertising\nrevenue.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18091.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.18056",
      "authors": [
        {
          "_id": "68d23a2c1ca7156988a8ee4a",
          "name": "Yunheng Li",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee4b",
          "name": "Jing Cheng",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee4c",
          "name": "Shaoyong Jia",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee4d",
          "name": "Hangyi Kuang",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee4e",
          "name": "Shaohui Jiao",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee4f",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "68d23a2c1ca7156988a8ee50",
          "name": "Ming-Ming Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T17:30:15.000Z",
      "submittedOnDailyAt": "2025-09-23T04:45:19.836Z",
      "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
      "submittedOnDailyBy": {
        "_id": "67485bfd768f8d6a509d5cd7",
        "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
        "isPro": false,
        "fullname": "Yunheng Li",
        "user": "lyhisme",
        "type": "user"
      },
      "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
      "upvotes": 19,
      "discussionId": "68d23a2c1ca7156988a8ee51",
      "ai_summary": "TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "multimodal large language models",
        "video temporal grounding",
        "Group Relative Policy Optimization",
        "on-policy sampling",
        "off-policy supervision",
        "non-linear soft advantage computation",
        "Chain-of-Thought training",
        "few-shot generalization"
      ]
    },
    "publishedAt": "2025-09-22T13:30:15.000Z",
    "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
    "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67485bfd768f8d6a509d5cd7",
      "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
      "fullname": "Yunheng Li",
      "name": "lyhisme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17765",
      "authors": [
        {
          "_id": "68d20ca51ca7156988a8ed02",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed03",
          "name": "Zhifang Guo",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed04",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed05",
          "name": "Yunfei Chu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed06",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed07",
          "name": "Jinzheng He",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed08",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed09",
          "name": "Xian Shi",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0a",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0b",
          "name": "Xinfa Zhu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0c",
          "name": "Yuanjun Lv",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0d",
          "name": "Yongqi Wang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0e",
          "name": "Dake Guo",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed0f",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed10",
          "name": "Linhan Ma",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed11",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed12",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed13",
          "name": "Hongkun Hao",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed14",
          "name": "Zishan Guo",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed15",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed16",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed17",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed18",
          "name": "Xipin Wei",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed19",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1a",
          "name": "Keqin Chen",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1b",
          "name": "Xuejing Liu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1d",
          "name": "Mingkun Yang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1e",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed1f",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed20",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed21",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed22",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed23",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed24",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed25",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed26",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68d20ca51ca7156988a8ed27",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T13:26:24.000Z",
      "submittedOnDailyAt": "2025-09-23T01:27:51.983Z",
      "title": "Qwen3-Omni Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
      "upvotes": 15,
      "discussionId": "68d20ca51ca7156988a8ed28",
      "githubRepo": "https://github.com/QwenLM/Qwen3-Omni",
      "ai_summary": "Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.",
      "ai_keywords": [
        "multimodal model",
        "Thinker-Talker MoE architecture",
        "audio tasks",
        "audio-visual benchmarks",
        "open-source SOTA",
        "closed-source models",
        "Gemini-2.5-Pro",
        "Seed-ASR",
        "GPT-4o-Transcribe",
        "fluent text",
        "natural real-time speech",
        "text interaction",
        "speech understanding",
        "speech generation",
        "first-packet latency",
        "Talker",
        "discrete speech codecs",
        "multi-codebook scheme",
        "block-wise diffusion",
        "causal ConvNet",
        "cold-start settings",
        "multimodal reasoning",
        "Thinking model",
        "audio captioning model",
        "Qwen3-Omni-30B-A3B",
        "Qwen3-Omni-30B-A3B-Thinking",
        "Qwen3-Omni-30B-A3B-Captioner"
      ],
      "githubStars": 895
    },
    "publishedAt": "2025-09-22T09:26:24.000Z",
    "title": "Qwen3-Omni Technical Report",
    "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 108
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17437",
      "authors": [
        {
          "_id": "68d22fc51ca7156988a8ee0f",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee10",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee11",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee12",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee13",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee14",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "68d22fc51ca7156988a8ee15",
          "name": "Yu Rong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T07:28:09.000Z",
      "submittedOnDailyAt": "2025-09-23T04:30:26.856Z",
      "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "64e85b3edb3767299865e0e3",
        "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
        "isPro": false,
        "fullname": "Chen",
        "user": "Guizhen",
        "type": "user"
      },
      "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.",
      "upvotes": 12,
      "discussionId": "68d22fc51ca7156988a8ee16",
      "githubRepo": "https://github.com/DAMO-NLP-SG/GeoPQA",
      "ai_summary": "A two-stage reinforcement learning framework improves geometric reasoning and problem-solving in multimodal language models by first enhancing visual perception.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "multimodal language models",
        "geometric reasoning",
        "hallucination",
        "perceptual bottleneck",
        "Geo-Perception Question-Answering",
        "visual perception",
        "reasoning capabilities",
        "Qwen2.5-VL-3B-Instruct",
        "figure understanding",
        "perceptual grounding"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-22T03:28:09.000Z",
    "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
    "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e85b3edb3767299865e0e3",
      "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
      "fullname": "Chen",
      "name": "Guizhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.18084",
      "authors": [
        {
          "_id": "68d201a61ca7156988a8ec95",
          "name": "Jiawen Tian",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec96",
          "name": "Liqun Huang",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec97",
          "user": {
            "_id": "68d206c0a6f8ea66da0d416d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bcKpFMN2HwqTB41CDFJGP.png",
            "isPro": false,
            "fullname": "czr",
            "user": "cuizhongren",
            "type": "user"
          },
          "name": "Zhongren Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-23T02:40:28.782Z",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec98",
          "name": "Jingchao Qiao",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec99",
          "name": "Jiafeng Xu",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec9a",
          "name": "Xiao Ma",
          "hidden": false
        },
        {
          "_id": "68d201a61ca7156988a8ec9b",
          "name": "Zeyu Ren",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/8aoHWwElrdM0QhSVdIuVo.mp4"
      ],
      "publishedAt": "2025-09-22T17:57:07.000Z",
      "submittedOnDailyAt": "2025-09-23T00:54:36.399Z",
      "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
      "submittedOnDailyBy": {
        "_id": "6478597d91398856110a6738",
        "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
        "isPro": false,
        "fullname": "Xiao Ma",
        "user": "yusufma555",
        "type": "user"
      },
      "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.",
      "upvotes": 10,
      "discussionId": "68d201a61ca7156988a8ec9c",
      "projectPage": "https://bytewrist.github.io/"
    },
    "publishedAt": "2025-09-22T13:57:07.000Z",
    "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
    "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/8aoHWwElrdM0QhSVdIuVo.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478597d91398856110a6738",
      "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
      "fullname": "Xiao Ma",
      "name": "yusufma555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17396",
      "authors": [
        {
          "_id": "68d1fc2d1ca7156988a8ec7e",
          "name": "Minsoo Kim",
          "hidden": false
        },
        {
          "_id": "68d1fc2d1ca7156988a8ec7f",
          "name": "Arnav Kundu",
          "hidden": false
        },
        {
          "_id": "68d1fc2d1ca7156988a8ec80",
          "name": "Han-Byul Kim",
          "hidden": false
        },
        {
          "_id": "68d1fc2d1ca7156988a8ec81",
          "name": "Richa Dixit",
          "hidden": false
        },
        {
          "_id": "68d1fc2d1ca7156988a8ec82",
          "name": "Minsik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T06:56:35.000Z",
      "submittedOnDailyAt": "2025-09-23T01:24:27.136Z",
      "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
      "submittedOnDailyBy": {
        "_id": "63c0e2503bdc86f8108da51b",
        "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
        "isPro": false,
        "fullname": "Minsoo Kim",
        "user": "minsoo2333",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
      "upvotes": 9,
      "discussionId": "68d1fc2e1ca7156988a8ec83",
      "ai_summary": "EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.",
      "ai_keywords": [
        "Key-Value (KV) caching",
        "KV cache compression",
        "block-wise prefill",
        "episodic KV compression",
        "adaptive layer-wise budget allocation",
        "LongConvQA",
        "multi-turn conversations"
      ]
    },
    "publishedAt": "2025-09-22T02:56:35.000Z",
    "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
    "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17396.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63c0e2503bdc86f8108da51b",
      "avatarUrl": "/avatars/7d47f11992f030b3d831e45102581d1f.svg",
      "fullname": "Minsoo Kim",
      "name": "minsoo2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16941",
      "authors": [
        {
          "_id": "68d2125b1ca7156988a8ed84",
          "name": "Xiang Deng",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed85",
          "name": "Jeff Da",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed86",
          "name": "Edwin Pan",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed87",
          "name": "Yannis Yiming He",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed88",
          "name": "Charles Ide",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed89",
          "name": "Kanak Garg",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8a",
          "name": "Niklas Lauffer",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8b",
          "name": "Andrew Park",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8c",
          "name": "Nitin Pasari",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8d",
          "name": "Chetan Rane",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8e",
          "name": "Karmini Sampath",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed8f",
          "name": "Maya Krishnan",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed90",
          "name": "Srivatsa Kundurthy",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed91",
          "name": "Sean Hendryx",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed92",
          "name": "Zifan Wang",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed93",
          "name": "Chen Bo Calvin Zhang",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed94",
          "name": "Noah Jacobson",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed95",
          "name": "Bing Liu",
          "hidden": false
        },
        {
          "_id": "68d2125b1ca7156988a8ed96",
          "name": "Brad Kenstler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-21T06:28:17.000Z",
      "submittedOnDailyAt": "2025-09-23T01:52:13.007Z",
      "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.",
      "upvotes": 9,
      "discussionId": "68d2125c1ca7156988a8ed97",
      "projectPage": "https://scale.com/research/swe_bench_pro",
      "ai_summary": "SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.",
      "ai_keywords": [
        ""
      ]
    },
    "publishedAt": "2025-09-21T02:28:17.000Z",
    "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
    "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 108
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16117",
      "authors": [
        {
          "_id": "68d0bc4f8adc5cd018d15afb",
          "name": "Kaiwen Zheng",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15afc",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15afd",
          "name": "Haotian Ye",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15afe",
          "name": "Haoxiang Wang",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15aff",
          "name": "Qinsheng Zhang",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15b00",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15b01",
          "name": "Hang Su",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15b02",
          "name": "Stefano Ermon",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15b03",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "68d0bc4f8adc5cd018d15b04",
          "name": "Ming-Yu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-19T16:09:33.000Z",
      "submittedOnDailyAt": "2025-09-23T00:57:40.436Z",
      "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
      "submittedOnDailyBy": {
        "_id": "652bf7edc3cba555d5673c6e",
        "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
        "isPro": true,
        "fullname": "Kaiwen Zheng",
        "user": "worstcoder",
        "type": "user"
      },
      "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
      "upvotes": 9,
      "discussionId": "68d0bc508adc5cd018d15b05",
      "ai_summary": "Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.",
      "ai_keywords": [
        "online reinforcement learning",
        "diffusion models",
        "GRPO-style training",
        "reverse sampling process",
        "flow matching",
        "implicit policy improvement",
        "supervised learning objective",
        "black-box solvers",
        "likelihood estimation",
        "GenEval score",
        "reward models",
        "SD3.5-Medium"
      ]
    },
    "publishedAt": "2025-09-19T12:09:33.000Z",
    "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
    "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652bf7edc3cba555d5673c6e",
      "avatarUrl": "/avatars/78f6416c30203b30671f8423f061c657.svg",
      "fullname": "Kaiwen Zheng",
      "name": "worstcoder",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17985",
      "authors": [
        {
          "_id": "68d21e5d1ca7156988a8edc5",
          "name": "Geonung Kim",
          "hidden": false
        },
        {
          "_id": "68d21e5d1ca7156988a8edc6",
          "name": "Janghyeok Han",
          "hidden": false
        },
        {
          "_id": "68d21e5d1ca7156988a8edc7",
          "name": "Sunghyun Cho",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63040f5c7373aacccd889430/3COO8-8PZ0F305vXjZVjE.mp4"
      ],
      "publishedAt": "2025-09-22T16:28:47.000Z",
      "submittedOnDailyAt": "2025-09-23T02:48:15.077Z",
      "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "63040f5c7373aacccd889430",
        "avatarUrl": "/avatars/29b05c69445c48943f535ad381fb9464.svg",
        "isPro": false,
        "fullname": "geonung kim",
        "user": "comar",
        "type": "user"
      },
      "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.",
      "upvotes": 6,
      "discussionId": "68d21e5d1ca7156988a8edc8",
      "projectPage": "https://kimgeonung.github.io/VideoFrom3D/",
      "githubRepo": "https://github.com/KIMGEONUNG/VideoFrom3D",
      "ai_summary": "VideoFrom3D synthesizes high-quality 3D scene videos using a combination of image and video diffusion models, achieving style consistency without requiring paired datasets.",
      "ai_keywords": [
        "video diffusion model",
        "image diffusion model",
        "Sparse Anchor-view Generation",
        "Geometry-guided Generative Inbetweening",
        "Sparse Appearance-guided Sampling",
        "flow-based camera control",
        "structural guidance"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-09-22T12:28:47.000Z",
    "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
    "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63040f5c7373aacccd889430/3COO8-8PZ0F305vXjZVjE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63040f5c7373aacccd889430",
      "avatarUrl": "/avatars/29b05c69445c48943f535ad381fb9464.svg",
      "fullname": "geonung kim",
      "name": "comar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17158",
      "authors": [
        {
          "_id": "68d2139b1ca7156988a8ed99",
          "name": "Pierre Andrews",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9a",
          "name": "Amine Benhalloum",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9b",
          "name": "Gerard Moreno-Torres Bertran",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9c",
          "name": "Matteo Bettini",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9d",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9e",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8ed9f",
          "name": "Virginie Do",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda0",
          "name": "Romain Froger",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda1",
          "name": "Emilien Garreau",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda2",
          "name": "Jean-Baptiste Gaya",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda3",
          "name": "Hugo Laurenon",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda4",
          "name": "Maxime Lecanu",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda5",
          "name": "Kunal Malkan",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda6",
          "name": "Dheeraj Mekala",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda7",
          "name": "Pierre Mnard",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda8",
          "name": "Grgoire Mialon",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8eda9",
          "name": "Ulyana Piterbarg",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edaa",
          "name": "Mikhail Plekhanov",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edab",
          "name": "Mathieu Rita",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edac",
          "name": "Andrey Rusakov",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edad",
          "name": "Thomas Scialom",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edae",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edaf",
          "name": "Mengjue Wang",
          "hidden": false
        },
        {
          "_id": "68d2139b1ca7156988a8edb0",
          "name": "Ian Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-21T16:59:45.000Z",
      "submittedOnDailyAt": "2025-09-23T01:57:29.427Z",
      "title": "ARE: Scaling Up Agent Environments and Evaluations",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.",
      "upvotes": 6,
      "discussionId": "68d2139b1ca7156988a8edb1",
      "ai_summary": "Meta Agents Research Environments (ARE) facilitate the creation and execution of complex environments for agent research, and Gaia2, a benchmark built on ARE, evaluates general agent capabilities in dynamic, asynchronous settings.",
      "ai_keywords": [
        "Meta Agents Research Environments",
        "ARE",
        "synthetic applications",
        "real-world deployment",
        "abstractions",
        "complex environments",
        "rules",
        "tools",
        "content",
        "verifiers",
        "Gaia2",
        "benchmark",
        "general agent capabilities",
        "ambiguities",
        "noise",
        "dynamic environments",
        "collaboration",
        "temporal constraints",
        "asynchronous",
        "failure modes",
        "reasoning",
        "efficiency",
        "budget scaling curves",
        "architectures",
        "adaptive compute strategies"
      ]
    },
    "publishedAt": "2025-09-21T12:59:45.000Z",
    "title": "ARE: Scaling Up Agent Environments and Evaluations",
    "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 108
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16596",
      "authors": [
        {
          "_id": "68d205331ca7156988a8ecbd",
          "user": {
            "_id": "66384be673c2c55f2ded89fa",
            "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
            "isPro": false,
            "fullname": "Junjie Ye",
            "user": "Junjie-Ye",
            "type": "user"
          },
          "name": "Junjie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-23T02:40:25.849Z",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecbe",
          "name": "Yuming Yang",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecbf",
          "name": "Yang Nan",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc0",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc1",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc2",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc3",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc4",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc5",
          "name": "Zhongchao Shi",
          "hidden": false
        },
        {
          "_id": "68d205331ca7156988a8ecc6",
          "name": "Jianping Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-20T09:40:32.000Z",
      "submittedOnDailyAt": "2025-09-23T00:56:48.151Z",
      "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
      "submittedOnDailyBy": {
        "_id": "655c6b1abfb531437a54c0e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
        "isPro": false,
        "fullname": "Yuming Yang",
        "user": "Umean",
        "type": "user"
      },
      "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.",
      "upvotes": 6,
      "discussionId": "68d205331ca7156988a8ecc7",
      "ai_summary": "Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.",
      "ai_keywords": [
        "large language models",
        "pre-training",
        "supervised fine-tuning",
        "closed-book question answering",
        "token",
        "parameter updates",
        "knowledge enhancement"
      ]
    },
    "publishedAt": "2025-09-20T05:40:32.000Z",
    "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
    "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655c6b1abfb531437a54c0e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
      "fullname": "Yuming Yang",
      "name": "Umean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17671",
      "authors": [
        {
          "_id": "68d23b0d1ca7156988a8ee53",
          "name": "Selva Ta",
          "hidden": false
        },
        {
          "_id": "68d23b0d1ca7156988a8ee54",
          "name": "Mahmut El Huseyni",
          "hidden": false
        },
        {
          "_id": "68d23b0d1ca7156988a8ee55",
          "name": "zay Ezerceli",
          "hidden": false
        },
        {
          "_id": "68d23b0d1ca7156988a8ee56",
          "name": "Reyhan Bayraktar",
          "hidden": false
        },
        {
          "_id": "68d23b0d1ca7156988a8ee57",
          "name": "Fatma Betl Terziolu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T12:14:11.000Z",
      "submittedOnDailyAt": "2025-09-23T04:48:36.360Z",
      "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
      "submittedOnDailyBy": {
        "_id": "6422eab8e2029ade06eeee2c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
        "isPro": false,
        "fullname": "Mahmud ElHuseyni ",
        "user": "MElHuseyni",
        "type": "user"
      },
      "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
      "upvotes": 4,
      "discussionId": "68d23b0d1ca7156988a8ee58",
      "ai_summary": "Turk-LettuceDetect, a suite of hallucination detection models for Turkish RAG applications, achieves high performance using fine-tuned encoder architectures on a machine-translated RAGTruth dataset.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Retrieval-Augmented Generation (RAG)",
        "hallucination detection",
        "token-level classification",
        "ModernBERT",
        "TurkEmbed4STS",
        "EuroBERT",
        "RAGTruth benchmark dataset",
        "F1-score",
        "computational efficiency",
        "long contexts",
        "multilingual NLP"
      ]
    },
    "publishedAt": "2025-09-22T08:14:11.000Z",
    "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
    "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17671.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422eab8e2029ade06eeee2c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
      "fullname": "Mahmud ElHuseyni ",
      "name": "MElHuseyni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15709",
      "authors": [
        {
          "_id": "68d2322a1ca7156988a8ee18",
          "name": "Zhuangzhuang He",
          "hidden": false
        },
        {
          "_id": "68d2322a1ca7156988a8ee19",
          "name": "Zhou Kaiyu",
          "hidden": false
        },
        {
          "_id": "68d2322a1ca7156988a8ee1a",
          "name": "Haoyue Bai",
          "hidden": false
        },
        {
          "_id": "68d2322a1ca7156988a8ee1b",
          "name": "Fengbin Zhu",
          "hidden": false
        },
        {
          "_id": "68d2322a1ca7156988a8ee1c",
          "name": "Yonghui Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-19T07:33:50.000Z",
      "submittedOnDailyAt": "2025-09-23T04:11:14.501Z",
      "title": "Understanding Embedding Scaling in Collaborative Filtering",
      "submittedOnDailyBy": {
        "_id": "6621e88fa11ce46061d25a16",
        "avatarUrl": "/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg",
        "isPro": false,
        "fullname": "Zhuangzhuang He",
        "user": "bruno888",
        "type": "user"
      },
      "summary": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
      "upvotes": 3,
      "discussionId": "68d2322a1ca7156988a8ee1d",
      "ai_summary": "Large-scale experiments reveal double-peak and logarithmic performance patterns in collaborative filtering models as embedding dimensions scale, and provide theoretical insights into their causes.",
      "ai_keywords": [
        "collaborative filtering models",
        "embedding dimensions",
        "double-peak phenomenon",
        "logarithmic curve",
        "noise robustness"
      ]
    },
    "publishedAt": "2025-09-19T03:33:50.000Z",
    "title": "Understanding Embedding Scaling in Collaborative Filtering",
    "summary": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15709.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6621e88fa11ce46061d25a16",
      "avatarUrl": "/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg",
      "fullname": "Zhuangzhuang He",
      "name": "bruno888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15248",
      "authors": [
        {
          "_id": "68d209c41ca7156988a8ecd4",
          "name": "Zitong Yang",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecd5",
          "name": "Aonan Zhang",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecd6",
          "name": "Hong Liu",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecd7",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecd8",
          "name": "Emmanuel Cands",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecd9",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "68d209c41ca7156988a8ecda",
          "name": "Ruoming Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T22:28:27.000Z",
      "submittedOnDailyAt": "2025-09-23T01:20:05.684Z",
      "title": "Synthetic bootstrapped pretraining",
      "submittedOnDailyBy": {
        "_id": "65f3e68a138c6ab771434e2d",
        "avatarUrl": "/avatars/7bfbdb1949f73b3d8f88ae2ff73900bb.svg",
        "isPro": false,
        "fullname": "Aonan Zhang",
        "user": "AonanZhang",
        "type": "user"
      },
      "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.",
      "upvotes": 3,
      "discussionId": "68d209c51ca7156988a8ecdb",
      "ai_summary": "Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.",
      "ai_keywords": [
        "Synthetic Bootstrapped Pretraining",
        "SBP",
        "language model",
        "pretraining procedure",
        "inter-document correlations",
        "causal correlations",
        "Bayesian interpretation",
        "latent concepts"
      ]
    },
    "publishedAt": "2025-09-17T18:28:27.000Z",
    "title": "Synthetic bootstrapped pretraining",
    "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15248.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f3e68a138c6ab771434e2d",
      "avatarUrl": "/avatars/7bfbdb1949f73b3d8f88ae2ff73900bb.svg",
      "fullname": "Aonan Zhang",
      "name": "AonanZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.18095",
      "authors": [
        {
          "_id": "68d22a351ca7156988a8edfa",
          "name": "Zilin Xiao",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8edfb",
          "name": "Qi Ma",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8edfc",
          "name": "Mengting Gu",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8edfd",
          "name": "Chun-cheng Jason Chen",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8edfe",
          "name": "Xintao Chen",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8edff",
          "name": "Vicente Ordonez",
          "hidden": false
        },
        {
          "_id": "68d22a351ca7156988a8ee00",
          "name": "Vijai Mohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T17:59:42.000Z",
      "submittedOnDailyAt": "2025-09-23T03:34:46.809Z",
      "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
      "submittedOnDailyBy": {
        "_id": "67eb818141abf40cd87ab303",
        "avatarUrl": "/avatars/8fd19f5fcac50946be63d55d265e68b0.svg",
        "isPro": false,
        "fullname": "Zilin Xiao",
        "user": "MrZilinXiao",
        "type": "user"
      },
      "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
      "upvotes": 2,
      "discussionId": "68d22a351ca7156988a8ee01",
      "ai_summary": "MetaEmbed, a new framework for multimodal retrieval, uses learnable Meta Tokens to provide compact yet expressive multi-vector embeddings, enabling scalable and efficient retrieval performance.",
      "ai_keywords": [
        "MetaEmbed",
        "Meta Tokens",
        "multimodal embeddings",
        "Matryoshka Multi-Vector Retrieval",
        "Massive Multimodal Embedding Benchmark",
        "Visual Document Retrieval Benchmark"
      ]
    },
    "publishedAt": "2025-09-22T13:59:42.000Z",
    "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
    "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67eb818141abf40cd87ab303",
      "avatarUrl": "/avatars/8fd19f5fcac50946be63d55d265e68b0.svg",
      "fullname": "Zilin Xiao",
      "name": "MrZilinXiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17818",
      "authors": [
        {
          "_id": "68d20b381ca7156988a8ece1",
          "name": "Yiyang Chen",
          "hidden": false
        },
        {
          "_id": "68d20b381ca7156988a8ece2",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "68d20b381ca7156988a8ece3",
          "name": "Xiujun Ma",
          "hidden": false
        },
        {
          "_id": "68d20b381ca7156988a8ece4",
          "name": "Yue Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T14:13:31.000Z",
      "submittedOnDailyAt": "2025-09-23T01:21:45.969Z",
      "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
      "upvotes": 2,
      "discussionId": "68d20b381ca7156988a8ece5",
      "projectPage": "https://yychen233.github.io/ContextFlow-page/",
      "ai_summary": "ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "Rectified Flow solver",
        "Adaptive Context Enrichment",
        "self-attention",
        "Key-Value pairs",
        "Guidance Responsiveness Metric"
      ]
    },
    "publishedAt": "2025-09-22T10:13:31.000Z",
    "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
    "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 108
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17641",
      "authors": [
        {
          "_id": "68d22a281ca7156988a8edf4",
          "name": "Hyunjong Ok",
          "hidden": false
        },
        {
          "_id": "68d22a281ca7156988a8edf5",
          "name": "Suho Yoo",
          "hidden": false
        },
        {
          "_id": "68d22a281ca7156988a8edf6",
          "name": "Hyeonjun Kim",
          "hidden": false
        },
        {
          "_id": "68d22a281ca7156988a8edf7",
          "name": "Jaeho Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T11:45:22.000Z",
      "submittedOnDailyAt": "2025-09-23T03:34:10.651Z",
      "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
      "submittedOnDailyBy": {
        "_id": "631974d51328b6caf9fe328f",
        "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
        "isPro": false,
        "fullname": "Hyunjong Ok",
        "user": "HJOK",
        "type": "user"
      },
      "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
      "upvotes": 2,
      "discussionId": "68d22a281ca7156988a8edf8",
      "ai_summary": "AuditoryBench++ and AIR-CoT enhance text-only models' auditory reasoning and knowledge integration, outperforming existing models in multimodal interactions.",
      "ai_keywords": [
        "AuditoryBench++",
        "AIR-CoT",
        "auditory imagination reasoning",
        "span detection",
        "knowledge injection",
        "multimodal interactions"
      ]
    },
    "publishedAt": "2025-09-22T07:45:22.000Z",
    "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
    "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631974d51328b6caf9fe328f",
      "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
      "fullname": "Hyunjong Ok",
      "name": "HJOK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17336",
      "authors": [
        {
          "_id": "68d20f911ca7156988a8ed61",
          "name": "Tianyu Fu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed62",
          "name": "Anyang Su",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed63",
          "name": "Chenxu Zhao",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed64",
          "name": "Hanning Wang",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed65",
          "name": "Minghui Wu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed66",
          "name": "Zhe Yu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed67",
          "name": "Fei Hu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed68",
          "name": "Mingjia Shi",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed69",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6a",
          "name": "Jiayao Wang",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6b",
          "name": "Yuyang Chen",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6c",
          "name": "Ruiyang Yu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6d",
          "name": "Siran Peng",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6e",
          "name": "Menglin Li",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed6f",
          "name": "Nan Huang",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed70",
          "name": "Haitian Wei",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed71",
          "name": "Jiawei Yu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed72",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed73",
          "name": "Xilin Zhao",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed74",
          "name": "Kai Gu",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed75",
          "name": "Ping Jiang",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed76",
          "name": "Sifan Zhou",
          "hidden": false
        },
        {
          "_id": "68d20f911ca7156988a8ed77",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-22T03:13:58.000Z",
      "submittedOnDailyAt": "2025-09-23T01:40:17.413Z",
      "title": "Mano Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
      "upvotes": 2,
      "discussionId": "68d20f911ca7156988a8ed78",
      "ai_summary": "A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.",
      "ai_keywords": [
        "vision-language models",
        "multi-modal foundation model",
        "simulated environment",
        "three-stage training pipeline",
        "supervised fine-tuning",
        "offline reinforcement learning",
        "online reinforcement learning",
        "verification module",
        "Mind2Web",
        "OSWorld",
        "domain-specific data",
        "iterative training",
        "holistic reward design"
      ]
    },
    "publishedAt": "2025-09-21T23:13:58.000Z",
    "title": "Mano Report",
    "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 108
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.18083",
      "authors": [
        {
          "_id": "68d24adf1ca7156988a8ee94",
          "name": "Valentin Lacombe",
          "hidden": false
        },
        {
          "_id": "68d24adf1ca7156988a8ee95",
          "name": "Valentin Quesnel",
          "hidden": false
        },
        {
          "_id": "68d24adf1ca7156988a8ee96",
          "name": "Damien Sileo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/8eF3782GxQiM5xUWR24vj.png"
      ],
      "publishedAt": "2025-09-22T17:56:38.000Z",
      "submittedOnDailyAt": "2025-09-23T05:54:55.151Z",
      "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
      "submittedOnDailyBy": {
        "_id": "5fc0bcb41160c47d1d43856b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
        "isPro": false,
        "fullname": "Damien Sileo",
        "user": "sileod",
        "type": "user"
      },
      "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
      "upvotes": 1,
      "discussionId": "68d24ae01ca7156988a8ee97",
      "projectPage": "https://github.com/sileod/reasoning_core",
      "githubRepo": "https://github.com/sileod/reasoning_core",
      "ai_summary": "Reasoning Core is a scalable RLVR environment that generates diverse symbolic reasoning problems to enhance LLM capabilities.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large Language Models (LLMs)",
        "PDDL planning",
        "first-order logic",
        "context-free grammar parsing",
        "causal reasoning",
        "system equation solving",
        "high-generality problem distributions",
        "verification via external tools",
        "continuous difficulty control"
      ]
    },
    "publishedAt": "2025-09-22T13:56:38.000Z",
    "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
    "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/8eF3782GxQiM5xUWR24vj.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.18083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fc0bcb41160c47d1d43856b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
      "fullname": "Damien Sileo",
      "name": "sileod",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17428",
      "authors": [
        {
          "_id": "68d24c461ca7156988a8eeae",
          "name": "Hyesung Jeon",
          "hidden": false
        },
        {
          "_id": "68d24c461ca7156988a8eeaf",
          "name": "Seojune Lee",
          "hidden": false
        },
        {
          "_id": "68d24c461ca7156988a8eeb0",
          "name": "Beomseok Kang",
          "hidden": false
        },
        {
          "_id": "68d24c461ca7156988a8eeb1",
          "name": "Yulhwa Kim",
          "hidden": false
        },
        {
          "_id": "68d24c461ca7156988a8eeb2",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/aUmk7ue3qOnZzLtHih-XT.png"
      ],
      "publishedAt": "2025-09-22T07:21:41.000Z",
      "submittedOnDailyAt": "2025-09-23T06:15:05.820Z",
      "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for\n  Parameter-Efficient Fine-Tuning on Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6400208acafc9d549863af59",
        "avatarUrl": "/avatars/6c383c810a038ce61e803f1d75132471.svg",
        "isPro": false,
        "fullname": "Hyesung Jeon",
        "user": "hjeon2k",
        "type": "user"
      },
      "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.",
      "upvotes": 1,
      "discussionId": "68d24c471ca7156988a8eeb3",
      "githubRepo": "https://github.com/vantaa89/qwha",
      "ai_summary": "QWHA integrates Walsh-Hadamard Transform-based adapters into quantized models to reduce quantization errors and computational overhead, improving low-bit quantization accuracy and training speed.",
      "ai_keywords": [
        "quantization",
        "parameter-efficient fine-tuning",
        "quantization-aware PEFT",
        "quantization error",
        "low-rank adaptation",
        "Fourier-related transform",
        "FT-based adapters",
        "Walsh-Hadamard Transform",
        "WHT",
        "adaptive parameter selection",
        "value refinement",
        "low-bit quantization accuracy",
        "training speedups"
      ]
    },
    "publishedAt": "2025-09-22T03:21:41.000Z",
    "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for\n  Parameter-Efficient Fine-Tuning on Large Language Models",
    "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/aUmk7ue3qOnZzLtHih-XT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6400208acafc9d549863af59",
      "avatarUrl": "/avatars/6c383c810a038ce61e803f1d75132471.svg",
      "fullname": "Hyesung Jeon",
      "name": "hjeon2k",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16633",
      "authors": [
        {
          "_id": "68d22a211ca7156988a8edee",
          "name": "Abhirama Subramanyam Penamakuri",
          "hidden": false
        },
        {
          "_id": "68d22a211ca7156988a8edef",
          "name": "Navlika Singh",
          "hidden": false
        },
        {
          "_id": "68d22a211ca7156988a8edf0",
          "name": "Piyush Arora",
          "hidden": false
        },
        {
          "_id": "68d22a211ca7156988a8edf1",
          "name": "Anand Mishra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-20T11:12:23.000Z",
      "submittedOnDailyAt": "2025-09-23T03:34:34.342Z",
      "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
      "submittedOnDailyBy": {
        "_id": "6549d555fc80ab2d8ca1469a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6549d555fc80ab2d8ca1469a/Z6-tS-9-6-aUiHCAW5ql8.jpeg",
        "isPro": false,
        "fullname": "Abhirama Subramanyam",
        "user": "abhiram4572",
        "type": "user"
      },
      "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
      "upvotes": 1,
      "discussionId": "68d22a221ca7156988a8edf2",
      "githubRepo": "https://github.com/vl2g/MPA",
      "ai_summary": "The Model Parity Aligner (MPA) framework improves Small Vision-Language Models (S-VLMs) by leveraging unlabeled images and knowledge transfer from Large Vision-Language Models (L-VLMs) to reduce performance gaps in vision and language tasks.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Small Vision-Language Models",
        "Model Parity Aligner",
        "knowledge transfer",
        "knowledge distillation",
        "VQA",
        "TextVQA",
        "ST-VQA",
        "ChartQA",
        "OKVQA",
        "specialized reasoning",
        "text recognition",
        "chart interpretation",
        "commonsense understanding",
        "factual understanding"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-20T07:12:23.000Z",
    "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
    "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6549d555fc80ab2d8ca1469a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6549d555fc80ab2d8ca1469a/Z6-tS-9-6-aUiHCAW5ql8.jpeg",
      "fullname": "Abhirama Subramanyam",
      "name": "abhiram4572",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16591",
      "authors": [
        {
          "_id": "68d217261ca7156988a8edb3",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb4",
          "name": "Mengjie Liu",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb5",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb6",
          "name": "Mengzhang Cai",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb7",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb8",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "68d217261ca7156988a8edb9",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-20T09:30:25.000Z",
      "submittedOnDailyAt": "2025-09-23T03:49:26.707Z",
      "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
      "submittedOnDailyBy": {
        "_id": "6625ef13605f46d05c1d0031",
        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
        "isPro": false,
        "fullname": "Zheng Liu",
        "user": "starriver030515",
        "type": "user"
      },
      "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.",
      "upvotes": 1,
      "discussionId": "68d217261ca7156988a8edba",
      "githubRepo": "https://github.com/starriver030515/HAPO",
      "ai_summary": "Heterogeneous Adaptive Policy Optimization (HAPO) enhances reinforcement learning in LLMs by dynamically adapting token optimization based on entropy, improving performance across various model scales.",
      "ai_keywords": [
        "Heterogeneous Adaptive Policy Optimization",
        "HAPO",
        "token-aware algorithm",
        "token entropy",
        "Adaptive Temperature Sampling",
        "Token Level Group Average",
        "Differential Advantage Redistribution",
        "Asymmetric Adaptive Clipping"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-20T05:30:25.000Z",
    "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
    "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ef13605f46d05c1d0031",
      "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
      "fullname": "Zheng Liu",
      "name": "starriver030515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16415",
      "authors": [
        {
          "_id": "68d229631ca7156988a8eddb",
          "name": "Zhengri Wu",
          "hidden": false
        },
        {
          "_id": "68d229631ca7156988a8eddc",
          "name": "Yiran Wang",
          "hidden": false
        },
        {
          "_id": "68d229631ca7156988a8eddd",
          "name": "Yu Wen",
          "hidden": false
        },
        {
          "_id": "68d229631ca7156988a8edde",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "68d229631ca7156988a8eddf",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "68d229631ca7156988a8ede0",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/u348NeyvlFMCYWemMJdoi.mp4"
      ],
      "publishedAt": "2025-09-19T20:57:03.000Z",
      "submittedOnDailyAt": "2025-09-23T03:31:25.680Z",
      "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": true,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
      "upvotes": 1,
      "discussionId": "68d229641ca7156988a8ede1",
      "projectPage": "https://aigeeksgroup.github.io/StereoAdapter/",
      "githubRepo": "https://github.com/AIGeeksGroup/StereoAdapter",
      "ai_summary": "StereoAdapter is a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular encoder with a recurrent stereo refinement module for underwater stereo depth estimation, improving accuracy and robustness.",
      "ai_keywords": [
        "stereo depth estimation",
        "parameter-efficient",
        "self-supervised framework",
        "LoRA-adapted",
        "monocular foundation encoder",
        "recurrent stereo refinement module",
        "dynamic LoRA adaptation",
        "UW-StereoDepth-40K dataset",
        "TartanAir",
        "SQUID",
        "BlueROV2"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-19T16:57:03.000Z",
    "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
    "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/u348NeyvlFMCYWemMJdoi.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14856",
      "authors": [
        {
          "_id": "68d2371c1ca7156988a8ee2e",
          "name": "Hanyang Guo",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee2f",
          "name": "Xunjin Zheng",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee30",
          "name": "Zihan Liao",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee31",
          "name": "Hang Yu",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee32",
          "name": "Peng DI",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee33",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "68d2371c1ca7156988a8ee34",
          "name": "Hong-Ning Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T11:24:09.000Z",
      "submittedOnDailyAt": "2025-09-23T04:34:40.984Z",
      "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
      "submittedOnDailyBy": {
        "_id": "6430bdd8cd31d174a9f900fb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
        "isPro": false,
        "fullname": "Ziyin Zhang",
        "user": "Geralt-Targaryen",
        "type": "user"
      },
      "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
      "upvotes": 1,
      "discussionId": "68d2371c1ca7156988a8ee35",
      "ai_summary": "A new benchmark, CodeFuse-CR-Bench, evaluates LLMs in repository-level code review with comprehensive, context-rich data and a novel evaluation framework.",
      "ai_keywords": [
        "Large Language Models",
        "automated code review",
        "CodeFuse-CR-Bench",
        "repository-level CR evaluation",
        "Pull-Request problem domains",
        "rule-based checks",
        "model-based judgments",
        "review quality",
        "Gemini 2.5 Pro"
      ]
    },
    "publishedAt": "2025-09-18T07:24:09.000Z",
    "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
    "summary": "Automated code review (CR) is a key application for Large Language Models\n(LLMs), but progress is hampered by a \"reality gap\": existing benchmarks\nevaluate models on isolated sub-tasks using simplified, context-poor data. This\nfails to reflect the holistic context-rich nature of real-world CR. To bridge\nthis gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware\nbenchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601\nhigh-quality instances from 70 Python projects covering nine Pull-Request (PR)\nproblem domains, where each instance provides rich, multi-faceted context\nincluding the associated issue, PR details, and repository state, enabling\nend-to-end evaluation. Beyond superficial metrics, we also propose a novel\nevaluation framework that combines rule-based checks for location and syntax\nwith model-based judgments of review quality. We present the first large-scale\nassessment of state-of-the-art LLMs on this comprehensive CR task. Our results\nestablish crucial baselines and reveal that (1) no single LLM dominates all\naspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive\nperformance; and (3) different LLMs exhibit varying robustness to redundant\ncontext. These findings highlight the necessity of holistic, multi-dimensional\nevaluation and provide actionable insights for advancing truly intelligent yet\npractical CR assistants.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14856.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.17191",
      "authors": [
        {
          "_id": "68d2362b1ca7156988a8ee22",
          "name": "Jinchao Ge",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee23",
          "name": "Tengfei Cheng",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee24",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee25",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee26",
          "name": "Shiya Huang",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee27",
          "name": "Judith Bishop",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee28",
          "name": "Gillian Shepherd",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee29",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee2a",
          "name": "Ling Chen",
          "hidden": false
        },
        {
          "_id": "68d2362b1ca7156988a8ee2b",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-21T18:36:54.000Z",
      "submittedOnDailyAt": "2025-09-23T04:25:36.285Z",
      "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": true,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
      "upvotes": 0,
      "discussionId": "68d2362b1ca7156988a8ee2c",
      "ai_summary": "VaseVL, an SFT-then-RL system, enhances MLLMs for ancient Greek pottery analysis by addressing performance gaps through taxonomy-conditioned rewards, achieving state-of-the-art results in style classification and historical attribution.",
      "ai_keywords": [
        "SFT",
        "RL",
        "VaseVL",
        "taxonomy",
        "type-specific performance gaps",
        "type-conditioned",
        "compositionality-oriented rewards",
        "VaseVQA",
        "compositional robustness"
      ]
    },
    "publishedAt": "2025-09-21T14:36:54.000Z",
    "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
    "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.17191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.16548",
      "authors": [
        {
          "_id": "68d237f71ca7156988a8ee42",
          "name": "Yuyang Ding",
          "hidden": false
        },
        {
          "_id": "68d237f71ca7156988a8ee43",
          "name": "Xinyu Shi",
          "hidden": false
        },
        {
          "_id": "68d237f71ca7156988a8ee44",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "68d237f71ca7156988a8ee45",
          "name": "Xiaobo Liang",
          "hidden": false
        },
        {
          "_id": "68d237f71ca7156988a8ee46",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "68d237f71ca7156988a8ee47",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-20T06:19:55.000Z",
      "submittedOnDailyAt": "2025-09-23T04:33:07.311Z",
      "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
      "submittedOnDailyBy": {
        "_id": "626cf0f65651e31a7a2b9779",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
        "isPro": false,
        "fullname": "Ding",
        "user": "dyyyyyyyy",
        "type": "user"
      },
      "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
      "upvotes": 0,
      "discussionId": "68d237f71ca7156988a8ee48",
      "ai_summary": "SCAN, a self-denoising Monte Carlo framework, improves PRM performance with synthetic data, achieving high F1 scores and surpassing human-annotated baselines.",
      "ai_keywords": [
        "Process reward models",
        "Monte Carlo estimation",
        "self-denoising",
        "noise distribution",
        "annotation models",
        "ProcessBench",
        "F1 score",
        "PRM800K"
      ]
    },
    "publishedAt": "2025-09-20T02:19:55.000Z",
    "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
    "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.16548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626cf0f65651e31a7a2b9779",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
      "fullname": "Ding",
      "name": "dyyyyyyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09873",
      "authors": [
        {
          "_id": "68cccbb13df9ac65e93dc69a",
          "name": "James Jewitt",
          "hidden": false
        },
        {
          "_id": "68cccbb13df9ac65e93dc69b",
          "user": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
            "isPro": false,
            "fullname": "Leo",
            "user": "hao-li",
            "type": "user"
          },
          "name": "Hao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:16.570Z",
          "hidden": false
        },
        {
          "_id": "68cccbb13df9ac65e93dc69c",
          "name": "Bram Adams",
          "hidden": false
        },
        {
          "_id": "68cccbb13df9ac65e93dc69d",
          "name": "Gopi Krishnan Rajbahadur",
          "hidden": false
        },
        {
          "_id": "68cccbb13df9ac65e93dc69e",
          "name": "Ahmed E. Hassan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T21:46:20.000Z",
      "submittedOnDailyAt": "2025-09-23T01:35:42.612Z",
      "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
      "submittedOnDailyBy": {
        "_id": "62b4f3b7464e664268bf4e85",
        "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
        "isPro": false,
        "fullname": "Leo",
        "user": "hao-li",
        "type": "user"
      },
      "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.",
      "upvotes": 0,
      "discussionId": "68cccbb13df9ac65e93dc69f",
      "ai_summary": "The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.",
      "ai_keywords": [
        "license conflicts",
        "Hugging Face",
        "datasets",
        "models",
        "GitHub projects",
        "SPDX",
        "rule engine",
        "license compliance",
        "governance challenge",
        "automated compliance"
      ]
    },
    "publishedAt": "2025-09-11T17:46:20.000Z",
    "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
    "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b4f3b7464e664268bf4e85",
      "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
      "fullname": "Leo",
      "name": "hao-li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]