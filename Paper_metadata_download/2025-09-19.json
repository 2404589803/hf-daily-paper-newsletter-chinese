[
  {
    "paper": {
      "id": "2509.15221",
      "authors": [
        {
          "_id": "68ccb2e43df9ac65e93dc5b9",
          "user": {
            "_id": "6432c6152bfb2b0ec7572479",
            "avatarUrl": "/avatars/7c2f908f511cdfa38a19e967d86f9c40.svg",
            "isPro": false,
            "fullname": "Zhaoyang Liu",
            "user": "zyliu",
            "type": "user"
          },
          "name": "Zhaoyang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:49:00.775Z",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5ba",
          "user": {
            "_id": "6502f241b1792803da7e8def",
            "avatarUrl": "/avatars/2b6c8917a381ffdc36f6aebac13d0e57.svg",
            "isPro": false,
            "fullname": "JingJing Xie",
            "user": "ownerEli",
            "type": "user"
          },
          "name": "JingJing Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:58.643Z",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bb",
          "user": {
            "_id": "642b9861bb77f8456634b048",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
            "isPro": false,
            "fullname": "Zichen Ding",
            "user": "heroding77",
            "type": "user"
          },
          "name": "Zichen Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:49:03.065Z",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bc",
          "name": "Zehao Li",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bd",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5be",
          "name": "Zhenyu Wu",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bf",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c0",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c1",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c2",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c3",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c4",
          "name": "Qingyun Li",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c5",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c6",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c7",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c8",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c9",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5ca",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cb",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cc",
          "name": "Qifeng Chen",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cd",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:59:22.000Z",
      "submittedOnDailyAt": "2025-09-19T00:03:42.415Z",
      "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
      "upvotes": 56,
      "discussionId": "68ccb2e43df9ac65e93dc5ce",
      "githubRepo": "https://github.com/OpenGVLab/ScaleCUA",
      "ai_summary": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.",
      "ai_keywords": [
        "Vision-Language Models",
        "computer use agents",
        "GUIs",
        "closed-loop pipeline",
        "automated agents",
        "human experts",
        "WebArena-Lite-v2",
        "ScreenSpot-Pro",
        "MMBench-GUI L1-Hard",
        "OSWorld-G"
      ],
      "githubStars": 42
    },
    "publishedAt": "2025-09-18T13:59:22.000Z",
    "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
    "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15221.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15207",
      "authors": [
        {
          "_id": "68ccb7983df9ac65e93dc626",
          "user": {
            "_id": "647ffddeb82adfa7cc1a10d9",
            "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
            "isPro": false,
            "fullname": "zhu",
            "user": "xuekai",
            "type": "user"
          },
          "name": "Xuekai Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:45.314Z",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc627",
          "user": {
            "_id": "649e6761f9134a06ed1e0cea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
            "isPro": false,
            "fullname": "Daixuan Cheng",
            "user": "daixuancheng",
            "type": "user"
          },
          "name": "Daixuan Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:48.340Z",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc628",
          "name": "Dinghuai Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc629",
          "name": "Hengli Li",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62a",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62b",
          "name": "Che Jiang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62c",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62d",
          "name": "Ermo Hua",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62e",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62f",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc630",
          "name": "Qizheng Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc631",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc632",
          "name": "Fanghao Shao",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc633",
          "name": "Bo Xue",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc634",
          "name": "Yunchong Song",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc635",
          "user": {
            "_id": "65574c70d16b524f1d2345c1",
            "avatarUrl": "/avatars/8edbe9ebea85c570bc19b63bf3a727d9.svg",
            "isPro": false,
            "fullname": "Zhenjie Yang",
            "user": "jayyoung0802",
            "type": "user"
          },
          "name": "Zhenjie Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:37.248Z",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc636",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc637",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc638",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc639",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63a",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63b",
          "name": "Hongyuan Mei",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63c",
          "name": "Zhouhan Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
      ],
      "publishedAt": "2025-09-18T17:56:36.000Z",
      "submittedOnDailyAt": "2025-09-19T00:24:38.079Z",
      "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "649e6761f9134a06ed1e0cea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
        "isPro": false,
        "fullname": "Daixuan Cheng",
        "user": "daixuancheng",
        "type": "user"
      },
      "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
      "upvotes": 54,
      "discussionId": "68ccb7983df9ac65e93dc63d",
      "githubRepo": "https://github.com/Xuekai-Zhu/FlowRL",
      "ai_summary": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.",
      "ai_keywords": [
        "FlowRL",
        "reward distribution",
        "flow balancing",
        "reinforcement learning",
        "reward-maximizing methods",
        "PPO",
        "GRPO",
        "normalized target distribution",
        "learnable partition function",
        "reverse KL divergence",
        "diverse exploration",
        "generalizable reasoning trajectories",
        "math reasoning",
        "code reasoning"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-09-18T13:56:36.000Z",
    "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
    "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15207.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649e6761f9134a06ed1e0cea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
      "fullname": "Daixuan Cheng",
      "name": "daixuancheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.15194",
      "authors": [
        {
          "_id": "68ccb4383df9ac65e93dc5d0",
          "user": {
            "_id": "65a05abf07184d32fa002d41",
            "avatarUrl": "/avatars/3a23e7e568d2024381ed31b56c1c461a.svg",
            "isPro": false,
            "fullname": "Yujun Zhou",
            "user": "yujunzhou",
            "type": "user"
          },
          "name": "Yujun Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:53.689Z",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d1",
          "user": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
            "isPro": false,
            "fullname": "Zhenwen Liang",
            "user": "invokerliang",
            "type": "user"
          },
          "name": "Zhenwen Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:56.281Z",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d2",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d3",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d4",
          "name": "Kishan Panaganti",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d5",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d6",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d7",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d8",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d9",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:50:04.000Z",
      "submittedOnDailyAt": "2025-09-19T00:09:24.304Z",
      "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
      "submittedOnDailyBy": {
        "_id": "5feab3a28a3201f8e554c969",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
        "isPro": false,
        "fullname": "Wenhao Yu",
        "user": "wyu1",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
      "upvotes": 27,
      "discussionId": "68ccb4383df9ac65e93dc5da",
      "githubRepo": "https://github.com/YujunZhou/EVOL-RL",
      "ai_summary": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.",
      "ai_keywords": [
        "reinforcement learning from verifiable rewards",
        "RLVR",
        "label-free methods",
        "confidence minimization",
        "self-consistency",
        "majority-vote objectives",
        "entropy collapse",
        "Test-Time Reinforcement Learning",
        "TTRL",
        "EVolution-Oriented and Label-free Reinforcement Learning",
        "EVOL-RL",
        "GRPO",
        "asymmetric clipping",
        "entropy regularizer",
        "pass@1",
        "pass@n",
        "AIME24",
        "Qwen3-4B-Base",
        "AIME25",
        "GPQA"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-09-18T13:50:04.000Z",
    "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
    "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5feab3a28a3201f8e554c969",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
      "fullname": "Wenhao Yu",
      "name": "wyu1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14760",
      "authors": [
        {
          "_id": "68ccc6a13df9ac65e93dc66a",
          "user": {
            "_id": "689ec537196ab997b13dc977",
            "avatarUrl": "/avatars/28e0ba07ca26056833e1197a6aa0d14f.svg",
            "isPro": false,
            "fullname": "Haoran Zhang",
            "user": "zzzhr97",
            "type": "user"
          },
          "name": "Haoran Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:23.170Z",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc66b",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc66c",
          "name": "Xuyang Hu",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc66d",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc66e",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc66f",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "68ccc6a13df9ac65e93dc670",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T09:08:53.000Z",
      "submittedOnDailyAt": "2025-09-19T02:48:26.705Z",
      "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
      "submittedOnDailyBy": {
        "_id": "63f3502a520c14618925825a",
        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
        "isPro": false,
        "fullname": "Yafu Li",
        "user": "yaful",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
      "upvotes": 26,
      "discussionId": "68ccc6a13df9ac65e93dc671",
      "githubRepo": "https://github.com/zzzhr97/SpecBench",
      "ai_summary": "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.",
      "ai_keywords": [
        "Large language models",
        "specification alignment",
        "Test-Time Deliberation",
        "hierarchical reflection",
        "revision",
        "SpecBench",
        "safety-helpfulness trade-off",
        "Self-Refine",
        "TPO",
        "MoreThink"
      ]
    },
    "publishedAt": "2025-09-18T05:08:53.000Z",
    "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
    "summary": "Large language models (LLMs) are increasingly applied in diverse real-world\nscenarios, each governed by bespoke behavioral and safety specifications (spec)\ncustom-tailored by users or organizations. These spec, categorized into\nsafety-spec and behavioral-spec, vary across scenarios and evolve with changing\npreferences and requirements. We formalize this challenge as specification\nalignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec\nfrom both behavioral and safety perspectives. To address this challenge, we\npropose Align3, a lightweight method that employs Test-Time Deliberation (TTD)\nwith hierarchical reflection and revision to reason over the specification\nboundaries. We further present SpecBench, a unified benchmark for measuring\nspecification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts.\nExperiments on 15 reasoning and 18 instruct models with several TTD methods,\nincluding Self-Refine, TPO, and MoreThink, yield three key findings: (i)\ntest-time deliberation enhances specification alignment; (ii) Align3 advances\nthe safety-helpfulness trade-off frontier with minimal overhead; (iii)\nSpecBench effectively reveals alignment gaps. These results highlight the\npotential of test-time deliberation as an effective strategy for reasoning over\nthe real-world specification boundaries.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f3502a520c14618925825a",
      "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
      "fullname": "Yafu Li",
      "name": "yaful",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15185",
      "authors": [
        {
          "_id": "68ccc7243df9ac65e93dc679",
          "user": {
            "_id": "6662a450b1fff5575fdf0fbd",
            "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
            "isPro": false,
            "fullname": "Xiaoyu Yue",
            "user": "YueXY233",
            "type": "user"
          },
          "name": "Xiaoyu Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:19.187Z",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67a",
          "user": {
            "_id": "64b7aa374df206a3ed1947d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7aa374df206a3ed1947d2/Ostk72ehOR6yUX-PhUvyQ.jpeg",
            "isPro": false,
            "fullname": "wzd",
            "user": "GoodEnough",
            "type": "user"
          },
          "name": "Zidong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T07:00:14.082Z",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67c",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67d",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67e",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc67f",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "68ccc7243df9ac65e93dc680",
          "name": "Luping Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:47:40.000Z",
      "submittedOnDailyAt": "2025-09-19T01:31:06.584Z",
      "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
      "submittedOnDailyBy": {
        "_id": "6662a450b1fff5575fdf0fbd",
        "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
        "isPro": false,
        "fullname": "Xiaoyu Yue",
        "user": "YueXY233",
        "type": "user"
      },
      "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
      "upvotes": 21,
      "discussionId": "68ccc7253df9ac65e93dc681",
      "ai_summary": "Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.",
      "ai_keywords": [
        "autoregressive models",
        "next-token prediction",
        "high-level visual semantics",
        "local and conditional dependence",
        "inter-step semantic inconsistency",
        "spatial invariance deficiency",
        "self-supervised objectives",
        "Self-guided Training for AutoRegressive models",
        "ST-AR",
        "FID improvement",
        "LlamaGen-L",
        "LlamaGen-XL"
      ]
    },
    "publishedAt": "2025-09-18T13:47:40.000Z",
    "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
    "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6662a450b1fff5575fdf0fbd",
      "avatarUrl": "/avatars/2a6065269f1980213625a9cfd8d42fbd.svg",
      "fullname": "Xiaoyu Yue",
      "name": "YueXY233",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.13160",
      "authors": [
        {
          "_id": "68ccb6e23df9ac65e93dc60d",
          "name": "Liang Hu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc60e",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc60f",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc610",
          "name": "Yanle Ren",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc611",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc612",
          "name": "Kaiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc613",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc614",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc615",
          "name": "Tianci He",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc616",
          "name": "Fei Hu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc617",
          "name": "Yali Liao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc618",
          "name": "Zaiyuan Wang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc619",
          "name": "Chenghao Yang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61a",
          "name": "Qianyu Yang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61b",
          "name": "Mingren Yin",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61c",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61e",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61f",
          "name": "Xiying Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc620",
          "name": "Zhenwei Zhu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc621",
          "name": "Hongseok Namkoong",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc622",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc623",
          "name": "Yuwen Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T15:13:13.000Z",
      "submittedOnDailyAt": "2025-09-19T00:20:55.990Z",
      "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
      "upvotes": 11,
      "discussionId": "68ccb6e23df9ac65e93dc624",
      "ai_summary": "FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.",
      "ai_keywords": [
        "LLM-based agents",
        "search proficiency",
        "knowledge-grounded reasoning",
        "FinSearchComp",
        "Time-Sensitive Data Fetching",
        "Simple Historical Lookup",
        "Complex Historical Investigation",
        "web search",
        "financial plugins"
      ]
    },
    "publishedAt": "2025-09-16T11:13:13.000Z",
    "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
    "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 67
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15212",
      "authors": [
        {
          "_id": "68ccb5443df9ac65e93dc5e8",
          "name": "Yuming Jiang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5e9",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ea",
          "name": "Shengke Xue",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5eb",
          "name": "Yaxi Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ec",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ed",
          "name": "Sicong Leng",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ee",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ef",
          "name": "Jiayan Guo",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f0",
          "name": "Kexiang Wang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f1",
          "name": "Mingxiu Chen",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f2",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f3",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f4",
          "name": "Xin Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:58:02.000Z",
      "submittedOnDailyAt": "2025-09-19T00:13:55.962Z",
      "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
      "upvotes": 9,
      "discussionId": "68ccb5453df9ac65e93dc5f5",
      "githubRepo": "https://github.com/alibaba-damo-academy/RynnVLA-001",
      "ai_summary": "RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.",
      "ai_keywords": [
        "Ego-Centric Video Generative Pretraining",
        "Human-Centric Trajectory-Aware Modeling",
        "Image-to-Video model",
        "keypoint trajectories",
        "ActionVAE",
        "variational autoencoder",
        "latent embeddings",
        "VLA model"
      ],
      "githubStars": 169
    },
    "publishedAt": "2025-09-18T13:58:02.000Z",
    "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14476",
      "authors": [
        {
          "_id": "68ccb6a63df9ac65e93dc603",
          "name": "Jiasen Lu",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc604",
          "name": "Liangchen Song",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc605",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc606",
          "name": "Byeongjoo Ahn",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc607",
          "name": "Yanjun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc608",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc609",
          "name": "Afshin Dehghan",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc60a",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T23:11:18.000Z",
      "submittedOnDailyAt": "2025-09-19T00:19:42.928Z",
      "title": "AToken: A Unified Tokenizer for Vision",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
      "upvotes": 6,
      "discussionId": "68ccb6a63df9ac65e93dc60b",
      "ai_summary": "AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.",
      "ai_keywords": [
        "unified visual tokenizer",
        "4D latent space",
        "pure transformer architecture",
        "4D rotary position embeddings",
        "adversarial-free training objective",
        "perceptual loss",
        "Gram matrix loss",
        "progressive training curriculum",
        "continuous latent tokens",
        "discrete latent tokens",
        "rFID",
        "ImageNet accuracy",
        "rFVD",
        "MSRVTT retrieval",
        "PSNR",
        "classification accuracy",
        "visual generation tasks",
        "text-to-video generation",
        "image-to-3D synthesis",
        "multimodal LLMs"
      ]
    },
    "publishedAt": "2025-09-17T19:11:18.000Z",
    "title": "AToken: A Unified Tokenizer for Vision",
    "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14476.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15130",
      "authors": [
        {
          "_id": "68ccb5de3df9ac65e93dc5fc",
          "name": "Chenxi Song",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5fd",
          "name": "Yanming Yang",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5fe",
          "name": "Tong Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5ff",
          "name": "Ruibo Li",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc600",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T16:40:47.000Z",
      "submittedOnDailyAt": "2025-09-19T00:16:15.925Z",
      "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
      "upvotes": 5,
      "discussionId": "68ccb5df3df9ac65e93dc601",
      "projectPage": "https://worldforge-agi.github.io/",
      "ai_summary": "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.",
      "ai_keywords": [
        "video diffusion models",
        "spatial intelligence tasks",
        "latent world priors",
        "controllability",
        "geometric inconsistency",
        "retraining",
        "fine-tuning",
        "WorldForge",
        "inference-time framework",
        "Intra-Step Recursive Refinement",
        "denoising step",
        "Flow-Gated Latent Fusion",
        "optical flow similarity",
        "Dual-Path Self-Corrective Guidance",
        "trajectory injection",
        "trajectory drift",
        "photorealistic content generation",
        "trajectory consistency",
        "visual fidelity",
        "plug-and-play paradigm",
        "generative priors"
      ]
    },
    "publishedAt": "2025-09-18T12:40:47.000Z",
    "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
    "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14638",
      "authors": [
        {
          "_id": "68ccb51f3df9ac65e93dc5dc",
          "user": {
            "_id": "67ade8d6d53a41cd62ee300c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ade8d6d53a41cd62ee300c/ThE7nKEkVLmDxq_flbWdj.png",
            "isPro": false,
            "fullname": "Li",
            "user": "Mingsong07",
            "type": "user"
          },
          "name": "Mingsong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:48:51.131Z",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5dd",
          "name": "Lin Liu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5de",
          "name": "Hongjun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5df",
          "name": "Haoxing Chen",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e0",
          "name": "Xijun Gu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e1",
          "name": "Shizhan Liu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e2",
          "name": "Dong Gong",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e3",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e4",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e5",
          "name": "Jianguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T05:33:38.000Z",
      "submittedOnDailyAt": "2025-09-19T00:13:05.587Z",
      "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
      "upvotes": 4,
      "discussionId": "68ccb5203df9ac65e93dc5e6",
      "projectPage": "https://huggingface.co/datasets/inclusionAI/MultiEdit",
      "ai_summary": "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.",
      "ai_keywords": [
        "instruction-based image editing",
        "IBIE",
        "editing tasks",
        "sample counts",
        "noisy image-caption pairs",
        "biases",
        "dataset construction",
        "editing types",
        "style transfer",
        "semantic operations",
        "person reference editing",
        "in-image text editing",
        "multi-modal large language models",
        "MLLMs",
        "visual-adaptive editing instructions",
        "high-fidelity edited images",
        "fine-tuning",
        "foundational open-source models",
        "MultiEdit-Train",
        "MultiEdit-Test benchmark",
        "standard editing benchmark"
      ]
    },
    "publishedAt": "2025-09-18T01:33:38.000Z",
    "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
    "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15178",
      "authors": [
        {
          "_id": "68ccc1663df9ac65e93dc664",
          "name": "Zaiquan Yang",
          "hidden": false
        },
        {
          "_id": "68ccc1663df9ac65e93dc665",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68ccc1663df9ac65e93dc666",
          "name": "Gerhard Hancke",
          "hidden": false
        },
        {
          "_id": "68ccc1663df9ac65e93dc667",
          "name": "Rynson W. H. Lau",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:35:50.000Z",
      "submittedOnDailyAt": "2025-09-19T01:09:43.882Z",
      "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
      "submittedOnDailyBy": {
        "_id": "6351463b8445bbe32e944f6c",
        "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
        "isPro": false,
        "fullname": "Yuhao Liu",
        "user": "LeoLau",
        "type": "user"
      },
      "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as grounding tokens,\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(e.g., attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
      "upvotes": 2,
      "discussionId": "68ccc1673df9ac65e93dc668",
      "githubRepo": "https://github.com/zaiquanyang/LLaVA_Next_STVG",
      "ai_summary": "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.",
      "ai_keywords": [
        "spatio-temporal video grounding",
        "multimodal large language models",
        "zero-shot solution",
        "grounding tokens",
        "decomposed spatio-temporal highlighting",
        "temporal-augmented assembling",
        "logit-guided re-attention",
        "latent variables",
        "spatial prompts",
        "temporal prompts",
        "temporal consistency"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-18T13:35:50.000Z",
    "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
    "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as grounding tokens,\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(e.g., attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6351463b8445bbe32e944f6c",
      "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
      "fullname": "Yuhao Liu",
      "name": "LeoLau",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14977",
      "authors": [
        {
          "_id": "68ccaff53df9ac65e93dc5ab",
          "user": {
            "_id": "6576dc779091da7dc69a0acd",
            "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
            "isPro": false,
            "fullname": "chaoyinshe",
            "user": "chaoyinshe",
            "type": "user"
          },
          "name": "Chaoyin She",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-19T06:49:12.488Z",
          "hidden": false
        },
        {
          "_id": "68ccaff53df9ac65e93dc5ac",
          "name": "Ruifang Lu",
          "hidden": false
        },
        {
          "_id": "68ccaff53df9ac65e93dc5ad",
          "name": "Lida Chen",
          "hidden": false
        },
        {
          "_id": "68ccaff53df9ac65e93dc5ae",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "68ccaff53df9ac65e93dc5af",
          "name": "Qinghua Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T14:07:53.000Z",
      "submittedOnDailyAt": "2025-09-19T05:22:49.128Z",
      "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
      "submittedOnDailyBy": {
        "_id": "6576dc779091da7dc69a0acd",
        "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
        "isPro": false,
        "fullname": "chaoyinshe",
        "user": "chaoyinshe",
        "type": "user"
      },
      "summary": "Ultrasound imaging has become the preferred imaging modality for early cancer\nscreening due to its advantages of non-ionizing radiation, low cost, and\nreal-time imaging capabilities. However, conventional ultrasound diagnosis\nheavily relies on physician expertise, presenting challenges of high\nsubjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer\npromising solutions for this issue, but existing general-purpose models\ndemonstrate limited knowledge in ultrasound medical tasks, with poor\ngeneralization in multi-organ lesion recognition and low efficiency across\nmulti-task diagnostics. To address these limitations, we propose EchoVLM, a\nvision-language model specifically designed for ultrasound medical imaging. The\nmodel employs a Mixture of Experts (MoE) architecture trained on data spanning\nseven anatomical regions. This design enables the model to perform multiple\ntasks, including ultrasound report generation, diagnosis and visual\nquestion-answering (VQA). The experimental results demonstrated that EchoVLM\nachieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and\nROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report\ngeneration task. These findings suggest that EchoVLM has substantial potential\nto enhance diagnostic accuracy in ultrasound imaging, thereby providing a\nviable technical solution for future clinical applications. Source code and\nmodel weights are available at https://github.com/Asunatan/EchoVLM.",
      "upvotes": 0,
      "discussionId": "68ccaff53df9ac65e93dc5b0",
      "githubRepo": "https://github.com/Asunatan/EchoVLM",
      "ai_summary": "EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.",
      "ai_keywords": [
        "vision-language models",
        "Mixture of Experts",
        "ultrasound report generation",
        "diagnosis",
        "visual question-answering",
        "BLEU-1 scores",
        "ROUGE-1 scores"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-18T10:07:53.000Z",
    "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
    "summary": "Ultrasound imaging has become the preferred imaging modality for early cancer\nscreening due to its advantages of non-ionizing radiation, low cost, and\nreal-time imaging capabilities. However, conventional ultrasound diagnosis\nheavily relies on physician expertise, presenting challenges of high\nsubjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer\npromising solutions for this issue, but existing general-purpose models\ndemonstrate limited knowledge in ultrasound medical tasks, with poor\ngeneralization in multi-organ lesion recognition and low efficiency across\nmulti-task diagnostics. To address these limitations, we propose EchoVLM, a\nvision-language model specifically designed for ultrasound medical imaging. The\nmodel employs a Mixture of Experts (MoE) architecture trained on data spanning\nseven anatomical regions. This design enables the model to perform multiple\ntasks, including ultrasound report generation, diagnosis and visual\nquestion-answering (VQA). The experimental results demonstrated that EchoVLM\nachieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and\nROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report\ngeneration task. These findings suggest that EchoVLM has substantial potential\nto enhance diagnostic accuracy in ultrasound imaging, thereby providing a\nviable technical solution for future clinical applications. Source code and\nmodel weights are available at https://github.com/Asunatan/EchoVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576dc779091da7dc69a0acd",
      "avatarUrl": "/avatars/dd825a1308e687b23a349ccb64f9a91a.svg",
      "fullname": "chaoyinshe",
      "name": "chaoyinshe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.06482",
      "authors": [
        {
          "_id": "68cbb3b45a7803ff3be42ec2",
          "user": {
            "_id": "68cbaffdbba5bcf511763e56",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
            "isPro": false,
            "fullname": "Zhongxiang Xie",
            "user": "zx-Xie",
            "type": "user"
          },
          "name": "Zhongxiang Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-18T13:25:51.321Z",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec3",
          "name": "Shuangxi Miao",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec4",
          "name": "Yuhan Jiang",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec5",
          "name": "Zhewei Zhang",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec6",
          "name": "Jing Yao",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec7",
          "name": "Xuecao Li",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec8",
          "name": "Jianxi Huang",
          "hidden": false
        },
        {
          "_id": "68cbb3b45a7803ff3be42ec9",
          "name": "Pedram Ghamisi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T09:46:33.000Z",
      "submittedOnDailyAt": "2025-09-19T01:25:17.604Z",
      "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
      "submittedOnDailyBy": {
        "_id": "68cbaffdbba5bcf511763e56",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
        "isPro": false,
        "fullname": "Zhongxiang Xie",
        "user": "zx-Xie",
        "type": "user"
      },
      "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.",
      "upvotes": 0,
      "discussionId": "68cbb3b45a7803ff3be42eca",
      "githubRepo": "https://github.com/zxXie-Air/FSG-Net",
      "ai_summary": "FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.",
      "ai_keywords": [
        "Frequency-Spatial Synergistic Gated Network",
        "FSG-Net",
        "Discrepancy-Aware Wavelet Interaction Module",
        "DAWIM",
        "Synergistic Temporal-Spatial Attention Module",
        "STSAM",
        "Lightweight Gated Fusion Unit",
        "LGFU",
        "change detection",
        "high-resolution remote sensing images",
        "Earth observation",
        "false alarms",
        "semantic gap",
        "high-level semantics",
        "shallow layers",
        "CDD",
        "GZ-CD",
        "LEVIR-CD",
        "F1-scores"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-08T05:46:33.000Z",
    "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
    "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68cbaffdbba5bcf511763e56",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FWcmzd2m3h4NXZp0VYenS.png",
      "fullname": "Zhongxiang Xie",
      "name": "zx-Xie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]