[
  {
    "paper": {
      "id": "2506.17450",
      "authors": [
        {
          "_id": "68620adf9e7509383d29ab98",
          "user": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "isPro": true,
            "fullname": "Jiacheng Chen",
            "user": "cccjc",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab99",
          "name": "Ramin Mehran",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9a",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9b",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9c",
          "name": "Sanghyun Woo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T19:38:34.000Z",
      "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
      "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
      "submittedOnDailyBy": {
        "_id": "655bca95360e4f90cb61ba83",
        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
        "isPro": true,
        "fullname": "Jiacheng Chen",
        "user": "cccjc",
        "type": "user"
      },
      "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
      "upvotes": 24,
      "discussionId": "68620adf9e7509383d29ab9d",
      "projectPage": "https://blenderfusion.github.io/",
      "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
      "ai_keywords": [
        "diffusion model",
        "source masking",
        "simulated object jittering"
      ]
    },
    "publishedAt": "2025-06-20T15:38:34.000Z",
    "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
    "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655bca95360e4f90cb61ba83",
      "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
      "fullname": "Jiacheng Chen",
      "name": "cccjc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21862",
      "authors": [
        {
          "_id": "6861eea79e7509383d29ab2f",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab30",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab31",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab32",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T02:29:58.000Z",
      "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
      "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
      "upvotes": 20,
      "discussionId": "6861eea89e7509383d29ab33",
      "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
      "ai_keywords": [
        "token compression strategy",
        "Semantic Connected Components (SCC)",
        "spatio-temporal token compression strategy",
        "video question answering",
        "long video understanding",
        "comprehensive multi-choice benchmarks"
      ]
    },
    "publishedAt": "2025-06-26T22:29:58.000Z",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21356",
      "authors": [
        {
          "_id": "6861fb7a9e7509383d29ab4b",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4c",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4d",
          "name": "Yi Jin",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab50",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab51",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab52",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab53",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab54",
          "name": "Weichao Chen",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab55",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab56",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab57",
          "name": "Shengjie Zhao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab58",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T15:09:21.000Z",
      "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
      "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": false,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
      "upvotes": 13,
      "discussionId": "6861fb7a9e7509383d29ab59",
      "projectPage": "https://vchitect.github.io/ShotBench-project/",
      "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
      "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "ShotBench",
        "QA pairs",
        "cinematic grammar",
        "fine-grained visual comprehension",
        "AI-assisted video generation",
        "ShotQA",
        "multimodal dataset",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "ShotVL",
        "AI-driven cinematic understanding",
        "state-of-the-art performance"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-26T11:09:21.000Z",
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
    "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20279",
      "authors": [
        {
          "_id": "686218679e7509383d29abb3",
          "name": "Changliang Xia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb4",
          "name": "Chengyou Jia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb5",
          "name": "Zhuohang Dang",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb6",
          "name": "Minnan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T09:40:50.000Z",
      "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
      "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
      "submittedOnDailyBy": {
        "_id": "6602548a68d519ed324b47c5",
        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
        "isPro": false,
        "fullname": "ChengyouJia",
        "user": "ChengyouJia",
        "type": "user"
      },
      "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
      "upvotes": 12,
      "discussionId": "686218689e7509383d29abb7",
      "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
      "githubRepo": "https://github.com/xcltql666/DenseDiT",
      "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
      "ai_keywords": [
        "dense prediction",
        "generative models",
        "visual priors",
        "parameter-reuse mechanism",
        "lightweight branches",
        "multi-scale context",
        "DenseWorld",
        "DenseDiT"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-25T05:40:50.000Z",
    "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
    "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21416",
      "authors": [
        {
          "_id": "685e084071131fa43be08acc",
          "user": {
            "_id": "6361dd166945df7441b893fa",
            "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
            "isPro": false,
            "fullname": "Bowen Chen ",
            "user": "chenbowen",
            "type": "user"
          },
          "name": "Bowen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acd",
          "name": "Mengyi Zhao",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ace",
          "name": "Haomiao Sun",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acf",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad0",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad1",
          "name": "Kang Du",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad2",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:04:16.000Z",
      "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
      "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
      "submittedOnDailyBy": {
        "_id": "6498038ece9190ebb8693034",
        "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
        "isPro": false,
        "fullname": "Zhao",
        "user": "Mengyi",
        "type": "user"
      },
      "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
      "upvotes": 11,
      "discussionId": "685e084071131fa43be08ad3",
      "projectPage": "https://bytedance.github.io/XVerse/",
      "githubRepo": "https://github.com/bytedance/XVerse",
      "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "text-to-image generation",
        "multi-subject controlled generation",
        "reference images",
        "token-specific text-stream modulation",
        "image latents",
        "multi-subject image synthesis",
        "semantic attributes"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-06-26T12:04:16.000Z",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6498038ece9190ebb8693034",
      "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
      "fullname": "Zhao",
      "name": "Mengyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22434",
      "authors": [
        {
          "_id": "686205ad9e7509383d29ab80",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab81",
          "name": "Mingkang Zhu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab82",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab83",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab84",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab85",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab86",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab87",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
      "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
      "upvotes": 7,
      "discussionId": "686205ad9e7509383d29ab88",
      "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
      "ai_keywords": [
        "Vision-Language Models",
        "self-supervised learning",
        "image triplets",
        "reasoning ability",
        "multi-image reasoning benchmarks",
        "general vision tasks"
      ]
    },
    "publishedAt": "2025-06-27T13:59:27.000Z",
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21656",
      "authors": [
        {
          "_id": "6861f2b89e7509383d29ab35",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab36",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab37",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab38",
          "name": "Xu Cao",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab39",
          "name": "Xiaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3a",
          "name": "Yixiao He",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3b",
          "name": "Wenming Ye",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3c",
          "name": "James Matthew Rehg",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3d",
          "name": "Ismini Lourentzou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T18:00:00.000Z",
      "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
      "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
      "upvotes": 5,
      "discussionId": "6861f2b99e7509383d29ab3e",
      "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
      "ai_keywords": [
        "vision-language models",
        "SpatialReasoner-R1",
        "Multi-Model Monte Carlo Tree Search",
        "M3CTS",
        "Long Chain-of-Thought",
        "LongCoT",
        "fine-grained Direct Preference Optimization",
        "fDPO",
        "segment-specific preference granularity",
        "descriptive grounding",
        "logical reasoning",
        "spatial reward mechanism",
        "visual consistency",
        "spatial grounding",
        "logical coherence",
        "SPATIALRGPT-Bench"
      ]
    },
    "publishedAt": "2025-06-26T14:00:00.000Z",
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19741",
      "authors": [
        {
          "_id": "686209869e7509383d29ab92",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab93",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab94",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab95",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:58:55.000Z",
      "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
      "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
      "upvotes": 3,
      "discussionId": "686209869e7509383d29ab96",
      "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
      "ai_keywords": [
        "diffusion distillation",
        "Noise Consistency Training",
        "NCT",
        "one-step generators",
        "adapter module",
        "noise consistency loss",
        "noise space",
        "conditional distribution",
        "generative modeling",
        "data-efficient",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-24T11:58:55.000Z",
    "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
    "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21411",
      "authors": [
        {
          "_id": "686237f69e7509383d29abe9",
          "user": {
            "_id": "64d5deb154bb9eb704f83122",
            "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
            "isPro": false,
            "fullname": "Yehui Tang",
            "user": "tangyehui",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abea",
          "name": "Xiaosong Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abeb",
          "user": {
            "_id": "64b78295479b934973e2c40e",
            "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Fangcheng2",
            "type": "user"
          },
          "name": "Fangcheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abec",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abed",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abee",
          "name": "Yaoyuan Wang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abef",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf1",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf2",
          "name": "Hui Zang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf3",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf4",
          "name": "Xiaojun Meng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf5",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf6",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf7",
          "name": "Binfan Zheng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf8",
          "name": "Can Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf9",
          "name": "Youliang Yan",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfa",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfb",
          "name": "Peifeng Qin",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfc",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfd",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfe",
          "user": {
            "_id": "658bdf7b925aadd43304f05c",
            "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
            "isPro": false,
            "fullname": "Yunhe Wang",
            "user": "MightyCrane",
            "type": "user"
          },
          "name": "Yunhe Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T16:40:21.000Z",
      "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
      "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
      "upvotes": 0,
      "discussionId": "686237f79e7509383d29abff",
      "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "Mixture of Grouped Experts (MoGE)",
        "large language models",
        "expert load balancing",
        "computational load",
        "inference phase",
        "sparse model",
        "Ascend NPUs",
        "system simulation",
        "speculative acceleration",
        "Dense models",
        "GLM-Z1-32B",
        "Qwen3-32B"
      ]
    },
    "publishedAt": "2025-05-27T12:40:21.000Z",
    "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 774
    },
    "isAuthorParticipating": false
  }
]