[
  {
    "paper": {
      "id": "2602.09877",
      "authors": [
        {
          "_id": "698c7abdeb12ea7453916869",
          "user": {
            "_id": "674006451d2302f6aa9b026d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png",
            "isPro": false,
            "fullname": "Chenxu Wang",
            "user": "xunyoyo",
            "type": "user"
          },
          "name": "Chenxu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-02-12T16:49:45.534Z",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686a",
          "name": "Chaozhuo Li",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686b",
          "name": "Songyang Liu",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686c",
          "name": "Zejian Chen",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686d",
          "name": "Jinyu Hou",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686e",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea745391686f",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916870",
          "name": "Litian Zhang",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916871",
          "name": "Qiwei Ye",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916872",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916873",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916874",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "698c7abdeb12ea7453916875",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T15:18:19.000Z",
      "submittedOnDailyAt": "2026-02-13T00:53:30.377Z",
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "submittedOnDailyBy": {
        "_id": "674006451d2302f6aa9b026d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png",
        "isPro": false,
        "fullname": "Chenxu Wang",
        "user": "xunyoyo",
        "type": "user"
      },
      "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.",
      "upvotes": 113,
      "discussionId": "698c7abdeb12ea7453916876",
      "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.",
      "ai_keywords": [
        "multi-agent systems",
        "large language models",
        "self-evolution",
        "safety alignment",
        "information-theoretic framework",
        "anthropic value distributions",
        "statistical blind spots",
        "self-evolving AI societies",
        "external oversight",
        "safety-preserving mechanisms"
      ]
    },
    "publishedAt": "2026-02-10T10:18:19.000Z",
    "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
    "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674006451d2302f6aa9b026d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png",
      "fullname": "Chenxu Wang",
      "name": "xunyoyo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.12125",
      "authors": [
        {
          "_id": "698e8f46cace060ff123ac51",
          "name": "Wenkai Yang",
          "hidden": false
        },
        {
          "_id": "698e8f46cace060ff123ac52",
          "name": "Weijie Liu",
          "hidden": false
        },
        {
          "_id": "698e8f46cace060ff123ac53",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "698e8f46cace060ff123ac54",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "698e8f46cace060ff123ac55",
          "name": "Saiyong Yang",
          "hidden": false
        },
        {
          "_id": "698e8f46cace060ff123ac56",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T16:14:29.000Z",
      "submittedOnDailyAt": "2026-02-13T00:24:06.722Z",
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
      "upvotes": 34,
      "discussionId": "698e8f46cace060ff123ac57",
      "githubRepo": "https://github.com/RUCBM/G-OPD",
      "githubRepoAddedBy": "user",
      "ai_summary": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.",
      "ai_keywords": [
        "on-policy distillation",
        "logit distribution",
        "dense KL-constrained RL",
        "reward scaling factor",
        "reward extrapolation",
        "reward correction",
        "teacher-student size pairings",
        "domain-specific RL",
        "strong-to-weak distillation"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2026-02-12T11:14:29.000Z",
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10934",
      "authors": [
        {
          "_id": "698d585765c0d15a6d1621fe",
          "user": {
            "_id": "66c893b2e51ba3009235b1c0",
            "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg",
            "isPro": false,
            "fullname": "yitian gong",
            "user": "fdugyt",
            "type": "user"
          },
          "name": "Yitian Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:26:16.742Z",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d1621ff",
          "name": "Kuangwei Chen",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162200",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162201",
          "name": "Xiaogui Yang",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162202",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162203",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162204",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162205",
          "name": "Mingshu Chen",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162206",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162207",
          "name": "Qingyuan Cheng",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162208",
          "name": "Shimin Li",
          "hidden": false
        },
        {
          "_id": "698d585765c0d15a6d162209",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T15:13:27.000Z",
      "submittedOnDailyAt": "2026-02-13T01:47:00.074Z",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "submittedOnDailyBy": {
        "_id": "66c893b2e51ba3009235b1c0",
        "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg",
        "isPro": false,
        "fullname": "yitian gong",
        "user": "fdugyt",
        "type": "user"
      },
      "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "upvotes": 33,
      "discussionId": "698d585765c0d15a6d16220a",
      "githubRepo": "https://github.com/OpenMOSS/MOSS-Audio-Tokenizer",
      "githubRepoAddedBy": "user",
      "ai_summary": "A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.",
      "ai_keywords": [
        "audio tokenization",
        "causal audio tokenizer",
        "Transformer architecture",
        "end-to-end learning",
        "quantizer",
        "encoder",
        "decoder",
        "discrete audio tokens",
        "autoregressive TTS",
        "automatic speech recognition",
        "codec",
        "pre-trained encoders",
        "semantic distillation",
        "heterogeneous CNN-based architectures"
      ],
      "githubStars": 45,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-02-11T10:13:27.000Z",
    "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
    "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c893b2e51ba3009235b1c0",
      "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg",
      "fullname": "yitian gong",
      "name": "fdugyt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.12099",
      "authors": [
        {
          "_id": "698e8ff2cace060ff123ac59",
          "name": "GigaBrain Team",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5a",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5b",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5c",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5d",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5e",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac5f",
          "name": "Jie Li",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac60",
          "name": "Jindi Lv",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac61",
          "name": "Jingyu Liu",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac62",
          "name": "Lv Feng",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac63",
          "name": "Mingming Yu",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac64",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac65",
          "name": "Qiuping Deng",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac66",
          "name": "Tianze Liu",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac67",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac68",
          "name": "Xinze Chen",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac69",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6a",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6b",
          "name": "Yifan Li",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6c",
          "name": "Yifei Nie",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6d",
          "name": "Yilong Li",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6e",
          "name": "Yukun Zhou",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac6f",
          "name": "Yun Ye",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac70",
          "name": "Zhichao Liu",
          "hidden": false
        },
        {
          "_id": "698e8ff2cace060ff123ac71",
          "name": "Zheng Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6426616ea5ec4a5cbc535634/w6OBVRe04BfXtAnLDzr3o.mp4"
      ],
      "publishedAt": "2026-02-12T15:55:19.000Z",
      "submittedOnDailyAt": "2026-02-13T00:24:14.150Z",
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6426616ea5ec4a5cbc535634",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/5IfSFYd9QOxz8K9QmBCst.png",
        "isPro": false,
        "fullname": "JeffWang",
        "user": "Jeff-Wang",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.",
      "upvotes": 27,
      "discussionId": "698e8ff2cace060ff123ac72",
      "projectPage": "https://gigabrain05m.github.io/",
      "githubRepo": "https://github.com/open-gigaai/giga-brain-0",
      "githubRepoAddedBy": "user",
      "ai_summary": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.",
      "ai_keywords": [
        "Vision-language-action models",
        "world models",
        "reinforcement learning",
        "cross-task adaptation",
        "RAMP",
        "RoboChallenge benchmark",
        "robotic manipulation"
      ],
      "githubStars": 2256,
      "organization": {
        "_id": "68d6587936e2de9610d9f5f0",
        "name": "open-gigaai",
        "fullname": "GigaAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
      }
    },
    "publishedAt": "2026-02-12T10:55:19.000Z",
    "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "summary": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6426616ea5ec4a5cbc535634/w6OBVRe04BfXtAnLDzr3o.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6426616ea5ec4a5cbc535634",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/5IfSFYd9QOxz8K9QmBCst.png",
      "fullname": "JeffWang",
      "name": "Jeff-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68d6587936e2de9610d9f5f0",
      "name": "open-gigaai",
      "fullname": "GigaAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12056",
      "authors": [
        {
          "_id": "698e8eb9cace060ff123ac42",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "698e8eb9cace060ff123ac43",
          "name": "Chenlong Deng",
          "hidden": false
        },
        {
          "_id": "698e8eb9cace060ff123ac44",
          "name": "Tongyu Wen",
          "hidden": false
        },
        {
          "_id": "698e8eb9cace060ff123ac45",
          "name": "Binyu Xie",
          "hidden": false
        },
        {
          "_id": "698e8eb9cace060ff123ac46",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T15:19:11.000Z",
      "submittedOnDailyAt": "2026-02-13T00:35:29.769Z",
      "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
      "submittedOnDailyBy": {
        "_id": "6710ac3fb4ee4920580a5f0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
        "isPro": false,
        "fullname": "Chenghao Zhang",
        "user": "SnowNation",
        "type": "user"
      },
      "summary": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
      "upvotes": 25,
      "discussionId": "698e8ebacace060ff123ac47",
      "githubRepo": "https://github.com/yxy-919/LawThinker-agent",
      "githubRepoAddedBy": "user",
      "ai_summary": "LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.",
      "ai_keywords": [
        "legal reasoning",
        "autonomous agent",
        "Explore-Verify-Memorize strategy",
        "DeepVerifier module",
        "knowledge accuracy",
        "fact-law relevance",
        "procedural compliance",
        "cross-round knowledge reuse",
        "dynamic judicial environments"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-02-12T10:19:11.000Z",
    "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
    "summary": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6710ac3fb4ee4920580a5f0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
      "fullname": "Chenghao Zhang",
      "name": "SnowNation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12280",
      "authors": [
        {
          "_id": "698ea022cace060ff123ae30",
          "name": "Huai-Hsun Cheng",
          "hidden": false
        },
        {
          "_id": "698ea022cace060ff123ae31",
          "name": "Siang-Ling Zhang",
          "hidden": false
        },
        {
          "_id": "698ea022cace060ff123ae32",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655f1770f74fa124d1172ec1/8bTk3z-6osRw_udarMcy2.mp4"
      ],
      "publishedAt": "2026-02-12T18:59:54.000Z",
      "submittedOnDailyAt": "2026-02-13T02:04:47.660Z",
      "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
      "submittedOnDailyBy": {
        "_id": "655f1770f74fa124d1172ec1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f1770f74fa124d1172ec1/bdYocZ1qN50CAfb2z2YLA.png",
        "isPro": false,
        "fullname": "Jie-Ying Lee",
        "user": "jayinnn",
        "type": "user"
      },
      "summary": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/",
      "upvotes": 18,
      "discussionId": "698ea023cace060ff123ae33",
      "projectPage": "https://stroke-of-surprise.github.io/",
      "githubRepo": "https://github.com/stroke-of-surprise/Stroke-Of-Surprise",
      "githubRepoAddedBy": "user",
      "ai_summary": "Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.",
      "ai_keywords": [
        "vector sketching",
        "semantic transformation",
        "Stroke of Surprise",
        "generative framework",
        "dual-branch Score Distillation Sampling",
        "sequential optimization",
        "structural subspace",
        "Overlay Loss",
        "visual anagrams"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "63e39e6499a032b1c950403d",
        "name": "NYCU",
        "fullname": "National Yang Ming Chiao Tung University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
      }
    },
    "publishedAt": "2026-02-12T13:59:54.000Z",
    "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
    "summary": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655f1770f74fa124d1172ec1/8bTk3z-6osRw_udarMcy2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12280.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655f1770f74fa124d1172ec1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f1770f74fa124d1172ec1/bdYocZ1qN50CAfb2z2YLA.png",
      "fullname": "Jie-Ying Lee",
      "name": "jayinnn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63e39e6499a032b1c950403d",
      "name": "NYCU",
      "fullname": "National Yang Ming Chiao Tung University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12153",
      "authors": [
        {
          "_id": "698e9c44cace060ff123ae0f",
          "name": "Sicheng Feng",
          "hidden": false
        },
        {
          "_id": "698e9c44cace060ff123ae10",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "698e9c44cace060ff123ae11",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "698e9c44cace060ff123ae12",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "698e9c44cace060ff123ae13",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T16:35:05.000Z",
      "submittedOnDailyAt": "2026-02-13T01:11:17.731Z",
      "title": "dVoting: Fast Voting for dLLMs",
      "submittedOnDailyBy": {
        "_id": "67a4a26d5e65aa63c6d30e68",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
        "isPro": false,
        "fullname": "Sicheng Feng",
        "user": "FSCCS",
        "type": "user"
      },
      "summary": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
      "upvotes": 15,
      "discussionId": "698e9c44cace060ff123ae14",
      "githubRepo": "https://github.com/fscdc/dVoting",
      "githubRepoAddedBy": "user",
      "ai_summary": "Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive modeling",
        "parallel test-time scaling",
        "token predictions",
        "iterative refinement",
        "consistency analysis",
        "voting technique"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2026-02-12T11:35:05.000Z",
    "title": "dVoting: Fast Voting for dLLMs",
    "summary": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12153.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a4a26d5e65aa63c6d30e68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
      "fullname": "Sicheng Feng",
      "name": "FSCCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11748",
      "authors": [
        {
          "_id": "698ec188cace060ff123aed7",
          "name": "Futing Wang",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aed8",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aed9",
          "name": "Yun Luo",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aeda",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aedb",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aedc",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aedd",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aede",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "698ec188cace060ff123aedf",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T09:24:32.000Z",
      "submittedOnDailyAt": "2026-02-13T03:46:21.481Z",
      "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "644915c5e87a77e872e61350",
        "avatarUrl": "/avatars/46ba7bdf04ad4c1b0ad79155010dc684.svg",
        "isPro": false,
        "fullname": "Luo",
        "user": "ramiroluo",
        "type": "user"
      },
      "summary": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.\n  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.\n  To bridge this gap, we propose Length-Incentivized Exploration(\\method).\n  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.\n  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.\n  As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
      "upvotes": 13,
      "discussionId": "698ec188cace060ff123aee0",
      "githubRepo": "https://github.com/LINs-lab/LIE",
      "githubRepoAddedBy": "user",
      "ai_summary": "Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.",
      "ai_keywords": [
        "In-Context Exploration",
        "State Coverage theory",
        "Shallow Exploration Trap",
        "Length-Incentivized Exploration",
        "autoregressive generation",
        "redundancy penalty",
        "state coverage"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "643cb10025681c3afab0f1a6",
        "name": "Westlake-University",
        "fullname": "Westlake University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
      }
    },
    "publishedAt": "2026-02-12T04:24:32.000Z",
    "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
    "summary": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.\n  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.\n  To bridge this gap, we propose Length-Incentivized Exploration(\\method).\n  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.\n  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.\n  As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11748.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644915c5e87a77e872e61350",
      "avatarUrl": "/avatars/46ba7bdf04ad4c1b0ad79155010dc684.svg",
      "fullname": "Luo",
      "name": "ramiroluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "643cb10025681c3afab0f1a6",
      "name": "Westlake-University",
      "fullname": "Westlake University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05548",
      "authors": [
        {
          "_id": "698e8c95cace060ff123ac11",
          "name": "Zhiqi Yu",
          "hidden": false
        },
        {
          "_id": "698e8c95cace060ff123ac12",
          "name": "Zhangquan Chen",
          "hidden": false
        },
        {
          "_id": "698e8c95cace060ff123ac13",
          "name": "Mengting Liu",
          "hidden": false
        },
        {
          "_id": "698e8c95cace060ff123ac14",
          "name": "Heye Zhang",
          "hidden": false
        },
        {
          "_id": "698e8c95cace060ff123ac15",
          "name": "Liangqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T11:07:14.000Z",
      "submittedOnDailyAt": "2026-02-13T00:01:39.201Z",
      "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
      "submittedOnDailyBy": {
        "_id": "663058bc2653ec94f4a6235f",
        "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
        "isPro": false,
        "fullname": "Liangqiong Qu",
        "user": "Liangqiong-QU",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
      "upvotes": 10,
      "discussionId": "698e8c95cace060ff123ac16",
      "githubRepo": "https://github.com/HKU-HealthAI/A-GRAE",
      "githubRepoAddedBy": "user",
      "ai_summary": "Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "GRPO",
        "Group Relative Advantage Estimation",
        "GRAE",
        "asymmetric suppression",
        "curriculum learning",
        "sample-difficulty focus",
        "exploration incentives",
        "large language models",
        "multi-modal large language models"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2026-02-05T06:07:14.000Z",
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663058bc2653ec94f4a6235f",
      "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
      "fullname": "Liangqiong Qu",
      "name": "Liangqiong-QU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12092",
      "authors": [
        {
          "_id": "698e9081cace060ff123ac81",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac82",
          "name": "Jiaxuan Guo",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac83",
          "name": "Lijun Li",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac84",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac85",
          "name": "Sujin Chen",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac86",
          "name": "Guanxu Chen",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac87",
          "name": "Zhijie Zheng",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac88",
          "name": "Qihao Lin",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac89",
          "name": "Lewen Yan",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8a",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8b",
          "name": "Yijin Zhou",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8c",
          "name": "Yuyao Wu",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8d",
          "name": "Shaoxiong Guo",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8e",
          "name": "Tianyi Du",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac8f",
          "name": "Jingyi Yang",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac90",
          "name": "Xuhao Hu",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac91",
          "name": "Ziqi Miao",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac92",
          "name": "Xiaoya Lu",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac93",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "698e9081cace060ff123ac94",
          "name": "Xia Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T15:43:14.000Z",
      "submittedOnDailyAt": "2026-02-13T03:54:07.740Z",
      "title": "DeepSight: An All-in-One LM Safety Toolkit",
      "submittedOnDailyBy": {
        "_id": "641d3efac3983aa9491677b9",
        "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg",
        "isPro": false,
        "fullname": "Lijun Li",
        "user": "adwardlee",
        "type": "user"
      },
      "summary": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "upvotes": 8,
      "discussionId": "698e9081cace060ff123ac95",
      "projectPage": "https://github.com/AI45Lab/DeepScan/",
      "githubRepo": "https://github.com/AI45Lab/DeepSafe",
      "githubRepoAddedBy": "user",
      "ai_summary": "DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal Large Language Models",
        "safety evaluation",
        "safety diagnosis",
        "safety alignment",
        "DeepSafe",
        "DeepScan"
      ],
      "githubStars": 31
    },
    "publishedAt": "2026-02-12T10:43:14.000Z",
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "summary": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "641d3efac3983aa9491677b9",
      "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg",
      "fullname": "Lijun Li",
      "name": "adwardlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09070",
      "authors": [
        {
          "_id": "698ebb55cace060ff123aec0",
          "name": "Yufan Wen",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec1",
          "name": "Zhaocheng Liu",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec2",
          "name": "YeGuo Hua",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec3",
          "name": "Ziyi Guo",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec4",
          "name": "Lihua Zhang",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec5",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "698ebb55cace060ff123aec6",
          "name": "Jian Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T09:39:42.000Z",
      "submittedOnDailyAt": "2026-02-13T03:23:58.688Z",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "submittedOnDailyBy": {
        "_id": "633e570be7d5ce7bfe037a53",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
        "isPro": false,
        "fullname": "Zhaocheng Liu",
        "user": "zhaocheng",
        "type": "user"
      },
      "summary": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "upvotes": 8,
      "discussionId": "698ebb55cace060ff123aec7",
      "ai_summary": "NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.",
      "ai_keywords": [
        "Vision-Language Models",
        "Valence-Arousal trajectories",
        "Dual-Branch Injection",
        "Global Semantic Anchor",
        "Token-Level Affective Adapter",
        "residual injection",
        "overfitting risks"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-02-09T04:39:42.000Z",
    "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
    "summary": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e570be7d5ce7bfe037a53",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
      "fullname": "Zhaocheng Liu",
      "name": "zhaocheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11731",
      "authors": [
        {
          "_id": "698eb9d9cace060ff123aea4",
          "name": "Jingxuan Wei",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aea5",
          "name": "Honghao He",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aea6",
          "name": "Caijun Jia",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aea7",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aea8",
          "name": "Zheng Sun",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aea9",
          "name": "Yuhang Xu",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aeaa",
          "name": "Yuanyuan Lin",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aeab",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aeac",
          "name": "Yuchen Wu",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aead",
          "name": "Bihui Yu",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aeae",
          "name": "Xiangxiang Zhang",
          "hidden": false
        },
        {
          "_id": "698eb9d9cace060ff123aeaf",
          "name": "Cheng Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T08:54:02.000Z",
      "submittedOnDailyAt": "2026-02-13T05:37:54.762Z",
      "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
      "submittedOnDailyBy": {
        "_id": "64be296a46cc3cdfbb057f7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg",
        "isPro": false,
        "fullname": "Cheng Tan",
        "user": "chengtan9907",
        "type": "user"
      },
      "summary": "Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.",
      "upvotes": 4,
      "discussionId": "698eb9dacace060ff123aeb0",
      "ai_summary": "Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.",
      "ai_keywords": [
        "multimodal large language models",
        "visual perception",
        "visual generation",
        "optical decompression",
        "visual tokens",
        "Domain-Specific Language",
        "visual algebra benchmark",
        "visual reasoning",
        "logical topology",
        "deterministic visual proofs"
      ]
    },
    "publishedAt": "2026-02-12T03:54:02.000Z",
    "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
    "summary": "Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11731.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64be296a46cc3cdfbb057f7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg",
      "fullname": "Cheng Tan",
      "name": "chengtan9907",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12205",
      "authors": [
        {
          "_id": "698ea0f9cace060ff123ae3a",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae3b",
          "name": "Ruihang Li",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae3c",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae3d",
          "name": "Chaofan Ma",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae3e",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae3f",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae40",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae41",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae42",
          "name": "Hongjian Liu",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae43",
          "name": "Zhixiong Zhang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae44",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae45",
          "name": "Tianhang Wang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae46",
          "name": "Zhenglin Cheng",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae47",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae48",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae49",
          "name": "Kaicheng Yu",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae4a",
          "name": "Jingjing Chen",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae4b",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae4c",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "698ea0f9cace060ff123ae4d",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T17:44:24.000Z",
      "submittedOnDailyAt": "2026-02-13T03:37:56.296Z",
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
      "upvotes": 3,
      "discussionId": "698ea0f9cace060ff123ae4e",
      "projectPage": "https://deepgenteam.github.io/",
      "githubRepo": "https://github.com/DeepGenTeam/DeepGen",
      "githubRepoAddedBy": "user",
      "ai_summary": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.",
      "ai_keywords": [
        "unified multimodal models",
        "image generation",
        "image editing",
        "parameter scale",
        "VLM layers",
        "DiT representations",
        "Stacked Channel Bridging",
        "think tokens",
        "data-centric training strategy",
        "alignment pre-training",
        "joint supervised fine-tuning",
        "reinforcement learning",
        "MR-GRPO",
        "generation quality",
        "human preferences",
        "visual artifacts"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "683ebd0d913d82e703e77286",
        "name": "sii-research",
        "fullname": "Shanghai Innovation Institute",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"
      }
    },
    "publishedAt": "2026-02-12T12:44:24.000Z",
    "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12205.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "683ebd0d913d82e703e77286",
      "name": "sii-research",
      "fullname": "Shanghai Innovation Institute",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11683",
      "authors": [
        {
          "_id": "698e905bcace060ff123ac79",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "698e905bcace060ff123ac7a",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "698e905bcace060ff123ac7b",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "698e905bcace060ff123ac7c",
          "name": "Haoliang Wang",
          "hidden": false
        },
        {
          "_id": "698e905bcace060ff123ac7d",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "698e905bcace060ff123ac7e",
          "name": "Saayan Mitra",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T08:01:01.000Z",
      "submittedOnDailyAt": "2026-02-13T01:01:30.987Z",
      "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
      "submittedOnDailyBy": {
        "_id": "6190ab805ca89a28e9f66873",
        "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
        "isPro": false,
        "fullname": "Xin Xu",
        "user": "XinXuNLPer",
        "type": "user"
      },
      "summary": "Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.",
      "upvotes": 3,
      "discussionId": "698e905bcace060ff123ac7f",
      "ai_summary": "ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.",
      "ai_keywords": [
        "latent reasoning",
        "continuous representations",
        "latent space",
        "discrete token space",
        "model confidence",
        "thinking trajectories",
        "soft embeddings",
        "reasoning efficiency",
        "inference-time routing",
        "ThinkRouter",
        "Pass@1",
        "explicit CoT",
        "latent reasoning baselines"
      ]
    },
    "publishedAt": "2026-02-12T03:01:01.000Z",
    "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
    "summary": "Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6190ab805ca89a28e9f66873",
      "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
      "fullname": "Xin Xu",
      "name": "XinXuNLPer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11541",
      "authors": [
        {
          "_id": "698e8b35cace060ff123ac03",
          "name": "Hanbing Liu",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac04",
          "name": "Chunhao Tian",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac05",
          "name": "Nan An",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac06",
          "name": "Ziyuan Wang",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac07",
          "name": "Pinyan Lu",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac08",
          "name": "Changyuan Yu",
          "hidden": false
        },
        {
          "_id": "698e8b35cace060ff123ac09",
          "name": "Qi Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T04:01:30.000Z",
      "submittedOnDailyAt": "2026-02-13T00:05:48.943Z",
      "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
      "submittedOnDailyBy": {
        "_id": "643f6c06b410b176e9a1bb76",
        "avatarUrl": "/avatars/3827c219a557e0b0ff5f51b04f28b0b4.svg",
        "isPro": false,
        "fullname": "HanbingLiu",
        "user": "leolhb",
        "type": "user"
      },
      "summary": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
      "upvotes": 3,
      "discussionId": "698e8b36cace060ff123ac0a",
      "ai_summary": "Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.",
      "ai_keywords": [
        "tool-augmented agents",
        "large language model",
        "sequential decision making",
        "context space",
        "stochastic tool executions",
        "planning",
        "hierarchical world model",
        "risk-calibrated cost"
      ],
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-02-11T23:01:30.000Z",
    "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
    "summary": "We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f6c06b410b176e9a1bb76",
      "avatarUrl": "/avatars/3827c219a557e0b0ff5f51b04f28b0b4.svg",
      "fullname": "HanbingLiu",
      "name": "leolhb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11337",
      "authors": [
        {
          "_id": "698e9615cace060ff123adc9",
          "name": "Yejin Kim",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adca",
          "name": "Wilbert Pumacay",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adcb",
          "name": "Omar Rayyan",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adcc",
          "name": "Max Argus",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adcd",
          "name": "Winson Han",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adce",
          "name": "Eli VanderBilt",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adcf",
          "name": "Jordi Salvador",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add0",
          "name": "Abhay Deshpande",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add1",
          "name": "Rose Hendrix",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add2",
          "name": "Snehal Jauhri",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add3",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add4",
          "name": "Nur Muhammad Mahi Shafiullah",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add5",
          "name": "Maya Guru",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add6",
          "name": "Ainaz Eftekhar",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add7",
          "name": "Karen Farley",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add8",
          "name": "Donovan Clay",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123add9",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adda",
          "name": "Arjun Guru",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123addb",
          "name": "Piper Wolters",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123addc",
          "name": "Alvaro Herrasti",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123addd",
          "name": "Ying-Chun Lee",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123adde",
          "name": "Georgia Chalvatzaki",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123addf",
          "name": "Yuchen Cui",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123ade0",
          "name": "Ali Farhadi",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123ade1",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "698e9615cace060ff123ade2",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Yxr-Vb4IoaqDv5GexlM2t.mp4"
      ],
      "publishedAt": "2026-02-11T20:16:31.000Z",
      "submittedOnDailyAt": "2026-02-13T00:42:09.045Z",
      "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, ho = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
      "upvotes": 3,
      "discussionId": "698e9616cace060ff123ade3",
      "projectPage": "https://allenai.org/blog/molmospaces",
      "githubRepo": "https://github.com/allenai/molmospaces",
      "githubRepoAddedBy": "user",
      "ai_summary": "MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.",
      "ai_keywords": [
        "robot policies",
        "embodied tasks",
        "sim-to-real correlation",
        "zero-shot policies",
        "prompt phrasing",
        "initial joint positions",
        "camera occlusion"
      ],
      "githubStars": 69,
      "organization": {
        "_id": "65e6310cc7738c6b88970c23",
        "name": "ai21labs",
        "fullname": "AI21",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"
      }
    },
    "publishedAt": "2026-02-11T15:16:31.000Z",
    "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
    "summary": "Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, ho = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Yxr-Vb4IoaqDv5GexlM2t.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11337.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65e6310cc7738c6b88970c23",
      "name": "ai21labs",
      "fullname": "AI21",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08194",
      "authors": [
        {
          "_id": "698e2bc1cace060ff123ab84",
          "user": {
            "_id": "67ab6e798f8b45f100fd1a61",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HA3mwJWthvIKBizC-PfZN.png",
            "isPro": false,
            "fullname": "Konstantinos Mitsides",
            "user": "kmitsides",
            "type": "user"
          },
          "name": "Konstantinos Mitsides",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T20:22:43.876Z",
          "hidden": false
        },
        {
          "_id": "698e2bc1cace060ff123ab85",
          "name": "Maxence Faldor",
          "hidden": false
        },
        {
          "_id": "698e2bc1cace060ff123ab86",
          "name": "Antoine Cully",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/wvhX89LfF8A34qtgeGtC7.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/414Kd8ujb5smeuM53HIWG.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/Gm1iXRYJNSYln1w7G_wj2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/jNdBH2xk9BrY9C3ZrZJZ5.png"
      ],
      "publishedAt": "2026-02-09T01:24:40.000Z",
      "submittedOnDailyAt": "2026-02-13T00:20:05.164Z",
      "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
      "submittedOnDailyBy": {
        "_id": "67ab6e798f8b45f100fd1a61",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HA3mwJWthvIKBizC-PfZN.png",
        "isPro": false,
        "fullname": "Konstantinos Mitsides",
        "user": "kmitsides",
        "type": "user"
      },
      "summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a 16% improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
      "upvotes": 3,
      "discussionId": "698e2bc1cace060ff123ab87",
      "projectPage": "https://konstantinosmitsides.github.io/dreaming-in-code",
      "githubRepo": "https://github.com/konstantinosmitsides/dreaming-in-code",
      "githubRepoAddedBy": "user",
      "ai_summary": "Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.",
      "ai_keywords": [
        "foundation models",
        "open-ended learning",
        "environment synthesis",
        "curriculum control",
        "long-horizon progression",
        "skill acquisition"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "650987fc2feb9570c5137ac2",
        "name": "ImperialCollegeLondon",
        "fullname": "Imperial College London",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
      }
    },
    "publishedAt": "2026-02-08T20:24:40.000Z",
    "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
    "summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a 16% improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/wvhX89LfF8A34qtgeGtC7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/414Kd8ujb5smeuM53HIWG.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/Gm1iXRYJNSYln1w7G_wj2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67ab6e798f8b45f100fd1a61/jNdBH2xk9BrY9C3ZrZJZ5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ab6e798f8b45f100fd1a61",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HA3mwJWthvIKBizC-PfZN.png",
      "fullname": "Konstantinos Mitsides",
      "name": "kmitsides",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "650987fc2feb9570c5137ac2",
      "name": "ImperialCollegeLondon",
      "fullname": "Imperial College London",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.12262",
      "authors": [
        {
          "_id": "698ebb22cace060ff123aeb2",
          "name": "Tunyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb3",
          "name": "Xinxi Zhang",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb4",
          "name": "Ligong Han",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb5",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb6",
          "name": "Xiaoxiao He",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb7",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb8",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeb9",
          "name": "Kai Xu",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aeba",
          "name": "Akash Srivastava",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aebb",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aebc",
          "name": "Vladimir Pavlovic",
          "hidden": false
        },
        {
          "_id": "698ebb22cace060ff123aebd",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b7a545c5a90c1aa84ca807/795Z1MGniQ8rMBwNCl24L.png"
      ],
      "publishedAt": "2026-02-12T18:52:35.000Z",
      "submittedOnDailyAt": "2026-02-13T03:21:43.394Z",
      "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
      "submittedOnDailyBy": {
        "_id": "66b7a545c5a90c1aa84ca807",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b7a545c5a90c1aa84ca807/_aJOGL5k-8-75cI9mo7Iy.png",
        "isPro": false,
        "fullname": "Tunyu Zhang",
        "user": "Tyrion279",
        "type": "user"
      },
      "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
      "upvotes": 2,
      "discussionId": "698ebb23cace060ff123aebe",
      "githubRepo": "https://github.com/Tyrion58/T3D",
      "githubRepoAddedBy": "user",
      "ai_summary": "A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.",
      "ai_keywords": [
        "diffusion large language models",
        "trajectory self-distillation",
        "self-distillation",
        "Direct Discriminative Optimization",
        "reverse-KL objective",
        "mode-seeking distillation",
        "generative trajectories",
        "few-step decoding",
        "text generation"
      ],
      "githubStars": 5
    },
    "publishedAt": "2026-02-12T13:52:35.000Z",
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b7a545c5a90c1aa84ca807/795Z1MGniQ8rMBwNCl24L.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b7a545c5a90c1aa84ca807",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b7a545c5a90c1aa84ca807/_aJOGL5k-8-75cI9mo7Iy.png",
      "fullname": "Tunyu Zhang",
      "name": "Tyrion279",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12164",
      "authors": [
        {
          "_id": "698ea97ecace060ff123ae78",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "698ea97ecace060ff123ae79",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "698ea97ecace060ff123ae7a",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "698ea97ecace060ff123ae7b",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "698ea97ecace060ff123ae7c",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698ea97ecace060ff123ae7d",
          "name": "Bo Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T16:46:00.000Z",
      "submittedOnDailyAt": "2026-02-13T02:04:10.210Z",
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "submittedOnDailyBy": {
        "_id": "667cf204268f6622dac71961",
        "avatarUrl": "/avatars/90e1928beb2a685e82e19758e4a6b7ae.svg",
        "isPro": false,
        "fullname": "shiyang",
        "user": "sY713",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
      "upvotes": 2,
      "discussionId": "698ea97ecace060ff123ae7e",
      "ai_summary": "Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.",
      "ai_keywords": [
        "large language models",
        "scientific reasoning",
        "co-evolving paradigms",
        "sparse supervision",
        "unsupervised learning",
        "verifier",
        "geometric reward mechanism",
        "consensus",
        "reliability",
        "diversity",
        "self-iteration",
        "scientific benchmarks",
        "scalability"
      ],
      "organization": {
        "_id": "690af7a885f71496ea396393",
        "name": "InternScience",
        "fullname": "Intern Science",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
      }
    },
    "publishedAt": "2026-02-12T11:46:00.000Z",
    "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "summary": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667cf204268f6622dac71961",
      "avatarUrl": "/avatars/90e1928beb2a685e82e19758e4a6b7ae.svg",
      "fullname": "shiyang",
      "name": "sY713",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "690af7a885f71496ea396393",
      "name": "InternScience",
      "fullname": "Intern Science",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12116",
      "authors": [
        {
          "_id": "698ec6ebcace060ff123aee2",
          "name": "Pinyi Zhang",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee3",
          "name": "Ting-En Lin",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee4",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee5",
          "name": "Jingyang Chen",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee6",
          "name": "Zongqi Wang",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee7",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee8",
          "name": "Ze Xu",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aee9",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aeea",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "698ec6ebcace060ff123aeeb",
          "name": "Yongbin Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T16:07:22.000Z",
      "submittedOnDailyAt": "2026-02-13T04:12:00.551Z",
      "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
      "submittedOnDailyBy": {
        "_id": "62e0ef42edb0462c8d51818d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
        "isPro": false,
        "fullname": "Ting-En Lin",
        "user": "tnlin",
        "type": "user"
      },
      "summary": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
      "upvotes": 2,
      "discussionId": "698ec6eccace060ff123aeec",
      "githubRepo": "https://github.com/Tongyi-ConvAI/Qwen-Character/tree/main/Character-GenRM",
      "githubRepoAddedBy": "user",
      "ai_summary": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.",
      "ai_keywords": [
        "personalized alignment",
        "large language models",
        "reinforcement learning",
        "reward models",
        "generative reward models",
        "test-time user-based scaling",
        "adaptive personas",
        "scoring rubrics",
        "user clustering",
        "user prototypes",
        "dual-granularity scaling",
        "preference signals",
        "generalization",
        "out-of-distribution dataset"
      ],
      "githubStars": 11,
      "organization": {
        "_id": "66f62545af8dfaaa2817d92d",
        "name": "Tongyi-ConvAI",
        "fullname": "Tongyi-ConvAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62e0ef42edb0462c8d51818d/0XudhGKJsc60h2nG462zf.jpeg"
      }
    },
    "publishedAt": "2026-02-12T11:07:22.000Z",
    "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
    "summary": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e0ef42edb0462c8d51818d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
      "fullname": "Ting-En Lin",
      "name": "tnlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66f62545af8dfaaa2817d92d",
      "name": "Tongyi-ConvAI",
      "fullname": "Tongyi-ConvAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62e0ef42edb0462c8d51818d/0XudhGKJsc60h2nG462zf.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11964",
      "authors": [
        {
          "_id": "698e946fcace060ff123acee",
          "name": "Romain Froger",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acef",
          "name": "Pierre Andrews",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf0",
          "name": "Matteo Bettini",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf1",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf2",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf3",
          "name": "Virginie Do",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf4",
          "name": "Emilien Garreau",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf5",
          "name": "Jean-Baptiste Gaya",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf6",
          "name": "Hugo Laurenon",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf7",
          "name": "Maxime Lecanu",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf8",
          "name": "Kunal Malkan",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acf9",
          "name": "Dheeraj Mekala",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acfa",
          "name": "Pierre Mnard",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acfb",
          "name": "Gerard Moreno-Torres Bertran",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acfc",
          "name": "Ulyana Piterbarg",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acfd",
          "name": "Mikhail Plekhanov",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acfe",
          "name": "Mathieu Rita",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123acff",
          "name": "Andrey Rusakov",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad00",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad01",
          "name": "Mengjue Wang",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad02",
          "name": "Ian Yu",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad03",
          "name": "Amine Benhalloum",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad04",
          "name": "Grgoire Mialon",
          "hidden": false
        },
        {
          "_id": "698e946fcace060ff123ad05",
          "name": "Thomas Scialom",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T13:58:27.000Z",
      "submittedOnDailyAt": "2026-02-13T00:33:23.603Z",
      "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.",
      "upvotes": 2,
      "discussionId": "698e946fcace060ff123ad06",
      "ai_summary": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.",
      "ai_keywords": [
        "large language model agents",
        "asynchronous environments",
        "temporal constraints",
        "multi-agent collaboration",
        "write-action verifier",
        "reinforcement learning from verifiable rewards",
        "pass@1",
        "sim2real gap",
        "Agents Research Environments platform"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2026-02-12T08:58:27.000Z",
    "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
    "summary": "We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11964.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11761",
      "authors": [
        {
          "_id": "698e93e8cace060ff123acbe",
          "name": "MiniCPM Team",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acbf",
          "name": "Wenhao An",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc0",
          "name": "Yingfa Chen",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc1",
          "name": "Yewei Fang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc2",
          "name": "Jiayi Li",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc3",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc4",
          "name": "Yaohui Li",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc5",
          "name": "Yishan Li",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc6",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc7",
          "name": "Biyuan Lin",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc8",
          "name": "Chuan Liu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acc9",
          "name": "Hezi Liu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acca",
          "name": "Siyuan Liu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123accb",
          "name": "Hongya Lyu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123accc",
          "name": "Yinxu Pan",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123accd",
          "name": "Shixin Ren",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acce",
          "name": "Xingyu Shen",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123accf",
          "name": "Zhou Su",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd0",
          "name": "Haojun Sun",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd1",
          "name": "Yangang Sun",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd2",
          "name": "Zhen Leng Thai",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd3",
          "name": "Xin Tian",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd4",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd5",
          "name": "Xiaorong Wang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd6",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd7",
          "name": "Bo Wu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd8",
          "name": "Xiaoyue Xu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acd9",
          "name": "Dong Xu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acda",
          "name": "Shuaikang Xue",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acdb",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acdc",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acdd",
          "name": "Jinqian Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acde",
          "name": "Letian Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acdf",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace0",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace1",
          "name": "Xinyuan Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace2",
          "name": "Zhu Zhang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace3",
          "name": "Hengyu Zhao",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace4",
          "name": "Jiacheng Zhao",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace5",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace6",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace7",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace8",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123ace9",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123acea",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "698e93e8cace060ff123aceb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T09:37:05.000Z",
      "submittedOnDailyAt": "2026-02-13T00:30:57.943Z",
      "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.",
      "upvotes": 2,
      "discussionId": "698e93e9cace060ff123acec",
      "githubRepo": "https://github.com/OpenBMB/MiniCPM",
      "githubRepoAddedBy": "user",
      "ai_summary": "MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.",
      "ai_keywords": [
        "large language models",
        "Transformer architecture",
        "sparse attention",
        "linear attention",
        "hybrid architecture",
        "layer selection algorithm",
        "hybrid positional encoding",
        "continual training framework",
        "inference speed",
        "sequence length",
        "token context"
      ],
      "githubStars": 8599,
      "organization": {
        "_id": "633fe81429b5a95f6e16e34a",
        "name": "openbmb",
        "fullname": "OpenBMB",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"
      }
    },
    "publishedAt": "2026-02-12T04:37:05.000Z",
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "summary": "The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11761.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "633fe81429b5a95f6e16e34a",
      "name": "openbmb",
      "fullname": "OpenBMB",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11636",
      "authors": [
        {
          "_id": "698e9502cace060ff123ad90",
          "name": "Changti Wu",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad91",
          "name": "Jiahuai Mao",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad92",
          "name": "Yuzhuo Miao",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad93",
          "name": "Shijie Lian",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad94",
          "name": "Bin Yu",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad95",
          "name": "Xiaopeng Lin",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad96",
          "name": "Cong Huang",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad97",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "698e9502cace060ff123ad98",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T06:38:49.000Z",
      "submittedOnDailyAt": "2026-02-13T00:36:06.374Z",
      "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
      "submittedOnDailyBy": {
        "_id": "67b55e66d454cc4d10d21cfd",
        "avatarUrl": "/avatars/3b18014fa7e603a5940175896f89372a.svg",
        "isPro": false,
        "fullname": "Changti Wu",
        "user": "MaplesWCT",
        "type": "user"
      },
      "summary": "Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at https://github.com/ChangtiWu/ScalSelect{ScalSelect}.",
      "upvotes": 2,
      "discussionId": "698e9502cace060ff123ad99",
      "githubRepo": "https://github.com/ChangtiWu/ScalSelect",
      "githubRepoAddedBy": "user",
      "ai_summary": "ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.",
      "ai_keywords": [
        "vision-language models",
        "data selection",
        "training-free",
        "linear-time complexity",
        "visual features",
        "instruction tokens",
        "dominant subspace",
        "importance scoring",
        "multimodal data selection"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "68896d3a716ee5bfb1428441",
        "name": "ZGCA",
        "fullname": "Zhongguancun Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
      }
    },
    "publishedAt": "2026-02-12T01:38:49.000Z",
    "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
    "summary": "Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at https://github.com/ChangtiWu/ScalSelect{ScalSelect}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67b55e66d454cc4d10d21cfd",
      "avatarUrl": "/avatars/3b18014fa7e603a5940175896f89372a.svg",
      "fullname": "Changti Wu",
      "name": "MaplesWCT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68896d3a716ee5bfb1428441",
      "name": "ZGCA",
      "fullname": "Zhongguancun Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11598",
      "authors": [
        {
          "_id": "698e95e3cace060ff123ad9b",
          "name": "Zedong Chu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ad9c",
          "name": "Shichao Xie",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ad9d",
          "name": "Xiaolong Wu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ad9e",
          "name": "Yanfen Shen",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ad9f",
          "name": "Minghua Luo",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada0",
          "name": "Zhengbo Wang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada1",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada2",
          "name": "Xiaoxu Leng",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada3",
          "name": "Junjun Hu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada4",
          "name": "Mingyang Yin",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada5",
          "name": "Jia Lu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada6",
          "name": "Yingnan Guo",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada7",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada8",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123ada9",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adaa",
          "name": "Yanqing Zhu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adab",
          "name": "Yuxiang Zhao",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adac",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adad",
          "name": "Yirong Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adae",
          "name": "Ye He",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adaf",
          "name": "Jiahang Wang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb0",
          "name": "Yang Cai",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb1",
          "name": "Tianlin Zhang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb2",
          "name": "Li Gao",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb3",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb4",
          "name": "Mingchao Sun",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb5",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb6",
          "name": "Chiyu Wang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb7",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb8",
          "name": "Hongyu Pan",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adb9",
          "name": "Honglin Han",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adba",
          "name": "Zhining Gu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adbb",
          "name": "Kuan Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adbc",
          "name": "Jianfang Zhang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adbd",
          "name": "Di Jing",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adbe",
          "name": "Zihao Guan",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adbf",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc0",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc1",
          "name": "Di Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc2",
          "name": "Xiangpo Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc3",
          "name": "Menglin Yang",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc4",
          "name": "Hongguang Xing",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc5",
          "name": "Weiguo Li",
          "hidden": false
        },
        {
          "_id": "698e95e3cace060ff123adc6",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/NSe-04vDg-XUwZ878OihA.mp4"
      ],
      "publishedAt": "2026-02-12T05:30:20.000Z",
      "submittedOnDailyAt": "2026-02-13T00:39:27.438Z",
      "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.\n  To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 km^2). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
      "upvotes": 2,
      "discussionId": "698e95e4cace060ff123adc7",
      "projectPage": "https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/",
      "githubRepo": "https://github.com/amap-cvlab/ABot-Navigation/tree/ABot-N0",
      "githubRepoAddedBy": "user",
      "ai_summary": "A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.",
      "ai_keywords": [
        "Vision-Language-Action",
        "LLM-based Cognitive Brain",
        "Flow Matching-based Action Expert",
        "hierarchical architecture",
        "embodied navigation",
        "expert trajectories",
        "3D scenes",
        "Agentic Navigation System",
        "hierarchical topological memory",
        "long-horizon missions"
      ],
      "githubStars": 23
    },
    "publishedAt": "2026-02-12T00:30:20.000Z",
    "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
    "summary": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.\n  To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 km^2). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/NSe-04vDg-XUwZ878OihA.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11598.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11298",
      "authors": [
        {
          "_id": "698e94cacace060ff123ad08",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad09",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0a",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0b",
          "name": "Chen-Yo Sun",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0c",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0d",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0e",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad0f",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad10",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad11",
          "name": "Rohin Arora",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad12",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad13",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad14",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad15",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad16",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad17",
          "name": "Alan Jeffares",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad18",
          "name": "Albert Jiang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad19",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1a",
          "name": "Amlie Hliou",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1b",
          "name": "Andrew Bai",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1c",
          "name": "Angele Lenglemetz",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1d",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1e",
          "name": "Anton Eliseev",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad1f",
          "name": "Antonia Calvi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad20",
          "name": "Arjun Majumdar",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad21",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad22",
          "name": "Baptiste Rozire",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad23",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad24",
          "name": "Benjamin Tibi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad25",
          "name": "Clmence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad26",
          "name": "Connor Chen",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad27",
          "name": "Corentin Barreau",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad28",
          "name": "Corentin Sautier",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad29",
          "name": "Cyprien Courtot",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2a",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2b",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2c",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2d",
          "name": "Enguerrand Paquin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2e",
          "name": "Faruk Ahmed",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad2f",
          "name": "Federico Baldassarre",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad30",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad31",
          "name": "Gatan Ecrepont",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad32",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad33",
          "name": "Genevieve Hayes",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad34",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad35",
          "name": "Giada Pistilli",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad36",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad37",
          "name": "Gunjan Dhanuka",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad38",
          "name": "Gunshi Gupta",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad39",
          "name": "Han Zhou",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3a",
          "name": "Indraneel Mukherjee",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3b",
          "name": "Irene Zhang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3c",
          "name": "Jaeyoung Kim",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3d",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3e",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad3f",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad40",
          "name": "John Harvill",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad41",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad42",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad43",
          "name": "Julien Tauran",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad44",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad45",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad46",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad47",
          "name": "Laurence Aitchison",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad48",
          "name": "Lonard Blier",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad49",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4a",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4b",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4c",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4d",
          "name": "Maarten Buyl",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4e",
          "name": "Manan Sharma",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad4f",
          "name": "Margaret Jennings",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad50",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad51",
          "name": "Mark Prins",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad52",
          "name": "Mathieu Poire",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad53",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad54",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad55",
          "name": "Matthieu Futeral",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad56",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad57",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad58",
          "name": "Mert Unsal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad59",
          "name": "Mia Chiquier",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5a",
          "name": "Nathan Grinsztajn",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5b",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5c",
          "name": "Olivier Bousquet",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5d",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5e",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad5f",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad60",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad61",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad62",
          "name": "Philomne Chagniot",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad63",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad64",
          "name": "Piotr Mio",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad65",
          "name": "Prateek Gupta",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad66",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad67",
          "name": "Quentin Torroba",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad68",
          "name": "Ram Ramrakhya",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad69",
          "name": "Rishi Shah",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6a",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6b",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6c",
          "name": "Rosalie Millner",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6d",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6e",
          "name": "Samuel Humeau",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad6f",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad70",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad71",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad72",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad73",
          "name": "Tho Cachet",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad74",
          "name": "Theo Simon Sorg",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad75",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad76",
          "name": "Thomas Chabal",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad77",
          "name": "Thomas Foubert",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad78",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad79",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7a",
          "name": "Tim Lawson",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7b",
          "name": "Tom Bewley",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7c",
          "name": "Tom Edwards",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7d",
          "name": "Tyler Wang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7e",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad7f",
          "name": "Van Phung",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad80",
          "name": "Vedant Nanda",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad81",
          "name": "Victor Jouault",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad82",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad83",
          "name": "Vladislav Bataev",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad84",
          "name": "Wassim Bouaziz",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad85",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad86",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad87",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad88",
          "name": "Xingran Guo",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad89",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad8a",
          "name": "Yannic Neuhaus",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad8b",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad8c",
          "name": "Zaccharie Ramzi",
          "hidden": false
        },
        {
          "_id": "698e94cacace060ff123ad8d",
          "name": "Zhenlin Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T19:17:10.000Z",
      "submittedOnDailyAt": "2026-02-13T00:34:54.682Z",
      "title": "Voxtral Realtime",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.",
      "upvotes": 2,
      "discussionId": "698e94cacace060ff123ad8e",
      "projectPage": "https://mistral.ai/news/voxtral-transcribe-2",
      "ai_summary": "Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.",
      "ai_keywords": [
        "automatic speech recognition",
        "streaming",
        "end-to-end training",
        "causal audio encoder",
        "Ada RMS-Norm",
        "Delayed Streams Modeling",
        "pretraining",
        "large-scale dataset",
        "latency",
        "alignment"
      ],
      "organization": {
        "_id": "64edf4004f42c35eea1b1632",
        "name": "mistralai",
        "fullname": "Mistral AI_",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"
      }
    },
    "publishedAt": "2026-02-11T14:17:10.000Z",
    "title": "Voxtral Realtime",
    "summary": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11298.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64edf4004f42c35eea1b1632",
      "name": "mistralai",
      "fullname": "Mistral AI_",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11792",
      "authors": [
        {
          "_id": "698ecc34cace060ff123aef5",
          "name": "Hongbo Zhang",
          "hidden": false
        },
        {
          "_id": "698ecc34cace060ff123aef6",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "698ecc34cace060ff123aef7",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "698ecc34cace060ff123aef8",
          "name": "Guangsheng Bao",
          "hidden": false
        },
        {
          "_id": "698ecc34cace060ff123aef9",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "698ecc34cace060ff123aefa",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T10:17:32.000Z",
      "submittedOnDailyAt": "2026-02-13T04:34:34.494Z",
      "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
      "submittedOnDailyBy": {
        "_id": "6182ac4df82814e44d033d51",
        "avatarUrl": "/avatars/ff56e4a0654388be9b4c052237d8e3cb.svg",
        "isPro": false,
        "fullname": "Hongbo Zhang",
        "user": "StevenZHB",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the k smallest nearest-neighbor edit distances. Min-kNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
      "upvotes": 1,
      "discussionId": "698ecc34cace060ff123aefb",
      "projectPage": "https://stevenzhb.github.io/detect-rlvr-data/",
      "githubRepo": "https://github.com/StevenZHB/Detect_RLVR_Data",
      "githubRepoAddedBy": "user",
      "ai_summary": "Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "reward feedback",
        "self-generated reasoning trajectories",
        "membership inference",
        "RL contamination detection",
        "Min-kNN Distance",
        "behavioral signature"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "66bb231e40d36c70d6ad0c4b",
        "name": "WestlakeNLP",
        "fullname": "Text Intelligence Lab of Westlake University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"
      }
    },
    "publishedAt": "2026-02-12T05:17:32.000Z",
    "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the k smallest nearest-neighbor edit distances. Min-kNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11792.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6182ac4df82814e44d033d51",
      "avatarUrl": "/avatars/ff56e4a0654388be9b4c052237d8e3cb.svg",
      "fullname": "Hongbo Zhang",
      "name": "StevenZHB",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66bb231e40d36c70d6ad0c4b",
      "name": "WestlakeNLP",
      "fullname": "Text Intelligence Lab of Westlake University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11543",
      "authors": [
        {
          "_id": "698ea783cace060ff123ae71",
          "name": "Jinrui Zhang",
          "hidden": false
        },
        {
          "_id": "698ea783cace060ff123ae72",
          "name": "Chaodong Xiao",
          "hidden": false
        },
        {
          "_id": "698ea783cace060ff123ae73",
          "name": "Aoqi Wu",
          "hidden": false
        },
        {
          "_id": "698ea783cace060ff123ae74",
          "name": "Xindong Zhang",
          "hidden": false
        },
        {
          "_id": "698ea783cace060ff123ae75",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T04:02:45.000Z",
      "submittedOnDailyAt": "2026-02-13T01:57:19.452Z",
      "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
      "submittedOnDailyBy": {
        "_id": "642f8ce583e266575cf2aa20",
        "avatarUrl": "/avatars/2a8c7a9bc0bfdde80078e3bb821f25b5.svg",
        "isPro": false,
        "fullname": "Jinrui Zhang",
        "user": "zjr2000",
        "type": "user"
      },
      "summary": "Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.",
      "upvotes": 1,
      "discussionId": "698ea784cace060ff123ae76",
      "githubRepo": "https://github.com/zjr2000/SPES",
      "githubRepoAddedBy": "user",
      "ai_summary": "A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.",
      "ai_keywords": [
        "mixture-of-experts",
        "language models",
        "decentralized training",
        "federated optimization",
        "expert synchronization",
        "expert merging",
        "sparse training",
        "memory efficiency"
      ],
      "githubStars": 11,
      "organization": {
        "_id": "646ecc368d316fde87b3b6e3",
        "name": "PolyUHK",
        "fullname": "The Hong Kong Polytechnic University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
      }
    },
    "publishedAt": "2026-02-11T23:02:45.000Z",
    "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
    "summary": "Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11543.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f8ce583e266575cf2aa20",
      "avatarUrl": "/avatars/2a8c7a9bc0bfdde80078e3bb821f25b5.svg",
      "fullname": "Jinrui Zhang",
      "name": "zjr2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "646ecc368d316fde87b3b6e3",
      "name": "PolyUHK",
      "fullname": "The Hong Kong Polytechnic University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11509",
      "authors": [
        {
          "_id": "698e90fecace060ff123ac97",
          "name": "David Wan",
          "hidden": false
        },
        {
          "_id": "698e90fecace060ff123ac98",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "698e90fecace060ff123ac99",
          "name": "Ziyang Wang",
          "hidden": false
        },
        {
          "_id": "698e90fecace060ff123ac9a",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "698e90fecace060ff123ac9b",
          "name": "Hyunji Lee",
          "hidden": false
        },
        {
          "_id": "698e90fecace060ff123ac9c",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T03:10:02.000Z",
      "submittedOnDailyAt": "2026-02-13T01:53:50.142Z",
      "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
      "submittedOnDailyBy": {
        "_id": "617df9bb402d4d8f8eee3737",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
        "isPro": false,
        "fullname": "Han Wang",
        "user": "HanNight",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.",
      "upvotes": 1,
      "discussionId": "698e90fecace060ff123ac9d",
      "githubRepo": "https://github.com/meetdavidwan/murgat",
      "githubRepoAddedBy": "user",
      "ai_summary": "MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.",
      "ai_keywords": [
        "Multimodal large language models",
        "multimodal grounding",
        "multimodal reasoning",
        "fact-level attribution",
        "automatic evaluation framework",
        "hallucination",
        "structured grounding"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-11T22:10:02.000Z",
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "summary": "Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617df9bb402d4d8f8eee3737",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
      "fullname": "Han Wang",
      "name": "HanNight",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10575",
      "authors": [
        {
          "_id": "698ed966cace060ff123af29",
          "name": "Chenhao Zhang",
          "hidden": false
        },
        {
          "_id": "698ed966cace060ff123af2a",
          "name": "Yazhe Niu",
          "hidden": false
        },
        {
          "_id": "698ed966cace060ff123af2b",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/9QM8-L4a_QhnfeQh05hja.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/7nIgI1NW6f2deE2CSJMMw.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/hba5woADggvcU6ObolDRB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/FJ-AqQ9tVvvcPIxSb3G_E.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/GQcEbv4IMQEMwADsxyPGL.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/cempnjDOf46SFsa4OAsSN.png"
      ],
      "publishedAt": "2026-02-11T06:59:36.000Z",
      "submittedOnDailyAt": "2026-02-13T05:36:58.003Z",
      "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "647daf00cfca67bc50f9a99f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
        "isPro": false,
        "fullname": "Chenhao(Leo) Zhang",
        "user": "MING-ZCH",
        "type": "user"
      },
      "summary": "Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.\n  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.",
      "upvotes": 1,
      "discussionId": "698ed966cace060ff123af2c",
      "projectPage": "https://metaphorstar.github.io/",
      "githubRepo": "https://github.com/MING-ZCH/MetaphorStar",
      "githubRepoAddedBy": "user",
      "ai_summary": "MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Visual Question Answering",
        "Theory of Mind",
        "visual reinforcement learning",
        "image implication tasks",
        "fine-grained dataset",
        "visual RL method",
        "benchmark",
        "model parameter scaling",
        "training data scaling",
        "model architectures",
        "training strategies"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-11T01:59:36.000Z",
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "summary": "Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.\n  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/9QM8-L4a_QhnfeQh05hja.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/7nIgI1NW6f2deE2CSJMMw.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/hba5woADggvcU6ObolDRB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/FJ-AqQ9tVvvcPIxSb3G_E.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/GQcEbv4IMQEMwADsxyPGL.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647daf00cfca67bc50f9a99f/cempnjDOf46SFsa4OAsSN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10575.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647daf00cfca67bc50f9a99f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
      "fullname": "Chenhao(Leo) Zhang",
      "name": "MING-ZCH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08277",
      "authors": [
        {
          "_id": "698beabb6052d3bed96309e9",
          "name": "Xiangbo Gao",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309ea",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309eb",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309ec",
          "name": "Yuheng Wu",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309ed",
          "name": "Suofei Feng",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309ee",
          "name": "Qing Yin",
          "hidden": false
        },
        {
          "_id": "698beabb6052d3bed96309ef",
          "name": "Zhengzhong Tu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T05:15:39.000Z",
      "submittedOnDailyAt": "2026-02-13T02:41:26.676Z",
      "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.",
      "upvotes": 1,
      "discussionId": "698beabb6052d3bed96309f0",
      "projectPage": "https://xiangbogaobarry.github.io/PISCO/",
      "githubRepo": "https://github.com/taco-group/PISCO",
      "githubRepoAddedBy": "user",
      "ai_summary": "Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.",
      "ai_keywords": [
        "video diffusion model",
        "video instance insertion",
        "sparse keyframe control",
        "variable-information guidance",
        "distribution-preserving temporal masking",
        "geometry-aware conditioning",
        "video editing",
        "inpainting",
        "temporal generation",
        "spatial-temporal placement"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "693049768605dfa68334b46d",
        "name": "TexasAMUniversity",
        "fullname": "Texas A&M University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"
      }
    },
    "publishedAt": "2026-02-09T00:15:39.000Z",
    "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08277.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "693049768605dfa68334b46d",
      "name": "TexasAMUniversity",
      "fullname": "Texas A&M University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10585",
      "authors": [
        {
          "_id": "698e8cd0cace060ff123ac1f",
          "name": "Guangzhi Xiong",
          "hidden": false
        },
        {
          "_id": "698e8cd0cace060ff123ac20",
          "name": "Sanchit Sinha",
          "hidden": false
        },
        {
          "_id": "698e8cd0cace060ff123ac21",
          "name": "Aidong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T07:19:25.000Z",
      "submittedOnDailyAt": "2026-02-13T00:03:10.509Z",
      "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
      "submittedOnDailyBy": {
        "_id": "657e56d11e3e9c41a4a57d2c",
        "avatarUrl": "/avatars/1a6e7cff2693e1523f87ad24f4529872.svg",
        "isPro": false,
        "fullname": "Guangzhi Xiong",
        "user": "TeddyXGZ",
        "type": "user"
      },
      "summary": "The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.",
      "upvotes": 0,
      "discussionId": "698e8cd0cace060ff123ac22",
      "githubRepo": "https://github.com/Teddy-XiongGZ/NAE",
      "githubRepoAddedBy": "user",
      "ai_summary": "Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.",
      "ai_keywords": [
        "Generalized Additive Models",
        "mixture of experts framework",
        "neural networks",
        "feature interactions",
        "dynamic gating mechanism",
        "targeted regularization",
        "feature attributions",
        "predictive accuracy"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-11T02:19:25.000Z",
    "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
    "summary": "The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657e56d11e3e9c41a4a57d2c",
      "avatarUrl": "/avatars/1a6e7cff2693e1523f87ad24f4529872.svg",
      "fullname": "Guangzhi Xiong",
      "name": "TeddyXGZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]