[
  {
    "paper": {
      "id": "2511.03276",
      "authors": [
        {
          "_id": "690c1cdc60494e4fa76756a9",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756aa",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756ab",
          "name": "Longxu Dou",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756ac",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756ad",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756ae",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756af",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "690c1cdc60494e4fa76756b0",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62a7362fd1e7a011fd4e31a7/QrHOFNxOkfy93f6tPGJdN.jpeg"
      ],
      "publishedAt": "2025-11-05T08:17:42.000Z",
      "submittedOnDailyAt": "2025-11-06T01:32:43.418Z",
      "title": "Diffusion Language Models are Super Data Learners",
      "submittedOnDailyBy": {
        "_id": "62a7362fd1e7a011fd4e31a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png",
        "isPro": false,
        "fullname": "Jinjie Ni",
        "user": "jinjieni",
        "type": "user"
      },
      "summary": "Under strictly controlled pre-training settings, we observe a Crossover: when\nunique data is limited, diffusion language models (DLMs) consistently surpass\nautoregressive (AR) models by training for more epochs. The crossover shifts\nlater with more or higher-quality data, earlier with larger models, and\npersists across dense and sparse architectures. We attribute the gains to three\ncompounding factors: (1) any-order modeling, (2) super-dense compute from\niterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;\ninput or parameter noise improves AR under data constraint but cannot close the\ngap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B\nunique Python tokens overtakes an AR coder trained with strictly matched\nsettings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag\nand > 33% on MMLU using only 1B tokens, without any special tricks, just by\nrepeating standard pre-training data. We also show that rising validation\ncross-entropy does not imply degraded downstream performance in this regime.",
      "upvotes": 45,
      "discussionId": "690c1cdc60494e4fa76756b1",
      "projectPage": "https://github.com/JinjieNi/dlms-are-super-data-learners",
      "githubRepo": "https://github.com/JinjieNi/MegaDLMs",
      "ai_summary": "Diffusion language models outperform autoregressive models in low-data settings due to any-order modeling, iterative bidirectional denoising, and Monte Carlo augmentation, and maintain advantages even at scale.",
      "ai_keywords": [
        "diffusion language models",
        "autoregressive models",
        "any-order modeling",
        "iterative bidirectional denoising",
        "Monte Carlo augmentation",
        "HellaSwag",
        "MMLU"
      ],
      "githubStars": 20,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-11-05T03:17:42.000Z",
    "title": "Diffusion Language Models are Super Data Learners",
    "summary": "Under strictly controlled pre-training settings, we observe a Crossover: when\nunique data is limited, diffusion language models (DLMs) consistently surpass\nautoregressive (AR) models by training for more epochs. The crossover shifts\nlater with more or higher-quality data, earlier with larger models, and\npersists across dense and sparse architectures. We attribute the gains to three\ncompounding factors: (1) any-order modeling, (2) super-dense compute from\niterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;\ninput or parameter noise improves AR under data constraint but cannot close the\ngap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B\nunique Python tokens overtakes an AR coder trained with strictly matched\nsettings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag\nand > 33% on MMLU using only 1B tokens, without any special tricks, just by\nrepeating standard pre-training data. We also show that rising validation\ncross-entropy does not imply degraded downstream performance in this regime.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62a7362fd1e7a011fd4e31a7/QrHOFNxOkfy93f6tPGJdN.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a7362fd1e7a011fd4e31a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png",
      "fullname": "Jinjie Ni",
      "name": "jinjieni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01294",
      "authors": [
        {
          "_id": "690c12a360494e4fa7675647",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "690c12a360494e4fa7675648",
          "name": "Dingyou Wang",
          "hidden": false
        },
        {
          "_id": "690c12a360494e4fa7675649",
          "name": "Jiaming Hu",
          "hidden": false
        },
        {
          "_id": "690c12a360494e4fa767564a",
          "name": "Qixuan Zhang",
          "hidden": false
        },
        {
          "_id": "690c12a360494e4fa767564b",
          "name": "Jingyi Yu",
          "hidden": false
        },
        {
          "_id": "690c12a360494e4fa767564c",
          "name": "Lan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T07:21:42.000Z",
      "submittedOnDailyAt": "2025-11-06T00:47:13.918Z",
      "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
      "submittedOnDailyBy": {
        "_id": "636d12455aaed143cd665607",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
        "isPro": false,
        "fullname": "ZLW",
        "user": "ZarkLngeW",
        "type": "user"
      },
      "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for objects with\nhigh degrees of freedom (DoF), remains a significant challenge. Existing\nmethods typically rely on motion sequences or strong assumptions from\nhand-curated datasets, which hinders scalability. In this paper, we introduce\nKinematify, an automated framework that synthesizes articulated objects\ndirectly from arbitrary RGB images or textual descriptions. Our method\naddresses two core challenges: (i) inferring kinematic topologies for high-DoF\nobjects and (ii) estimating joint parameters from static geometry. To achieve\nthis, we combine MCTS search for structural inference with geometry-driven\noptimization for joint reasoning, producing physically consistent and\nfunctionally valid descriptions. We evaluate Kinematify on diverse inputs from\nboth synthetic and real-world environments, demonstrating improvements in\nregistration and kinematic topology accuracy over prior work.",
      "upvotes": 7,
      "discussionId": "690c12a360494e4fa767564d",
      "ai_summary": "Kinematify is an automated framework that synthesizes articulated objects from RGB images or textual descriptions, addressing challenges in inferring kinematic topologies and estimating joint parameters.",
      "ai_keywords": [
        "articulated objects",
        "motion planning",
        "policy learning",
        "degrees of freedom",
        "MCTS search",
        "geometry-driven optimization",
        "registration",
        "kinematic topology accuracy"
      ],
      "organization": {
        "_id": "6427195aba6cef0a6eb9e5ed",
        "name": "DEEMOSTECH",
        "fullname": "DEEMOS Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642704738a61aabae65be84d/VU59bPTS9RR4pFC7LEQmo.png"
      }
    },
    "publishedAt": "2025-11-03T02:21:42.000Z",
    "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
    "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for objects with\nhigh degrees of freedom (DoF), remains a significant challenge. Existing\nmethods typically rely on motion sequences or strong assumptions from\nhand-curated datasets, which hinders scalability. In this paper, we introduce\nKinematify, an automated framework that synthesizes articulated objects\ndirectly from arbitrary RGB images or textual descriptions. Our method\naddresses two core challenges: (i) inferring kinematic topologies for high-DoF\nobjects and (ii) estimating joint parameters from static geometry. To achieve\nthis, we combine MCTS search for structural inference with geometry-driven\noptimization for joint reasoning, producing physically consistent and\nfunctionally valid descriptions. We evaluate Kinematify on diverse inputs from\nboth synthetic and real-world environments, demonstrating improvements in\nregistration and kinematic topology accuracy over prior work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "6427195aba6cef0a6eb9e5ed",
      "name": "DEEMOSTECH",
      "fullname": "DEEMOS Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642704738a61aabae65be84d/VU59bPTS9RR4pFC7LEQmo.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03628",
      "authors": [
        {
          "_id": "690c0eb360494e4fa7675642",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "690c0eb360494e4fa7675643",
          "name": "Fenghai Li",
          "hidden": false
        },
        {
          "_id": "690c0eb360494e4fa7675644",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T16:47:26.000Z",
      "submittedOnDailyAt": "2025-11-06T00:28:04.227Z",
      "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.",
      "upvotes": 3,
      "discussionId": "690c0eb360494e4fa7675645",
      "projectPage": "https://trade-bench.live/",
      "githubRepo": "https://github.com/ulab-uiuc/live-trade-bench",
      "ai_summary": "LiveTradeBench evaluates LLMs in dynamic trading environments to assess decision-making under real-time uncertainty and market volatility.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "live trading environment",
        "market prices",
        "news",
        "portfolio management",
        "multi-asset allocation",
        "risk management",
        "cross-asset reasoning",
        "multi-market evaluation",
        "U.S. stocks",
        "Polymarket prediction markets",
        "volatility",
        "liquidity",
        "information flow",
        "sequential decision making",
        "consistency under live uncertainty"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-11-05T11:47:26.000Z",
    "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
    "summary": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03146",
      "authors": [
        {
          "_id": "690c0d0560494e4fa7675623",
          "name": "Kaiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675624",
          "name": "Chenghao Yang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675625",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675626",
          "name": "Sihang Yuan",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675627",
          "name": "Qiuyue Wang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675628",
          "name": "Chaoyi Huang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675629",
          "name": "Guosheng Zhu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562b",
          "name": "Huawenyu Lu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562c",
          "name": "Jianing Wen",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562d",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562e",
          "name": "Lishu Luo",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa767562f",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675630",
          "name": "Sijin Wu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675631",
          "name": "Xiaolei Zhu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675632",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675633",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675634",
          "name": "Yi Lin",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675635",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675636",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "690c0d0560494e4fa7675637",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T03:09:16.000Z",
      "submittedOnDailyAt": "2025-11-06T00:21:01.663Z",
      "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As reasoning models scale rapidly, the essential role of multimodality in\nhuman cognition has come into sharp relief, driving a growing need to probe\nvision-centric cognitive behaviors. Yet, existing multimodal benchmarks either\noveremphasize textual reasoning or fall short of systematically capturing\nvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs\ninsufficiently assessed. To address this limitation, we introduce MME-CC\n(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded\nbenchmark that organizes 11 representative reasoning tasks into three\nfundamental categories of visual information: spatial, geometric, and\nknowledge-based reasoning, and provides fine-grained analyses of MLLMs'\ncognitive capacity across these dimensions. Based on MME-CC, we conduct\nextensive experiments over 16 representative MLLMs. Our study reveals that\nclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.\n30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak\n(less than or equal to 30%). We further identify common error patterns,\nincluding orientation mistakes, fragile cross-view identity persistence, and\npoor adherence to counterfactual instructions, and observe that\nChain-of-Thought typically follows a three-stage process (extract -> reason ->\nverify) with heavy reliance on visual extraction. We hope this work catalyzes a\nshift toward treating the cognitive capacity of MLLMs as central to both\nevaluation and model design.",
      "upvotes": 3,
      "discussionId": "690c0d0560494e4fa7675638",
      "projectPage": "https://randomtutu.github.io/MME-CC/",
      "ai_summary": "MME-CC is a vision-grounded benchmark that evaluates multimodal large language models' cognitive capacity across spatial, geometric, and knowledge-based reasoning tasks, revealing weaknesses in spatial and geometric reasoning.",
      "ai_keywords": [
        "multimodal benchmarks",
        "vision-centric cognitive behaviors",
        "MME-CC",
        "visual information",
        "spatial reasoning",
        "geometric reasoning",
        "knowledge-based reasoning",
        "MLLMs",
        "cognitive capacity",
        "closed-source models",
        "orientation mistakes",
        "cross-view identity persistence",
        "counterfactual instructions",
        "Chain-of-Thought"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-04T22:09:16.000Z",
    "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity",
    "summary": "As reasoning models scale rapidly, the essential role of multimodality in\nhuman cognition has come into sharp relief, driving a growing need to probe\nvision-centric cognitive behaviors. Yet, existing multimodal benchmarks either\noveremphasize textual reasoning or fall short of systematically capturing\nvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs\ninsufficiently assessed. To address this limitation, we introduce MME-CC\n(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded\nbenchmark that organizes 11 representative reasoning tasks into three\nfundamental categories of visual information: spatial, geometric, and\nknowledge-based reasoning, and provides fine-grained analyses of MLLMs'\ncognitive capacity across these dimensions. Based on MME-CC, we conduct\nextensive experiments over 16 representative MLLMs. Our study reveals that\nclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.\n30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak\n(less than or equal to 30%). We further identify common error patterns,\nincluding orientation mistakes, fragile cross-view identity persistence, and\npoor adherence to counterfactual instructions, and observe that\nChain-of-Thought typically follows a three-stage process (extract -> reason ->\nverify) with heavy reliance on visual extraction. We hope this work catalyzes a\nshift toward treating the cognitive capacity of MLLMs as central to both\nevaluation and model design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03001",
      "authors": [
        {
          "_id": "690c0e1c60494e4fa767563a",
          "name": "Gyeom Hwangbo",
          "hidden": false
        },
        {
          "_id": "690c0e1c60494e4fa767563b",
          "name": "Hyungjoo Chae",
          "hidden": false
        },
        {
          "_id": "690c0e1c60494e4fa767563c",
          "name": "Minseok Kang",
          "hidden": false
        },
        {
          "_id": "690c0e1c60494e4fa767563d",
          "name": "Hyeonjong Ju",
          "hidden": false
        },
        {
          "_id": "690c0e1c60494e4fa767563e",
          "name": "Soohyun Oh",
          "hidden": false
        },
        {
          "_id": "690c0e1c60494e4fa767563f",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hmrVsF8OLhsfTzRcjJyOn.png"
      ],
      "publishedAt": "2025-11-04T21:13:51.000Z",
      "submittedOnDailyAt": "2025-11-06T00:25:38.643Z",
      "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied\n  Environments with Tool Augmentation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite recent progress in using Large Language Models (LLMs) for\nautomatically generating 3D scenes, generated scenes often lack realistic\nspatial layouts and object attributes found in real-world environments. As this\nproblem stems from insufficiently detailed, coarse-grained instructions,\nadvancing 3D scene synthesis guided by more detailed, fine-grained instructions\nthat reflect real-world environments becomes crucial. Without such realistic\nscenes, training embodied agents in unrealistic environments can lead them to\nlearn priors that diverge significantly from real-world physics and semantics,\ndegrading their performance when deployed. Thus, verifying the alignment\nbetween the fine-grained instruction and the generated scene is essential for\neffective learning. However, current evaluation methods, such as CLIPScore and\nvision-language models (VLMs), often fail to reliably assess such alignment.\nThis shortcoming arises primarily from their shallow understanding of 3D\nscenes, which often leads to improperly grounded scene components. To address\nthis, we introduce LEGO-Eval, an evaluation framework equipped with diverse\ntools designed to explicitly ground scene components, enabling more accurate\nalignment assessments. We also present LEGO-Bench, a benchmark of detailed\ninstructions that specify complex layouts and attributes of real-world\nenvironments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge\nby 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with\nLEGO-Bench reveals significant limitations in current generation methods.\nAcross all evaluated approaches, success rates reached at most 10% in\ngenerating scenes that fully align with fine-grained instructions.",
      "upvotes": 1,
      "discussionId": "690c0e1d60494e4fa7675640",
      "projectPage": "https://gyeomh.github.io/LEGO-Eval/",
      "ai_summary": "LEGO-Eval and LEGO-Bench improve the evaluation and generation of realistic 3D scenes by aligning detailed instructions with scene components, outperforming existing methods.",
      "ai_keywords": [
        "Large Language Models",
        "3D scene synthesis",
        "fine-grained instructions",
        "real-world environments",
        "CLIPScore",
        "vision-language models",
        "LEGO-Eval",
        "LEGO-Bench",
        "scene-instruction alignment",
        "F1 score"
      ]
    },
    "publishedAt": "2025-11-04T16:13:51.000Z",
    "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied\n  Environments with Tool Augmentation",
    "summary": "Despite recent progress in using Large Language Models (LLMs) for\nautomatically generating 3D scenes, generated scenes often lack realistic\nspatial layouts and object attributes found in real-world environments. As this\nproblem stems from insufficiently detailed, coarse-grained instructions,\nadvancing 3D scene synthesis guided by more detailed, fine-grained instructions\nthat reflect real-world environments becomes crucial. Without such realistic\nscenes, training embodied agents in unrealistic environments can lead them to\nlearn priors that diverge significantly from real-world physics and semantics,\ndegrading their performance when deployed. Thus, verifying the alignment\nbetween the fine-grained instruction and the generated scene is essential for\neffective learning. However, current evaluation methods, such as CLIPScore and\nvision-language models (VLMs), often fail to reliably assess such alignment.\nThis shortcoming arises primarily from their shallow understanding of 3D\nscenes, which often leads to improperly grounded scene components. To address\nthis, we introduce LEGO-Eval, an evaluation framework equipped with diverse\ntools designed to explicitly ground scene components, enabling more accurate\nalignment assessments. We also present LEGO-Bench, a benchmark of detailed\ninstructions that specify complex layouts and attributes of real-world\nenvironments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge\nby 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with\nLEGO-Bench reveals significant limitations in current generation methods.\nAcross all evaluated approaches, success rates reached at most 10% in\ngenerating scenes that fully align with fine-grained instructions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hmrVsF8OLhsfTzRcjJyOn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02818",
      "authors": [
        {
          "_id": "690b348b60494e4fa76754f6",
          "name": "Mohamed Bouadi",
          "hidden": false
        },
        {
          "_id": "690b348b60494e4fa76754f7",
          "user": {
            "_id": "66fce04d927ec45504514afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
            "isPro": false,
            "fullname": "Pratinav Seth",
            "user": "pratinavsetharya",
            "type": "user"
          },
          "name": "Pratinav Seth",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-05T16:24:08.560Z",
          "hidden": false
        },
        {
          "_id": "690b348b60494e4fa76754f8",
          "name": "Aditya Tanna",
          "hidden": false
        },
        {
          "_id": "690b348b60494e4fa76754f9",
          "name": "Vinay Kumar Sankarapu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-11-06T04:18:13.289Z",
      "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
      "submittedOnDailyBy": {
        "_id": "66fce04d927ec45504514afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
        "isPro": false,
        "fullname": "Pratinav Seth",
        "user": "pratinavsetharya",
        "type": "user"
      },
      "summary": "Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .",
      "upvotes": 1,
      "discussionId": "690b348c60494e4fa76754fa",
      "ai_summary": "Orion-MSP, a tabular in-context learning architecture, addresses limitations in current models by incorporating multi-scale processing, block-sparse attention, and a Perceiver-style memory, achieving state-of-the-art performance on diverse benchmarks.",
      "ai_keywords": [
        "tabular in-context learning",
        "TabPFN",
        "TabICL",
        "gradient-boosted trees",
        "multi-scale processing",
        "block-sparse attention",
        "Perceiver-style memory"
      ],
      "organization": {
        "_id": "69034619c56aefa86350a727",
        "name": "Lexsi",
        "fullname": "Lexsi Labs",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cbbcb9f488db9bb3beeaa1/b0eHmC8iYVQCyqqaLFfA6.png"
      }
    },
    "publishedAt": "2025-11-04T13:43:44.000Z",
    "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
    "summary": "Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fce04d927ec45504514afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
      "fullname": "Pratinav Seth",
      "name": "pratinavsetharya",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "69034619c56aefa86350a727",
      "name": "Lexsi",
      "fullname": "Lexsi Labs",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cbbcb9f488db9bb3beeaa1/b0eHmC8iYVQCyqqaLFfA6.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.02802",
      "authors": [
        {
          "_id": "690b34a160494e4fa76754fc",
          "name": "Aditya Tanna",
          "hidden": false
        },
        {
          "_id": "690b34a160494e4fa76754fd",
          "user": {
            "_id": "66fce04d927ec45504514afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
            "isPro": false,
            "fullname": "Pratinav Seth",
            "user": "pratinavsetharya",
            "type": "user"
          },
          "name": "Pratinav Seth",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-05T16:24:05.630Z",
          "hidden": false
        },
        {
          "_id": "690b34a160494e4fa76754fe",
          "name": "Mohamed Bouadi",
          "hidden": false
        },
        {
          "_id": "690b34a160494e4fa76754ff",
          "name": "Utsav Avaiya",
          "hidden": false
        },
        {
          "_id": "690b34a160494e4fa7675500",
          "name": "Vinay Kumar Sankarapu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T18:25:17.000Z",
      "submittedOnDailyAt": "2025-11-06T04:16:02.558Z",
      "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
      "submittedOnDailyBy": {
        "_id": "66fce04d927ec45504514afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
        "isPro": false,
        "fullname": "Pratinav Seth",
        "user": "pratinavsetharya",
        "type": "user"
      },
      "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune .",
      "upvotes": 1,
      "discussionId": "690b34a160494e4fa7675501",
      "ai_summary": "TabTune is a unified library that standardizes the workflow for tabular foundation models, supporting various adaptation strategies and evaluation metrics.",
      "ai_keywords": [
        "tabular foundation models",
        "zero-shot inference",
        "meta-learning",
        "supervised fine-tuning",
        "parameter-efficient fine-tuning",
        "model-aware preprocessing",
        "calibration",
        "fairness"
      ],
      "organization": {
        "_id": "69034619c56aefa86350a727",
        "name": "Lexsi",
        "fullname": "Lexsi Labs",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cbbcb9f488db9bb3beeaa1/b0eHmC8iYVQCyqqaLFfA6.png"
      }
    },
    "publishedAt": "2025-11-04T13:25:17.000Z",
    "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
    "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02802.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fce04d927ec45504514afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fce04d927ec45504514afd/lA_bTErY7JywT6xbzdflo.jpeg",
      "fullname": "Pratinav Seth",
      "name": "pratinavsetharya",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "69034619c56aefa86350a727",
      "name": "Lexsi",
      "fullname": "Lexsi Labs",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cbbcb9f488db9bb3beeaa1/b0eHmC8iYVQCyqqaLFfA6.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.02358",
      "authors": [
        {
          "_id": "690aafdad70e173c84528f82",
          "user": {
            "_id": "64d4615cf8082bf19b916492",
            "avatarUrl": "/avatars/8e1b59565ec5e4b31090cf1b911781b9.svg",
            "isPro": false,
            "fullname": "wongyukim",
            "user": "wongyukim",
            "type": "user"
          },
          "name": "Wongyu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-05T16:24:45.834Z",
          "hidden": false
        },
        {
          "_id": "690aafdad70e173c84528f83",
          "name": "Hochang Lee",
          "hidden": false
        },
        {
          "_id": "690aafdad70e173c84528f84",
          "name": "Sanghak Lee",
          "hidden": false
        },
        {
          "_id": "690aafdad70e173c84528f85",
          "name": "Yoonsung Kim",
          "hidden": false
        },
        {
          "_id": "690aafdad70e173c84528f86",
          "name": "Jaehyun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T08:24:41.000Z",
      "submittedOnDailyAt": "2025-11-06T00:27:02.825Z",
      "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation",
      "submittedOnDailyBy": {
        "_id": "64d4615cf8082bf19b916492",
        "avatarUrl": "/avatars/8e1b59565ec5e4b31090cf1b911781b9.svg",
        "isPro": false,
        "fullname": "wongyukim",
        "user": "wongyukim",
        "type": "user"
      },
      "summary": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.",
      "upvotes": 1,
      "discussionId": "690aafdbd70e173c84528f87",
      "ai_summary": "M-Solomon, a multimodal embedder, adaptively augments queries using a Multimodal LLM, improving performance and reducing embedding latency compared to baselines.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "embedders",
        "query augmentation",
        "multimodal environments",
        "Multimodal LLM (MLLM)",
        "adaptive query augmentation"
      ]
    },
    "publishedAt": "2025-11-04T03:24:41.000Z",
    "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation",
    "summary": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d4615cf8082bf19b916492",
      "avatarUrl": "/avatars/8e1b59565ec5e4b31090cf1b911781b9.svg",
      "fullname": "wongyukim",
      "name": "wongyukim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  }
]