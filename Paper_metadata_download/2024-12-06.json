[
    {
        "paper": {
            "id": "2412.04467",
            "authors": [
                {
                    "_id": "6752642e9106cbed4a680b9d",
                    "user": {
                        "_id": "6527b7280ae663e384eb8499",
                        "avatarUrl": "/avatars/fbead641c9d244019abc29ba9de1c3b0.svg",
                        "isPro": false,
                        "fullname": "Senqiao Yang",
                        "user": "Senqiao",
                        "type": "user"
                    },
                    "name": "Senqiao Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:21.290Z",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680b9e",
                    "user": {
                        "_id": "65b31d77f32c79fea79ba4d1",
                        "avatarUrl": "/avatars/8c51c1ff337ac372c7caafa4fa5019dc.svg",
                        "isPro": false,
                        "fullname": "Yukang Chen",
                        "user": "Yukang2",
                        "type": "user"
                    },
                    "name": "Yukang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:45:09.922Z",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680b9f",
                    "user": {
                        "_id": "64d104a37a7305c5895bd720",
                        "avatarUrl": "/avatars/2d9eff3a2dff6d02e45ae8964fb91f27.svg",
                        "isPro": false,
                        "fullname": "zt tian",
                        "user": "tianzhuotao",
                        "type": "user"
                    },
                    "name": "Zhuotao Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:45:21.690Z",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680ba0",
                    "user": {
                        "_id": "64c3257a027e6d966673712a",
                        "avatarUrl": "/avatars/abac7539c8e90c2780ecc445536d8c2c.svg",
                        "isPro": false,
                        "fullname": "Chengyao Wang",
                        "user": "wangchengyao",
                        "type": "user"
                    },
                    "name": "Chengyao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:45:28.672Z",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680ba1",
                    "user": {
                        "_id": "647ff779d037515be1169e35",
                        "avatarUrl": "/avatars/741096302cbdeb3670a8f31cf2d31d5d.svg",
                        "isPro": false,
                        "fullname": "Jingyao Li",
                        "user": "JingyaoLi",
                        "type": "user"
                    },
                    "name": "Jingyao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:45:35.074Z",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680ba2",
                    "name": "Bei Yu",
                    "hidden": false
                },
                {
                    "_id": "6752642e9106cbed4a680ba3",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:59:53.000Z",
            "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
            "summary": "Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip .",
            "upvotes": 48,
            "discussionId": "6752642f9106cbed4a680bfe"
        },
        "publishedAt": "2024-12-05T21:43:21.620Z",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04467.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "6527b7280ae663e384eb8499",
            "avatarUrl": "/avatars/fbead641c9d244019abc29ba9de1c3b0.svg",
            "fullname": "Senqiao Yang",
            "name": "Senqiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.04455",
            "authors": [
                {
                    "_id": "6752744f8681347ff4207fc2",
                    "user": {
                        "_id": "63f08dc79cf89c9ed1bb89cd",
                        "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
                        "isPro": false,
                        "fullname": "Zhoues",
                        "user": "Zhoues",
                        "type": "user"
                    },
                    "name": "Enshen Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:08:56.444Z",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc3",
                    "name": "Qi Su",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc4",
                    "user": {
                        "_id": "650bf938677f9e45963d672e",
                        "avatarUrl": "/avatars/7d4159067b5005a3a635e36b26b7b239.svg",
                        "isPro": false,
                        "fullname": "Cheng Chi",
                        "user": "ChuckChi",
                        "type": "user"
                    },
                    "name": "Cheng Chi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:13.512Z",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc5",
                    "name": "Zhizheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc6",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc7",
                    "name": "Tiejun Huang",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc8",
                    "user": {
                        "_id": "65b722dbe02a17f0f8d1cc6b",
                        "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
                        "isPro": false,
                        "fullname": "Lu Sheng",
                        "user": "lsheng2024",
                        "type": "user"
                    },
                    "name": "Lu Sheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:56:27.453Z",
                    "hidden": false
                },
                {
                    "_id": "6752744f8681347ff4207fc9",
                    "name": "He Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:58:27.000Z",
            "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and\n  Proactive Robotic Failure Detection",
            "summary": "Automatic detection and prevention of open-set failures are crucial in\nclosed-loop robotic systems. Recent studies often struggle to simultaneously\nidentify unexpected failures reactively after they occur and prevent\nforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a\nnovel paradigm leveraging the vision-language model (VLM) for both open-set\nreactive and proactive failure detection. The core of our method is to\nformulate both tasks as a unified set of spatio-temporal constraint\nsatisfaction problems and use VLM-generated code to evaluate them for real-time\nmonitoring. To enhance the accuracy and efficiency of monitoring, we further\nintroduce constraint elements that abstract constraint-related entities or\ntheir parts into compact geometric elements. This approach offers greater\ngenerality, simplifies tracking, and facilitates constraint-aware visual\nprogramming by leveraging these elements as visual prompts. Experiments show\nthat CaM achieves a 28.7% higher success rate and reduces execution time by\n31.8% under severe disturbances compared to baselines across three simulators\nand a real-world setting. Moreover, CaM can be integrated with open-loop\ncontrol policies to form closed-loop systems, enabling long-horizon tasks in\ncluttered scenes with dynamic environments.",
            "upvotes": 30,
            "discussionId": "675274518681347ff420802a"
        },
        "publishedAt": "2024-12-05T22:49:44.338Z",
        "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04455.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "fullname": "Zhoues",
            "name": "Zhoues",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.04454",
            "authors": [
                {
                    "_id": "67525cba61e5a1edbc685410",
                    "user": {
                        "_id": "601d29ab913ad3afd7b7ddb8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620447944896-601d29ab913ad3afd7b7ddb8.jpeg",
                        "isPro": true,
                        "fullname": "Yiheng Xu",
                        "user": "ranpox",
                        "type": "user"
                    },
                    "name": "Yiheng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:31.388Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685411",
                    "user": {
                        "_id": "656832dfbd65fd41ee7aa8cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg",
                        "isPro": false,
                        "fullname": "Zekun Wang",
                        "user": "kugwzk",
                        "type": "user"
                    },
                    "name": "Zekun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T10:37:55.632Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685412",
                    "user": {
                        "_id": "65f944d5056d465a38f49361",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PWp43VsXeltiPifGxGwrn.jpeg",
                        "isPro": false,
                        "fullname": "Junli Wang",
                        "user": "ZeonLap",
                        "type": "user"
                    },
                    "name": "Junli Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:27.583Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685413",
                    "user": {
                        "_id": "669ca7e678115e16bdfc9bfc",
                        "avatarUrl": "/avatars/6d7214e494cc50350cf06d590e63e393.svg",
                        "isPro": false,
                        "fullname": "Lu Dunjie",
                        "user": "ludunjie",
                        "type": "user"
                    },
                    "name": "Dunjie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:49:01.891Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685414",
                    "user": {
                        "_id": "618767e4238063b4615d042b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636263880877-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Tianbao Xie",
                        "user": "tianbaoxiexxx",
                        "type": "user"
                    },
                    "name": "Tianbao Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:29.425Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685415",
                    "user": {
                        "_id": "6461c2905dba83471db3be53",
                        "avatarUrl": "/avatars/6e36cf86201d590ac729a75d4a439cde.svg",
                        "isPro": false,
                        "fullname": "Amrita Saha",
                        "user": "amritasaha87",
                        "type": "user"
                    },
                    "name": "Amrita Saha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:48:52.752Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685416",
                    "user": {
                        "_id": "65f84fd980481173afd91233",
                        "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
                        "isPro": false,
                        "fullname": "Doyen",
                        "user": "doyensahoo",
                        "type": "user"
                    },
                    "name": "Doyen Sahoo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T09:48:40.670Z",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685417",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "67525cba61e5a1edbc685418",
                    "user": {
                        "_id": "649dbcc4e0fff1ed099dc80a",
                        "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
                        "isPro": false,
                        "fullname": "Caiming Xiong",
                        "user": "cxiong",
                        "type": "user"
                    },
                    "name": "Caiming Xiong",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-12-06T02:09:01.442Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:58:26.000Z",
            "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
            "summary": "Graphical User Interfaces (GUIs) are critical to human-computer interaction,\nyet automating GUI tasks remains challenging due to the complexity and\nvariability of visual environments. Existing approaches often rely on textual\nrepresentations of GUIs, which introduce limitations in generalization,\nefficiency, and scalability. In this paper, we introduce Aguvis, a unified pure\nvision-based framework for autonomous GUI agents that operates across various\nplatforms. Our approach leverages image-based observations, and grounding\ninstructions in natural language to visual elements, and employs a consistent\naction space to ensure cross-platform generalization. To address the\nlimitations of previous work, we integrate explicit planning and reasoning\nwithin the model, enhancing its ability to autonomously navigate and interact\nwith complex digital environments. We construct a large-scale dataset of GUI\nagent trajectories, incorporating multimodal reasoning and grounding, and\nemploy a two-stage training pipeline that first focuses on general GUI\ngrounding, followed by planning and reasoning. Through comprehensive\nexperiments, we demonstrate that Aguvis surpasses previous state-of-the-art\nmethods in both offline and real-world online scenarios, achieving, to our\nknowledge, the first fully autonomous pure vision GUI agent capable of\nperforming tasks independently without collaboration with external\nclosed-source models. We open-sourced all datasets, models, and training\nrecipes to facilitate future research at https://aguvis-project.github.io/.",
            "upvotes": 23,
            "discussionId": "67525cbd61e5a1edbc68563b"
        },
        "publishedAt": "2024-12-05T22:09:04.197Z",
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04454.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "601d29ab913ad3afd7b7ddb8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620447944896-601d29ab913ad3afd7b7ddb8.jpeg",
            "fullname": "Yiheng Xu",
            "name": "ranpox",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 22
        }
    },
    {
        "paper": {
            "id": "2412.03895",
            "authors": [
                {
                    "_id": "6752a5285281c3cae4a81f18",
                    "name": "Donghoon Ahn",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f19",
                    "name": "Jiwon Kang",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1a",
                    "name": "Sanghyun Lee",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1b",
                    "user": {
                        "_id": "66012e9c9e1cf5eb41ee0c4c",
                        "avatarUrl": "/avatars/dfbde0da4408ba86e599732f24a232b1.svg",
                        "isPro": false,
                        "fullname": "Jaewon Min",
                        "user": "Min-Jaewon",
                        "type": "user"
                    },
                    "name": "Jaewon Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T10:37:52.730Z",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1c",
                    "name": "Minjae Kim",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1d",
                    "name": "Wooseok Jang",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1e",
                    "name": "Hyoungwon Cho",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f1f",
                    "name": "Sayak Paul",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f20",
                    "name": "SeonHwa Kim",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f21",
                    "name": "Eunju Cha",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f22",
                    "name": "Kyong Hwan Jin",
                    "hidden": false
                },
                {
                    "_id": "6752a5285281c3cae4a81f23",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T06:09:56.000Z",
            "title": "A Noise is Worth Diffusion Guidance",
            "summary": "Diffusion models excel in generating high-quality images. However, current\ndiffusion models struggle to produce reliable images without guidance methods,\nsuch as classifier-free guidance (CFG). Are guidance methods truly necessary?\nObserving that noise obtained via diffusion inversion can reconstruct\nhigh-quality images without guidance, we focus on the initial noise of the\ndenoising pipeline. By mapping Gaussian noise to `guidance-free noise', we\nuncover that small low-magnitude low-frequency components significantly enhance\nthe denoising process, removing the need for guidance and thus improving both\ninference throughput and memory. Expanding on this, we propose \\ours, a novel\nmethod that replaces guidance methods with a single refinement of the initial\nnoise. This refined noise enables high-quality image generation without\nguidance, within the same diffusion pipeline. Our noise-refining model\nleverages efficient noise-space learning, achieving rapid convergence and\nstrong performance with just 50K text-image pairs. We validate its\neffectiveness across diverse metrics and analyze how refined noise can\neliminate the need for guidance. See our project page:\nhttps://cvlab-kaist.github.io/NoiseRefine/.",
            "upvotes": 19,
            "discussionId": "6752a52f5281c3cae4a8214d"
        },
        "publishedAt": "2024-12-06T02:18:49.514Z",
        "title": "A Noise is Worth Diffusion Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03895.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbb4d09726f7e58f96b70b",
            "avatarUrl": "/avatars/766daad8f5c7ebef33cef07521b23665.svg",
            "fullname": "AHN DONGHOON",
            "name": "sunovivid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        }
    },
    {
        "paper": {
            "id": "2412.04424",
            "authors": [
                {
                    "_id": "67526d9bba72c8bc0779edcf",
                    "name": "Jiuhai Chen",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd0",
                    "name": "Jianwei Yang",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd1",
                    "name": "Haiping Wu",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd2",
                    "name": "Dianqi Li",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd3",
                    "name": "Jianfeng Gao",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd4",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:02.653Z",
                    "hidden": false
                },
                {
                    "_id": "67526d9bba72c8bc0779edd5",
                    "name": "Bin Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:50:39.000Z",
            "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision\n  Encoder and Depth-Breadth Fusion",
            "summary": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL",
            "upvotes": 16,
            "discussionId": "67526d9cba72c8bc0779ee38"
        },
        "publishedAt": "2024-12-05T22:33:31.340Z",
        "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04424.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "fullname": "JiuhaiChen",
            "name": "jiuhai",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        }
    },
    {
        "paper": {
            "id": "2412.03679",
            "authors": [
                {
                    "_id": "67525bedeeb66c5ab7dc99fa",
                    "user": {
                        "_id": "6469949654873f0043b09c22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
                        "isPro": false,
                        "fullname": "Seungone Kim",
                        "user": "seungone",
                        "type": "user"
                    },
                    "name": "Seungone Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:33.388Z",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc99fb",
                    "name": "Juyoung Suk",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc99fc",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc99fd",
                    "name": "Vijay Viswanathan",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc99fe",
                    "name": "Seongyun Lee",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc99ff",
                    "name": "Yizhong Wang",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc9a00",
                    "name": "Kiril Gashteovski",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc9a01",
                    "name": "Carolin Lawrence",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc9a02",
                    "name": "Sean Welleck",
                    "hidden": false
                },
                {
                    "_id": "67525bedeeb66c5ab7dc9a03",
                    "name": "Graham Neubig",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T19:20:32.000Z",
            "title": "Evaluating Language Models as Synthetic Data Generators",
            "summary": "Given the increasing use of synthetic data in language model (LM)\npost-training, an LM's ability to generate high-quality data has become nearly\nas crucial as its ability to solve problems directly. While prior works have\nfocused on developing effective data generation methods, they lack systematic\ncomparison of different LMs as data generators in a unified setting. To address\nthis gap, we propose AgoraBench, a benchmark that provides standardized\nsettings and metrics to evaluate LMs' data generation abilities. Through\nsynthesizing 1.26 million training instances using 6 LMs and training 99\nstudent models, we uncover key insights about LMs' data generation\ncapabilities. First, we observe that LMs exhibit distinct strengths. For\ninstance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet\nperforms better at enhancing existing ones. Furthermore, our analysis reveals\nthat an LM's data generation ability doesn't necessarily correlate with its\nproblem-solving ability. Instead, multiple intrinsic features of data\nquality-including response quality, perplexity, and instruction\ndifficulty-collectively serve as better indicators. Finally, we demonstrate\nthat strategic choices in output format and cost-conscious model selection\nsignificantly impact data generation effectiveness.",
            "upvotes": 15,
            "discussionId": "67525beeeeb66c5ab7dc9a98"
        },
        "publishedAt": "2024-12-05T21:56:59.355Z",
        "title": "Evaluating Language Models as Synthetic Data Generators",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03679.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "fullname": "Seungone Kim",
            "name": "seungone",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 19
        }
    },
    {
        "paper": {
            "id": "2412.01339",
            "authors": [
                {
                    "_id": "674e7c274b7915defe769665",
                    "name": "Jaskirat Singh",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe769666",
                    "name": "Lindsey Li",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe769667",
                    "name": "Weijia Shi",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe769668",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe769669",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe76966a",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe76966b",
                    "name": "Michael F. Cohen",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe76966c",
                    "name": "Stephen Gould",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe76966d",
                    "name": "Liang Zheng",
                    "hidden": false
                },
                {
                    "_id": "674e7c274b7915defe76966e",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T10:06:57.000Z",
            "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
            "summary": "Text-based adversarial guidance using a negative prompt has emerged as a\nwidely adopted approach to push the output features away from undesired\nconcepts. While useful, performing adversarial guidance using text alone can be\ninsufficient to capture complex visual concepts and avoid undesired visual\nelements like copyrighted characters. In this paper, for the first time we\nexplore an alternate modality in this direction by performing adversarial\nguidance directly using visual features from a reference image or other images\nin a batch. In particular, we introduce negative token merging (NegToMe), a\nsimple but effective training-free approach which performs adversarial guidance\nby selectively pushing apart matching semantic features (between reference and\noutput generation) during the reverse diffusion process. When used w.r.t. other\nimages in the same batch, we observe that NegToMe significantly increases\noutput diversity (racial, gender, visual) without sacrificing output image\nquality. Similarly, when used w.r.t. a reference copyrighted asset, NegToMe\nhelps reduce visual similarity with copyrighted content by 34.57%. NegToMe is\nsimple to implement using just few-lines of code, uses only marginally higher\n(<4%) inference times and generalizes to different diffusion architectures like\nFlux, which do not natively support the use of a separate negative prompt. Code\nis available at https://negtome.github.io",
            "upvotes": 13,
            "discussionId": "674e7c2f4b7915defe769907"
        },
        "publishedAt": "2024-12-05T23:24:45.801Z",
        "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60cc389a0844fb1605fef405/Vm9q1-gplF2kpYeqUC8yz.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01339.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "60cc389a0844fb1605fef405",
            "avatarUrl": "/avatars/ec11f85735e0525439e8821cf6d12e53.svg",
            "fullname": "Jaskirat Singh",
            "name": "jsingh",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2412.01506",
            "authors": [
                {
                    "_id": "674f2822ecb7d1fab211ac7b",
                    "user": {
                        "_id": "6513df3175940371506cb73e",
                        "avatarUrl": "/avatars/5f48fd02377f62864c5db5555b9bf413.svg",
                        "isPro": true,
                        "fullname": "Jianfeng Xiang",
                        "user": "JeffreyXiang",
                        "type": "user"
                    },
                    "name": "Jianfeng Xiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:43:35.349Z",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac7c",
                    "user": {
                        "_id": "674ec756d7e624639d198b7c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dLuxrPo-Nhc3SHy74EUDU.png",
                        "isPro": false,
                        "fullname": "Zelong Lv",
                        "user": "Maxtir",
                        "type": "user"
                    },
                    "name": "Zelong Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:43:14.516Z",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac7d",
                    "user": {
                        "_id": "641d402c043963b1c0a578b8",
                        "avatarUrl": "/avatars/d1a7297ca7d4cdbbe484a54d7d223551.svg",
                        "isPro": false,
                        "fullname": "Sicheng Xu",
                        "user": "sicxu",
                        "type": "user"
                    },
                    "name": "Sicheng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:52.189Z",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac7e",
                    "name": "Yu Deng",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac7f",
                    "user": {
                        "_id": "629d87735a13ba8233e63dbd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629d87735a13ba8233e63dbd/4G1r1otoOzszvSr-HeM9f.jpeg",
                        "isPro": true,
                        "fullname": "Ruicheng Wang",
                        "user": "Ruicheng",
                        "type": "user"
                    },
                    "name": "Ruicheng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:43:08.729Z",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac80",
                    "user": {
                        "_id": "64560bb04cfac492278df590",
                        "avatarUrl": "/avatars/cf7acf16ac452a551d32a774ff172d8a.svg",
                        "isPro": false,
                        "fullname": "Bowen Zhang",
                        "user": "bowenzhang",
                        "type": "user"
                    },
                    "name": "Bowen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:42:58.378Z",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac81",
                    "name": "Dong Chen",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac82",
                    "name": "Xin Tong",
                    "hidden": false
                },
                {
                    "_id": "674f2822ecb7d1fab211ac83",
                    "user": {
                        "_id": "66a59f6b94d2b190a22b7c80",
                        "avatarUrl": "/avatars/9b59c0bb3e171d5246e3cb3aa8377a77.svg",
                        "isPro": false,
                        "fullname": "jiaolong yang",
                        "user": "godjiaolongge",
                        "type": "user"
                    },
                    "name": "Jiaolong Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:42:47.364Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T13:58:38.000Z",
            "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
            "summary": "We introduce a novel 3D generation method for versatile and high-quality 3D\nasset creation. The cornerstone is a unified Structured LATent (SLAT)\nrepresentation which allows decoding to different output formats, such as\nRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a\nsparsely-populated 3D grid with dense multiview visual features extracted from\na powerful vision foundation model, comprehensively capturing both structural\n(geometry) and textural (appearance) information while maintaining flexibility\nduring decoding. We employ rectified flow transformers tailored for SLAT as our\n3D generation models and train models with up to 2 billion parameters on a\nlarge 3D asset dataset of 500K diverse objects. Our model generates\nhigh-quality results with text or image conditions, significantly surpassing\nexisting methods, including recent ones at similar scales. We showcase flexible\noutput format selection and local 3D editing capabilities which were not\noffered by previous models. Code, model, and data will be released.",
            "upvotes": 13,
            "discussionId": "674f2827ecb7d1fab211ada0"
        },
        "publishedAt": "2024-12-05T21:33:52.623Z",
        "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6513df3175940371506cb73e/4QcckHqeHrkc-6UL7AyR8.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01506.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6513df3175940371506cb73e",
            "avatarUrl": "/avatars/5f48fd02377f62864c5db5555b9bf413.svg",
            "fullname": "Jianfeng Xiang",
            "name": "JeffreyXiang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        }
    },
    {
        "paper": {
            "id": "2412.03632",
            "authors": [
                {
                    "_id": "6752727b61e5a1edbc6f45fc",
                    "user": {
                        "_id": "6375d136dee28348a9c63cbf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                        "isPro": false,
                        "fullname": "zehuan-huang",
                        "user": "huanngzh",
                        "type": "user"
                    },
                    "name": "Zehuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:41:33.167Z",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f45fd",
                    "user": {
                        "_id": "6346aaa3f06b237ba4e297b0",
                        "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
                        "isPro": false,
                        "fullname": "Yuan-Chen Guo",
                        "user": "bennyguo",
                        "type": "user"
                    },
                    "name": "Yuan-Chen Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:41:38.686Z",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f45fe",
                    "name": "Haoran Wang",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f45ff",
                    "name": "Ran Yi",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f4600",
                    "name": "Lizhuang Ma",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f4601",
                    "user": {
                        "_id": "638066faf022c8a5803f7eb8",
                        "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
                        "isPro": false,
                        "fullname": "Yanpei Cao",
                        "user": "pookiefoof",
                        "type": "user"
                    },
                    "name": "Yan-Pei Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-06T10:42:23.272Z",
                    "hidden": false
                },
                {
                    "_id": "6752727b61e5a1edbc6f4602",
                    "user": {
                        "_id": "65b722dbe02a17f0f8d1cc6b",
                        "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
                        "isPro": false,
                        "fullname": "Lu Sheng",
                        "user": "lsheng2024",
                        "type": "user"
                    },
                    "name": "Lu Sheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:15.578Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T18:48:20.000Z",
            "title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy",
            "summary": "Existing multi-view image generation methods often make invasive\nmodifications to pre-trained text-to-image (T2I) models and require full\nfine-tuning, leading to (1) high computational costs, especially with large\nbase models and high-resolution images, and (2) degradation in image quality\ndue to optimization difficulties and scarce high-quality 3D data. In this\npaper, we propose the first adapter-based solution for multi-view image\ngeneration, and introduce MV-Adapter, a versatile plug-and-play adapter that\nenhances T2I models and their derivatives without altering the original network\nstructure or feature space. By updating fewer parameters, MV-Adapter enables\nefficient training and preserves the prior knowledge embedded in pre-trained\nmodels, mitigating overfitting risks. To efficiently model the 3D geometric\nknowledge within the adapter, we introduce innovative designs that include\nduplicated self-attention layers and parallel attention architecture, enabling\nthe adapter to inherit the powerful priors of the pre-trained models to model\nthe novel 3D knowledge. Moreover, we present a unified condition encoder that\nseamlessly integrates camera parameters and geometric information, facilitating\napplications such as text- and image-based 3D generation and texturing.\nMV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion\nXL (SDXL), and demonstrates adaptability and versatility. It can also be\nextended to arbitrary view generation, enabling broader applications. We\ndemonstrate that MV-Adapter sets a new quality standard for multi-view image\ngeneration, and opens up new possibilities due to its efficiency, adaptability\nand versatility.",
            "upvotes": 12,
            "discussionId": "6752728261e5a1edbc6f48f0"
        },
        "publishedAt": "2024-12-05T22:44:40.045Z",
        "title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6375d136dee28348a9c63cbf/a4snsodPSO2fkoZgbzv8E.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03632.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "fullname": "zehuan-huang",
            "name": "huanngzh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        }
    },
    {
        "paper": {
            "id": "2412.04431",
            "authors": [
                {
                    "_id": "67526cb3067cdbb1447b7879",
                    "name": "Jian Han",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787a",
                    "name": "Jinlai Liu",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787b",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787c",
                    "name": "Bin Yan",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787d",
                    "name": "Yuqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787e",
                    "name": "Zehuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b787f",
                    "name": "Bingyue Peng",
                    "hidden": false
                },
                {
                    "_id": "67526cb3067cdbb1447b7880",
                    "name": "Xiaobing Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:53:02.000Z",
            "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution\n  Image Synthesis",
            "summary": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of\ngenerating high-resolution, photorealistic images following language\ninstruction. Infinity redefines visual autoregressive model under a bitwise\ntoken prediction framework with an infinite-vocabulary tokenizer & classifier\nand bitwise self-correction mechanism, remarkably improving the generation\ncapacity and details. By theoretically scaling the tokenizer vocabulary size to\ninfinity and concurrently scaling the transformer size, our method\nsignificantly unleashes powerful scaling capabilities compared to vanilla VAR.\nInfinity sets a new record for autoregressive text-to-image models,\noutperforming top-tier diffusion models like SD3-Medium and SDXL. Notably,\nInfinity surpasses SD3-Medium by improving the GenEval benchmark score from\n0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a\nwin rate of 66%. Without extra optimization, Infinity generates a high-quality\n1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and\nestablishing it as the fastest text-to-image model. Models and codes will be\nreleased to promote further exploration of Infinity for visual generation and\nunified tokenizer modeling.",
            "upvotes": 9,
            "discussionId": "67526cb6067cdbb1447b791a"
        },
        "publishedAt": "2024-12-05T22:27:00.463Z",
        "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04431.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b796079ebb7e6c7ddcdabf",
            "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
            "fullname": "Liao Qu",
            "name": "leo1117",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.04146",
            "authors": [
                {
                    "_id": "6752cf141a15db83fd3777ef",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T10:37:49.071Z",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f0",
                    "name": "Qichao Sun",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f1",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f2",
                    "name": "Fulong Ye",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f3",
                    "name": "Zhichao Liao",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f4",
                    "name": "Wanquan Feng",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f5",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6752cf141a15db83fd3777f6",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T13:16:47.000Z",
            "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent\n  Diffusion Models",
            "summary": "Recent advances in garment-centric image generation from text and image\nprompts based on diffusion models are impressive. However, existing methods\nlack support for various combinations of attire, and struggle to preserve the\ngarment details while maintaining faithfulness to the text prompts, limiting\ntheir performance across diverse scenarios. In this paper, we focus on a new\ntask, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing\nmethod for customizing characters conditioned on any combination of garments\nand any personalized text prompts. AnyDressing comprises two primary networks\nnamed GarmentsNet and DressingNet, which are respectively dedicated to\nextracting detailed clothing features and generating customized images.\nSpecifically, we propose an efficient and scalable module called\nGarment-Specific Feature Extractor in GarmentsNet to individually encode\ngarment textures in parallel. This design prevents garment confusion while\nensuring network efficiency. Meanwhile, we design an adaptive\nDressing-Attention mechanism and a novel Instance-Level Garment Localization\nLearning strategy in DressingNet to accurately inject multi-garment features\ninto their corresponding regions. This approach efficiently integrates\nmulti-garment texture cues into generated images and further enhances\ntext-image consistency. Additionally, we introduce a Garment-Enhanced Texture\nLearning strategy to improve the fine-grained texture details of garments.\nThanks to our well-craft design, AnyDressing can serve as a plug-in module to\neasily integrate with any community control extensions for diffusion models,\nimproving the diversity and controllability of synthesized images. Extensive\nexperiments show that AnyDressing achieves state-of-the-art results.",
            "upvotes": 8,
            "discussionId": "6752cf181a15db83fd3779a0"
        },
        "publishedAt": "2024-12-06T06:48:12.142Z",
        "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6752cd83ffaeeb979db974ae/MJYs23es9i9zCA7ixBNJ3.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04146.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6752cd83ffaeeb979db974ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
            "fullname": "Xinghui Li",
            "name": "Crayon-Shinchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.04315",
            "authors": [
                {
                    "_id": "675270943d5dc4e06f207c72",
                    "user": {
                        "_id": "608f6d72283d0a8d7be9d1f9",
                        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                        "isPro": false,
                        "fullname": "Chaojun XIAO",
                        "user": "xcjthu",
                        "type": "user"
                    },
                    "name": "Chaojun Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:08:58.922Z",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c73",
                    "name": "Jie Cai",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c74",
                    "name": "Weilin Zhao",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c75",
                    "name": "Guoyang Zeng",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c76",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c77",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "675270943d5dc4e06f207c78",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T16:31:13.000Z",
            "title": "Densing Law of LLMs",
            "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``capacity density'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the effective parameter size of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
            "upvotes": 8,
            "discussionId": "675270953d5dc4e06f207cb4"
        },
        "publishedAt": "2024-12-05T22:34:33.613Z",
        "title": "Densing Law of LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04315.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "608f6d72283d0a8d7be9d1f9",
            "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
            "fullname": "Chaojun XIAO",
            "name": "xcjthu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2412.04280",
            "authors": [
                {
                    "_id": "6752613e5d8bc6e501691fee",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:25.700Z",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691fef",
                    "user": {
                        "_id": "644b71ddb2e7823a76abcf91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
                        "isPro": false,
                        "fullname": "zhou wei",
                        "user": "WeiChow",
                        "type": "user"
                    },
                    "name": "Wei Chow",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:23.481Z",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691ff0",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691ff1",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691ff2",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691ff3",
                    "name": "Hanwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6752613e5d8bc6e501691ff4",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T16:00:59.000Z",
            "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based\n  Image Editing",
            "summary": "We present HumanEdit, a high-quality, human-rewarded dataset specifically\ndesigned for instruction-guided image editing, enabling precise and diverse\nimage manipulations through open-form language instructions. Previous\nlarge-scale editing datasets often incorporate minimal human feedback, leading\nto challenges in aligning datasets with human preferences. HumanEdit bridges\nthis gap by employing human annotators to construct data pairs and\nadministrators to provide feedback. With meticulously curation, HumanEdit\ncomprises 5,751 images and requires more than 2,500 hours of human effort\nacross four stages, ensuring both accuracy and reliability for a wide range of\nimage editing tasks. The dataset includes six distinct types of editing\ninstructions: Action, Add, Counting, Relation, Remove, and Replace,\nencompassing a broad spectrum of real-world scenarios. All images in the\ndataset are accompanied by masks, and for a subset of the data, we ensure that\nthe instructions are sufficiently detailed to support mask-free editing.\nFurthermore, HumanEdit offers comprehensive diversity and high-resolution 1024\ntimes 1024 content sourced from various domains, setting a new versatile\nbenchmark for instructional image editing datasets. With the aim of advancing\nfuture research and establishing evaluation benchmarks in the field of image\nediting, we release HumanEdit at\nhttps://huggingface.co/datasets/BryanW/HumanEdit.",
            "upvotes": 7,
            "discussionId": "675261455d8bc6e501692494"
        },
        "publishedAt": "2024-12-05T21:30:42.373Z",
        "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04280.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "fullname": "Jinbin Bai",
            "name": "BryanW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.04378",
            "authors": [
                {
                    "_id": "6752e52e8b6b5be84ef0065b",
                    "user": {
                        "_id": "60d5fc283039da71be392aaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624636332039-noauth.png",
                        "isPro": false,
                        "fullname": "Yassine Ouali",
                        "user": "youali",
                        "type": "user"
                    },
                    "name": "Yassine Ouali",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:55:38.196Z",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef0065c",
                    "user": {
                        "_id": "64a6e0923987f4dd3c37087d",
                        "avatarUrl": "/avatars/88bc86ddce1b38a012b07c81f5c61183.svg",
                        "isPro": false,
                        "fullname": "Adrian Bulat",
                        "user": "adrianb1",
                        "type": "user"
                    },
                    "name": "Adrian Bulat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:55:40.423Z",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef0065d",
                    "user": {
                        "_id": "62430990422d7ec108c0c57d",
                        "avatarUrl": "/avatars/23ed460f9c544252d904a670c3a8fff1.svg",
                        "isPro": false,
                        "fullname": "Alexandros Xenos",
                        "user": "alxenos",
                        "type": "user"
                    },
                    "name": "Alexandros Xenos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:55:36.081Z",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef0065e",
                    "name": "Anestis Zaganidis",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef0065f",
                    "name": "Ioannis Maniadis Metaxas",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef00660",
                    "name": "Georgios Tzimiropoulos",
                    "hidden": false
                },
                {
                    "_id": "6752e52e8b6b5be84ef00661",
                    "name": "Brais Martinez",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T17:54:27.000Z",
            "title": "Discriminative Fine-tuning of LVLMs",
            "summary": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown capable of\ndetailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include: (1) A carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components. (2) A parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters. (3) Significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
            "upvotes": 6,
            "discussionId": "6752e52f8b6b5be84ef006c7"
        },
        "publishedAt": "2024-12-06T06:51:25.027Z",
        "title": "Discriminative Fine-tuning of LVLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04378.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64a6e0923987f4dd3c37087d",
            "avatarUrl": "/avatars/88bc86ddce1b38a012b07c81f5c61183.svg",
            "fullname": "Adrian Bulat",
            "name": "adrianb1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.02142",
            "authors": [
                {
                    "_id": "675286e49106cbed4a723624",
                    "name": "Junda Wu",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723625",
                    "name": "Hanjia Lyu",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723626",
                    "name": "Yu Xia",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723627",
                    "name": "Zhehao Zhang",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723628",
                    "name": "Joe Barrow",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723629",
                    "name": "Ishita Kumar",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362a",
                    "name": "Mehrnoosh Mirtaheri",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362b",
                    "name": "Hongjie Chen",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362c",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362d",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:08:51.410Z",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362e",
                    "name": "Tong Yu",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72362f",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723630",
                    "name": "Jiuxiang Gu",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723631",
                    "name": "Nesreen K. Ahmed",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723632",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723633",
                    "name": "Xiang Chen",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723634",
                    "name": "Hanieh Deilamsalehy",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723635",
                    "name": "Namyong Park",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723636",
                    "name": "Sungchul Kim",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723637",
                    "name": "Huanrui Yang",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723638",
                    "name": "Subrata Mitra",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a723639",
                    "name": "Zhengmian Hu",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72363a",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72363b",
                    "name": "Dang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72363c",
                    "name": "Yue Zhao",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72363d",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "675286e49106cbed4a72363e",
                    "name": "Julian McAuley",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-03T03:59:03.000Z",
            "title": "Personalized Multimodal Large Language Models: A Survey",
            "summary": "Multimodal Large Language Models (MLLMs) have become increasingly important\ndue to their state-of-the-art performance and ability to integrate multiple\ndata modalities, such as text, images, and audio, to perform complex tasks with\nhigh accuracy. This paper presents a comprehensive survey on personalized\nmultimodal large language models, focusing on their architecture, training\nmethods, and applications. We propose an intuitive taxonomy for categorizing\nthe techniques used to personalize MLLMs to individual users, and discuss the\ntechniques accordingly. Furthermore, we discuss how such techniques can be\ncombined or adapted when appropriate, highlighting their advantages and\nunderlying rationale. We also provide a succinct summary of personalization\ntasks investigated in existing research, along with the evaluation metrics\ncommonly used. Additionally, we summarize the datasets that are useful for\nbenchmarking personalized MLLMs. Finally, we outline critical open challenges.\nThis survey aims to serve as a valuable resource for researchers and\npractitioners seeking to understand and advance the development of personalized\nmultimodal large language models.",
            "upvotes": 6,
            "discussionId": "675286e59106cbed4a72366f"
        },
        "publishedAt": "2024-12-06T00:08:56.179Z",
        "title": "Personalized Multimodal Large Language Models: A Survey",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02142.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.04062",
            "authors": [
                {
                    "_id": "67527072dc10e013a0cf4761",
                    "name": "Yefei He",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4762",
                    "name": "Feng Chen",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4763",
                    "name": "Yuanyu He",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4764",
                    "name": "Shaoxuan He",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4765",
                    "name": "Hong Zhou",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4766",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67527072dc10e013a0cf4767",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T10:57:08.000Z",
            "title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial\n  Locality",
            "summary": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining.",
            "upvotes": 6,
            "discussionId": "67527074dc10e013a0cf47e4"
        },
        "publishedAt": "2024-12-05T22:34:30.289Z",
        "title": "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/xwazVYjndQ2V4YPmBjRd3.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/kZikXyTfmUgseEGXGfNkQ.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04062.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.01820",
            "authors": [
                {
                    "_id": "6752678b4ca8baad001c64db",
                    "user": {
                        "_id": "6625ce8074ae2df4e3effa92",
                        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
                        "isPro": false,
                        "fullname": "Jiayuan Rao 饶珈源",
                        "user": "Homie0609",
                        "type": "user"
                    },
                    "name": "Jiayuan Rao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:14.974Z",
                    "hidden": false
                },
                {
                    "_id": "6752678b4ca8baad001c64dc",
                    "user": {
                        "_id": "632c7a0d1d303f5f9acf01b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
                        "isPro": false,
                        "fullname": "Haoning Wu",
                        "user": "haoningwu",
                        "type": "user"
                    },
                    "name": "Haoning Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:16.901Z",
                    "hidden": false
                },
                {
                    "_id": "6752678b4ca8baad001c64dd",
                    "name": "Hao Jiang",
                    "hidden": false
                },
                {
                    "_id": "6752678b4ca8baad001c64de",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "6752678b4ca8baad001c64df",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6752678b4ca8baad001c64e0",
                    "name": "Weidi Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:58:04.000Z",
            "title": "Towards Universal Soccer Video Understanding",
            "summary": "As a globally celebrated sport, soccer has attracted widespread interest from\nfans all over the world. This paper aims to develop a comprehensive multi-modal\nframework for soccer video understanding. Specifically, we make the following\ncontributions in this paper: (i) we introduce SoccerReplay-1988, the largest\nmulti-modal soccer dataset to date, featuring videos and detailed annotations\nfrom 1,988 complete matches, with an automated annotation pipeline; (ii) we\npresent the first visual-language foundation model in the soccer domain,\nMatchVision, which leverages spatiotemporal information across soccer videos\nand excels in various downstream tasks; (iii) we conduct extensive experiments\nand ablation studies on event classification, commentary generation, and\nmulti-view foul recognition. MatchVision demonstrates state-of-the-art\nperformance on all of them, substantially outperforming existing models, which\nhighlights the superiority of our proposed data and model. We believe that this\nwork will offer a standard paradigm for sports understanding research.",
            "upvotes": 6,
            "discussionId": "6752678e4ca8baad001c65bb"
        },
        "publishedAt": "2024-12-05T21:58:07.580Z",
        "title": "Towards Universal Soccer Video Understanding",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/RufCeZRUUkcm12kqrJV3h.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01820.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "fullname": "Haoning Wu",
            "name": "haoningwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.03304",
            "authors": [
                {
                    "_id": "675193c76af9371edd506f32",
                    "user": {
                        "_id": "62c2175d756039dd0dd20509",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677753847837-62c2175d756039dd0dd20509.jpeg",
                        "isPro": false,
                        "fullname": "Shivalika Singh",
                        "user": "shivi",
                        "type": "user"
                    },
                    "name": "Shivalika Singh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:32.994Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f33",
                    "user": {
                        "_id": "62a0803e3e7d1dda046d475a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0803e3e7d1dda046d475a/Oi1qHSaX7_PU1pS8qaXKv.jpeg",
                        "isPro": false,
                        "fullname": "Angelika Romanou",
                        "user": "angelika",
                        "type": "user"
                    },
                    "name": "Angelika Romanou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:21.884Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f34",
                    "name": "Clémentine Fourrier",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f35",
                    "user": {
                        "_id": "5fcc1929563427b03e9af259",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620642852901-5fcc1929563427b03e9af259.jpeg",
                        "isPro": false,
                        "fullname": "David Adelani",
                        "user": "Davlan",
                        "type": "user"
                    },
                    "name": "David I. Adelani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:24.007Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f36",
                    "user": {
                        "_id": "65727254948b9e1dbc1eb717",
                        "avatarUrl": "/avatars/af0c1d4c5ad16f7c856538dd7c276cf6.svg",
                        "isPro": false,
                        "fullname": "Jian Gang Ngui",
                        "user": "Jian-Gang",
                        "type": "user"
                    },
                    "name": "Jian Gang Ngui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:17.762Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f37",
                    "user": {
                        "_id": "60420dccc15e823a685f2b03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60420dccc15e823a685f2b03/Dn7QTyy9SZ7jKN6xpufVD.png",
                        "isPro": false,
                        "fullname": "Daniel Vila",
                        "user": "dvilasuero",
                        "type": "user"
                    },
                    "name": "Daniel Vila-Suero",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:35.570Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f38",
                    "name": "Peerat Limkonchotiwat",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f39",
                    "name": "Kelly Marchisio",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3a",
                    "name": "Wei Qi Leong",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3b",
                    "name": "Yosephine Susanto",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3c",
                    "name": "Raymond Ng",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3d",
                    "name": "Shayne Longpre",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3e",
                    "name": "Wei-Yin Ko",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f3f",
                    "name": "Madeline Smith",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f40",
                    "name": "Antoine Bosselut",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f41",
                    "name": "Alice Oh",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f42",
                    "name": "Andre F. T. Martins",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f43",
                    "user": {
                        "_id": "61bf40824b4300d0fb0acf59",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg",
                        "isPro": false,
                        "fullname": "Leshem Choshen",
                        "user": "borgr",
                        "type": "user"
                    },
                    "name": "Leshem Choshen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T15:57:19.561Z",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f44",
                    "name": "Daphne Ippolito",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f45",
                    "name": "Enzo Ferrante",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f46",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f47",
                    "name": "Beyza Ermis",
                    "hidden": false
                },
                {
                    "_id": "675193c76af9371edd506f48",
                    "name": "Sara Hooker",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T13:27:09.000Z",
            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases\n  in Multilingual Evaluation",
            "summary": "Cultural biases in multilingual datasets pose significant challenges for\ntheir effectiveness as global benchmarks. These biases stem not only from\nlanguage but also from the cultural knowledge required to interpret questions,\nreducing the practical utility of translated datasets like MMLU. Furthermore,\ntranslation often introduces artifacts that can distort the meaning or clarity\nof questions in the target language. A common practice in multilingual\nevaluation is to rely on machine-translated evaluation sets, but simply\ntranslating a dataset is insufficient to address these challenges. In this\nwork, we trace the impact of both of these issues on multilingual evaluations\nand ensuing model performances. Our large-scale evaluation of state-of-the-art\nopen and proprietary models illustrates that progress on MMLU depends heavily\non learning Western-centric concepts, with 28% of all questions requiring\nculturally sensitive knowledge. Moreover, for questions requiring geographic\nknowledge, an astounding 84.9% focus on either North American or European\nregions. Rankings of model evaluations change depending on whether they are\nevaluated on the full portion or the subset of questions annotated as\nculturally sensitive, showing the distortion to model rankings when blindly\nrelying on translated MMLU. We release Global-MMLU, an improved MMLU with\nevaluation coverage across 42 languages -- with improved overall quality by\nengaging with compensated professional and community annotators to verify\ntranslation quality while also rigorously evaluating cultural biases present in\nthe original dataset. This comprehensive Global-MMLU set also includes\ndesignated subsets labeled as culturally sensitive and culturally agnostic to\nallow for more holistic, complete evaluation.",
            "upvotes": 5,
            "discussionId": "675193ca6af9371edd506ff7"
        },
        "publishedAt": "2024-12-06T05:25:52.091Z",
        "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03304.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60420dccc15e823a685f2b03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60420dccc15e823a685f2b03/Dn7QTyy9SZ7jKN6xpufVD.png",
            "fullname": "Daniel Vila",
            "name": "dvilasuero",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 240
        }
    },
    {
        "paper": {
            "id": "2412.01169",
            "authors": [
                {
                    "_id": "67507dcb10a0006992dd4612",
                    "name": "Shufan Li",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4613",
                    "name": "Konstantinos Kallidromitis",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4614",
                    "name": "Akash Gokul",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4615",
                    "name": "Zichun Liao",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4616",
                    "name": "Yusuke Kato",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4617",
                    "name": "Kazuki Kozuka",
                    "hidden": false
                },
                {
                    "_id": "67507dcb10a0006992dd4618",
                    "name": "Aditya Grover",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T06:13:01.000Z",
            "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
            "summary": "We introduce OmniFlow, a novel generative model designed for any-to-any\ngeneration tasks such as text-to-image, text-to-audio, and audio-to-image\nsynthesis. OmniFlow advances the rectified flow (RF) framework used in\ntext-to-image models to handle the joint distribution of multiple modalities.\nIt outperforms previous any-to-any models on a wide range of tasks, such as\ntext-to-image and text-to-audio synthesis. Our work offers three key\ncontributions: First, we extend RF to a multi-modal setting and introduce a\nnovel guidance mechanism, enabling users to flexibly control the alignment\nbetween different modalities in the generated outputs. Second, we propose a\nnovel architecture that extends the text-to-image MMDiT architecture of Stable\nDiffusion 3 and enables audio and text generation. The extended modules can be\nefficiently pretrained individually and merged with the vanilla text-to-image\nMMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design\nchoices of rectified flow transformers for large-scale audio and text\ngeneration, providing valuable insights into optimizing performance across\ndiverse modalities. The Code will be available at\nhttps://github.com/jacklishufan/OmniFlows.",
            "upvotes": 5,
            "discussionId": "67507dcd10a0006992dd46d8"
        },
        "publishedAt": "2024-12-05T22:03:01.518Z",
        "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01169.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6310531914aa81e1044363ed",
            "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
            "fullname": "Shufan Li",
            "name": "jacklishufan",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2412.04106",
            "authors": [
                {
                    "_id": "67526571c87c395cd2fc896e",
                    "user": {
                        "_id": "632c7a0d1d303f5f9acf01b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
                        "isPro": false,
                        "fullname": "Haoning Wu",
                        "user": "haoningwu",
                        "type": "user"
                    },
                    "name": "Haoning Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:19.410Z",
                    "hidden": false
                },
                {
                    "_id": "67526571c87c395cd2fc896f",
                    "name": "Ziheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "67526571c87c395cd2fc8970",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "67526571c87c395cd2fc8971",
                    "name": "Weidi Xie",
                    "hidden": false
                },
                {
                    "_id": "67526571c87c395cd2fc8972",
                    "name": "Yanfeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T16:34:22.000Z",
            "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation\n  towards Unannotated Modalities",
            "summary": "Medical image segmentation has recently demonstrated impressive progress with\ndeep neural networks, yet the heterogeneous modalities and scarcity of mask\nannotations limit the development of segmentation models on unannotated\nmodalities. This paper investigates a new paradigm for leveraging generative\nmodels in medical applications: controllably synthesizing data for unannotated\nmodalities, without requiring registered data pairs. Specifically, we make the\nfollowing contributions in this paper: (i) we collect and curate a large-scale\nradiology image-text dataset, MedGen-1M, comprising modality labels,\nattributes, region, and organ information, along with a subset of organ mask\nannotations, to support research in controllable medical image generation; (ii)\nwe propose a diffusion-based data engine, termed MRGen, which enables\ngeneration conditioned on text prompts and masks, synthesizing MR images for\ndiverse modalities lacking mask annotations, to train segmentation models on\nunannotated modalities; (iii) we conduct extensive experiments across various\nmodalities, illustrating that our data engine can effectively synthesize\ntraining samples and extend MRI segmentation towards unannotated modalities.",
            "upvotes": 5,
            "discussionId": "67526572c87c395cd2fc89bd"
        },
        "publishedAt": "2024-12-05T21:51:46.197Z",
        "title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/ByItoHoqR4ePi_nqCE3CJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04106.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "fullname": "Haoning Wu",
            "name": "haoningwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.04139",
            "authors": [
                {
                    "_id": "675282e85b1cc8f5a17f70d2",
                    "user": {
                        "_id": "60f8435644e75317cc02ed51",
                        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
                        "isPro": false,
                        "fullname": "Jungwoo Park",
                        "user": "affjljoo3581",
                        "type": "user"
                    },
                    "name": "Jungwoo Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:08:53.485Z",
                    "hidden": false
                },
                {
                    "_id": "675282e85b1cc8f5a17f70d3",
                    "name": "Young Jin Ahn",
                    "hidden": false
                },
                {
                    "_id": "675282e85b1cc8f5a17f70d4",
                    "name": "Kee-Eung Kim",
                    "hidden": false
                },
                {
                    "_id": "675282e85b1cc8f5a17f70d5",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T13:06:03.000Z",
            "title": "Monet: Mixture of Monosemantic Experts for Transformers",
            "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance} mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust} model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
            "upvotes": 4,
            "discussionId": "675282ea5b1cc8f5a17f71a8"
        },
        "publishedAt": "2024-12-05T23:54:17.477Z",
        "title": "Monet: Mixture of Monosemantic Experts for Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04139.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f8435644e75317cc02ed51",
            "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
            "fullname": "Jungwoo Park",
            "name": "affjljoo3581",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.19574",
            "authors": [
                {
                    "_id": "675269205281c3cae493244d",
                    "user": {
                        "_id": "662f3c239e6d371ab7903b01",
                        "avatarUrl": "/avatars/e36d774f4371a05f27c0950c624f8d3e.svg",
                        "isPro": false,
                        "fullname": "xumingyu",
                        "user": "xumingyu16",
                        "type": "user"
                    },
                    "name": "Mingyu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T09:09:12.672Z",
                    "hidden": false
                },
                {
                    "_id": "675269205281c3cae493244e",
                    "name": "Wei Cheng",
                    "hidden": false
                },
                {
                    "_id": "675269205281c3cae493244f",
                    "name": "Bingning Wang",
                    "hidden": false
                },
                {
                    "_id": "675269205281c3cae4932450",
                    "name": "Weipeng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T09:42:38.000Z",
            "title": "KV Shifting Attention Enhances Language Modeling",
            "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
            "upvotes": 3,
            "discussionId": "675269225281c3cae4932491"
        },
        "publishedAt": "2024-12-06T04:23:17.860Z",
        "title": "KV Shifting Attention Enhances Language Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19574.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "662f3c239e6d371ab7903b01",
            "avatarUrl": "/avatars/e36d774f4371a05f27c0950c624f8d3e.svg",
            "fullname": "xumingyu",
            "name": "xumingyu16",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.04003",
            "authors": [
                {
                    "_id": "6752a3acd34997a5fbc79614",
                    "name": "Lingfeng Ming",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79615",
                    "name": "Bo Zeng",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79616",
                    "name": "Chenyang Lyu",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79617",
                    "name": "Tianqi Shi",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79618",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79619",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961a",
                    "name": "Yefeng Liu",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961b",
                    "name": "Yiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961c",
                    "name": "Linlong Xu",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961d",
                    "name": "Yangyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961e",
                    "name": "Xiaohu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc7961f",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79620",
                    "name": "Heng Liu",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79621",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79622",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79623",
                    "name": "Zifu Shang",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79624",
                    "name": "Haijun Li",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79625",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79626",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "6752a3acd34997a5fbc79627",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T09:26:58.000Z",
            "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement",
            "summary": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
            "upvotes": 2,
            "discussionId": "6752a3add34997a5fbc79673"
        },
        "publishedAt": "2024-12-06T02:14:58.244Z",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04003.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5314
        }
    },
    {
        "paper": {
            "id": "2412.03704",
            "authors": [
                {
                    "_id": "675320b0cb11c5f0308bf62b",
                    "name": "Wang Xiyao",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf62c",
                    "name": "Yang Zhengyuan",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf62d",
                    "name": "Li Linjie",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf62e",
                    "name": "Lu Hongjin",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf62f",
                    "name": "Xu Yuancheng",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf630",
                    "name": "Lin Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf631",
                    "name": "Lin Kevin",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf632",
                    "name": "Huang Furong",
                    "hidden": false
                },
                {
                    "_id": "675320b0cb11c5f0308bf633",
                    "name": "Wang Lijuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T20:35:07.000Z",
            "title": "Scaling Inference-Time Search with Vision Value Model for Improved\n  Visual Comprehension",
            "summary": "Despite significant advancements in vision-language models (VLMs), there\nlacks effective approaches to enhance response quality by scaling\ninference-time computation. This capability is known to be a core step towards\nthe self-improving models in recent large language model studies. In this\npaper, we present Vision Value Model (VisVM) that can guide VLM inference-time\nsearch to generate responses with better visual comprehension. Specifically,\nVisVM not only evaluates the generated sentence quality in the current search\nstep, but also anticipates the quality of subsequent sentences that may result\nfrom the current step, thus providing a long-term value. In this way, VisVM\nsteers VLMs away from generating sentences prone to hallucinations or\ninsufficient detail, thereby producing higher quality responses. Experimental\nresults demonstrate that VisVM-guided search significantly enhances VLMs'\nability to generate descriptive captions with richer visual details and fewer\nhallucinations, compared with greedy decoding and search methods with other\nvisual reward signals. Furthermore, we find that self-training the model with\nthe VisVM-guided captions improve VLM's performance across a wide range of\nmultimodal benchmarks, indicating the potential for developing self-improving\nVLMs. Our value model and code are available at\nhttps://github.com/si0wang/VisVM.",
            "upvotes": 1,
            "discussionId": "675320b2cb11c5f0308bf6d6"
        },
        "publishedAt": "2024-12-06T11:06:22.941Z",
        "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03704.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655fed9fdef5905d38b84af3",
            "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
            "fullname": "Xiyao Wang",
            "name": "russwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2412.04262",
            "authors": [
                {
                    "_id": "6752b7fa706d6371eb7ff47d",
                    "user": {
                        "_id": "6262ce003662a749162c08ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262ce003662a749162c08ed/NuY6APYss9cXwuFC-CpFT.jpeg",
                        "isPro": false,
                        "fullname": "Ethan Bradley",
                        "user": "ethanbradley",
                        "type": "user"
                    },
                    "name": "Ethan Bradley",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-06T10:37:51.109Z",
                    "hidden": false
                },
                {
                    "_id": "6752b7fa706d6371eb7ff47e",
                    "name": "Muhammad Roman",
                    "hidden": false
                },
                {
                    "_id": "6752b7fa706d6371eb7ff47f",
                    "name": "Karen Rafferty",
                    "hidden": false
                },
                {
                    "_id": "6752b7fa706d6371eb7ff480",
                    "name": "Barry Devereux",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T15:42:59.000Z",
            "title": "SynFinTabs: A Dataset of Synthetic Financial Tables for Information and\n  Table Extraction",
            "summary": "Table extraction from document images is a challenging AI problem, and\nlabelled data for many content domains is difficult to come by. Existing table\nextraction datasets often focus on scientific tables due to the vast amount of\nacademic articles that are readily available, along with their source code.\nHowever, there are significant layout and typographical differences between\ntables found across scientific, financial, and other domains. Current datasets\noften lack the words, and their positions, contained within the tables, instead\nrelying on unreliable OCR to extract these features for training modern machine\nlearning models on natural language processing tasks. Therefore, there is a\nneed for a more general method of obtaining labelled data. We present\nSynFinTabs, a large-scale, labelled dataset of synthetic financial tables. Our\nhope is that our method of generating these synthetic tables is transferable to\nother domains. To demonstrate the effectiveness of our dataset in training\nmodels to extract information from table images, we create FinTabQA, a layout\nlarge language model trained on an extractive question-answering task. We test\nour model using real-world financial tables and compare it to a\nstate-of-the-art generative model and discuss the results. We make the dataset,\nmodel, and dataset generation code publicly available.",
            "upvotes": 0,
            "discussionId": "6752b7fb706d6371eb7ff4b9"
        },
        "publishedAt": "2024-12-06T11:24:19.459Z",
        "title": "SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04262.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6262ce003662a749162c08ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262ce003662a749162c08ed/NuY6APYss9cXwuFC-CpFT.jpeg",
            "fullname": "Ethan Bradley",
            "name": "ethanbradley",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.04448",
            "authors": [
                {
                    "_id": "675265f58be943d9cb551721",
                    "name": "Longtao Zheng",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551722",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551723",
                    "name": "Hanzhong Guo",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551724",
                    "name": "Jiachun Pan",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551725",
                    "name": "Zhenxiong Tan",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551726",
                    "name": "Jiahao Lu",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551727",
                    "name": "Chuanxin Tang",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551728",
                    "name": "Bo An",
                    "hidden": false
                },
                {
                    "_id": "675265f58be943d9cb551729",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:57:26.000Z",
            "title": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation",
            "summary": "Recent advances in video diffusion models have unlocked new potential for\nrealistic audio-driven talking video generation. However, achieving seamless\naudio-lip synchronization, maintaining long-term identity consistency, and\nproducing natural, audio-aligned expressions in generated talking videos remain\nsignificant challenges. To address these challenges, we propose Memory-guided\nEMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation\napproach to generate identity-consistent and expressive talking videos. Our\napproach is built around two key modules: (1) a memory-guided temporal module,\nwhich enhances long-term identity consistency and motion smoothness by\ndeveloping memory states to store information from a longer past context to\nguide temporal modeling via linear attention; and (2) an emotion-aware audio\nmodule, which replaces traditional cross attention with multi-modal attention\nto enhance audio-video interaction, while detecting emotions from audio to\nrefine facial expressions via emotion adaptive layer norm. Extensive\nquantitative and qualitative results demonstrate that MEMO generates more\nrealistic talking videos across diverse image and audio types, outperforming\nstate-of-the-art methods in overall quality, audio-lip synchronization,\nidentity consistency, and expression-emotion alignment.",
            "upvotes": 0,
            "discussionId": "675265f88be943d9cb5517de"
        },
        "publishedAt": "2024-12-06T10:28:41.069Z",
        "title": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63db5dc49f2687298a1547bf/dUdUF4NvpNvZPuX59VKhd.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/63db5dc49f2687298a1547bf/2UY8S34f4DKCUNCJZri8s.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/63db5dc49f2687298a1547bf/MRND-NAfGCdUGkyCndYfQ.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/63db5dc49f2687298a1547bf/E133y3IFK11oAs6bKciWF.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/63db5dc49f2687298a1547bf/mtDz1rNbpXUHYWo3Tn3Tq.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04448.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63db5dc49f2687298a1547bf",
            "avatarUrl": "/avatars/a3f87dbf41b836f043f01b61ffd820ea.svg",
            "fullname": "Longtao Zheng",
            "name": "ltzheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.04363",
            "authors": [
                {
                    "_id": "67530912232ea247a627e51a",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "67530912232ea247a627e51b",
                    "name": "Alexander M. Rush",
                    "hidden": false
                },
                {
                    "_id": "67530912232ea247a627e51c",
                    "name": "Tanya Goyal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T17:22:04.000Z",
            "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
            "summary": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
            "upvotes": 0,
            "discussionId": "67530912232ea247a627e562"
        },
        "publishedAt": "2024-12-06T09:25:11.216Z",
        "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04363.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ffdbaab09f82a81a222c27",
            "avatarUrl": "/avatars/3aca53f6555f4548489844902fe4a80a.svg",
            "fullname": "Wenting Zhao",
            "name": "wentingzhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        }
    }
]