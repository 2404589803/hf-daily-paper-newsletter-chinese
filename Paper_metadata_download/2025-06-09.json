[
  {
    "paper": {
      "id": "2506.01111",
      "authors": [
        {
          "_id": "6845b6a33ec10bdd8ab4da1b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1c",
          "name": "Xinyuan Xie",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1d",
          "name": "Zheshu Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1e",
          "name": "Liyan Zhao",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1f",
          "name": "Owen Lee",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da20",
          "name": "Zhan Su",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da21",
          "name": "Qilin Sun",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da22",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T18:29:17.000Z",
      "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
      "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
      "upvotes": 18,
      "discussionId": "6845b6a43ec10bdd8ab4da23",
      "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
      "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
      "ai_keywords": [
        "audio captioning",
        "auditory perception",
        "auditory scene analysis",
        "pretrained models",
        "large language model",
        "FusionAudio",
        "CLAP-based audio encoder",
        "audio-text alignment",
        "instruction following"
      ]
    },
    "publishedAt": "2025-06-01T14:29:17.000Z",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
    "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05984",
      "authors": [
        {
          "_id": "68463ee43ec10bdd8ab4da6f",
          "name": "Cheng-Han Chiang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da70",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da71",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da72",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da73",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da74",
          "name": "Radu Kopetz",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da75",
          "name": "Yao Qian",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da76",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da77",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da78",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da79",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:05:48.000Z",
      "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
      "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
      "upvotes": 6,
      "discussionId": "68463ee43ec10bdd8ab4da7a",
      "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
      "ai_keywords": [
        "audio-aware large language models",
        "ALLMs",
        "speaking styles",
        "SLMs",
        "voice style instruction",
        "role-playing",
        "emotion",
        "volume",
        "speaking pace",
        "word emphasis",
        "pitch control",
        "non-verbal elements",
        "GPT-4o-audio",
        "Gemini-2.5-pro",
        "human evaluation",
        "agreement",
        "speaking style control",
        "natural dialogues"
      ]
    },
    "publishedAt": "2025-06-06T07:05:48.000Z",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05629",
      "authors": [
        {
          "_id": "68464b273ec10bdd8ab4da86",
          "user": {
            "_id": "64a6518132cf858d6386ac52",
            "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
            "isPro": false,
            "fullname": "Ananth Muppidi",
            "user": "ananthmuppidi",
            "type": "user"
          },
          "name": "Ananth Muppidi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da87",
          "user": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "isPro": false,
            "fullname": "Abhilash Nandy",
            "user": "abhi1nandy2",
            "type": "user"
          },
          "name": "Abhilash Nandy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da88",
          "user": {
            "_id": "65238ea295df08170c93933d",
            "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
            "isPro": false,
            "fullname": "Sambaran Bandyopadhyay",
            "user": "sambaran",
            "type": "user"
          },
          "name": "Sambaran Bandyopadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T23:13:22.000Z",
      "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
      "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
      "upvotes": 5,
      "discussionId": "68464b273ec10bdd8ab4da89",
      "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
      "ai_keywords": [
        "soft prompting",
        "parameter-efficient fine-tuning",
        "pre-trained models",
        "downstream tasks",
        "Input Dependent Soft Prompting technique",
        "self-Attention Mechanism",
        "zero shot domain transfer"
      ]
    },
    "publishedAt": "2025-06-05T19:13:22.000Z",
    "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05523",
      "authors": [
        {
          "_id": "6846657d3ec10bdd8ab4daca",
          "name": "Zikui Cai",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacb",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacc",
          "name": "Anirudh Satheesh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacd",
          "name": "Ankit Nakhawa",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dace",
          "name": "Hyunwoo Jae",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacf",
          "name": "Keenan Powell",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad0",
          "name": "Minghui Liu",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad1",
          "name": "Neel Jay",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad2",
          "name": "Sungbin Oh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad3",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad4",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad5",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad6",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T19:12:45.000Z",
      "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
      "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
      "upvotes": 4,
      "discussionId": "6846657d3ec10bdd8ab4dad7",
      "projectPage": "https://morse-500.github.io/",
      "githubRepo": "https://github.com/morse-benchmark/morse-500",
      "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
      "ai_keywords": [
        "Vision-language models",
        "MORSE-500",
        "multimodal reasoning",
        "video benchmark",
        "scripted clips",
        "reasoning categories",
        "Manim",
        "Matplotlib",
        "MoviePy",
        "generative video models",
        "controllable generation",
        "spatial capabilities",
        "temporal capabilities",
        "abstract reasoning",
        "planning tasks"
      ]
    },
    "publishedAt": "2025-06-05T15:12:45.000Z",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06276",
      "authors": [
        {
          "_id": "68466dfb3ec10bdd8ab4dae2",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae3",
          "name": "Tianrong Chen",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae4",
          "name": "David Berthelot",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae5",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae6",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae7",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae8",
          "name": "Laurent Dinh",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae9",
          "name": "Miguel Angel Bautista",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daea",
          "name": "Josh Susskind",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daeb",
          "name": "Shuangfei Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:58:39.000Z",
      "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
      "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "6164e72d73996c363c52e66d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
        "isPro": false,
        "fullname": "Jiatao Gu",
        "user": "thomagram",
        "type": "user"
      },
      "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
      "upvotes": 3,
      "discussionId": "68466dfb3ec10bdd8ab4daec",
      "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
      "ai_keywords": [
        "normalizing flows",
        "Transformer Autoregressive Flow",
        "TARFlow",
        "theoretical universality",
        "deep-shallow design",
        "pretrained autoencoders",
        "latent space",
        "guidance algorithm",
        "end-to-end normalizing flow",
        "exact maximum likelihood training",
        "class-conditional",
        "text-conditional image generation",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-06-06T13:58:39.000Z",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
    "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6164e72d73996c363c52e66d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
      "fullname": "Jiatao Gu",
      "name": "thomagram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06199",
      "authors": [
        {
          "_id": "684637733ec10bdd8ab4da66",
          "name": "Hongyan Zhi",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da67",
          "name": "Peihao Chen",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da68",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da69",
          "name": "Yubo Dong",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6a",
          "name": "Quanxi Wu",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6b",
          "name": "Lei Han",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6c",
          "name": "Mingkui Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T16:00:31.000Z",
      "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
      "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
      "submittedOnDailyBy": {
        "_id": "674b2406591d7232820252cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
        "isPro": false,
        "fullname": "Hongyan Zhi",
        "user": "Hoyard",
        "type": "user"
      },
      "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
      "upvotes": 2,
      "discussionId": "684637733ec10bdd8ab4da6d",
      "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
      "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
      "ai_keywords": [
        "3D flow world model",
        "moving object auto-detect pipeline",
        "video diffusion-based world model",
        "3D optical flow dataset",
        "ManiFlow-110k",
        "3D optical flow trajectories",
        "flow-guided rendering mechanism",
        "GPT-4o",
        "closed-loop planning",
        "optimization policy",
        "cross-embodiment adaptation"
      ]
    },
    "publishedAt": "2025-06-06T12:00:31.000Z",
    "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
    "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674b2406591d7232820252cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
      "fullname": "Hongyan Zhi",
      "name": "Hoyard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05573",
      "authors": [
        {
          "_id": "6846902a3ec10bdd8ab4db61",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db62",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db63",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db64",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db65",
          "name": "Yiqiang Feng",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db66",
          "name": "Yadong Mu",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db67",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
      ],
      "publishedAt": "2025-06-05T20:30:28.000Z",
      "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
      "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
      "upvotes": 2,
      "discussionId": "6846902a3ec10bdd8ab4db68",
      "projectPage": "https://wgsxm.github.io/projects/partcrafter",
      "githubRepo": "https://github.com/wgsxm/PartCrafter",
      "ai_summary": "PartCrafter generates complex 3D scenes from single images using a unified compositional architecture with a diffusion transformer, enabling part-aware generation and hierarchical attention.",
      "ai_keywords": [
        "structured 3D generative model",
        "3D meshes",
        "RGB image",
        "compositional generation",
        "denoising",
        "3D parts",
        "pretrained 3D mesh diffusion transformer",
        "compositional latent space",
        "latent tokens",
        "hierarchical attention mechanism",
        "part-level supervision",
        "3D object datasets",
        "decomposable 3D meshes"
      ]
    },
    "publishedAt": "2025-06-05T16:30:28.000Z",
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
    "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04255",
      "authors": [
        {
          "_id": "684312988f9ec8394c514883",
          "user": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "isPro": false,
            "fullname": "Kunal Pai",
            "user": "guineapig",
            "type": "user"
          },
          "name": "Kunal Pai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514884",
          "user": {
            "_id": "62a0dbe7bff710e3fb05f9ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
            "isPro": false,
            "fullname": "Parth Shah",
            "user": "helloparthshah",
            "type": "user"
          },
          "name": "Parth Shah",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514885",
          "name": "Harshil Patel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T17:33:16.000Z",
      "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
      "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
      "submittedOnDailyBy": {
        "_id": "65c43d6d2b723dbc4ddc29d2",
        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
        "isPro": false,
        "fullname": "Kunal Pai",
        "user": "guineapig",
        "type": "user"
      },
      "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
      "upvotes": 2,
      "discussionId": "684312998f9ec8394c514886",
      "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
      "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
      "ai_keywords": [
        "Hierarchical Agent System",
        "Hybrid Intelligent Resource Utilization",
        "HASHIRU",
        "CEO agent",
        "employee agents",
        "Ollama",
        "external APIs",
        "economic model",
        "hiring/firing costs",
        "autonomous API tool creation",
        "academic paper review",
        "safety assessments",
        "GSM8K",
        "JEEBench",
        "SVAMP",
        "Gemini 2.0 Flash",
        "self-improvement",
        "autonomous cost model generation",
        "tool integration",
        "budget management"
      ]
    },
    "publishedAt": "2025-06-01T13:33:16.000Z",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c43d6d2b723dbc4ddc29d2",
      "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
      "fullname": "Kunal Pai",
      "name": "guineapig",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]