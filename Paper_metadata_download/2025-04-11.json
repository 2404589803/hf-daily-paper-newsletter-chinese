[
  {
    "paper": {
      "id": "2504.07491",
      "authors": [
        {
          "_id": "67f8a3db7de2391a06a3b2e0",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e1",
          "name": "Angang Du",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e2",
          "name": "Bohong Yin",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e3",
          "user": {
            "_id": "67503f270dfe827c4068a408",
            "avatarUrl": "/avatars/4591c8229c7815bfd6dc4b98aea85ca8.svg",
            "isPro": false,
            "fullname": "Bowei Xing",
            "user": "xingbowei",
            "type": "user"
          },
          "name": "Bowei Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:27:42.989Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e4",
          "name": "Bowen Qu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e5",
          "name": "Bowen Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e6",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e7",
          "user": {
            "_id": "644ce4e416703fd670260e2e",
            "avatarUrl": "/avatars/db43b13c6913af31cc97f5be7bf30091.svg",
            "isPro": false,
            "fullname": "Chenlin Zhang",
            "user": "tzzcl",
            "type": "user"
          },
          "name": "Chenlin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:30:16.472Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e8",
          "user": {
            "_id": "64c21fb42426d683e56b42bf",
            "avatarUrl": "/avatars/60359fe204e32af831d701d2975c4599.svg",
            "isPro": false,
            "fullname": "Du",
            "user": "DuChenZhuang",
            "type": "user"
          },
          "name": "Chenzhuang Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:30:30.673Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e9",
          "name": "Chu Wei",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ea",
          "user": {
            "_id": "5eefd87c5e979253a010eee5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1603575136094-5eefd87c5e979253a010eee5.jpeg",
            "isPro": false,
            "fullname": "Congcong Wang",
            "user": "congcongwang",
            "type": "user"
          },
          "name": "Congcong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:40:18.553Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2eb",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ec",
          "name": "Dikang Du",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ed",
          "user": {
            "_id": "67652998288b8433a92f3c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yJCzGJ8gl6JyXRc7A9IeI.png",
            "isPro": false,
            "fullname": "wang",
            "user": "dongliangwang",
            "type": "user"
          },
          "name": "Dongliang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:40:52.991Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ee",
          "user": {
            "_id": "6331606f18711776b4655e67",
            "avatarUrl": "/avatars/1479c2ca743b9f92d845b0ed23fcd07b.svg",
            "isPro": false,
            "fullname": "Enming Yuan",
            "user": "EnmingYuan",
            "type": "user"
          },
          "name": "Enming Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:01.062Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ef",
          "user": {
            "_id": "67aed930cc96f87ce3c3132f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/JDrhmbCRcuCtKir7i9z9n.png",
            "isPro": false,
            "fullname": "Lu",
            "user": "Enzhe",
            "type": "user"
          },
          "name": "Enzhe Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:12.500Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f0",
          "name": "Fang Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f1",
          "user": {
            "_id": "6343d01a08c017b2c042305d",
            "avatarUrl": "/avatars/790c4104d80da9887d481f9efb494d81.svg",
            "isPro": false,
            "fullname": "Flood Sung",
            "user": "floodsung",
            "type": "user"
          },
          "name": "Flood Sung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:24:56.624Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f2",
          "name": "Guangda Wei",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f3",
          "user": {
            "_id": "63b4c71758f367a212c4f9ef",
            "avatarUrl": "/avatars/d61736e0ae8b333a7c24eb411378698c.svg",
            "isPro": false,
            "fullname": "Lai",
            "user": "Guokun",
            "type": "user"
          },
          "name": "Guokun Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:40.586Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f4",
          "name": "Han Zhu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f5",
          "user": {
            "_id": "67bdb4ff599d450529afecf4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/clUC8MtK-qlVAfJQ7v99H.png",
            "isPro": false,
            "fullname": "Hao Ding",
            "user": "HaoDing",
            "type": "user"
          },
          "name": "Hao Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:51.300Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f6",
          "name": "Hao Hu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f7",
          "user": {
            "_id": "64ec364e7e2ec711a7601cde",
            "avatarUrl": "/avatars/6ba47d496586de65df183f056d35982b.svg",
            "isPro": false,
            "fullname": "Hao Yang",
            "user": "hayayanghao",
            "type": "user"
          },
          "name": "Hao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:24:58.807Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f8",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f9",
          "user": {
            "_id": "63047ed2412a1b9d381b09c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
            "isPro": true,
            "fullname": "Haoning Wu, Teo",
            "user": "teowu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:01.006Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fa",
          "user": {
            "_id": "67f7ef911f10ddb81f2d6d3d",
            "avatarUrl": "/avatars/6d87cde1056a80f8effbc21b4949e690.svg",
            "isPro": false,
            "fullname": "Haotian Yao",
            "user": "xdedmyao",
            "type": "user"
          },
          "name": "Haotian Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:00.040Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fb",
          "user": {
            "_id": "64c206a3fd4d5966b453ed85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c206a3fd4d5966b453ed85/NemBrHcAJFm2ws_VQG8ia.jpeg",
            "isPro": false,
            "fullname": "Haoyu Lu",
            "user": "Nealeon",
            "type": "user"
          },
          "name": "Haoyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:25.550Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fc",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fd",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:35.087Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fe",
          "user": {
            "_id": "61860e1258cb1f8c362f9441",
            "avatarUrl": "/avatars/8dbc8209ad0d918453c1ffacc8f61e7f.svg",
            "isPro": false,
            "fullname": "Huabin Zheng",
            "user": "zhenghuabin",
            "type": "user"
          },
          "name": "Huabin Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:42.807Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ff",
          "user": {
            "_id": "64f83e01d493d8b0d2ab4cd3",
            "avatarUrl": "/avatars/788d42871df1be2c9b79b2916de3e4d0.svg",
            "isPro": false,
            "fullname": "Jiaming Li",
            "user": "blabluble",
            "type": "user"
          },
          "name": "Jiaming Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:50.314Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b300",
          "user": {
            "_id": "6404982cad54665351d7c1e0",
            "avatarUrl": "/avatars/8fb6d01802cbd4a1cbb7f6a0d83faa3a.svg",
            "isPro": false,
            "fullname": "jianlin su",
            "user": "bojone",
            "type": "user"
          },
          "name": "Jianlin Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:57.607Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b301",
          "user": {
            "_id": "63be6bf6da08ed0544f1eb7a",
            "avatarUrl": "/avatars/19b5be6d3296da402d8822e51d6376e2.svg",
            "isPro": false,
            "fullname": "jianzhouWang",
            "user": "jianzhouWang",
            "type": "user"
          },
          "name": "Jianzhou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:04.777Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b302",
          "name": "Jiaqi Deng",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b303",
          "name": "Jiezhong Qiu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b304",
          "name": "Jin Xie",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b305",
          "name": "Jinhong Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b306",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b307",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b308",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b309",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30a",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30b",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30c",
          "user": {
            "_id": "6309d1e6a58e1be42eb6eb5e",
            "avatarUrl": "/avatars/2a7a437e801389a9f79b49c164f85817.svg",
            "isPro": false,
            "fullname": "dong",
            "user": "mengnan",
            "type": "user"
          },
          "name": "Mengfan Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:53.204Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30d",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30e",
          "name": "Nuo Xu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30f",
          "name": "Pengyu Cheng",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b310",
          "name": "Qizheng Gu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b311",
          "name": "Runjie Zhou",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b312",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b313",
          "name": "Sihan Cao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b314",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b315",
          "name": "Tianhui Song",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b316",
          "name": "Tongtong Bai",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b317",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b318",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b319",
          "name": "Weixiao Huang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31a",
          "name": "Weixin Xu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31b",
          "name": "Xiaokun Yuan",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31c",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31d",
          "name": "Xingzhe Wu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31e",
          "name": "Xinxing Zu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31f",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b320",
          "name": "Xinyuan Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b321",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b322",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b323",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b324",
          "name": "Yangyang Hu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b325",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b326",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b327",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b328",
          "name": "Yibo Miao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b329",
          "name": "Yidao Qin",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32a",
          "name": "Yimin Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32b",
          "name": "Yiping Bao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32c",
          "name": "Yiqin Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32d",
          "name": "Yongsheng Kang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32e",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32f",
          "name": "Yulun Du",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b330",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b331",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b332",
          "name": "Yuzi Yan",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b333",
          "name": "Zaida Zhou",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b334",
          "name": "Zhaowei Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b335",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b336",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b337",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b338",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b339",
          "user": {
            "_id": "66561c5a8ec33cfd8c724cf1",
            "avatarUrl": "/avatars/88ce5bc8ce2d7b1ca97a33d7863bf184.svg",
            "isPro": false,
            "fullname": "Zihao Huang",
            "user": "EdwardHzh",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:37.864Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b33a",
          "name": "Zijia Zhao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b33b",
          "name": "Ziwei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
      ],
      "publishedAt": "2025-04-10T06:48:26.000Z",
      "submittedOnDailyAt": "2025-04-11T03:40:59.047Z",
      "title": "Kimi-VL Technical Report",
      "submittedOnDailyBy": {
        "_id": "63047ed2412a1b9d381b09c9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
        "isPro": true,
        "fullname": "Haoning Wu, Teo",
        "user": "teowu",
        "type": "user"
      },
      "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
      "upvotes": 44,
      "discussionId": "67f8a3de7de2391a06a3b420"
    },
    "publishedAt": "2025-04-10T02:48:26.000Z",
    "title": "Kimi-VL Technical Report",
    "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63047ed2412a1b9d381b09c9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
      "fullname": "Haoning Wu, Teo",
      "name": "teowu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 44
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07956",
      "authors": [
        {
          "_id": "67f886f1a0a44c8f05b7a124",
          "name": "Yukun Qi",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a125",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a126",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a127",
          "name": "Xikun Bao",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a128",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a129",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:22.829Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12a",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12b",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12c",
          "name": "Zhongang Qi",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12d",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:03.000Z",
      "submittedOnDailyAt": "2025-04-11T01:36:38.929Z",
      "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
      "upvotes": 27,
      "discussionId": "67f886f7a0a44c8f05b7a282",
      "projectPage": "https://vlm-reasoning.github.io/VCR-Bench/",
      "githubRepo": "https://github.com/zhishuifeiqian/VCR-Bench"
    },
    "publishedAt": "2025-04-10T13:59:03.000Z",
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 87
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07957",
      "authors": [
        {
          "_id": "67f8914516d43f88b3177ec1",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec2",
          "name": "Shenxi Wu",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec3",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec4",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:05.800Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec5",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec6",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec7",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec8",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec9",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177eca",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:12.000Z",
      "submittedOnDailyAt": "2025-04-11T02:20:10.515Z",
      "title": "MM-IFEngine: Towards Multimodal Instruction Following",
      "submittedOnDailyBy": {
        "_id": "646cd947da8e99940b6e55cf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
        "isPro": false,
        "fullname": "Shengyuan Ding",
        "user": "ChrisDing1105",
        "type": "user"
      },
      "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
      "upvotes": 21,
      "discussionId": "67f8914816d43f88b3177fa7",
      "projectPage": "https://syuan03.github.io/MM-IFEngine/",
      "githubRepo": "https://github.com/SYuan03/MM-IFEngine"
    },
    "publishedAt": "2025-04-10T13:59:12.000Z",
    "title": "MM-IFEngine: Towards Multimodal Instruction Following",
    "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646cd947da8e99940b6e55cf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
      "fullname": "Shengyuan Ding",
      "name": "ChrisDing1105",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07960",
      "authors": [
        {
          "_id": "67f86e6884277ab58c40ce8a",
          "user": {
            "_id": "6740a5730bb4a675446a80ad",
            "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
            "isPro": true,
            "fullname": "Zhong-Yu Li",
            "user": "lzyhha",
            "type": "user"
          },
          "name": "Zhong-Yu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:42.946Z",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8b",
          "name": "Ruoyi Du",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8c",
          "name": "Juncheng Yan",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8d",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8e",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8f",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce90",
          "name": "Zhanyu Ma",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce91",
          "name": "Ming-Ming Cheng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
      ],
      "publishedAt": "2025-04-10T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-11T01:51:23.709Z",
      "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
      "submittedOnDailyBy": {
        "_id": "6740a5730bb4a675446a80ad",
        "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
        "isPro": true,
        "fullname": "Zhong-Yu Li",
        "user": "lzyhha",
        "type": "user"
      },
      "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.",
      "upvotes": 18,
      "discussionId": "67f86e6a84277ab58c40cf08",
      "projectPage": "https://visualcloze.github.io/",
      "githubRepo": "https://github.com/lzyhha/VisualCloze"
    },
    "publishedAt": "2025-04-10T13:59:42.000Z",
    "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
    "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6740a5730bb4a675446a80ad",
      "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
      "fullname": "Zhong-Yu Li",
      "name": "lzyhha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07943",
      "authors": [
        {
          "_id": "67f88643f5889583ce0499ae",
          "name": "Yunhan Yang",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499af",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b0",
          "user": {
            "_id": "638ee900ee7e45e0474a5712",
            "avatarUrl": "/avatars/eadb5ae2fc92bd9af5516acbc8f1bdf0.svg",
            "isPro": false,
            "fullname": "Yukun Huang",
            "user": "KevinHuang",
            "type": "user"
          },
          "name": "Yukun Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:25.048Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b1",
          "name": "Zi-Xin Zou",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b2",
          "name": "Zhipeng Yu",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b3",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b4",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b5",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:53:31.000Z",
      "submittedOnDailyAt": "2025-04-11T01:33:34.186Z",
      "title": "HoloPart: Generative 3D Part Amodal Segmentation",
      "submittedOnDailyBy": {
        "_id": "64b4eecf2fc8324fcb63b404",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
        "isPro": false,
        "fullname": "Yunhan Yang",
        "user": "yhyang-myron",
        "type": "user"
      },
      "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
      "upvotes": 18,
      "discussionId": "67f88646f5889583ce049a90",
      "projectPage": "https://vast-ai-research.github.io/HoloPart",
      "githubRepo": "https://github.com/VAST-AI-Research/HoloPart"
    },
    "publishedAt": "2025-04-10T13:53:31.000Z",
    "title": "HoloPart: Generative 3D Part Amodal Segmentation",
    "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eecf2fc8324fcb63b404",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
      "fullname": "Yunhan Yang",
      "name": "yhyang-myron",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07128",
      "authors": [
        {
          "_id": "67f888115ebbf40d96641662",
          "name": "Sara Vera MarjanoviÄ‡",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641663",
          "user": {
            "_id": "631a523c04f8ed65eff16fb4",
            "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
            "isPro": false,
            "fullname": "Arkil Patel",
            "user": "arkilpatel",
            "type": "user"
          },
          "name": "Arkil Patel",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-11T03:11:10.866Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641664",
          "name": "Vaibhav Adlakha",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641665",
          "name": "Milad Aghajohari",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641666",
          "name": "Parishad BehnamGhader",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641667",
          "name": "Mehar Bhatia",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641668",
          "user": {
            "_id": "6512e852a76fd5945b19e9a1",
            "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
            "isPro": false,
            "fullname": "Aditi Khandelwal",
            "user": "aditi184",
            "type": "user"
          },
          "name": "Aditi Khandelwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:16.938Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641669",
          "name": "Austin Kraft",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166a",
          "user": {
            "_id": "6270c58780d5f35f8dbe42be",
            "avatarUrl": "/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg",
            "isPro": false,
            "fullname": "Benno Krojer",
            "user": "BennoKrojer",
            "type": "user"
          },
          "name": "Benno Krojer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:15.125Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166b",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lu",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han LÃ¹",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:20.675Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166c",
          "name": "Nicholas Meade",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166d",
          "user": {
            "_id": "63340b24f68a3fb7efa62b3a",
            "avatarUrl": "/avatars/44c960437c037553d90b1ca24c952977.svg",
            "isPro": false,
            "fullname": "Dongchan Shin",
            "user": "ShinDC",
            "type": "user"
          },
          "name": "Dongchan Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:18.900Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166e",
          "name": "Amirhossein Kazemnejad",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166f",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641670",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641671",
          "name": "Karolina StaÅ„czak",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641672",
          "user": {
            "_id": "624734dc4c731bb6bfab8af7",
            "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
            "isPro": false,
            "fullname": "Siva Reddy",
            "user": "sivareddyg",
            "type": "user"
          },
          "name": "Siva Reddy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-11T03:26:50.276Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T00:36:08.000Z",
      "submittedOnDailyAt": "2025-04-11T02:00:19.953Z",
      "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "6512e852a76fd5945b19e9a1",
        "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
        "isPro": false,
        "fullname": "Aditi Khandelwal",
        "user": "aditi184",
        "type": "user"
      },
      "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs.",
      "upvotes": 16,
      "discussionId": "67f888125ebbf40d966416bd",
      "projectPage": "https://mcgill-nlp.github.io/thoughtology/",
      "githubRepo": "https://github.com/mcgill-NLP/thoughtology"
    },
    "publishedAt": "2025-04-01T20:36:08.000Z",
    "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
    "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6512e852a76fd5945b19e9a1",
      "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
      "fullname": "Aditi Khandelwal",
      "name": "aditi184",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07964",
      "authors": [
        {
          "_id": "67f88c848178ca61d74e001d",
          "user": {
            "_id": "671002fd13203512e7b8f9e3",
            "avatarUrl": "/avatars/313d8ea313ed300750cfdaaca44fdb6e.svg",
            "isPro": false,
            "fullname": "Zhongyang Li",
            "user": "Lzy01241010",
            "type": "user"
          },
          "name": "Zhongyang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:08.340Z",
          "hidden": false
        },
        {
          "_id": "67f88c848178ca61d74e001e",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67f88c848178ca61d74e001f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:10.558Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-11T02:02:00.382Z",
      "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
      "upvotes": 11,
      "discussionId": "67f88c858178ca61d74e0061",
      "githubRepo": "https://github.com/tianyi-lab/C3PO"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
    "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07964.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07830",
      "authors": [
        {
          "_id": "67f8886e334eb1a3942c4f3f",
          "user": {
            "_id": "64881deb8e004bb92b0f4845",
            "avatarUrl": "/avatars/30a1e016d469bf7eb42c713351a9f65c.svg",
            "isPro": false,
            "fullname": "Genglin Liu",
            "user": "genglinliu",
            "type": "user"
          },
          "name": "Genglin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:12.907Z",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f40",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f41",
          "name": "Elisa Kreiss",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f42",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f43",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T15:06:54.000Z",
      "submittedOnDailyAt": "2025-04-11T01:44:23.981Z",
      "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
      "submittedOnDailyBy": {
        "_id": "625913bd5f80a3c1aad074b6",
        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
        "isPro": false,
        "fullname": "Salman Rahman",
        "user": "salmannyu",
        "type": "user"
      },
      "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
      "upvotes": 11,
      "discussionId": "67f8886f334eb1a3942c4f5f"
    },
    "publishedAt": "2025-04-10T11:06:54.000Z",
    "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
    "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625913bd5f80a3c1aad074b6",
      "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
      "fullname": "Salman Rahman",
      "name": "salmannyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07934",
      "authors": [
        {
          "_id": "67f88bbaf1410163f7c3b68a",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68b",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68c",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68d",
          "name": "Hongjin Lu",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68e",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68f",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b690",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b691",
          "name": "Furong Huang",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b692",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:49:05.000Z",
      "submittedOnDailyAt": "2025-04-11T01:57:16.470Z",
      "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.",
      "upvotes": 6,
      "discussionId": "67f88bbbf1410163f7c3b6f4",
      "githubRepo": "https://github.com/si0wang/ThinkLite-VL"
    },
    "publishedAt": "2025-04-10T13:49:05.000Z",
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
    "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04974",
      "authors": [
        {
          "_id": "67f8932aa1d82990423d99b5",
          "user": {
            "_id": "65031d01cccc7b28a388c719",
            "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "MingLiiii",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b6",
          "user": {
            "_id": "64668f982da1abc242355cbb",
            "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
            "isPro": false,
            "fullname": "Ruiyi Zhang",
            "user": "zhangry868",
            "type": "user"
          },
          "name": "Ruiyi Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b7",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b8",
          "name": "Jiuxiang Gu",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b9",
          "name": "Yufan Zhou",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99ba",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:03.265Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bb",
          "name": "Wanrong Zhu",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bc",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bd",
          "name": "Tong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T12:01:59.000Z",
      "submittedOnDailyAt": "2025-04-11T02:27:52.838Z",
      "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
      "upvotes": 2,
      "discussionId": "67f8932fa1d82990423d9b2a"
    },
    "publishedAt": "2025-04-07T08:01:59.000Z",
    "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
    "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  }
]