[
    {
        "paper": {
            "id": "2411.18203",
            "authors": [
                {
                    "_id": "67481f451cf82b1a47ba94f2",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "qq8933",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-28T09:19:17.745Z",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f3",
                    "name": "Jingdi Lei",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f4",
                    "user": {
                        "_id": "656ae4088fb1ddf0d5ec9ac5",
                        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
                        "isPro": false,
                        "fullname": "Junxian Li",
                        "user": "Duke-de-Artois",
                        "type": "user"
                    },
                    "name": "Junxian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-28T09:19:13.767Z",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f5",
                    "name": "Xunzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f6",
                    "name": "Yujie Liu",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f7",
                    "user": {
                        "_id": "646a11791556443f24b582e9",
                        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                        "isPro": false,
                        "fullname": "Zonglin Yang",
                        "user": "ZonglinY",
                        "type": "user"
                    },
                    "name": "Zonglin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-28T11:38:00.768Z",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f8",
                    "name": "Jiatong Li",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94f9",
                    "name": "Weida Wang",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94fa",
                    "name": "Suorong Yang",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94fb",
                    "user": {
                        "_id": "6407d3d3a7bc7c3865addc4b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6407d3d3a7bc7c3865addc4b/NEMP6xShRq-5H_rQ5ganY.jpeg",
                        "isPro": false,
                        "fullname": "Jianbo Wu",
                        "user": "jwu323",
                        "type": "user"
                    },
                    "name": "Jianbo Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-28T09:19:16.199Z",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94fc",
                    "name": "Peng Ye",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94fd",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67481f451cf82b1a47ba94fe",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T10:28:57.000Z",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "summary": "Vision-language models~(VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.",
            "upvotes": 13,
            "discussionId": "67481f461cf82b1a47ba9533"
        },
        "publishedAt": "2024-11-28T23:00:30.168Z",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18203.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
            "fullname": "Di Zhang",
            "name": "qq8933",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 111
        }
    },
    {
        "paper": {
            "id": "2411.17176",
            "authors": [
                {
                    "_id": "67497408647c7aabef05d682",
                    "user": {
                        "_id": "6602548a68d519ed324b47c5",
                        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                        "isPro": false,
                        "fullname": "ChengyouJia",
                        "user": "ChengyouJia",
                        "type": "user"
                    },
                    "name": "Chengyou Jia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-29T09:30:09.129Z",
                    "hidden": false
                },
                {
                    "_id": "67497408647c7aabef05d683",
                    "user": {
                        "_id": "6749852ae60112b9012a1a74",
                        "avatarUrl": "/avatars/c5773f87da9f2bc460c24dbc01f812c5.svg",
                        "isPro": false,
                        "fullname": "Changliang Xia",
                        "user": "xcll",
                        "type": "user"
                    },
                    "name": "Changliang Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-29T15:13:17.676Z",
                    "hidden": false
                },
                {
                    "_id": "67497408647c7aabef05d684",
                    "user": {
                        "_id": "672098263fb4d4644a0caed5",
                        "avatarUrl": "/avatars/ec321749642d8ec78352aedc8a6120a3.svg",
                        "isPro": false,
                        "fullname": "Zhuohang Dang",
                        "user": "Ericiand",
                        "type": "user"
                    },
                    "name": "Zhuohang Dang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-29T15:13:23.532Z",
                    "hidden": false
                },
                {
                    "_id": "67497408647c7aabef05d685",
                    "user": {
                        "_id": "6345a93afe134dfd7a0cfabd",
                        "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
                        "isPro": false,
                        "fullname": "wu weijia",
                        "user": "weijiawu",
                        "type": "user"
                    },
                    "name": "Weijia Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-29T15:13:57.453Z",
                    "hidden": false
                },
                {
                    "_id": "67497408647c7aabef05d686",
                    "name": "Hangwei Qian",
                    "hidden": false
                },
                {
                    "_id": "67497408647c7aabef05d687",
                    "name": "Minnan Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T07:31:12.000Z",
            "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
            "summary": "Despite the significant advancements in text-to-image (T2I) generative\nmodels, users often face a trial-and-error challenge in practical scenarios.\nThis challenge arises from the complexity and uncertainty of tedious steps such\nas crafting suitable prompts, selecting appropriate models, and configuring\nspecific arguments, making users resort to labor-intensive attempts for desired\nimages. This paper proposes Automatic T2I generation, which aims to automate\nthese tedious steps, allowing users to simply describe their needs in a\nfreestyle chatting way. To systematically study this problem, we first\nintroduce ChatGenBench, a novel benchmark designed for Automatic T2I. It\nfeatures high-quality paired data with diverse freestyle inputs, enabling\ncomprehensive evaluation of automatic T2I models across all steps.\nAdditionally, recognizing Automatic T2I as a complex multi-step reasoning task,\nwe propose ChatGen-Evo, a multi-stage evolution strategy that progressively\nequips models with essential automation skills. Through extensive evaluation\nacross step-wise accuracy and image quality, ChatGen-Evo significantly enhances\nperformance over various baselines. Our evaluation also uncovers valuable\ninsights for advancing automatic T2I. All our data, code, and models will be\navailable in https://chengyou-jia.github.io/ChatGen-Home",
            "upvotes": 11,
            "discussionId": "6749740e647c7aabef05d8a7"
        },
        "publishedAt": "2024-11-29T03:36:19.490Z",
        "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17176.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "fullname": "ChengyouJia",
            "name": "ChengyouJia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.17041",
            "authors": [
                {
                    "_id": "674952f5ad6772de5d595ae8",
                    "user": {
                        "_id": "64c3732de6c3860fba66ceb0",
                        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
                        "isPro": false,
                        "fullname": "JaeminKim",
                        "user": "kjm981995",
                        "type": "user"
                    },
                    "name": "Jaemin Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-29T09:33:20.413Z",
                    "hidden": false
                },
                {
                    "_id": "674952f5ad6772de5d595ae9",
                    "name": "Bryan S Kim",
                    "hidden": false
                },
                {
                    "_id": "674952f5ad6772de5d595aea",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T02:14:47.000Z",
            "title": "Free^2Guide: Gradient-Free Path Integral Control for Enhancing\n  Text-to-Video Generation with Large Vision-Language Models",
            "summary": "Diffusion models have achieved impressive results in generative tasks like\ntext-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving\naccurate text alignment in T2V generation remains challenging due to the\ncomplex temporal dependency across frames. Existing reinforcement learning\n(RL)-based approaches to enhance text alignment often require differentiable\nreward functions or are constrained to limited prompts, hindering their\nscalability and applicability. In this paper, we propose Free^2Guide, a novel\ngradient-free framework for aligning generated videos with text prompts without\nrequiring additional model training. Leveraging principles from path integral\ncontrol, Free^2Guide approximates guidance for diffusion models using\nnon-differentiable reward functions, thereby enabling the integration of\npowerful black-box Large Vision-Language Models (LVLMs) as reward model.\nAdditionally, our framework supports the flexible ensembling of multiple reward\nmodels, including large-scale image-based models, to synergistically enhance\nalignment without incurring substantial computational overhead. We demonstrate\nthat Free^2Guide significantly improves text alignment across various\ndimensions and enhances the overall quality of generated videos.",
            "upvotes": 3,
            "discussionId": "674952f7ad6772de5d595b76"
        },
        "publishedAt": "2024-11-29T06:31:14.840Z",
        "title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17041.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "fullname": "JaeminKim",
            "name": "kjm981995",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.18350",
            "authors": [
                {
                    "_id": "674818fd93aea005d45001bd",
                    "user": {
                        "_id": "63f222ecf4e30ffd2bd08405",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f222ecf4e30ffd2bd08405/AJoV9-gbiwKcOFPIGPDvD.jpeg",
                        "isPro": false,
                        "fullname": "Riza Velioglu",
                        "user": "rizavelioglu",
                        "type": "user"
                    },
                    "name": "Riza Velioglu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-28T09:19:21.403Z",
                    "hidden": false
                },
                {
                    "_id": "674818fd93aea005d45001be",
                    "user": {
                        "_id": "66c7334bf486e7ed57231858",
                        "avatarUrl": "/avatars/2f315177d64b683b32d7d4ef6a6c91eb.svg",
                        "isPro": false,
                        "fullname": "Petra Bevandic",
                        "user": "PetraBevandic",
                        "type": "user"
                    },
                    "name": "Petra Bevandic",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-29T15:25:20.925Z",
                    "hidden": false
                },
                {
                    "_id": "674818fd93aea005d45001bf",
                    "name": "Robin Chan",
                    "hidden": false
                },
                {
                    "_id": "674818fd93aea005d45001c0",
                    "name": "Barbara Hammer",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T13:53:09.000Z",
            "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction\n  using Diffusion Models",
            "summary": "This paper introduces Virtual Try-Off (VTOFF), a novel task focused on\ngenerating standardized garment images from single photos of clothed\nindividuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses\nmodels, VTOFF aims to extract a canonical garment image, posing unique\nchallenges in capturing garment shape, texture, and intricate patterns. This\nwell-defined target makes VTOFF particularly effective for evaluating\nreconstruction fidelity in generative models. We present TryOffDiff, a model\nthat adapts Stable Diffusion with SigLIP-based visual conditioning to ensure\nhigh fidelity and detail retention. Experiments on a modified VITON-HD dataset\nshow that our approach outperforms baseline methods based on pose transfer and\nvirtual try-on with fewer pre- and post-processing steps. Our analysis reveals\nthat traditional image generation metrics inadequately assess reconstruction\nquality, prompting us to rely on DISTS for more accurate evaluation. Our\nresults highlight the potential of VTOFF to enhance product imagery in\ne-commerce applications, advance generative model evaluation, and inspire\nfuture work on high-fidelity reconstruction. Demo, code, and models are\navailable at: https://rizavelioglu.github.io/tryoffdiff/",
            "upvotes": 3,
            "discussionId": "6748190193aea005d450026f"
        },
        "publishedAt": "2024-11-29T04:44:39.366Z",
        "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18350.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f222ecf4e30ffd2bd08405/AJoV9-gbiwKcOFPIGPDvD.jpeg",
            "fullname": "Riza Velioglu",
            "name": "rizavelioglu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.17863",
            "authors": [
                {
                    "_id": "6749a26de60308c9c195f72e",
                    "user": {
                        "_id": "6463eb7a31063f253c21fdcc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6463eb7a31063f253c21fdcc/V0hI_xB0HE4qhLieXlPME.jpeg",
                        "isPro": false,
                        "fullname": "Jeovane H Alves",
                        "user": "jeohalves",
                        "type": "user"
                    },
                    "name": "Jeovane Honorio Alves",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-29T14:56:01.691Z",
                    "hidden": false
                },
                {
                    "_id": "6749a26de60308c9c195f72f",
                    "user": {
                        "_id": "6412db862a07dd87bd30096a",
                        "avatarUrl": "/avatars/8742da8a6a1e8a1748b9c7dab8329907.svg",
                        "isPro": false,
                        "fullname": "Radu State",
                        "user": "RaduState",
                        "type": "user"
                    },
                    "name": "Radu State",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-29T12:52:13.681Z",
                    "hidden": false
                },
                {
                    "_id": "6749a26de60308c9c195f730",
                    "name": "Cinthia Obladen de Almendra Freitas",
                    "hidden": false
                },
                {
                    "_id": "6749a26de60308c9c195f731",
                    "name": "Jean Paul Barddal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T20:26:47.000Z",
            "title": "LongKey: Keyphrase Extraction for Long Documents",
            "summary": "In an era of information overload, manually annotating the vast and growing\ncorpus of documents and scholarly papers is increasingly impractical. Automated\nkeyphrase extraction addresses this challenge by identifying representative\nterms within texts. However, most existing methods focus on short documents (up\nto 512 tokens), leaving a gap in processing long-context documents. In this\npaper, we introduce LongKey, a novel framework for extracting keyphrases from\nlengthy documents, which uses an encoder-based language model to capture\nextended text intricacies. LongKey uses a max-pooling embedder to enhance\nkeyphrase candidate representation. Validated on the comprehensive LDKP\ndatasets and six diverse, unseen datasets, LongKey consistently outperforms\nexisting unsupervised and language model-based keyphrase extraction methods.\nOur findings demonstrate LongKey's versatility and superior performance,\nmarking an advancement in keyphrase extraction for varied text lengths and\ndomains.",
            "upvotes": 1,
            "discussionId": "6749a26ee60308c9c195f773"
        },
        "publishedAt": "2024-11-29T06:16:41.623Z",
        "title": "LongKey: Keyphrase Extraction for Long Documents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17863.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6463eb7a31063f253c21fdcc/V0hI_xB0HE4qhLieXlPME.jpeg",
            "fullname": "Jeovane H Alves",
            "name": "jeohalves",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]