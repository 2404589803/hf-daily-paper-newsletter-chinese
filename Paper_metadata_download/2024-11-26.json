[
    {
        "paper": {
            "id": "2411.15138",
            "authors": [
                {
                    "_id": "674455fdde9997dd26a9ed4b",
                    "user": {
                        "_id": "638d7d33c8912be69c1a5849",
                        "avatarUrl": "/avatars/475efc364e10f3594467bd45260d8999.svg",
                        "isPro": false,
                        "fullname": "Xin Huang",
                        "user": "xanderhuang",
                        "type": "user"
                    },
                    "name": "Xin Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T20:38:54.950Z",
                    "hidden": false
                },
                {
                    "_id": "674455fdde9997dd26a9ed4c",
                    "name": "Tengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "674455fdde9997dd26a9ed4d",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "674455fdde9997dd26a9ed4e",
                    "name": "Qing Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T18:59:39.000Z",
            "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
            "summary": "We present Material Anything, a fully-automated, unified diffusion framework\ndesigned to generate physically-based materials for 3D objects. Unlike existing\nmethods that rely on complex pipelines or case-specific optimizations, Material\nAnything offers a robust, end-to-end solution adaptable to objects under\ndiverse lighting conditions. Our approach leverages a pre-trained image\ndiffusion model, enhanced with a triple-head architecture and rendering loss to\nimprove stability and material quality. Additionally, we introduce confidence\nmasks as a dynamic switcher within the diffusion model, enabling it to\neffectively handle both textured and texture-less objects across varying\nlighting conditions. By employing a progressive material generation strategy\nguided by these confidence masks, along with a UV-space material refiner, our\nmethod ensures consistent, UV-ready material outputs. Extensive experiments\ndemonstrate our approach outperforms existing methods across a wide range of\nobject categories and lighting conditions.",
            "upvotes": 31,
            "discussionId": "674455ffde9997dd26a9ef6c"
        },
        "publishedAt": "2024-11-26T00:04:35.029Z",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638d7d33c8912be69c1a5849/oZQKTezv_U6qELj8B0lbE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15138.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/475efc364e10f3594467bd45260d8999.svg",
            "fullname": "Xin Huang",
            "name": "xanderhuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.15466",
            "authors": [
                {
                    "_id": "67453096586b9927fa10108e",
                    "name": "Chaehun Shin",
                    "hidden": false
                },
                {
                    "_id": "67453096586b9927fa10108f",
                    "name": "Jooyoung Choi",
                    "hidden": false
                },
                {
                    "_id": "67453096586b9927fa101090",
                    "name": "Heeseung Kim",
                    "hidden": false
                },
                {
                    "_id": "67453096586b9927fa101091",
                    "name": "Sungroh Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-23T06:17:43.000Z",
            "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot\n  Subject-Driven Image Generator",
            "summary": "Subject-driven text-to-image generation aims to produce images of a new\nsubject within a desired context by accurately capturing both the visual\ncharacteristics of the subject and the semantic content of a text prompt.\nTraditional methods rely on time- and resource-intensive fine-tuning for\nsubject alignment, while recent zero-shot approaches leverage on-the-fly image\nprompting, often sacrificing subject alignment. In this paper, we introduce\nDiptych Prompting, a novel zero-shot approach that reinterprets as an\ninpainting task with precise subject alignment by leveraging the emergent\nproperty of diptych generation in large-scale text-to-image models. Diptych\nPrompting arranges an incomplete diptych with the reference image in the left\npanel, and performs text-conditioned inpainting on the right panel. We further\nprevent unwanted content leakage by removing the background in the reference\nimage and improve fine-grained details in the generated subject by enhancing\nattention weights between the panels during inpainting. Experimental results\nconfirm that our approach significantly outperforms zero-shot image prompting\nmethods, resulting in images that are visually preferred by users.\nAdditionally, our method supports not only subject-driven generation but also\nstylized image generation and subject-driven image editing, demonstrating\nversatility across diverse image generation applications. Project page:\nhttps://diptychprompting.github.io/",
            "upvotes": 26,
            "discussionId": "6745309a586b9927fa101214"
        },
        "publishedAt": "2024-11-26T00:56:52.128Z",
        "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15466.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/590633e163da425bf041e3988d8dd015.svg",
            "fullname": "shin",
            "name": "chaehun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.16594",
            "authors": [
                {
                    "_id": "674547c6cc0f907b7bb44733",
                    "name": "Dawei Li",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44734",
                    "name": "Bohan Jiang",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44735",
                    "name": "Liangjie Huang",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44736",
                    "name": "Alimohammad Beigi",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44737",
                    "name": "Chengshuai Zhao",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44738",
                    "name": "Zhen Tan",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb44739",
                    "name": "Amrita Bhattacharjee",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473a",
                    "name": "Yuxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473b",
                    "name": "Canyu Chen",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473c",
                    "name": "Tianhao Wu",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473d",
                    "name": "Kai Shu",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473e",
                    "name": "Lu Cheng",
                    "hidden": false
                },
                {
                    "_id": "674547c6cc0f907b7bb4473f",
                    "name": "Huan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T17:28:44.000Z",
            "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
            "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\nhttps://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and\nhttps://llm-as-a-judge.github.io.",
            "upvotes": 16,
            "discussionId": "674547c7cc0f907b7bb447a5"
        },
        "publishedAt": "2024-11-26T02:30:59.013Z",
        "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16594.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483af58571c2dcfa98cae82/d8Vnh_EG1KxeEoNAAQXHZ.jpeg",
            "fullname": "Canyu Chen",
            "name": "canyuchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.16205",
            "authors": [
                {
                    "_id": "674546c8c868984c8a473c03",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "674546c8c868984c8a473c04",
                    "name": "Xun Wu",
                    "hidden": false
                },
                {
                    "_id": "674546c8c868984c8a473c05",
                    "name": "Shuming Ma",
                    "hidden": false
                },
                {
                    "_id": "674546c8c868984c8a473c06",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T09:05:36.000Z",
            "title": "MH-MoE:Multi-Head Mixture-of-Experts",
            "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
            "upvotes": 14,
            "discussionId": "674546c9c868984c8a473c4c"
        },
        "publishedAt": "2024-11-26T02:26:15.710Z",
        "title": "MH-MoE:Multi-Head Mixture-of-Experts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16205.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "fullname": "HUANG SHAOHAN",
            "name": "buaahsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        }
    },
    {
        "paper": {
            "id": "2411.15611",
            "authors": [
                {
                    "_id": "67458484aa42422dcbcb4381",
                    "user": {
                        "_id": "64f9f44c3c2a6f731f9ead10",
                        "avatarUrl": "/avatars/c53a47a2d321e9d8b1bdf5babdfb206c.svg",
                        "isPro": false,
                        "fullname": "Carlo Alberto Barbano",
                        "user": "carloalbertobarbano",
                        "type": "user"
                    },
                    "name": "Carlo Alberto Barbano",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-26T08:28:39.551Z",
                    "hidden": false
                },
                {
                    "_id": "67458484aa42422dcbcb4382",
                    "user": {
                        "_id": "666875d810b4ced2a569c089",
                        "avatarUrl": "/avatars/5080e0a0bb44b911f6621e16476d5cd2.svg",
                        "isPro": false,
                        "fullname": "Luca Molinaro",
                        "user": "luca-molinaro",
                        "type": "user"
                    },
                    "name": "Luca Molinaro",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-26T08:27:48.920Z",
                    "hidden": false
                },
                {
                    "_id": "67458484aa42422dcbcb4383",
                    "name": "Emanuele Aiello",
                    "hidden": false
                },
                {
                    "_id": "67458484aa42422dcbcb4384",
                    "name": "Marco Grangetto",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-23T17:26:50.000Z",
            "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
            "summary": "We present a way to learn novel concepts by only using their textual\ndescription. We call this method Knowledge Transfer. Similarly to human\nperception, we leverage cross-modal interaction to introduce new concepts. We\nhypothesize that in a pre-trained visual encoder there are enough low-level\nfeatures already learned (e.g. shape, appearance, color) that can be used to\ndescribe previously unknown high-level concepts. Provided with a textual\ndescription of the novel concept, our method works by aligning the known\nlow-level features of the visual encoder to its high-level textual description.\nWe show that Knowledge Transfer can successfully introduce novel concepts in\nmultimodal models, in a very efficient manner, by only requiring a single\ndescription of the target concept. Our approach is compatible with both\nseparate textual and visual encoders (e.g. CLIP) and shared parameters across\nmodalities. We also show that, following the same principle, Knowledge Transfer\ncan improve concepts already known by the model. Leveraging Knowledge Transfer\nwe improve zero-shot performance across different tasks such as classification,\nsegmentation, image-text retrieval, and captioning.",
            "upvotes": 13,
            "discussionId": "67458486aa42422dcbcb440b"
        },
        "publishedAt": "2024-11-26T06:50:48.708Z",
        "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15611.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/ed033f30f9f78ed68da976e6d01af0ea.svg",
            "fullname": "Emanuele Aiello",
            "name": "Ema97x",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.16657",
            "authors": [
                {
                    "_id": "674539fa5cb54d8d661272cc",
                    "name": "Zun Wang",
                    "hidden": false
                },
                {
                    "_id": "674539fa5cb54d8d661272cd",
                    "name": "Jialu Li",
                    "hidden": false
                },
                {
                    "_id": "674539fa5cb54d8d661272ce",
                    "name": "Han Lin",
                    "hidden": false
                },
                {
                    "_id": "674539fa5cb54d8d661272cf",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "674539fa5cb54d8d661272d0",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T18:41:56.000Z",
            "title": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation",
            "summary": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.",
            "upvotes": 12,
            "discussionId": "674539ff5cb54d8d66127468"
        },
        "publishedAt": "2024-11-26T01:32:36.633Z",
        "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16657.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b89798ff623abffb169eacda2ac32fde.svg",
            "fullname": "Han Lin",
            "name": "hanlincs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16318",
            "authors": [
                {
                    "_id": "674569ce9930041fff3f7bf0",
                    "name": "Duong H. Le",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf1",
                    "name": "Tuan Pham",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf2",
                    "name": "Sangho Lee",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf3",
                    "name": "Christopher Clark",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf4",
                    "name": "Aniruddha Kembhavi",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf5",
                    "name": "Stephan Mandt",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf6",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "674569ce9930041fff3f7bf7",
                    "name": "Jiasen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T12:11:05.000Z",
            "title": "One Diffusion to Generate Them All",
            "summary": "We introduce OneDiffusion, a versatile, large-scale diffusion model that\nseamlessly supports bidirectional image synthesis and understanding across\ndiverse tasks. It enables conditional generation from inputs such as text,\ndepth, pose, layout, and semantic maps, while also handling tasks like image\ndeblurring, upscaling, and reverse processes such as depth estimation and\nsegmentation. Additionally, OneDiffusion allows for multi-view generation,\ncamera pose estimation, and instant personalization using sequential image\ninputs. Our model takes a straightforward yet effective approach by treating\nall tasks as frame sequences with varying noise scales during training,\nallowing any frame to act as a conditioning image at inference time. Our\nunified training framework removes the need for specialized architectures,\nsupports scalable multi-task training, and adapts smoothly to any resolution,\nenhancing both generalization and scalability. Experimental results demonstrate\ncompetitive performance across tasks in both generation and prediction such as\ntext-to-image, multiview generation, ID preservation, depth estimation and\ncamera pose estimation despite relatively small training dataset. Our code and\ncheckpoint are freely available at https://github.com/lehduong/OneDiffusion",
            "upvotes": 10,
            "discussionId": "674569d29930041fff3f7d48"
        },
        "publishedAt": "2024-11-26T04:56:04.365Z",
        "title": "One Diffusion to Generate Them All",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16318.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/77068771dd51df7519516cd502a88789.svg",
            "fullname": "Jiasenlu",
            "name": "Jiasenlu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16034",
            "authors": [
                {
                    "_id": "67455f372ec5738026c814d5",
                    "name": "Wang Bill Zhu",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814d6",
                    "name": "Deqing Fu",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814d7",
                    "name": "Kai Sun",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814d8",
                    "name": "Yi Lu",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814d9",
                    "name": "Zhaojiang Lin",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814da",
                    "name": "Seungwhan Moon",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814db",
                    "name": "Kanika Narang",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814dc",
                    "name": "Mustafa Canim",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814dd",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814de",
                    "name": "Anuj Kumar",
                    "hidden": false
                },
                {
                    "_id": "67455f372ec5738026c814df",
                    "name": "Xin Luna Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T01:45:42.000Z",
            "title": "VisualLens: Personalization through Visual History",
            "summary": "We hypothesize that a user's visual history with images reflecting their\ndaily life, offers valuable insights into their interests and preferences, and\ncan be leveraged for personalization. Among the many challenges to achieve this\ngoal, the foremost is the diversity and noises in the visual history,\ncontaining images not necessarily related to a recommendation task, not\nnecessarily reflecting the user's interest, or even not necessarily\npreference-relevant. Existing recommendation systems either rely on\ntask-specific user interaction logs, such as online shopping history for\nshopping recommendations, or focus on text signals. We propose a novel\napproach, VisualLens, that extracts, filters, and refines image\nrepresentations, and leverages these signals for personalization. We created\ntwo new benchmarks with task-agnostic visual histories, and show that our\nmethod improves over state-of-the-art recommendations by 5-10% on Hit@3, and\nimproves over GPT-4o by 2-5%. Our approach paves the way for personalized\nrecommendations in scenarios where traditional methods fail.",
            "upvotes": 10,
            "discussionId": "67455f382ec5738026c81579"
        },
        "publishedAt": "2024-11-26T04:23:59.343Z",
        "title": "VisualLens: Personalization through Visual History",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16034.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "fullname": "Deqing Fu",
            "name": "deqing",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.16681",
            "authors": [
                {
                    "_id": "67455e2f6338ac803d291a30",
                    "name": "Zechen Bai",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a31",
                    "name": "Jianxiong Gao",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a32",
                    "name": "Ziteng Gao",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a33",
                    "name": "Pichao Wang",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a34",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a35",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "67455e2f6338ac803d291a36",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T18:59:53.000Z",
            "title": "Factorized Visual Tokenization and Generation",
            "summary": "Visual tokenizers are fundamental to image generation. They convert visual\ndata into discrete tokens, enabling transformer-based models to excel at image\ngeneration. Despite their success, VQ-based tokenizers like VQGAN face\nsignificant limitations due to constrained vocabulary sizes. Simply expanding\nthe codebook often leads to training instability and diminishing performance\ngains, making scalability a critical challenge. In this work, we introduce\nFactorized Quantization (FQ), a novel approach that revitalizes VQ-based\ntokenizers by decomposing a large codebook into multiple independent\nsub-codebooks. This factorization reduces the lookup complexity of large\ncodebooks, enabling more efficient and scalable visual tokenization. To ensure\neach sub-codebook captures distinct and complementary information, we propose a\ndisentanglement regularization that explicitly reduces redundancy, promoting\ndiversity across the sub-codebooks. Furthermore, we integrate representation\nlearning into the training process, leveraging pretrained vision models like\nCLIP and DINO to infuse semantic richness into the learned representations.\nThis design ensures our tokenizer captures diverse semantic levels, leading to\nmore expressive and disentangled representations. Experiments show that the\nproposed FQGAN model substantially improves the reconstruction quality of\nvisual tokenizers, achieving state-of-the-art performance. We further\ndemonstrate that this tokenizer can be effectively adapted into auto-regressive\nimage generation. https://showlab.github.io/FQGAN",
            "upvotes": 10,
            "discussionId": "67455e306338ac803d291a84"
        },
        "publishedAt": "2024-11-26T04:06:00.281Z",
        "title": "Factorized Visual Tokenization and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16681.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
            "fullname": "Zechen Bai",
            "name": "ZechenBai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16489",
            "authors": [
                {
                    "_id": "6745580279d6f3a9da792790",
                    "name": "Zhen Huang",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792791",
                    "name": "Haoyang Zou",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792792",
                    "name": "Xuefeng Li",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792793",
                    "name": "Yixiu Liu",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792794",
                    "name": "Yuxiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792795",
                    "name": "Ethan Chern",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792796",
                    "name": "Shijie Xia",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792797",
                    "name": "Yiwei Qin",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792798",
                    "name": "Weizhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "6745580279d6f3a9da792799",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T15:31:27.000Z",
            "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple\n  Distillation, Big Progress or Bitter Lesson?",
            "summary": "This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.",
            "upvotes": 10,
            "discussionId": "6745580379d6f3a9da792801"
        },
        "publishedAt": "2024-11-26T03:41:30.559Z",
        "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16489.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5233
        }
    },
    {
        "paper": {
            "id": "2411.14522",
            "authors": [
                {
                    "_id": "6744834547d61b15da9ac6b4",
                    "user": {
                        "_id": "64aff95a1e39d3cd14efe6d5",
                        "avatarUrl": "/avatars/77efc2db6462dce2561116aa3a9d5b54.svg",
                        "isPro": false,
                        "fullname": "li",
                        "user": "foreverbeliever",
                        "type": "user"
                    },
                    "name": "Tianbin Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T20:38:04.236Z",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6b5",
                    "name": "Yanzhou Su",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6b6",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6b7",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6b8",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6b9",
                    "name": "Ziyan Huang",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6ba",
                    "name": "Guoan Wang",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6bb",
                    "name": "Chenglong Ma",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6bc",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6bd",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6be",
                    "name": "Yanjun Li",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6bf",
                    "name": "Pengcheng Chen",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c0",
                    "name": "Xiaowei Hu",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c1",
                    "name": "Zhongying Deng",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c2",
                    "name": "Yuanfeng Ji",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c3",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c4",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6744834547d61b15da9ac6c5",
                    "name": "Junjun He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-21T18:59:36.000Z",
            "title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A\n  Comprehensive Multimodal Dataset Towards General Medical AI",
            "summary": "Despite significant advancements in general artificial intelligence, such as\nGPT-4, their effectiveness in the medical domain (general medical AI, GMAI)\nremains constrained due to the absence of specialized medical knowledge. To\naddress this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal\nmedical dataset created by converting hundreds of specialized medical datasets\ninto meticulously constructed image-text pairs. This dataset features\ncomprehensive task coverage, diverse modalities, and high-quality image-text\ndata. Building upon this multimodal dataset, we propose GMAI-VL, a general\nmedical vision-language model with a progressively three-stage training\nstrategy. This approach significantly enhances the model's ability by\nintegrating visual and textual information, thereby improving its ability to\nprocess multimodal data and support accurate diagnosis and clinical\ndecision-making. Experimental evaluations demonstrate that GMAI-VL achieves\nstate-of-the-art results across a wide range of multimodal medical tasks, such\nas visual question answering and medical image diagnosis. Our contributions\ninclude the development of the GMAI-VL-5.5M dataset, the introduction of the\nGMAI-VL model, and the establishment of new benchmarks in multiple medical\ndomains. Code and dataset will be released at\nhttps://github.com/uni-medical/GMAI-VL.",
            "upvotes": 10,
            "discussionId": "6744834647d61b15da9ac703"
        },
        "publishedAt": "2024-11-26T02:02:17.103Z",
        "title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14522.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/77efc2db6462dce2561116aa3a9d5b54.svg",
            "fullname": "li",
            "name": "foreverbeliever",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16341",
            "authors": [
                {
                    "_id": "674540e9f9a4e557a0d01c8a",
                    "name": "Ahmed Heakl",
                    "hidden": false
                },
                {
                    "_id": "674540e9f9a4e557a0d01c8b",
                    "name": "Chaimaa Abi",
                    "hidden": false
                },
                {
                    "_id": "674540e9f9a4e557a0d01c8c",
                    "name": "Rania Hossam",
                    "hidden": false
                },
                {
                    "_id": "674540e9f9a4e557a0d01c8d",
                    "name": "Abdulrahman Mahmoud",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T12:37:07.000Z",
            "title": "From CISC to RISC: language-model guided assembly transpilation",
            "summary": "The transition from x86 to ARM architecture is becoming increasingly common\nacross various domains, primarily driven by ARM's energy efficiency and\nimproved performance across traditional sectors. However, this ISA shift poses\nsignificant challenges, mainly due to the extensive legacy ecosystem of x86\nsoftware and lack of portability across proprietary ecosystems and software\nstacks. This paper introduces CRT, a lightweight LLM-based transpiler that\nautomatically converts x86 assembly to ARM assembly. Our approach bridges the\nfundamental architectural gap between x86's CISC-based and ARM's RISC-based\ncomputing paradigms while preserving program semantics and optimizing\nperformance. We evaluate CRT on diverse real-world applications, achieving\n79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite,\nand an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2\nhardware (ARMv8), our transpiled code achieves 1.73times speedup compared to\nApple's Rosetta 2 virtualization engine, while delivering 2.41times memory\nefficiency and 1.47times better energy consumption. Through testing and\nanalysis, we show that CRT successfully navigates the CISC/RISC divide and\ngenerates correctly executable RISC code despite machine ``language'' barriers.\nWe release our code, models, training datasets, and benchmarks at:\nhttps://ahmedheakl.github.io/asm2asm/.",
            "upvotes": 6,
            "discussionId": "674540ebf9a4e557a0d01d0a"
        },
        "publishedAt": "2024-11-26T02:01:57.499Z",
        "title": "From CISC to RISC: language-model guided assembly transpilation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/fl63vdlpkcFxGuOZdM46Y.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16341.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        }
    },
    {
        "paper": {
            "id": "2411.16443",
            "authors": [
                {
                    "_id": "6745466156ed6a63d03086b0",
                    "name": "Hyojun Go",
                    "hidden": false
                },
                {
                    "_id": "6745466156ed6a63d03086b1",
                    "name": "Byeongjun Park",
                    "hidden": false
                },
                {
                    "_id": "6745466156ed6a63d03086b2",
                    "name": "Jiho Jang",
                    "hidden": false
                },
                {
                    "_id": "6745466156ed6a63d03086b3",
                    "name": "Jin-Young Kim",
                    "hidden": false
                },
                {
                    "_id": "6745466156ed6a63d03086b4",
                    "name": "Soonwoo Kwon",
                    "hidden": false
                },
                {
                    "_id": "6745466156ed6a63d03086b5",
                    "name": "Changick Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T14:46:17.000Z",
            "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting\n  Synthesis",
            "summary": "Text-based generation and editing of 3D scenes hold significant potential for\nstreamlining content creation through intuitive user interactions. While recent\nadvances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time\nrendering, existing methods are often specialized and task-focused, lacking a\nunified framework for both generation and editing. In this paper, we introduce\nSplatFlow, a comprehensive framework that addresses this gap by enabling direct\n3DGS generation and editing. SplatFlow comprises two main components: a\nmulti-view rectified flow (RF) model and a Gaussian Splatting Decoder\n(GSDecoder). The multi-view RF model operates in latent space, generating\nmulti-view images, depths, and camera poses simultaneously, conditioned on text\nprompts, thus addressing challenges like diverse scene scales and complex\ncamera trajectories in real-world settings. Then, the GSDecoder efficiently\ntranslates these latent outputs into 3DGS representations through a\nfeed-forward 3DGS method. Leveraging training-free inversion and inpainting\ntechniques, SplatFlow enables seamless 3DGS editing and supports a broad range\nof 3D tasks-including object editing, novel view synthesis, and camera pose\nestimation-within a unified framework without requiring additional complex\npipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K\ndatasets, demonstrating its versatility and effectiveness in various 3D\ngeneration, editing, and inpainting-based tasks.",
            "upvotes": 5,
            "discussionId": "6745466756ed6a63d03088a6"
        },
        "publishedAt": "2024-11-26T03:26:05.409Z",
        "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/653929a66da48e0d21e65e17/zY4pOzANlOLvN56BODb0R.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16443.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg",
            "fullname": "Byeongjun Park",
            "name": "byeongjun-park",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.14525",
            "authors": [
                {
                    "_id": "674549961934982716d2233c",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d2233d",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d2233e",
                    "name": "Yanjun Li",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d2233f",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22340",
                    "name": "Zhongying Deng",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22341",
                    "name": "Ziyan Huang",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22342",
                    "name": "Yanzhou Su",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22343",
                    "name": "Chenglong Ma",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22344",
                    "name": "Yuanfeng Ji",
                    "hidden": false
                },
                {
                    "_id": "674549961934982716d22345",
                    "name": "Junjun He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-21T19:00:01.000Z",
            "title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image\n  Segmentation",
            "summary": "Computed Tomography (CT) is one of the most popular modalities for medical\nimaging. By far, CT images have contributed to the largest publicly available\ndatasets for volumetric medical segmentation tasks, covering full-body\nanatomical structures. Large amounts of full-body CT images provide the\nopportunity to pre-train powerful models, e.g., STU-Net pre-trained in a\nsupervised fashion, to segment numerous anatomical structures. However, it\nremains unclear in which conditions these pre-trained models can be transferred\nto various downstream medical segmentation tasks, particularly segmenting the\nother modalities and diverse targets. To address this problem, a large-scale\nbenchmark for comprehensive evaluation is crucial for finding these conditions.\nThus, we collected 87 public datasets varying in modality, target, and sample\nsize to evaluate the transfer ability of full-body CT pre-trained models. We\nthen employed a representative model, STU-Net with multiple model scales, to\nconduct transfer learning across modalities and targets. Our experimental\nresults show that (1) there may be a bottleneck effect concerning the dataset\nsize in fine-tuning, with more improvement on both small- and large-scale\ndatasets than medium-size ones. (2) Models pre-trained on full-body CT\ndemonstrate effective modality transfer, adapting well to other modalities such\nas MRI. (3) Pre-training on the full-body CT not only supports strong\nperformance in structure detection but also shows efficacy in lesion detection,\nshowcasing adaptability across target tasks. We hope that this large-scale open\nevaluation of transfer learning can direct future research in volumetric\nmedical image segmentation.",
            "upvotes": 5,
            "discussionId": "674549971934982716d2239f"
        },
        "publishedAt": "2024-11-26T02:38:26.838Z",
        "title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14525.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/04f20baf200712f781b1a338a11644bf.svg",
            "fullname": "Ying Chen",
            "name": "cying",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.16085",
            "authors": [
                {
                    "_id": "67453b867baa7dd7dd0a5e4c",
                    "name": "Kaizhao Liang",
                    "hidden": false
                },
                {
                    "_id": "67453b867baa7dd7dd0a5e4d",
                    "name": "Lizhang Chen",
                    "hidden": false
                },
                {
                    "_id": "67453b867baa7dd7dd0a5e4e",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "67453b867baa7dd7dd0a5e4f",
                    "name": "Qiang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T04:36:01.000Z",
            "title": "Cautious Optimizers: Improving Training with One Line of Code",
            "summary": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searches for faster and more stable optimizers with only\nconstraint positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\nCautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing speed-up on Llama and\nMAE pretraining up to 1.47times. Code is available at\nhttps://github.com/kyleliang919/C-Optim",
            "upvotes": 5,
            "discussionId": "67453b877baa7dd7dd0a5e85"
        },
        "publishedAt": "2024-11-26T01:39:48.791Z",
        "title": "Cautious Optimizers: Improving Training with One Line of Code",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16085.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62140dcdcf7928035e8135ad/FTiirwS_L6IaLHmHwIo2g.png",
            "fullname": "Kaizhao Liang",
            "name": "kz919",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 34
        }
    },
    {
        "paper": {
            "id": "2411.14486",
            "authors": [
                {
                    "_id": "67453112edf0281a265ff746",
                    "name": "David Noever",
                    "hidden": false
                },
                {
                    "_id": "67453112edf0281a265ff747",
                    "name": "Forrest McKee",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-20T04:12:29.000Z",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI\n  Quiz",
            "summary": "This research introduces a novel evaluation framework designed to assess\nlarge language models' (LLMs) ability to acknowledge uncertainty on 675\nfundamentally unsolvable problems. Using a curated dataset of graduate-level\ngrand challenge questions with intentionally unknowable answers, we evaluated\ntwelve state-of-the-art LLMs, including both open and closed-source models, on\ntheir propensity to admit ignorance rather than generate plausible but\nincorrect responses. The best models scored in 62-68% accuracy ranges for\nadmitting the problem solution was unknown in fields ranging from biology to\nphilosophy and mathematics. We observed an inverse relationship between problem\ndifficulty and model accuracy, with GPT-4 demonstrating higher rates of\nuncertainty acknowledgment on more challenging problems (35.8%) compared to\nsimpler ones (20.0%). This pattern indicates that models may be more prone to\ngenerate speculative answers when problems appear more tractable. The study\nalso revealed significant variations across problem categories, with models\nshowing difficulty in acknowledging uncertainty in invention and NP-hard\nproblems while performing relatively better on philosophical and psychological\nchallenges. These results contribute to the growing body of research on\nartificial general intelligence (AGI) assessment by highlighting the importance\nof uncertainty recognition as a critical component of future machine\nintelligence evaluation. This impossibility test thus extends previous\ntheoretical frameworks for universal intelligence testing by providing\nempirical evidence of current limitations in LLMs' ability to recognize their\nown knowledge boundaries, suggesting new directions for improving model\ntraining architectures and evaluation approaches.",
            "upvotes": 5,
            "discussionId": "67453112edf0281a265ff7a3"
        },
        "publishedAt": "2024-11-26T00:53:46.070Z",
        "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14486.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
            "fullname": "David Noever",
            "name": "dnoever",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.12814",
            "authors": [
                {
                    "_id": "673f0f9690d6dbf6b905a252",
                    "user": {
                        "_id": "660adf0742025e0a8bb82b30",
                        "avatarUrl": "/avatars/d47371dc32ab04176021f6223745766a.svg",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "1Junlong",
                        "type": "user"
                    },
                    "name": "Junlong Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:34:27.090Z",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a253",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a254",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a255",
                    "name": "Guoan Wang",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a256",
                    "user": {
                        "_id": "64aff95a1e39d3cd14efe6d5",
                        "avatarUrl": "/avatars/77efc2db6462dce2561116aa3a9d5b54.svg",
                        "isPro": false,
                        "fullname": "li",
                        "user": "foreverbeliever",
                        "type": "user"
                    },
                    "name": "Tianbin Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T20:39:22.117Z",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a257",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a258",
                    "name": "Ruoyu Li",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a259",
                    "name": "He Yao",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a25a",
                    "name": "Junren Chen",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a25b",
                    "name": "JingWen Li",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a25c",
                    "name": "Yanzhou Su",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a25d",
                    "name": "Min Zhu",
                    "hidden": false
                },
                {
                    "_id": "673f0f9690d6dbf6b905a25e",
                    "name": "Junjun He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-19T19:06:29.000Z",
            "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
            "summary": "Interactive Medical Image Segmentation (IMIS) has long been constrained by\nthe limited availability of large-scale, diverse, and densely annotated\ndatasets, which hinders model generalization and consistent evaluation across\ndifferent models. In this paper, we introduce the IMed-361M benchmark dataset,\na significant advancement in general IMIS research. First, we collect and\nstandardize over 6.4 million medical images and their corresponding ground\ntruth masks from multiple data sources. Then, leveraging the strong object\nrecognition capabilities of a vision foundational model, we automatically\ngenerated dense interactive masks for each image and ensured their quality\nthrough rigorous quality control and granularity management. Unlike previous\ndatasets, which are limited by specific modalities or sparse annotations,\nIMed-361M spans 14 modalities and 204 segmentation targets, totaling 361\nmillion masks-an average of 56 masks per image. Finally, we developed an IMIS\nbaseline network on this dataset that supports high-quality mask generation\nthrough interactive inputs, including clicks, bounding boxes, text prompts, and\ntheir combinations. We evaluate its performance on medical image segmentation\ntasks from multiple perspectives, demonstrating superior accuracy and\nscalability compared to existing interactive segmentation models. To facilitate\nresearch on foundational models in medical computer vision, we release the\nIMed-361M and model at https://github.com/uni-medical/IMIS-Bench.",
            "upvotes": 4,
            "discussionId": "673f0f9790d6dbf6b905a2b8"
        },
        "publishedAt": "2024-11-26T12:11:59.231Z",
        "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/660adf0742025e0a8bb82b30/w3esM5ajh4ntAPmM4Jn2P.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12814.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d47371dc32ab04176021f6223745766a.svg",
            "fullname": "Cheng",
            "name": "1Junlong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16035",
            "authors": [
                {
                    "_id": "674550443dd665eec817d2f3",
                    "user": {
                        "_id": "605d07fa8a3450814bada877",
                        "avatarUrl": "/avatars/eafe986982057fbaba962b99d5543477.svg",
                        "isPro": false,
                        "fullname": "Charlie Snell",
                        "user": "sea-snell",
                        "type": "user"
                    },
                    "name": "Charlie Snell",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-26T04:49:22.384Z",
                    "hidden": false
                },
                {
                    "_id": "674550443dd665eec817d2f4",
                    "name": "Eric Wallace",
                    "hidden": false
                },
                {
                    "_id": "674550443dd665eec817d2f5",
                    "name": "Dan Klein",
                    "hidden": false
                },
                {
                    "_id": "674550443dd665eec817d2f6",
                    "name": "Sergey Levine",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T01:48:09.000Z",
            "title": "Predicting Emergent Capabilities by Finetuning",
            "summary": "A fundamental open challenge in modern LLM scaling is the lack of\nunderstanding around emergent capabilities. In particular, language model\npretraining loss is known to be highly predictable as a function of compute.\nHowever, downstream capabilities are far less predictable -- sometimes even\nexhibiting emergent jumps -- which makes it challenging to anticipate the\ncapabilities of future models. In this work, we first pose the task of\nemergence prediction: given access to current LLMs that have random few-shot\naccuracy on a task, can we predict whether future models (GPT-N+1) will have\nnon-trivial accuracy on that task? We then discover a simple insight for this\nproblem: finetuning LLMs on a given task can shift the point in scaling at\nwhich emergence occurs towards less capable models. To operationalize this\ninsight, we can finetune LLMs with varying amounts of data and fit a parametric\nfunction that predicts when emergence will occur (i.e., \"emergence laws\"). We\nvalidate this approach using four standard NLP benchmarks where large-scale\nopen-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and\nCoLA). Using only small-scale LLMs, we find that, in some cases, we can\naccurately predict whether models trained with up to 4x more compute have\nemerged. Finally, we present a case study of two realistic uses for emergence\nprediction.",
            "upvotes": 4,
            "discussionId": "674550453dd665eec817d346"
        },
        "publishedAt": "2024-11-26T03:06:49.177Z",
        "title": "Predicting Emergent Capabilities by Finetuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16035.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/eafe986982057fbaba962b99d5543477.svg",
            "fullname": "Charlie Snell",
            "name": "sea-snell",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.15862",
            "authors": [
                {
                    "_id": "6745c2f874d673071da3303f",
                    "name": "Yijiong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-24T14:38:59.000Z",
            "title": "LLMs Do Not Think Step-by-step In Implicit Reasoning",
            "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. But there is still gap between their efficacy and typical\nexplicit CoT methods. This leaves us a doubt that, does implicit CoT really\nequal to explicit CoT? Therefore, in this study, we address this question\nthrough experiments. We probe the information of intermediate steps from the\nmodel's hidden states when it is performing implicit CoT. The results\nsurprisingly indicate that LLMs hardly think about intermediate steps,\nsuggesting they may just rely on experience rather than strict step-by-step\nreasoning. Moreover, we find LLMs' implicit reasoning capabilities are\nsusceptible and unstable, reaffirming the necessity of explicit CoT to\neffectively support complex tasks.",
            "upvotes": 3,
            "discussionId": "6745c2f974d673071da330d9"
        },
        "publishedAt": "2024-11-26T11:18:22.695Z",
        "title": "LLMs Do Not Think Step-by-step In Implicit Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15862.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png",
            "fullname": "yuyijiong",
            "name": "yuyijiong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 34
        }
    },
    {
        "paper": {
            "id": "2411.15671",
            "authors": [
                {
                    "_id": "67455093d015d7e8717ff69e",
                    "name": "Ali Behrouz",
                    "hidden": false
                },
                {
                    "_id": "67455093d015d7e8717ff69f",
                    "name": "Ali Parviz",
                    "hidden": false
                },
                {
                    "_id": "67455093d015d7e8717ff6a0",
                    "name": "Mahdi Karami",
                    "hidden": false
                },
                {
                    "_id": "67455093d015d7e8717ff6a1",
                    "name": "Clayton Sanford",
                    "hidden": false
                },
                {
                    "_id": "67455093d015d7e8717ff6a2",
                    "name": "Bryan Perozzi",
                    "hidden": false
                },
                {
                    "_id": "67455093d015d7e8717ff6a3",
                    "name": "Vahab Mirrokni",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-23T23:24:42.000Z",
            "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
            "summary": "Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as\ndominant backbones of recent deep learning frameworks, mainly due to their\nefficiency, representational power, and/or ability to capture long-range\ndependencies. Adopting these sequence models for graph-structured data has\nrecently gained popularity as the alternative to Message Passing Neural\nNetworks (MPNNs). There is, however, a lack of a common foundation about what\nconstitutes a good graph sequence model, and a mathematical description of the\nbenefits and deficiencies in adopting different sequence models for learning on\ngraphs. To this end, we first present Graph Sequence Model (GSM), a unifying\nframework for adopting sequence models for graphs, consisting of three main\nsteps: (1) Tokenization, which translates the graph into a set of sequences;\n(2) Local Encoding, which encodes local neighborhoods around each node; and (3)\nGlobal Encoding, which employs a scalable sequence model to capture long-range\ndependencies within the sequences. This framework allows us to understand,\nevaluate, and compare the power of different sequence model backbones in graph\ntasks. Our theoretical evaluations of the representation power of Transformers\nand modern recurrent models through the lens of global and local graph tasks\nshow that there are both negative and positive sides for both types of models.\nBuilding on this observation, we present GSM++, a fast hybrid model that uses\nthe Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into\nhierarchical sequences, and then employs a hybrid architecture of Transformer\nto encode these sequences. Our theoretical and experimental results support the\ndesign of GSM++, showing that GSM++ outperforms baselines in most benchmark\nevaluations.",
            "upvotes": 3,
            "discussionId": "67455094d015d7e8717ff6ff"
        },
        "publishedAt": "2024-11-26T03:08:30.555Z",
        "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15671.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
            "fullname": "Ali Behrouz",
            "name": "AliBehrouz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.16665",
            "authors": [
                {
                    "_id": "6745ee44b7b160fc58aae8f0",
                    "name": "Or Hirschorn",
                    "hidden": false
                },
                {
                    "_id": "6745ee44b7b160fc58aae8f1",
                    "name": "Shai Avidan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T18:53:09.000Z",
            "title": "Edge Weight Prediction For Category-Agnostic Pose Estimation",
            "summary": "Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse\nobject categories with a single model, using one or a few annotated support\nimages. Recent works have shown that using a pose graph (i.e., treating\nkeypoints as nodes in a graph rather than isolated points) helps handle\nocclusions and break symmetry. However, these methods assume a static pose\ngraph with equal-weight edges, leading to suboptimal results. We introduce\nEdgeCape, a novel framework that overcomes these limitations by predicting the\ngraph's edge weights which optimizes localization. To further leverage\nstructural priors, we propose integrating Markovian Structural Bias, which\nmodulates the self-attention interaction between nodes based on the number of\nhops between them. We show that this improves the model's ability to capture\nglobal spatial dependencies. Evaluated on the MP-100 benchmark, which includes\n100 categories and over 20K images, EdgeCape achieves state-of-the-art results\nin the 1-shot setting and leads among similar-sized methods in the 5-shot\nsetting, significantly improving keypoint localization accuracy. Our code is\npublicly available.",
            "upvotes": 1,
            "discussionId": "6745ee48b7b160fc58aae9e6"
        },
        "publishedAt": "2024-11-26T14:22:39.927Z",
        "title": "Edge Weight Prediction For Category-Agnostic Pose Estimation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6318585a70b013d66093c9f5/ysmgQc4NGAaX5Rmxj2Se3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16665.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/25a3d2693e7427f2ce9111cc9bc3bf8c.svg",
            "fullname": "Or Hirschschorn",
            "name": "orhir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    }
]