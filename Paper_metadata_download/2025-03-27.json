[
  {
    "paper": {
      "id": "2503.19757",
      "authors": [
        {
          "_id": "67e3e1e20706b07bfb2713d6",
          "name": "Zhi Hou",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d7",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d8",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d9",
          "name": "Haonan Duan",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713da",
          "name": "Hengjun Pu",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713db",
          "name": "Ronglei Tong",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dc",
          "name": "Chengyang Zhao",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dd",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713de",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713df",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713e0",
          "name": "Yuntao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
      ],
      "publishedAt": "2025-03-25T15:19:56.000Z",
      "submittedOnDailyAt": "2025-03-27T01:27:55.746Z",
      "title": "Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy",
      "submittedOnDailyBy": {
        "_id": "643fa1c318afbc4d1f3e5e59",
        "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
        "isPro": false,
        "fullname": "Zhi Hou",
        "user": "zhihou",
        "type": "user"
      },
      "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
      "upvotes": 31,
      "discussionId": "67e3e1e40706b07bfb2714cd",
      "projectPage": "https://robodita.github.io",
      "githubRepo": "https://github.com/RoboDita/Dita",
      "ai_keywords": [
        "Transformer architectures",
        "multimodal diffusion process",
        "in-context conditioning",
        "action deltas",
        "environmental nuances",
        "cross-embodiment datasets",
        "long-horizon tasks",
        "10-shot finetuning",
        "third-person camera inputs",
        "generalist robot policy learning"
      ]
    },
    "publishedAt": "2025-03-25T11:19:56.000Z",
    "title": "Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy",
    "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fa1c318afbc4d1f3e5e59",
      "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
      "fullname": "Zhi Hou",
      "name": "zhihou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19990",
      "authors": [
        {
          "_id": "67e4d3df7e97884ba4150ec0",
          "name": "Kexian Tang",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec1",
          "name": "Junyao Gao",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec2",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec3",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec4",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec5",
          "name": "Zhening Xing",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec6",
          "name": "Wenran Liu",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec7",
          "name": "Kaifeng Lyu",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec8",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
      ],
      "publishedAt": "2025-03-25T18:21:07.000Z",
      "submittedOnDailyAt": "2025-03-27T03:00:01.698Z",
      "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\nto evaluate both spatial understanding and sequential\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
      "upvotes": 20,
      "discussionId": "67e4d3e07e97884ba4150f2b",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "LEGO-Puzzles",
        "visual question-answering (VQA)",
        "spatial understanding",
        "sequential reasoning",
        "Gemini-2.0-Flash",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T14:21:07.000Z",
    "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "summary": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\nto evaluate both spatial understanding and sequential\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20240",
      "authors": [
        {
          "_id": "67e4ae6787c92169aa3caa74",
          "name": "Prin Phunyaphibarn",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa75",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa76",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa77",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T05:11:38.000Z",
      "submittedOnDailyAt": "2025-03-27T00:22:30.335Z",
      "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.",
      "upvotes": 16,
      "discussionId": "67e4ae6a87c92169aa3cabc2",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "conditional diffusion models",
        "noise prediction",
        "dropout rate",
        "priors",
        "fine-tuning",
        "base model",
        "unconditional generation",
        "variance scaling",
        "Zero-1-to-3",
        "Versatile Diffusion",
        "DiT",
        "DynamiCrafter",
        "InstructPix2Pix"
      ]
    },
    "publishedAt": "2025-03-26T01:11:38.000Z",
    "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
    "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20314",
      "authors": [
        {
          "_id": "67e4b65a080a33e3955b340c",
          "name": "WanTeam",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340e",
          "name": "Ang Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340f",
          "name": "Baole Ai",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3410",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3411",
          "name": "Chaojie Mao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3412",
          "name": "Chen-Wei Xie",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3413",
          "name": "Di Chen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3414",
          "name": "Feiwu Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3415",
          "name": "Haiming Zhao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3416",
          "name": "Jianxiao Yang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3417",
          "name": "Jianyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3418",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3419",
          "name": "Jingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341a",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341b",
          "name": "Jinkai Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341c",
          "name": "Jixuan Chen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341d",
          "name": "Kai Zhu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341e",
          "name": "Kang Zhao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341f",
          "name": "Keyu Yan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3420",
          "name": "Lianghua Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3421",
          "name": "Mengyang Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3422",
          "name": "Ningyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3423",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3424",
          "name": "Pingyu Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3425",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3426",
          "name": "Ruili Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3427",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3428",
          "name": "Siyang Sun",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3429",
          "name": "Tao Fang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342a",
          "name": "Tianxing Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342b",
          "name": "Tianyi Gui",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342c",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342d",
          "name": "Tong Shen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342e",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342f",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3430",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3431",
          "name": "Wenmeng Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3432",
          "name": "Wente Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3433",
          "name": "Wenting Shen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3434",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3435",
          "name": "Xianzhong Shi",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3436",
          "name": "Xiaoming Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3437",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3438",
          "name": "Yan Kou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3439",
          "name": "Yangyu Lv",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343b",
          "name": "Yijing Liu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343c",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343e",
          "name": "Yitong Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343f",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3440",
          "name": "You Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3441",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3442",
          "name": "Yulin Pan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3443",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3444",
          "name": "Yuntao Hong",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3445",
          "name": "Yupeng Shi",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3446",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3447",
          "name": "Zeyinzi Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3448",
          "name": "Zhen Han",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3449",
          "name": "Zhi-Fan Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b344a",
          "name": "Ziyu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T08:25:43.000Z",
      "submittedOnDailyAt": "2025-03-27T00:52:37.426Z",
      "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
      "upvotes": 13,
      "discussionId": "67e4b663080a33e3955b371a",
      "ai_keywords": [
        "diffusion transformer",
        "VAE",
        "large-scale data curation",
        "automated evaluation metrics",
        "scaling laws",
        "image-to-video",
        "instruction-guided video editing",
        "personal video generation"
      ]
    },
    "publishedAt": "2025-03-26T04:25:43.000Z",
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20201",
      "authors": [
        {
          "_id": "67e4b04c8c0347025bd0fe84",
          "name": "Salaheddin Alzubi",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe85",
          "user": {
            "_id": "673f945e6cd62dbd4b02790d",
            "avatarUrl": "/avatars/3742e4e6b88d4f8b78d5c5308f55773e.svg",
            "isPro": false,
            "fullname": "Creston Brooks",
            "user": "cabxyz",
            "type": "user"
          },
          "name": "Creston Brooks",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T01:56:28.853Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe86",
          "name": "Purva Chiniya",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe87",
          "name": "Edoardo Contente",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe88",
          "name": "Chiara von Gerlach",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe89",
          "name": "Lucas Irwin",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8a",
          "name": "Yihan Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8b",
          "name": "Arda Kaz",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8c",
          "name": "Windsor Nguyen",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8d",
          "name": "Sewoong Oh",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8e",
          "name": "Himanshu Tyagi",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8f",
          "name": "Pramod Viswanath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:51:32.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:59.804Z",
      "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
      "upvotes": 12,
      "discussionId": "67e4b04c8c0347025bd0fed2",
      "ai_keywords": [
        "LLMs",
        "reasoning agents",
        "web search tools",
        "Open Search Tool",
        "Open Reasoning Agent",
        "DeepSeek-R1",
        "SimpleQA",
        "FRAMES",
        "GPT-4o Search Preview"
      ]
    },
    "publishedAt": "2025-03-25T23:51:32.000Z",
    "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19480",
      "authors": [
        {
          "_id": "67e3693eebafaa1efbed08d2",
          "user": {
            "_id": "67d30d9ae45dc43004b31425",
            "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
            "isPro": false,
            "fullname": "Shijie Ma",
            "user": "msj9817",
            "type": "user"
          },
          "name": "Shijie Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-26T20:44:37.274Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d3",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d4",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d5",
          "name": "Yuxin Guo",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d6",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d7",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:15:34.000Z",
      "submittedOnDailyAt": "2025-03-27T01:26:37.430Z",
      "title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers",
      "submittedOnDailyBy": {
        "_id": "67d30d9ae45dc43004b31425",
        "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
        "isPro": false,
        "fullname": "Shijie Ma",
        "user": "msj9817",
        "type": "user"
      },
      "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.",
      "upvotes": 10,
      "discussionId": "67e36940ebafaa1efbed0951",
      "projectPage": "https://mashijie1028.github.io/GenHancer/",
      "githubRepo": "https://github.com/mashijie1028/GenHancer",
      "ai_keywords": [
        "generative",
        "discriminative",
        "Contrastive Language-Image Pre-Training (CLIP)",
        "visual features",
        "reconstruction",
        "local tokens",
        "global visual tokens",
        "Conditioning mechanisms",
        "end-to-end training",
        "denoising configurations",
        "two-stage training",
        "lightweight denoisers",
        "continuous denoisers",
        "discrete denoisers",
        "Generation paradigms",
        "GenHancer",
        "MMVP-VLM benchmark",
        "OpenAICLIP",
        "multimodal large language models",
        "vision-centric performance"
      ]
    },
    "publishedAt": "2025-03-25T05:15:34.000Z",
    "title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers",
    "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d30d9ae45dc43004b31425",
      "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
      "fullname": "Shijie Ma",
      "name": "msj9817",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20757",
      "authors": [
        {
          "_id": "67e4c08fd9b7021d4a600fa4",
          "name": "Yunhai Hu",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa5",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T03:05:54.275Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa6",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa7",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:46:08.000Z",
      "submittedOnDailyAt": "2025-03-27T01:36:43.674Z",
      "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
      "upvotes": 6,
      "discussionId": "67e4c092d9b7021d4a60108b",
      "ai_keywords": [
        "MCTS-RAG",
        "retrieval-augmented generation (RAG)",
        "Monte Carlo Tree Search (MCTS)",
        "reasoning paths",
        "iterative decision-making",
        "knowledge suboptimally",
        "structured reasoning",
        "adaptive retrieval",
        "decision-making",
        "hallucinations",
        "factual accuracy",
        "response consistency",
        "ComplexWebQA",
        "GPQA",
        "FoolMeTwice",
        "small-scale LMs",
        "frontier LLMs (GPT-4o)",
        "scaling inference-time compute"
      ]
    },
    "publishedAt": "2025-03-26T13:46:08.000Z",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
    "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20215",
      "authors": [
        {
          "_id": "67e4f2507e97884ba4205660",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205661",
          "name": "Zhifang Guo",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205662",
          "name": "Jinzheng He",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205663",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205664",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205665",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205666",
          "name": "Keqin Chen",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205667",
          "name": "Jialin Wang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205668",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205669",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566a",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566b",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566c",
          "name": "Yunfei Chu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566d",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
      ],
      "publishedAt": "2025-03-26T04:17:55.000Z",
      "submittedOnDailyAt": "2025-03-27T05:14:09.555Z",
      "title": "Qwen2.5-Omni Technical Report",
      "submittedOnDailyBy": {
        "_id": "659e513ea9bc1f60189ac148",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
        "isPro": false,
        "fullname": "YuanjunLv",
        "user": "Bakerbunker",
        "type": "user"
      },
      "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
      "upvotes": 6,
      "discussionId": "67e4f2527e97884ba42056df",
      "projectPage": "https://qwenlm.github.io/blog/qwen2.5-omni/",
      "githubRepo": "https://github.com/QwenLM/Qwen2.5-Omni",
      "ai_keywords": [
        "multimodal model",
        "block-wise processing",
        "interleaved manner",
        "TMRoPE (Time-aligned Multimodal RoPE)",
        "position embedding",
        "Thinker-Talker architecture",
        "large language model",
        "dual-track autoregressive model",
        "end-to-end manner",
        "sliding-window DiT",
        "receptive field",
        "initial package delay",
        "Omni-Bench",
        "MMLU",
        "GSM8K",
        "end-to-end speech instruction following"
      ]
    },
    "publishedAt": "2025-03-26T00:17:55.000Z",
    "title": "Qwen2.5-Omni Technical Report",
    "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e513ea9bc1f60189ac148",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
      "fullname": "YuanjunLv",
      "name": "Bakerbunker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20672",
      "authors": [
        {
          "_id": "67e4b82c672b3d9c9cb42c70",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c71",
          "name": "Shishi Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c72",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c73",
          "name": "Qisheng Liao",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c74",
          "name": "Bohan Chen",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c75",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c76",
          "name": "Danqing Huang",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c77",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c78",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T16:04:57.000Z",
      "submittedOnDailyAt": "2025-03-27T01:00:23.038Z",
      "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
      "upvotes": 5,
      "discussionId": "67e4b831672b3d9c9cb42ebb",
      "ai_keywords": [
        "scalable, high-quality business content dataset",
        "Infographics-650K",
        "layer-wise retrieval-augmented infographic generation scheme",
        "layout-guided cross attention scheme",
        "cropped region latent space",
        "layout conditional CFG",
        "BizEval prompt set",
        "ablation experiments"
      ]
    },
    "publishedAt": "2025-03-26T12:04:57.000Z",
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
    "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20020",
      "authors": [
        {
          "_id": "67e4b288fa81c69f446da710",
          "name": "Gemini Robotics Team",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da711",
          "name": "Saminda Abeyruwan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da712",
          "name": "Joshua Ainslie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da713",
          "name": "Jean-Baptiste Alayrac",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da714",
          "name": "Montserrat Gonzalez Arenas",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da715",
          "name": "Travis Armstrong",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da716",
          "name": "Ashwin Balakrishna",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da717",
          "name": "Robert Baruch",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da718",
          "name": "Maria Bauza",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da719",
          "name": "Michiel Blokzijl",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71a",
          "name": "Steven Bohez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71b",
          "name": "Konstantinos Bousmalis",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71c",
          "name": "Anthony Brohan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71d",
          "name": "Thomas Buschmann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71e",
          "name": "Arunkumar Byravan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71f",
          "name": "Serkan Cabi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da720",
          "name": "Ken Caluwaerts",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da721",
          "name": "Federico Casarini",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da722",
          "name": "Oscar Chang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da723",
          "name": "Jose Enrique Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da724",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da725",
          "name": "Hao-Tien Lewis Chiang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da726",
          "name": "Krzysztof Choromanski",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da727",
          "name": "David D'Ambrosio",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da728",
          "name": "Sudeep Dasari",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da729",
          "name": "Todor Davchev",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72a",
          "name": "Coline Devin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72b",
          "name": "Norman Di Palo",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72c",
          "name": "Tianli Ding",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72d",
          "name": "Adil Dostmohamed",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72e",
          "name": "Danny Driess",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72f",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da730",
          "name": "Debidatta Dwibedi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da731",
          "name": "Michael Elabd",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da732",
          "name": "Claudio Fantacci",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da733",
          "name": "Cody Fong",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da734",
          "name": "Erik Frey",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da735",
          "name": "Chuyuan Fu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da736",
          "name": "Marissa Giustina",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da737",
          "name": "Keerthana Gopalakrishnan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da738",
          "name": "Laura Graesser",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da739",
          "name": "Leonard Hasenclever",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73a",
          "name": "Nicolas Heess",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73b",
          "name": "Brandon Hernaez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73c",
          "name": "Alexander Herzog",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73d",
          "name": "R. Alex Hofer",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73e",
          "name": "Jan Humplik",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73f",
          "name": "Atil Iscen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da740",
          "name": "Mithun George Jacob",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da741",
          "name": "Deepali Jain",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da742",
          "name": "Ryan Julian",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da743",
          "name": "Dmitry Kalashnikov",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da744",
          "name": "M. Emre Karagozler",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da745",
          "name": "Stefani Karp",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da746",
          "name": "Chase Kew",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da747",
          "name": "Jerad Kirkland",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da748",
          "name": "Sean Kirmani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da749",
          "name": "Yuheng Kuang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74a",
          "name": "Thomas Lampe",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74b",
          "name": "Antoine Laurens",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74c",
          "name": "Isabel Leal",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74d",
          "name": "Alex X. Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74e",
          "name": "Tsang-Wei Edward Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74f",
          "name": "Jacky Liang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da750",
          "name": "Yixin Lin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da751",
          "name": "Sharath Maddineni",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da752",
          "name": "Anirudha Majumdar",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da753",
          "name": "Assaf Hurwitz Michaely",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da754",
          "name": "Robert Moreno",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da755",
          "name": "Michael Neunert",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da756",
          "name": "Francesco Nori",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da757",
          "name": "Carolina Parada",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da758",
          "name": "Emilio Parisotto",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da759",
          "name": "Peter Pastor",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75a",
          "name": "Acorn Pooley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75b",
          "name": "Kanishka Rao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75c",
          "name": "Krista Reymann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75d",
          "name": "Dorsa Sadigh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75e",
          "name": "Stefano Saliceti",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75f",
          "name": "Pannag Sanketi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da760",
          "name": "Pierre Sermanet",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da761",
          "name": "Dhruv Shah",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da762",
          "name": "Mohit Sharma",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da763",
          "name": "Kathryn Shea",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da764",
          "name": "Charles Shu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da765",
          "name": "Vikas Sindhwani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da766",
          "name": "Sumeet Singh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da767",
          "name": "Radu Soricut",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da768",
          "name": "Jost Tobias Springenberg",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da769",
          "name": "Rachel Sterneck",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76a",
          "name": "Razvan Surdulescu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76b",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76c",
          "name": "Jonathan Tompson",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76d",
          "name": "Vincent Vanhoucke",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76e",
          "name": "Jake Varley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76f",
          "name": "Grace Vesom",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da770",
          "name": "Giulia Vezzani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da771",
          "name": "Oriol Vinyals",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da772",
          "name": "Ayzaan Wahid",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da773",
          "name": "Stefan Welker",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da774",
          "name": "Paul Wohlhart",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da775",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da776",
          "name": "Ted Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da777",
          "name": "Annie Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da778",
          "name": "Jinyu Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da779",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77a",
          "name": "Sichun Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77b",
          "name": "Ying Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77c",
          "name": "Zhuo Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77d",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77e",
          "name": "Rui Yao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77f",
          "name": "Sergey Yaroshenko",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da780",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da781",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da782",
          "name": "Jingwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da783",
          "name": "Tingnan Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da784",
          "name": "Allan Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da785",
          "name": "Yuxiang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T19:02:56.000Z",
      "submittedOnDailyAt": "2025-03-27T00:36:27.703Z",
      "title": "Gemini Robotics: Bringing AI into the Physical World",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
      "upvotes": 4,
      "discussionId": "67e4b28cfa81c69f446da8c7",
      "ai_keywords": [
        "Vision-Language-Action (VLA) generalist model",
        "multimodal model",
        "multimodal reasoning capabilities",
        "fine-tuning",
        "long-horizon, highly dexterous tasks",
        "short-horizon tasks",
        "robot embodiments",
        "embodied reasoning",
        "object detection",
        "pointing",
        "trajectory prediction",
        "grasp prediction",
        "multi-view correspondence",
        "3D bounding box predictions",
        "robotics applications",
        "safety considerations",
        "robotics foundation models",
        "general-purpose robots"
      ]
    },
    "publishedAt": "2025-03-25T15:02:56.000Z",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "summary": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19462",
      "authors": [
        {
          "_id": "67e3641cd8da46951f860d84",
          "name": "Haiyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d85",
          "name": "Xinyuan Chen",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d86",
          "name": "Yaohui Wang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d87",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d88",
          "name": "Yunhong Wang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d89",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T08:52:07.000Z",
      "submittedOnDailyAt": "2025-03-27T00:39:48.103Z",
      "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
      "submittedOnDailyBy": {
        "_id": "645b8bf6438d6cfbe1ae47ae",
        "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
        "isPro": false,
        "fullname": "Haiyu Zhang",
        "user": "aejion",
        "type": "user"
      },
      "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
      "upvotes": 4,
      "discussionId": "67e3641ed8da46951f860e12",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "iterative denoising",
        "inference steps",
        "diffusion distillation",
        "AccVideo",
        "synthetic dataset",
        "pretrained video diffusion model",
        "denoising trajectories",
        "trajectory-based few-step guidance",
        "noise-to-video mapping",
        "adversarial training strategy"
      ]
    },
    "publishedAt": "2025-03-25T04:52:07.000Z",
    "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
    "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b8bf6438d6cfbe1ae47ae",
      "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
      "fullname": "Haiyu Zhang",
      "name": "aejion",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20756",
      "authors": [
        {
          "_id": "67e4b0b850ca38886d7e78d0",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d1",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d2",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d3",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d4",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d5",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d6",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:45:29.000Z",
      "submittedOnDailyAt": "2025-03-27T00:28:51.612Z",
      "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
      "upvotes": 3,
      "discussionId": "67e4b0bb50ca38886d7e79d0",
      "ai_keywords": [
        "Knowledge Editing",
        "ADS-Edit",
        "multimodal knowledge editing dataset",
        "autonomous driving systems"
      ]
    },
    "publishedAt": "2025-03-26T13:45:29.000Z",
    "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
    "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19950",
      "authors": [
        {
          "_id": "67e4b27cfe1f5acc68028de9",
          "name": "Han Chen",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dea",
          "name": "Zicong Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028deb",
          "name": "Zining Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dec",
          "name": "Bingsheng He",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028ded",
          "name": "Pingyi Luo",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dee",
          "name": "Mian Lu",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028def",
          "name": "Yuqiang Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T16:24:45.000Z",
      "submittedOnDailyAt": "2025-03-27T02:17:05.033Z",
      "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
      "submittedOnDailyBy": {
        "_id": "6399c67bf78f75ae73146760",
        "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
        "isPro": false,
        "fullname": "CHEN Han",
        "user": "Concyclics",
        "type": "user"
      },
      "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
      "upvotes": 3,
      "discussionId": "67e4b27efe1f5acc68028e72",
      "githubRepo": "https://github.com/Concyclics/LogQuantKV",
      "ai_keywords": [
        "KV Cache",
        "large language model (LLM)",
        "token",
        "attention pattern",
        "log-based filtering mechanism",
        "throughput",
        "batch size",
        "accuracy",
        "Math Completion",
        "Code Completion",
        "compression ratio",
        "transformers library"
      ]
    },
    "publishedAt": "2025-03-25T12:24:45.000Z",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
    "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399c67bf78f75ae73146760",
      "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
      "fullname": "CHEN Han",
      "name": "Concyclics",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20198",
      "authors": [
        {
          "_id": "67e4b98039509b0149142daa",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dab",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dac",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dad",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dae",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:44:25.000Z",
      "submittedOnDailyAt": "2025-03-27T01:06:03.239Z",
      "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\n3~dalle3 in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
      "upvotes": 2,
      "discussionId": "67e4b98439509b0149142f3c",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "image tokenizer",
        "binary tokenizer",
        "multimodal autoregressive model",
        "font style",
        "text properties",
        "size",
        "color",
        "alignment",
        "SD3.5 Large",
        "GPT4o",
        "DALL-E 3",
        "long-text image generation"
      ]
    },
    "publishedAt": "2025-03-25T23:44:25.000Z",
    "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models",
    "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\n3~dalle3 in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19846",
      "authors": [
        {
          "_id": "67e4c449672b3d9c9cb8aa4b",
          "name": "Aaron Serianni",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4c",
          "name": "Tyler Zhu",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4d",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4e",
          "name": "Vikram V. Ramaswamy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:11:39.000Z",
      "submittedOnDailyAt": "2025-03-27T01:57:35.164Z",
      "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
      "submittedOnDailyBy": {
        "_id": "67e4c444692ba54248a6b337",
        "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
        "isPro": false,
        "fullname": "Aaron Serianni",
        "user": "serianni",
        "type": "user"
      },
      "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
      "upvotes": 2,
      "discussionId": "67e4c44a672b3d9c9cb8aaae",
      "ai_keywords": [
        "Attention-IoU",
        "attention maps",
        "internal representations",
        "image features",
        "Waterbirds dataset",
        "CelebA dataset",
        "accuracy disparities",
        "protected attribute",
        "attribute correlations",
        "confounding variables"
      ]
    },
    "publishedAt": "2025-03-25T13:11:39.000Z",
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "67e4c444692ba54248a6b337",
      "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
      "fullname": "Aaron Serianni",
      "name": "serianni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20271",
      "authors": [
        {
          "_id": "67e4dc6a38e4d1444c71ce70",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce71",
          "name": "Weitao Feng",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce72",
          "name": "Hardy Chen",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce73",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce74",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce75",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:38:31.000Z",
      "submittedOnDailyAt": "2025-03-27T06:03:39.751Z",
      "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
      "submittedOnDailyBy": {
        "_id": "604ae011caabafacfa48e3de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
        "isPro": false,
        "fullname": "Haoqin Tu",
        "user": "PahaII",
        "type": "user"
      },
      "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.",
      "upvotes": 1,
      "discussionId": "67e4dc6b38e4d1444c71cee2",
      "ai_keywords": [
        "vision large language models (VLLMs)",
        "output reward models (ORMs)",
        "process reward models (PRMs)",
        "vision-language benchmarks",
        "Chain-of-Thought (CoT)",
        "vision-language process reward data",
        "tree-search algorithm",
        "ViLBench"
      ]
    },
    "publishedAt": "2025-03-26T02:38:31.000Z",
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604ae011caabafacfa48e3de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
      "fullname": "Haoqin Tu",
      "name": "PahaII",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20220",
      "authors": [
        {
          "_id": "67e4b035af7d0551dc377e13",
          "name": "Weijie Guo",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e14",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e15",
          "name": "Wufei Ma",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e16",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T04:23:53.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:48.304Z",
      "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
      "submittedOnDailyBy": {
        "_id": "625f81afe1994410eef1c36a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
        "isPro": false,
        "fullname": "Wufei Ma",
        "user": "wufeim",
        "type": "user"
      },
      "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.",
      "upvotes": 1,
      "discussionId": "67e4b038af7d0551dc377f07",
      "projectPage": "https://analysis-by-synthesis.github.io/DINeMo/",
      "ai_keywords": [
        "neural mesh models",
        "analysis-by-synthesis",
        "robustness",
        "partial occlusion",
        "domain shifts",
        "part-contrastive learning",
        "pseudo-correspondence",
        "visual foundation models",
        "bidirectional pseudo-correspondence generation",
        "local appearance features",
        "global context information",
        "zero-shot",
        "few-shot",
        "fully-supervised methods"
      ]
    },
    "publishedAt": "2025-03-26T00:23:53.000Z",
    "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
    "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625f81afe1994410eef1c36a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
      "fullname": "Wufei Ma",
      "name": "wufeim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19953",
      "authors": [
        {
          "_id": "67e4b46cc90e5edf25f581f8",
          "name": "Stefan Stojanov",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581f9",
          "name": "David Wendt",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fa",
          "name": "Seungwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fb",
          "name": "Rahul Venkatesh",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fc",
          "name": "Kevin Feigelis",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fd",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fe",
          "name": "Daniel LK Yamins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:58:52.000Z",
      "submittedOnDailyAt": "2025-03-27T00:44:41.359Z",
      "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.",
      "upvotes": 1,
      "discussionId": "67e4b46ec90e5edf25f582db",
      "ai_keywords": [
        "self-supervised learning",
        "flow estimation",
        "occlusion estimation",
        "next-frame prediction model",
        "counterfactual probes",
        "motion estimation"
      ]
    },
    "publishedAt": "2025-03-25T13:58:52.000Z",
    "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
    "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16870",
      "authors": [
        {
          "_id": "67e4c860136c8a867191e52e",
          "name": "Anshumann",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e52f",
          "name": "Mohd Abbas Zaidi",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e530",
          "name": "Akhil Kedia",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e531",
          "name": "Jinwoo Ahn",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e532",
          "name": "Taehwak Kwon",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e533",
          "name": "Kangwook Lee",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e534",
          "name": "Haejun Lee",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e535",
          "name": "Joohyung Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:58:18.000Z",
      "submittedOnDailyAt": "2025-03-27T02:09:27.795Z",
      "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
      "submittedOnDailyBy": {
        "_id": "61765fe0b0715831eab6d465",
        "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
        "isPro": false,
        "fullname": "Mohd Abbas Zaidi",
        "user": "ya-mehdi",
        "type": "user"
      },
      "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
      "upvotes": 1,
      "discussionId": "67e4c861136c8a867191e58c",
      "ai_keywords": [
        "Knowledge distillation",
        "Large Language Models",
        "teacher output logits",
        "pre-computed",
        "cached",
        "sparse knowledge distillation",
        "Top-K probabilities",
        "biased estimates",
        "teacher probability distribution",
        "student",
        "suboptimal performance",
        "calibration",
        "importance-sampling-based method",
        "Random Sampling Knowledge Distillation",
        "unbiased estimates",
        "gradient in expectation",
        "storing significantly sparser logits",
        "cross-entropy based training",
        "competitive performance",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-21T01:58:18.000Z",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61765fe0b0715831eab6d465",
      "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
      "fullname": "Mohd Abbas Zaidi",
      "name": "ya-mehdi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]