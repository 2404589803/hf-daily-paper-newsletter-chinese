[
  {
    "paper": {
      "id": "2510.24668",
      "authors": [
        {
          "_id": "690188f7646208eac0d1f482",
          "name": "Mingyi Deng",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f483",
          "name": "Lijun Huang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f484",
          "name": "Yani Fan",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f485",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f486",
          "name": "Fashen Ren",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f487",
          "name": "Jinyi Bai",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f488",
          "name": "Fuzhen Yang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f489",
          "name": "Dayi Miao",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48a",
          "name": "Zhaoyang Yu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48b",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48c",
          "name": "Yanfei Zhang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48d",
          "name": "Fengwei Teng",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48e",
          "name": "Yingjia Wan",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f48f",
          "name": "Song Hu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f490",
          "name": "Yude Li",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f491",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f492",
          "name": "Conghao Hu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f493",
          "name": "Haoyu Li",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f494",
          "name": "Qirui Fu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f495",
          "name": "Tai Zhong",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f496",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f497",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f498",
          "name": "Nan Tang",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f499",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "690188f7646208eac0d1f49a",
          "name": "Yuyu Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:35:54.000Z",
      "submittedOnDailyAt": "2025-10-29T01:58:28.190Z",
      "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
      "submittedOnDailyBy": {
        "_id": "65f40e83653c231cbaf7defe",
        "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
        "isPro": false,
        "fullname": "Jiayi Zhang",
        "user": "didiforhugface",
        "type": "user"
      },
      "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
      "upvotes": 53,
      "discussionId": "690188f7646208eac0d1f49b",
      "ai_summary": "InteractComp evaluates search agents' ability to recognize and resolve query ambiguity through interaction, revealing significant gaps in current models' capabilities.",
      "ai_keywords": [
        "search agents",
        "query ambiguity",
        "interaction mechanisms",
        "benchmark",
        "expert-curated questions",
        "target-distractor methodology",
        "accuracy",
        "overconfidence",
        "reasoning deficits",
        "longitudinal analysis"
      ]
    },
    "publishedAt": "2025-10-28T13:35:54.000Z",
    "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
    "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24668.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f40e83653c231cbaf7defe",
      "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
      "fullname": "Jiayi Zhang",
      "name": "didiforhugface",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24701",
      "authors": [
        {
          "_id": "69017244646208eac0d1f30e",
          "name": "Tongyi DeepResearch Team",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f30f",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f310",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f311",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f312",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f313",
          "name": "Guangyu Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f314",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f315",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f316",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f317",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f318",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f319",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31a",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31b",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31c",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31d",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31e",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31f",
          "name": "Xinmiao Yu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f320",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f321",
          "name": "Xixi Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f322",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f323",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f324",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f325",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f326",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f327",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f328",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f329",
          "name": "Donglei Yu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32a",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32b",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32c",
          "name": "Jiayin Yang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32d",
          "name": "Jun Lin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32e",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32f",
          "name": "Kui Zeng",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f330",
          "name": "Li Yang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f331",
          "name": "Hailong Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f332",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f333",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f334",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f335",
          "name": "Qian Xiao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f336",
          "name": "Rui Min",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f337",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f338",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f339",
          "name": "Shaowei Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33a",
          "name": "Shen Huang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33b",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33c",
          "name": "Shihao Cai",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33d",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33e",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33f",
          "name": "Xin Guan",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f340",
          "name": "Xinyu Geng",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f341",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f342",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f343",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f344",
          "name": "Zijian Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f345",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:53:02.000Z",
      "submittedOnDailyAt": "2025-10-29T00:18:14.015Z",
      "title": "Tongyi DeepResearch Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
      "upvotes": 41,
      "discussionId": "69017244646208eac0d1f346",
      "projectPage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
      "ai_summary": "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.",
      "ai_keywords": [
        "agentic large language model",
        "end-to-end training framework",
        "agentic mid-training",
        "agentic post-training",
        "scalable reasoning",
        "information seeking",
        "data synthesis pipeline",
        "customized environments",
        "Humanity's Last Exam",
        "BrowseComp",
        "BrowseComp-ZH",
        "WebWalkerQA",
        "xbench-DeepSearch",
        "FRAMES",
        "xbench-DeepSearch-2510"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:53:02.000Z",
    "title": "Tongyi DeepResearch Technical Report",
    "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24701.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24699",
      "authors": [
        {
          "_id": "69017ce1646208eac0d1f3d6",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d7",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d8",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d9",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3da",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3db",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3dc",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3dd",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3de",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3df",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e0",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e2",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e3",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e4",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:50.000Z",
      "submittedOnDailyAt": "2025-10-29T01:03:50.549Z",
      "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
      "upvotes": 38,
      "discussionId": "69017ce2646208eac0d1f3e5",
      "ai_summary": "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.",
      "ai_keywords": [
        "LLM-based web agents",
        "ReAct-based agents",
        "context saturation",
        "context management",
        "cognitive workspace",
        "folding operation",
        "granular condensations",
        "deep consolidations",
        "BrowseComp",
        "BrowseComp-ZH",
        "DeepSeek-V3.1-671B-A37B",
        "OpenAI's o4-mini"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:50.000Z",
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24699.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23691",
      "authors": [
        {
          "_id": "69018071646208eac0d1f445",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f446",
          "name": "Xujing Li",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f447",
          "name": "Yining Ye",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f448",
          "name": "Junjie Fang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f449",
          "name": "Haoming Wang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44a",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44b",
          "name": "Shihao Liang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44c",
          "name": "Junting Lu",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44d",
          "name": "Zhiyong Wu",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44e",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f44f",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f450",
          "name": "Zili Li",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f451",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f452",
          "name": "Yu Miao",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f453",
          "name": "Bo Zhou",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f454",
          "name": "Yuanfan Li",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f455",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f456",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f457",
          "name": "Faming Wu",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f458",
          "name": "Zhengxuan Jiang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f459",
          "name": "Weihao Tan",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45a",
          "name": "Heyuan Yao",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45b",
          "name": "Shi Yan",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45c",
          "name": "Xiangyang Li",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45d",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45e",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "69018071646208eac0d1f45f",
          "name": "Guang Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:43:51.000Z",
      "submittedOnDailyAt": "2025-10-29T01:18:34.575Z",
      "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
      "upvotes": 33,
      "discussionId": "69018071646208eac0d1f460",
      "projectPage": "https://seed-tars.com/game-tars",
      "ai_summary": "Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies.",
      "ai_keywords": [
        "unified action space",
        "human-aligned native keyboard-mouse inputs",
        "decaying continual loss",
        "Sparse-Thinking strategy",
        "causal confusion",
        "open-world Minecraft tasks",
        "unseen web 3d games",
        "FPS benchmarks",
        "cross-game",
        "multimodal data",
        "generalist agents",
        "computer-use abilities"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2025-10-27T13:43:51.000Z",
    "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
    "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23691.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23763",
      "authors": [
        {
          "_id": "690168ee646208eac0d1f2fb",
          "name": "Siyin Wang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fc",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fd",
          "name": "Feihong Liu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fe",
          "name": "Xinzhe He",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2ff",
          "name": "Huangxuan Wu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f300",
          "name": "Junhao Shi",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f301",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f302",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f303",
          "name": "Jingjing Gong",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f304",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f305",
          "name": "Yugang Jiang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f306",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f307",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f308",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T18:49:03.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:28.984Z",
      "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang (SII)",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
      "upvotes": 28,
      "discussionId": "690168ee646208eac0d1f309",
      "projectPage": "https://OpenMOSS.github.io/RoboOmni",
      "githubRepo": "https://github.com/OpenMOSS/RoboOmni",
      "ai_summary": "RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Vision-Language-Action models",
        "cross-modal contextual instructions",
        "Perceiver-Thinker-Talker-Executor",
        "end-to-end omni-modal LLMs",
        "intention recognition",
        "interaction confirmation",
        "action execution",
        "spatiotemporal fusion",
        "OmniAction",
        "proactive intention recognition",
        "text-based baselines",
        "ASR-based baselines"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "fnlp",
        "fullname": "OpenMOSS (SII, Fudan NLP)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
      }
    },
    "publishedAt": "2025-10-27T14:49:03.000Z",
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang (SII)",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "fnlp",
      "fullname": "OpenMOSS (SII, Fudan NLP)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24717",
      "authors": [
        {
          "_id": "69019a46646208eac0d1f4fa",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f4fb",
          "name": "Ting Pan",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f4fc",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f4fd",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f4fe",
          "name": "Zhuoyan Luo",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f4ff",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f500",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f501",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f502",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f503",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "69019a46646208eac0d1f504",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-29T03:10:28.004Z",
      "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
      "submittedOnDailyBy": {
        "_id": "64dd9404f2c8f66d52604fb3",
        "avatarUrl": "/avatars/1d7fec043e0848a145e4dcaf8973a575.svg",
        "isPro": false,
        "fullname": "Haoge Deng",
        "user": "Bitterdhg",
        "type": "user"
      },
      "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
      "upvotes": 25,
      "discussionId": "69019a46646208eac0d1f505",
      "projectPage": "https://bitterdhg.github.io/URSA_page",
      "githubRepo": "https://github.com/baaivision/URSA",
      "ai_summary": "URSA, a discrete generative model, bridges the gap with continuous approaches in video generation by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving high-resolution and long-duration synthesis with fewer inference steps.",
      "ai_keywords": [
        "discrete generative modeling",
        "Uniform discRete diffuSion with metric pAth",
        "iterative global refinement",
        "discrete spatiotemporal tokens",
        "Linearized Metric Path",
        "Resolution-dependent Timestep Shifting",
        "asynchronous temporal fine-tuning",
        "interpolation",
        "image-to-video generation"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "61be9739d2f9358e24ca0a4f",
        "name": "BAAI",
        "fullname": "Beijing Academy of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
      }
    },
    "publishedAt": "2025-10-28T13:59:57.000Z",
    "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
    "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24717.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dd9404f2c8f66d52604fb3",
      "avatarUrl": "/avatars/1d7fec043e0848a145e4dcaf8973a575.svg",
      "fullname": "Haoge Deng",
      "name": "Bitterdhg",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "61be9739d2f9358e24ca0a4f",
      "name": "BAAI",
      "fullname": "Beijing Academy of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24657",
      "authors": [
        {
          "_id": "69018a99646208eac0d1f49d",
          "name": "Xuanpu Zhang",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f49e",
          "name": "Xuesong Niu",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f49f",
          "name": "Ruidong Chen",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a0",
          "name": "Dan Song",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a1",
          "name": "Jianhao Zeng",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a2",
          "name": "Penghui Du",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a3",
          "name": "Haoxiang Cao",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a4",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "69018a99646208eac0d1f4a5",
          "name": "An-an Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:22:44.000Z",
      "submittedOnDailyAt": "2025-10-29T02:06:38.798Z",
      "title": "Group Relative Attention Guidance for Image Editing",
      "submittedOnDailyBy": {
        "_id": "68e741ea3edb0ff47e20084e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
        "isPro": false,
        "fullname": "Wu Kai",
        "user": "KaiiWuu1993",
        "type": "user"
      },
      "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
      "upvotes": 15,
      "discussionId": "69018a99646208eac0d1f4a6",
      "projectPage": "https://little-misfit.github.io/GRAG-Image-Editing/",
      "githubRepo": "https://github.com/little-misfit/GRAG-Image-Editing",
      "ai_summary": "Group Relative Attention Guidance enhances image editing quality by modulating token deltas in Diffusion-in-Transformer models, providing fine-grained control over editing intensity.",
      "ai_keywords": [
        "Diffusion-in-Transformer",
        "MM-Attention",
        "Query tokens",
        "Key tokens",
        "bias vector",
        "Group Relative Attention Guidance",
        "Classifier-Free Guidance"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "665f02ce9f9e5b38d0a256a8",
        "name": "Kwai-Kolors",
        "fullname": "Kolors Team, Kuaishou Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
      }
    },
    "publishedAt": "2025-10-28T13:22:44.000Z",
    "title": "Group Relative Attention Guidance for Image Editing",
    "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68e741ea3edb0ff47e20084e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
      "fullname": "Wu Kai",
      "name": "KaiiWuu1993",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "665f02ce9f9e5b38d0a256a8",
      "name": "Kwai-Kolors",
      "fullname": "Kolors Team, Kuaishou Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24563",
      "authors": [
        {
          "_id": "690172eb646208eac0d1f357",
          "name": "Hongrui Jia",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f358",
          "name": "Jitong Liao",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f359",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35a",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35b",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35c",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35d",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35e",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35f",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f360",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:56:36.000Z",
      "submittedOnDailyAt": "2025-10-29T00:20:59.791Z",
      "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
      "upvotes": 15,
      "discussionId": "690172eb646208eac0d1f361",
      "projectPage": "https://osworld-mcp.github.io/",
      "ai_summary": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.",
      "ai_keywords": [
        "multimodal agents",
        "Model Context Protocol (MCP)",
        "OSWorld-MCP",
        "automated code-generation pipeline",
        "GUI operation",
        "task success rates",
        "tool invocation rates"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T11:56:36.000Z",
    "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
    "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24711",
      "authors": [
        {
          "_id": "6901814d646208eac0d1f468",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f469",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46a",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46b",
          "name": "Yujin Han",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46c",
          "name": "Zhekai Chen",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46d",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46e",
          "name": "Difan Zou",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f46f",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f470",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f471",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "6901814d646208eac0d1f472",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:59:02.000Z",
      "submittedOnDailyAt": "2025-10-29T01:21:59.327Z",
      "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
      "submittedOnDailyBy": {
        "_id": "637f70d6fab5db9101c3dfc8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
        "isPro": false,
        "fullname": "Yujie Wei",
        "user": "weilllllls",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
      "upvotes": 14,
      "discussionId": "6901814e646208eac0d1f473",
      "ai_summary": "ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoE",
        "Diffusion Transformers",
        "DiTs",
        "ProMoE",
        "two-step router",
        "conditional routing",
        "prototypical routing",
        "learnable prototypes",
        "latent space",
        "routing contrastive loss",
        "intra-expert coherence",
        "inter-expert diversity",
        "Rectified Flow",
        "DDPM"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:59:02.000Z",
    "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
    "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f70d6fab5db9101c3dfc8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
      "fullname": "Yujie Wei",
      "name": "weilllllls",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24697",
      "authors": [
        {
          "_id": "69017d55646208eac0d1f400",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f401",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f402",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f403",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f404",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f405",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f406",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f407",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f408",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f409",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40a",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40b",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40c",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40d",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:42.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:29.846Z",
      "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
      "upvotes": 14,
      "discussionId": "69017d55646208eac0d1f40e",
      "ai_summary": "WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "information seeking (IS)",
        "search efficiency",
        "target entities",
        "tree-structured reasoning",
        "WebLeaper",
        "Basic",
        "Union",
        "Reverse-Union",
        "training trajectories",
        "BrowserComp",
        "GAIA",
        "xbench-DeepSearch",
        "WideSearch",
        "Seal-0"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:42.000Z",
    "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
    "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24697.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24694",
      "authors": [
        {
          "_id": "69017e04646208eac0d1f42e",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f42f",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f430",
          "name": "Xixi Wu",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f431",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f432",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f433",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f434",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f435",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f436",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f437",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f438",
          "name": "Kewei Tu",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f439",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f43a",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f43b",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:50:40.000Z",
      "submittedOnDailyAt": "2025-10-29T01:08:19.441Z",
      "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
      "upvotes": 14,
      "discussionId": "69017e04646208eac0d1f43c",
      "ai_summary": "Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "Entity-aware Group Relative Policy Optimization (E-GRPO)",
        "entity-centric synthetic data",
        "entity match rate",
        "near-misses",
        "question-answering (QA)",
        "deep research benchmarks",
        "tool calls"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:50:40.000Z",
    "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
    "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24694.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24693",
      "authors": [
        {
          "_id": "69018af4646208eac0d1f4a8",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4a9",
          "name": "Zhikang Niu",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4aa",
          "name": "Qiuyang Xiao",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4ab",
          "name": "Zhisheng Zheng",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4ac",
          "name": "Ruoqi Yuan",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4ad",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4ae",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4af",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4b0",
          "name": "Jianze Liang",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4b1",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4b2",
          "name": "Leilei Sun",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4b3",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "69018af4646208eac0d1f4b4",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:50:34.000Z",
      "submittedOnDailyAt": "2025-10-29T02:16:52.438Z",
      "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
      "submittedOnDailyBy": {
        "_id": "6454d7cb9d37c3fb33266453",
        "avatarUrl": "/avatars/4942450f4c1017ea6312d23bac48d51a.svg",
        "isPro": false,
        "fullname": "Zihan Liu",
        "user": "LiuZH-19",
        "type": "user"
      },
      "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
      "upvotes": 14,
      "discussionId": "69018af5646208eac0d1f4b5",
      "projectPage": "https://internlm.github.io/StarBench/",
      "githubRepo": "https://github.com/InternLM/StarBench",
      "ai_summary": "STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.",
      "ai_keywords": [
        "Multi-modal Large Language Models",
        "Large Audio-Language Models",
        "audio benchmarks",
        "sound dynamics",
        "3D space",
        "STAR-Bench",
        "Foundational Acoustic Perception",
        "Holistic Spatio-Temporal Reasoning",
        "segment reordering",
        "static localization",
        "multi-source relations",
        "dynamic trajectories",
        "procedurally synthesized",
        "physics-simulated audio",
        "human annotation",
        "human performance",
        "closed-source models",
        "open-source models",
        "fine-grained perception",
        "knowledge",
        "reasoning"
      ],
      "githubStars": 14,
      "organization": {
        "_id": "64a2d5fa81252883206f24c9",
        "name": "internlm",
        "fullname": "Intern Large Models",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
      }
    },
    "publishedAt": "2025-10-28T13:50:34.000Z",
    "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
    "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d7cb9d37c3fb33266453",
      "avatarUrl": "/avatars/4942450f4c1017ea6312d23bac48d51a.svg",
      "fullname": "Zihan Liu",
      "name": "LiuZH-19",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "64a2d5fa81252883206f24c9",
      "name": "internlm",
      "fullname": "Intern Large Models",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24695",
      "authors": [
        {
          "_id": "69017da8646208eac0d1f419",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41a",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41b",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41c",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41d",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41e",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41f",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f420",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f421",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f422",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:50:47.000Z",
      "submittedOnDailyAt": "2025-10-29T01:07:28.731Z",
      "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
      "upvotes": 13,
      "discussionId": "69017da8646208eac0d1f423",
      "ai_summary": "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.",
      "ai_keywords": [
        "Zone of Proximal Development (ZPD)",
        "AgentFrontier Engine",
        "high-quality",
        "multidisciplinary data",
        "continued pre-training",
        "targeted post-training",
        "ZPD Exam",
        "dynamic benchmark",
        "complex reasoning tasks",
        "AgentFrontier-30B-A3B model",
        "Humanity's Last Exam"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:50:47.000Z",
    "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
    "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24695.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24514",
      "authors": [
        {
          "_id": "690173b7646208eac0d1f36c",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36d",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36e",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36f",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f370",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f371",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f372",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f373",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f374",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f375",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f376",
          "name": "Tieniu Tan",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f377",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:26:20.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:14.260Z",
      "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
      "submittedOnDailyBy": {
        "_id": "65e816bbcfd12cd15b052a0e",
        "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
        "isPro": false,
        "fullname": "Huanyu_Zhang",
        "user": "huanyu112",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
      "upvotes": 13,
      "discussionId": "690173b8646208eac0d1f378",
      "projectPage": "https://latent-sketchpad.github.io/",
      "githubRepo": "https://github.com/hwanyu112/Latent-Sketchpad",
      "ai_summary": "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Latent Sketchpad",
        "visual planning",
        "imagination",
        "sketching",
        "visual thinking",
        "internal visual scratchpad",
        "perceptual understanding",
        "generative visual thought",
        "autoregressive reasoning",
        "visual latents",
        "sketch images",
        "Context-Aware Vision Head",
        "Sketch Decoder",
        "MazePlanning",
        "Gemma3",
        "Qwen2.5-VL",
        "human-computer interaction"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-28T11:26:20.000Z",
    "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24514.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e816bbcfd12cd15b052a0e",
      "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
      "fullname": "Huanyu_Zhang",
      "name": "huanyu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24320",
      "authors": [
        {
          "_id": "69017878646208eac0d1f3aa",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ab",
          "name": "Jixuan Huang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ac",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ad",
          "name": "Boyang Hong",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ae",
          "name": "Dingwen Yang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3af",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b0",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b1",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b2",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b3",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b4",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b5",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b6",
          "name": "Yufei Xu",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b7",
          "name": "Jiecao Chen",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b8",
          "name": "Rui Zheng",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b9",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ba",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3bb",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T11:37:01.000Z",
      "submittedOnDailyAt": "2025-10-29T00:47:25.750Z",
      "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "653a6e5cae155b92bae77b74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
        "isPro": false,
        "fullname": "Zhiheng Xi",
        "user": "WooooDyy",
        "type": "user"
      },
      "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
      "upvotes": 13,
      "discussionId": "69017878646208eac0d1f3bc",
      "githubRepo": "https://github.com/WooooDyy/Critique-RL",
      "ai_summary": "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "actor",
        "critic",
        "indirect reward signals",
        "direct rule-based reward signals",
        "discriminability",
        "helpfulness",
        "Qwen2.5-7B"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "6447a7db3e7b3c11be6205f1",
        "name": "FudanNLP",
        "fullname": "Fudan NLP Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
      }
    },
    "publishedAt": "2025-10-28T07:37:01.000Z",
    "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
    "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24320.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "653a6e5cae155b92bae77b74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
      "fullname": "Zhiheng Xi",
      "name": "WooooDyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6447a7db3e7b3c11be6205f1",
      "name": "FudanNLP",
      "fullname": "Fudan NLP Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23642",
      "authors": [
        {
          "_id": "6901768a646208eac0d1f37d",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f37e",
          "name": "Songcheng Cai",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f37f",
          "name": "Xiangchao Chen",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f380",
          "name": "Jiarong Liang",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f381",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f382",
          "name": "Jiaqi Deng",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f383",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f384",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f385",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f386",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6901768a646208eac0d1f387",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-24T18:03:57.000Z",
      "submittedOnDailyAt": "2025-10-29T01:48:21.691Z",
      "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.",
      "upvotes": 13,
      "discussionId": "6901768a646208eac0d1f388",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder2/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder2",
      "ai_summary": "VisCoder2, a family of multi-language visualization models, outperforms open-source baselines and approaches proprietary models by leveraging VisCode-Multi-679K and VisPlotBench for iterative self-debugging and multi-turn correction.",
      "ai_keywords": [
        "large language models",
        "coding agents",
        "visualization code",
        "language coverage",
        "execution reliability",
        "iterative correction mechanisms",
        "VisCode-Multi-679K",
        "VisPlotBench",
        "multi-language visualization models",
        "VisCoder2",
        "execution pass rate",
        "symbolic languages",
        "compiler-dependent languages"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6313a90017838d05194fd282",
        "name": "TIGER-Lab",
        "fullname": "TIGER-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
      }
    },
    "publishedAt": "2025-10-24T14:03:57.000Z",
    "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
    "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "organization": {
      "_id": "6313a90017838d05194fd282",
      "name": "TIGER-Lab",
      "fullname": "TIGER-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24698",
      "authors": [
        {
          "_id": "69017d30646208eac0d1f3e7",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3e8",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3e9",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ea",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3eb",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ec",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ed",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ee",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ef",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f0",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f1",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f2",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:50.000Z",
      "submittedOnDailyAt": "2025-10-29T01:04:44.983Z",
      "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
      "upvotes": 12,
      "discussionId": "69017d31646208eac0d1f3f3",
      "ai_summary": "ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.",
      "ai_keywords": [
        "ParallelMuse",
        "Functionality-Specified Partial Rollout",
        "uncertainty-guided path reuse",
        "branch",
        "Compressed Reasoning Aggregation",
        "reasoning redundancy",
        "information compression",
        "exploratory token consumption"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:50.000Z",
    "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
    "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24698.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22037",
      "authors": [
        {
          "_id": "69018d27646208eac0d1f4d1",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d2",
          "name": "Sneha Kudugunta",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d3",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d4",
          "name": "I-Hung Hsu",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d5",
          "name": "Isaac Caswell",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d6",
          "name": "Alex Pentland",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d7",
          "name": "Sercan Arik",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d8",
          "name": "Chen-Yu Lee",
          "hidden": false
        },
        {
          "_id": "69018d27646208eac0d1f4d9",
          "name": "Sayna Ebrahimi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-24T21:45:22.000Z",
      "submittedOnDailyAt": "2025-10-29T02:16:01.482Z",
      "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality",
      "submittedOnDailyBy": {
        "_id": "64dc08f9e7bc8544f9b1ac32",
        "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
        "isPro": false,
        "fullname": "I-Hung Hsu",
        "user": "alexhsu",
        "type": "user"
      },
      "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.",
      "upvotes": 7,
      "discussionId": "69018d27646208eac0d1f4da",
      "ai_summary": "The study introduces ATLAS, a multilingual scaling law that improves out-of-sample generalization and provides insights into cross-lingual transfer, optimal scaling, and computational crossover points for model training.",
      "ai_keywords": [
        "Adaptive Transfer Scaling Law",
        "ATLAS",
        "multilingual scaling laws",
        "cross-lingual transfer matrix",
        "language-agnostic scaling law",
        "computational crossover points",
        "pretraining",
        "finetuning",
        "multilingual checkpoints"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-24T17:45:22.000Z",
    "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality",
    "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22037.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc08f9e7bc8544f9b1ac32",
      "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
      "fullname": "I-Hung Hsu",
      "name": "alexhsu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24645",
      "authors": [
        {
          "_id": "6901786f646208eac0d1f399",
          "name": "Zengzhuang Xu",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39a",
          "name": "Bingguang Hao",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39b",
          "name": "Zechuan Wang",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39c",
          "name": "Yuntao Wen",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39d",
          "name": "Maolin Wang",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f39f",
          "name": "Long Chen",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a0",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a1",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a2",
          "name": "Cunyin Peng",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a3",
          "name": "Chenyi Zhuang",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a4",
          "name": "Jinjie Gu",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a5",
          "name": "Leilei Gan",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a6",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "6901786f646208eac0d1f3a7",
          "name": "Shi Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:15:26.000Z",
      "submittedOnDailyAt": "2025-10-29T01:15:52.315Z",
      "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
      "submittedOnDailyBy": {
        "_id": "6550d7f0aae66b1676131620",
        "avatarUrl": "/avatars/0a55b7d52fc1e1655671fe5cf8a86262.svg",
        "isPro": false,
        "fullname": "Bingguang Hao",
        "user": "Bingguang",
        "type": "user"
      },
      "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
      "upvotes": 3,
      "discussionId": "6901786f646208eac0d1f3a8",
      "projectPage": "https://huggingface.co/datasets/Bingguang/FunReason-MT",
      "ai_summary": "FunReason-MT is a novel data synthesis framework that enhances multi-turn function calling in large language models by addressing challenges in environment interaction, query synthesis, and chain-of-thought generation, achieving state-of-the-art performance on the Berkeley Function-Calling Leaderboard.",
      "ai_keywords": [
        "Function calling",
        "large language models",
        "autonomous agents",
        "data synthesis",
        "Environment-API Graph Interactions",
        "Advanced Tool-Query Synthesis",
        "Guided Iterative Chain",
        "chain-of-thought generation",
        "Berkeley Function-Calling Leaderboard"
      ],
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2025-10-28T13:15:26.000Z",
    "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
    "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6550d7f0aae66b1676131620",
      "avatarUrl": "/avatars/0a55b7d52fc1e1655671fe5cf8a86262.svg",
      "fullname": "Bingguang Hao",
      "name": "Bingguang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24591",
      "authors": [
        {
          "_id": "690172c7646208eac0d1f348",
          "name": "Christine Ye",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f349",
          "name": "Sihan Yuan",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34a",
          "name": "Suchetha Cooray",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34b",
          "name": "Steven Dillmann",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34c",
          "name": "Ian L. V. Roque",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34d",
          "name": "Dalya Baron",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34e",
          "name": "Philipp Frank",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34f",
          "name": "Sergio Martin-Alvarez",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f350",
          "name": "Nolan Koblischke",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f351",
          "name": "Frank J Qu",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f352",
          "name": "Diyi Yang",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f353",
          "name": "Risa Wechsler",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f354",
          "name": "Ioana Ciuca",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T16:21:19.000Z",
      "submittedOnDailyAt": "2025-10-29T00:20:08.707Z",
      "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
      "upvotes": 3,
      "discussionId": "690172c8646208eac0d1f355",
      "ai_summary": "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.",
      "ai_keywords": [
        "ReplicationBench",
        "AI agents",
        "scientific research assistants",
        "faithfulness",
        "correctness",
        "research papers",
        "astrophysics",
        "experimental setup",
        "derivations",
        "data analysis",
        "codebase",
        "language models",
        "benchmark",
        "expert-validated",
        "data-driven science",
        "reliability"
      ]
    },
    "publishedAt": "2025-10-28T12:21:19.000Z",
    "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
    "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22099",
      "authors": [
        {
          "_id": "6901760b646208eac0d1f37a",
          "name": "Xuanming Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-25T00:50:47.000Z",
      "submittedOnDailyAt": "2025-10-29T00:35:45.044Z",
      "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
      "submittedOnDailyBy": {
        "_id": "65fc5109899083a2aad987c5",
        "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
        "isPro": false,
        "fullname": "XUANMING ZHANG",
        "user": "XUANMINGZHANG",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
      "upvotes": 2,
      "discussionId": "6901760c646208eac0d1f37b",
      "ai_summary": "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.",
      "ai_keywords": [
        "Information Bottleneck",
        "Dynamic Mode Steering",
        "linear probe",
        "dynamic activation steering",
        "adaptive",
        "self-contrastive decoding",
        "generalization",
        "memorization",
        "logical consistency",
        "factual accuracy"
      ],
      "organization": {
        "_id": "6112d84f8c2e1f4060908c9e",
        "name": "stanfordnlp",
        "fullname": "Stanford NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
      }
    },
    "publishedAt": "2025-10-24T20:50:47.000Z",
    "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
    "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc5109899083a2aad987c5",
      "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
      "fullname": "XUANMING ZHANG",
      "name": "XUANMINGZHANG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6112d84f8c2e1f4060908c9e",
      "name": "stanfordnlp",
      "fullname": "Stanford NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24448",
      "authors": [
        {
          "_id": "6901734b646208eac0d1f363",
          "name": "Pablo Acuaviva",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f364",
          "name": "Aram Davtyan",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f365",
          "name": "Mariam Hassan",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f366",
          "name": "Sebastian Stapf",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f367",
          "name": "Ahmad Rahimi",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f368",
          "name": "Alexandre Alahi",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f369",
          "name": "Paolo Favaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T14:12:11.000Z",
      "submittedOnDailyAt": "2025-10-29T00:50:57.135Z",
      "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
      "upvotes": 1,
      "discussionId": "6901734b646208eac0d1f36a",
      "ai_summary": "Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.",
      "ai_keywords": [
        "Video Diffusion Models",
        "VDMs",
        "large language models",
        "LLMs",
        "spatiotemporal data",
        "inductive biases",
        "ARC-AGI",
        "ConceptARC",
        "visual games",
        "route planning",
        "cellular automata",
        "visual foundation models"
      ]
    },
    "publishedAt": "2025-10-28T10:12:11.000Z",
    "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
    "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.21323",
      "authors": [
        {
          "_id": "6900561a22d452aac6dd4439",
          "user": {
            "_id": "660229531737e5cd4a6e7948",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
            "isPro": false,
            "fullname": "Shufan Shen",
            "user": "shufanshen",
            "type": "user"
          },
          "name": "Shufan Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-28T15:35:14.639Z",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443a",
          "name": "Junshu Sun",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443b",
          "name": "Qingming Huang",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443c",
          "name": "Shuhui Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-24T10:29:31.000Z",
      "submittedOnDailyAt": "2025-10-29T00:24:20.855Z",
      "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
      "submittedOnDailyBy": {
        "_id": "660229531737e5cd4a6e7948",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
        "isPro": false,
        "fullname": "Shufan Shen",
        "user": "shufanshen",
        "type": "user"
      },
      "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
      "upvotes": 1,
      "discussionId": "6900561b22d452aac6dd443d",
      "githubRepo": "https://github.com/ssfgunner/VL-SAE",
      "ai_summary": "VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.",
      "ai_keywords": [
        "sparse autoencoder",
        "vision-language representations",
        "multi-modal reasoning",
        "neuron-concept correlation",
        "cosine similarity",
        "distance-based encoder",
        "modality-specific decoders",
        "zero-shot image classification",
        "hallucination elimination"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "632fea4a9c9aa2bfdf5982f8",
        "name": "UCAS",
        "fullname": "ucas",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
      }
    },
    "publishedAt": "2025-10-24T06:29:31.000Z",
    "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
    "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660229531737e5cd4a6e7948",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
      "fullname": "Shufan Shen",
      "name": "shufanshen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "632fea4a9c9aa2bfdf5982f8",
      "name": "UCAS",
      "fullname": "ucas",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.20155",
      "authors": [
        {
          "_id": "6901b7bd646208eac0d1f567",
          "name": "Penghao Wang",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f568",
          "name": "Yiyang He",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f569",
          "name": "Xin Lv",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f56a",
          "name": "Yukai Zhou",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f56b",
          "name": "Lan Xu",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f56c",
          "name": "Jingyi Yu",
          "hidden": false
        },
        {
          "_id": "6901b7bd646208eac0d1f56d",
          "name": "Jiayuan Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T03:06:08.000Z",
      "submittedOnDailyAt": "2025-10-29T05:18:53.009Z",
      "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding",
      "submittedOnDailyBy": {
        "_id": "6462120032083c28b408e754",
        "avatarUrl": "/avatars/f02c6d23f03420b716d995c92f8e2213.svg",
        "isPro": false,
        "fullname": "Penghao Wang",
        "user": "AuWang",
        "type": "user"
      },
      "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.",
      "upvotes": 1,
      "discussionId": "6901b7bd646208eac0d1f56e",
      "projectPage": "https://authoritywang.github.io/partnext/",
      "githubRepo": "https://github.com/AuthorityWang/PartNeXt",
      "ai_summary": "PartNeXt, a high-quality, textured 3D dataset with fine-grained part labels, improves performance in class-agnostic part segmentation and 3D part-centric question answering, highlighting gaps in open-vocabulary part grounding.",
      "ai_keywords": [
        "PartNeXt",
        "class-agnostic part segmentation",
        "PartField",
        "SAMPart3D",
        "3D part-centric question answering",
        "3D-LLMs",
        "open-vocabulary part grounding",
        "Point-SAM",
        "structured 3D understanding"
      ],
      "githubStars": 66
    },
    "publishedAt": "2025-10-22T23:06:08.000Z",
    "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding",
    "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462120032083c28b408e754",
      "avatarUrl": "/avatars/f02c6d23f03420b716d995c92f8e2213.svg",
      "fullname": "Penghao Wang",
      "name": "AuWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]