[
  {
    "paper": {
      "id": "2501.12380",
      "authors": [
        {
          "_id": "67906f432565fc5140d72dc3",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc4",
          "name": "Lujing Xie",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc5",
          "name": "Haowei Zhang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc6",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc7",
          "name": "Yitao Long",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc8",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc9",
          "name": "Tongyan Hu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dca",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcb",
          "name": "Chuhan Li",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcc",
          "name": "Junyang Song",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcd",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dce",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcf",
          "name": "Weifeng Pan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd0",
          "name": "Ziyao Shangguan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd1",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd2",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd3",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd4",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd5",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T18:56:18.000Z",
      "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
      "summary": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\nfor evaluating foundation models in video understanding. MMVU includes 3,000\nexpert-annotated questions spanning 27 subjects across four core disciplines:\nScience, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\nprior benchmarks, MMVU features three key advancements. First, it challenges\nmodels to apply domain-specific knowledge and perform expert-level reasoning to\nanalyze specialized-domain videos, moving beyond the basic visual perception\ntypically assessed in current video benchmarks. Second, each example is\nannotated by human experts from scratch. We implement strict data quality\ncontrols to ensure the high quality of the dataset. Finally, each example is\nenriched with expert-annotated reasoning rationals and relevant domain\nknowledge, facilitating in-depth analysis. We conduct an extensive evaluation\nof 32 frontier multimodal foundation models on MMVU. The latest\nSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest\nperformance among the tested models. However, they still fall short of matching\nhuman expertise. Through in-depth error analyses and case studies, we offer\nactionable insights for future advancements in expert-level,\nknowledge-intensive video understanding for specialized domains.",
      "upvotes": 23,
      "discussionId": "67906f442565fc5140d72e4a"
    },
    "publishedAt": "2025-01-21T23:19:52.256Z",
    "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11425",
      "authors": [
        {
          "_id": "679080298ad1d8203a994f7f",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f80",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f81",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f82",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f83",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f84",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T11:46:04.000Z",
      "title": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
      "summary": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
      "upvotes": 15,
      "discussionId": "6790802b8ad1d8203a994fc7"
    },
    "publishedAt": "2025-01-22T00:20:57.292Z",
    "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12273",
      "authors": [
        {
          "_id": "67906c674932687e24e0cc08",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc09",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0a",
          "name": "Mo Li",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0b",
          "name": "Chuyu Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0c",
          "name": "Yunxin Liu",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0d",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0e",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0f",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T16:44:12.000Z",
      "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
      "summary": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.",
      "upvotes": 7,
      "discussionId": "67906c684932687e24e0cc61"
    },
    "publishedAt": "2025-01-21T22:56:36.701Z",
    "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12273.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11733",
      "authors": [
        {
          "_id": "6790791b203b95acf96ebf45",
          "user": {
            "_id": "628d7265db4cd1d1717c884f",
            "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
            "isPro": false,
            "fullname": "Zhenhailong Wang",
            "user": "mikewang",
            "type": "user"
          },
          "name": "Zhenhailong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-22T04:50:40.468Z",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf46",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf47",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf48",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf49",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4a",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4b",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4c",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T20:35:46.000Z",
      "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
      "summary": "Smartphones have become indispensable in modern life, yet navigating complex\ntasks on mobile devices often remains frustrating. Recent advancements in large\nmultimodal model (LMM)-based mobile agents have demonstrated the ability to\nperceive and act in mobile environments. However, current approaches face\nsignificant limitations: they fall short in addressing real-world human needs,\nstruggle with reasoning-intensive and long-horizon tasks, and lack mechanisms\nto learn and improve from prior experiences. To overcome these challenges, we\nintroduce Mobile-Agent-E, a hierarchical multi-agent framework capable of\nself-evolution through past experience. By hierarchical, we mean an explicit\nseparation of high-level planning and low-level action execution. The framework\ncomprises a Manager, responsible for devising overall plans by breaking down\ncomplex tasks into subgoals, and four subordinate agents--Perceptor, Operator,\nAction Reflector, and Notetaker--which handle fine-grained visual perception,\nimmediate action execution, error verification, and information aggregation,\nrespectively. Mobile-Agent-E also features a novel self-evolution module which\nmaintains a persistent long-term memory comprising Tips and Shortcuts. Tips are\ngeneral guidance and lessons learned from prior tasks on how to effectively\ninteract with the environment. Shortcuts are reusable, executable sequences of\natomic operations tailored for specific subroutines. The inclusion of Tips and\nShortcuts facilitates continuous refinement in performance and efficiency.\nAlongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring\ncomplex mobile tasks requiring long-horizon, multi-app interactions. Empirical\nresults show that Mobile-Agent-E achieves a 22% absolute improvement over\nprevious state-of-the-art approaches across three foundation model backbones.\nProject page: https://x-plug.github.io/MobileAgent.",
      "upvotes": 5,
      "discussionId": "67907920203b95acf96ec126"
    },
    "publishedAt": "2025-01-22T00:17:48.799Z",
    "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12326",
      "authors": [
        {
          "_id": "679078f902b4d94b0f2347c1",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c2",
          "name": "Yining Ye",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c3",
          "name": "Junjie Fang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c4",
          "name": "Haoming Wang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c5",
          "name": "Shihao Liang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c6",
          "name": "Shizuo Tian",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c7",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c8",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c9",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347ca",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cb",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cc",
          "name": "Kuanye Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cd",
          "name": "Jiale Yang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347ce",
          "name": "Yu Miao",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cf",
          "name": "Woyu Lin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d0",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d1",
          "name": "Xu Jiang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d2",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d3",
          "name": "Jingyu Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d4",
          "name": "Xiaojun Xiao",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d5",
          "name": "Kai Cai",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d6",
          "name": "Chuang Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d7",
          "name": "Yaowei Zheng",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d8",
          "name": "Chaolin Jin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d9",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347da",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347db",
          "name": "Minchao Wang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347dc",
          "name": "Haoli Chen",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347dd",
          "name": "Zhaojian Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347de",
          "name": "Haihua Yang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347df",
          "name": "Haifeng Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e0",
          "name": "Feng Lin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e1",
          "name": "Tao Peng",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e2",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e3",
          "name": "Guang Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T17:48:10.000Z",
      "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
      "summary": "This paper introduces UI-TARS, a native GUI agent model that solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots for context-aware understanding of UI elements and precise\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, involving multiple\nreasoning patterns such as task decomposition, reflection thinking, milestone\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\naddresses the data bottleneck by automatically collecting, filtering, and\nreflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns\nfrom its mistakes and adapts to unforeseen situations with minimal human\nintervention. We also analyze the evolution path of GUI agents to guide the\nfurther development of this domain.",
      "upvotes": 4,
      "discussionId": "679078ff02b4d94b0f2348e0"
    },
    "publishedAt": "2025-01-21T23:51:53.248Z",
    "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.10687",
      "authors": [
        {
          "_id": "6790856e3b0a6384a4117d0e",
          "name": "Linrui Tian",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d0f",
          "name": "Siqi Hu",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d10",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d11",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d12",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-18T07:51:29.000Z",
      "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
      "summary": "In this paper, we propose a novel audio-driven talking head method capable of\nsimultaneously generating highly expressive facial expressions and hand\ngestures. Unlike existing methods that focus on generating full-body or\nhalf-body poses, we investigate the challenges of co-speech gesture generation\nand identify the weak correspondence between audio features and full-body\ngestures as a key limitation. To address this, we redefine the task as a\ntwo-stage process. In the first stage, we generate hand poses directly from\naudio input, leveraging the strong correlation between audio signals and hand\nmovements. In the second stage, we employ a diffusion model to synthesize video\nframes, incorporating the hand poses generated in the first stage to produce\nrealistic facial expressions and body movements. Our experimental results\ndemonstrate that the proposed method outperforms state-of-the-art approaches,\nsuch as CyberHost and Vlogger, in terms of both visual quality and\nsynchronization accuracy. This work provides a new perspective on audio-driven\ngesture generation and a robust framework for creating expressive and natural\ntalking head animations.",
      "upvotes": 3,
      "discussionId": "679085813b0a6384a41183f1"
    },
    "publishedAt": "2025-01-22T00:49:10.316Z",
    "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10687.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65df1f1ee98700500d4c289c",
      "avatarUrl": "/avatars/be11bf61465df29ac997cc0fedad1cb9.svg",
      "fullname": "qi wang",
      "name": "lucaskingjade",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11223",
      "authors": [
        {
          "_id": "6790772b8d7df822f1fb4405",
          "name": "Maciej Besta",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4406",
          "name": "Julia Barth",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4407",
          "name": "Eric Schreiber",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4408",
          "name": "Ales Kubicek",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4409",
          "name": "Afonso Catarino",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440a",
          "name": "Robert Gerstenberger",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440b",
          "name": "Piotr Nyczyk",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440c",
          "name": "Patrick Iff",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440d",
          "name": "Yueling Li",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440e",
          "name": "Sam Houliston",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440f",
          "name": "Tomasz Sternal",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4410",
          "name": "Marcin Copik",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4411",
          "name": "Grzegorz Kwaśniewski",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4412",
          "name": "Jürgen Müller",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4413",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4414",
          "name": "Hannes Eberhard",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4415",
          "name": "Hubert Niewiadomski",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4416",
          "name": "Torsten Hoefler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T02:16:19.000Z",
      "title": "Reasoning Language Models: A Blueprint",
      "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending large language models\n(LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary\nnature, and complex architectures - uniquely combining Reinforcement Learning\n(RL), search heuristics, and LLMs - present accessibility and scalability\nchallenges. To address these, we propose a comprehensive blueprint that\norganizes RLM components into a modular framework, based on a survey and\nanalysis of all RLM works. This blueprint incorporates diverse reasoning\nstructures (chains, trees, graphs, and nested forms), reasoning strategies\n(e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models\nand others), and supervision schemes (Output-Based and Process-Based\nSupervision). We also provide detailed mathematical formulations and\nalgorithmic specifications to simplify RLM implementation. By showing how\nschemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as\nspecial cases, we demonstrate the blueprint's versatility and unifying\npotential. To illustrate its utility, we introduce x1, a modular implementation\nfor rapid RLM prototyping and experimentation. Using x1 and a literature\nreview, we provide key insights, such as multi-phase training for policy and\nvalue models, and the importance of familiar training distributions. Finally,\nwe outline how RLMs can integrate with a broader LLM ecosystem, including tools\nand databases. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.",
      "upvotes": 3,
      "discussionId": "6790772d8d7df822f1fb4493"
    },
    "publishedAt": "2025-01-21T23:42:44.747Z",
    "title": "Reasoning Language Models: A Blueprint",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12390",
      "authors": [
        {
          "_id": "67906d622ae55818ddfd0d93",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d94",
          "name": "Ziyang Chen",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d95",
          "name": "Aleksander Holynski",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d96",
          "name": "Alexei A. Efros",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d97",
          "name": "Andrew Owens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T18:59:46.000Z",
      "title": "GPS as a Control Signal for Image Generation",
      "summary": "We show that the GPS tags contained in photo metadata provide a useful\ncontrol signal for image generation. We train GPS-to-image models and use them\nfor tasks that require a fine-grained understanding of how images vary within a\ncity. In particular, we train a diffusion model to generate images conditioned\non both GPS and text. The learned model generates images that capture the\ndistinctive appearance of different neighborhoods, parks, and landmarks. We\nalso extract 3D models from 2D GPS-to-image models through score distillation\nsampling, using GPS conditioning to constrain the appearance of the\nreconstruction from each viewpoint. Our evaluations suggest that our\nGPS-conditioned models successfully learn to generate images that vary based on\nlocation, and that GPS conditioning improves estimated 3D structure.",
      "upvotes": 3,
      "discussionId": "67906d682ae55818ddfd0f53"
    },
    "publishedAt": "2025-01-21T23:41:48.239Z",
    "title": "GPS as a Control Signal for Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12390.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645ab0b7c266796265baefa4",
      "avatarUrl": "/avatars/bdac661996b63c4b2a56881707afa01f.svg",
      "fullname": "Chao Feng",
      "name": "chfeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12202",
      "authors": [
        {
          "_id": "67908409416b83605450716a",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716b",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716c",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716d",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716e",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716f",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507170",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507171",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507172",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507173",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507174",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507175",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507176",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507177",
          "name": "Yihang Lian",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507178",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507179",
          "name": "Ruining Tang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717a",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717b",
          "name": "Xinzhou Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717c",
          "name": "Jian Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717d",
          "name": "Xuhui Zuo",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717f",
          "name": "Biwen Lei",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507180",
          "name": "Haohan Weng",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507181",
          "name": "Jing Xu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507182",
          "name": "Yiling Zhu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507183",
          "name": "Xinhai Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507184",
          "name": "Lixin Xu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507185",
          "name": "Changrong Hu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507186",
          "name": "Tianyu Huang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507187",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507188",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507189",
          "name": "Meng Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718a",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718b",
          "name": "Yiwen Jia",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718c",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718d",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718e",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718f",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507190",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507191",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507192",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507193",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507194",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507195",
          "name": "Jie Xiao",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507196",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507197",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507198",
          "name": "Jinbao Xue",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507199",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719a",
          "name": "Chongqing Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719b",
          "name": "Xinming Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719c",
          "name": "Zhichao Hu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719d",
          "name": "Lei Qin",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719e",
          "name": "Jianbing Peng",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719f",
          "name": "Zhan Li",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a0",
          "name": "Minghui Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a1",
          "name": "Xipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a2",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a3",
          "name": "Paige Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a4",
          "name": "Yingkai Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a5",
          "name": "Haozhao Kuang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a6",
          "name": "Zhongyi Fan",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a7",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a8",
          "name": "Weihao Zhuang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a9",
          "name": "YingPing He",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071aa",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ab",
          "name": "Yong Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ac",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ad",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ae",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071af",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071b0",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T15:16:54.000Z",
      "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\n  Assets Generation",
      "summary": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\ngenerating high-resolution textured 3D assets. This system includes two\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\ncreate geometry that properly aligns with a given condition image, laying a\nsolid foundation for downstream applications. The texture synthesis model,\nbenefiting from strong geometric and diffusion priors, produces high-resolution\nand vibrant texture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\nplatform that simplifies the re-creation process of 3D assets. It allows both\nprofessional and amateur users to manipulate or even animate their meshes\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\noutperforms previous state-of-the-art models, including the open-source models\nand closed-source models in geometry details, condition alignment, texture\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\nin the open-source 3D community for large-scale foundation generative models.\nThe code and pre-trained weights of our models are available at:\nhttps://github.com/Tencent/Hunyuan3D-2",
      "upvotes": 1,
      "discussionId": "6790840d416b8360545072a7"
    },
    "publishedAt": "2025-01-22T00:37:32.486Z",
    "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.10893",
      "authors": [
        {
          "_id": "67907dd5e1d8fc832b3e7b0f",
          "name": "Hongjin Su",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b10",
          "name": "Ruoxi Sun",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b11",
          "name": "Jinsung Yoon",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b12",
          "name": "Pengcheng Yin",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b13",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b14",
          "name": "Sercan Ö. Arık",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-18T22:34:41.000Z",
      "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in\n  Realistic Environments",
      "summary": "Autonomous agents powered by large language models (LLMs) have the potential\nto enhance human capabilities, assisting with digital tasks from sending emails\nto performing data analysis. The abilities of existing LLMs at such tasks are\noften hindered by the lack of high-quality agent data from the corresponding\nenvironments they interact with. We propose Learn-by-interact, a data-centric\nframework to adapt LLM agents to any given environments without human\nannotations. Learn-by-interact synthesizes trajectories of agent-environment\ninteractions based on documentations, and constructs instructions by\nsummarizing or abstracting the interaction histories, a process called backward\nconstruction. We assess the quality of our synthetic data by using them in both\ntraining-based scenarios and training-free in-context learning (ICL), where we\ncraft innovative retrieval approaches optimized for agents. Extensive\nexperiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across\nrealistic coding, web, and desktop environments show the effectiveness of\nLearn-by-interact in various downstream agentic tasks -- baseline results are\nimproved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with\nCodestral-22B. We further demonstrate the critical role of backward\nconstruction, which provides up to 14.0\\% improvement for training. Our\nablation studies demonstrate the efficiency provided by our synthesized data in\nICL and the superiority of our retrieval pipeline over alternative approaches\nlike conventional retrieval-augmented generation (RAG). We expect that\nLearn-by-interact will serve as a foundation for agent data synthesis as LLMs\nare increasingly deployed at real-world environments.",
      "upvotes": 1,
      "discussionId": "67907dd9e1d8fc832b3e7c36"
    },
    "publishedAt": "2025-01-22T00:11:18.322Z",
    "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10893.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11873",
      "authors": [
        {
          "_id": "679071da11a3f67d8f498649",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864a",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864b",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864c",
          "name": "Kaiyue Wen",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864d",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864e",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864f",
          "name": "Ivan Titov",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498650",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498651",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498652",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T04:04:39.000Z",
      "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models",
      "summary": "This paper revisits the implementation of\nLoad-balancing Loss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E\nsum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i\nrepresents the frequency of expert i being selected, and p_i denotes the\naverage gating score of the expert i. Existing MoE training frameworks\nusually employ the parallel training strategy so that f_i and the LBL are\ncalculated within a micro-batch and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence (e.g., code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a global-batch to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize f_i across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n42.8B total parameters and 400B tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts.",
      "upvotes": 1,
      "discussionId": "679071db11a3f67d8f498680"
    },
    "publishedAt": "2025-01-21T23:27:52.660Z",
    "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/ddUbQV_yVPwD6P0TSR5lu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/f7Q4QULppOygZlsYBUvY9.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/9Jwx37bQkCjaWcccWbJ7b.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647ccbd6e07cf9bb2d485244",
      "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
      "fullname": "Zihan Qiu",
      "name": "QwQZh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.08331",
      "authors": [
        {
          "_id": "679086696d5aed184a333663",
          "name": "Ryan Burgert",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333664",
          "name": "Yuancheng Xu",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333665",
          "name": "Wenqi Xian",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333666",
          "name": "Oliver Pilarski",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333667",
          "name": "Pascal Clausen",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333668",
          "name": "Mingming He",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a333669",
          "name": "Li Ma",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366a",
          "name": "Yitong Deng",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366b",
          "user": {
            "_id": "66cb7dfcc5c61e4baa96469d",
            "avatarUrl": "/avatars/6cab6e06164371778d15e4a1a3278eac.svg",
            "isPro": false,
            "fullname": "Lingxiao Li",
            "user": "lingxiaol",
            "type": "user"
          },
          "name": "Lingxiao Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-22T05:47:27.544Z",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366c",
          "name": "Mohsen Mousavi",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366d",
          "name": "Michael Ryoo",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366e",
          "name": "Paul Debevec",
          "hidden": false
        },
        {
          "_id": "679086696d5aed184a33366f",
          "name": "Ning Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-14T18:59:10.000Z",
      "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise",
      "summary": "Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code\nand model checkpoints are available on GitHub:\nhttps://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.",
      "upvotes": 0,
      "discussionId": "6790866f6d5aed184a333805"
    },
    "publishedAt": "2025-01-22T01:03:06.008Z",
    "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12375",
      "authors": [
        {
          "_id": "679069d37b150be8ddef0657",
          "name": "Sili Chen",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef0658",
          "name": "Hengkai Guo",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef0659",
          "name": "Shengnan Zhu",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef065a",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef065b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef065c",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "679069d37b150be8ddef065d",
          "name": "Bingyi Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T18:53:30.000Z",
      "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
      "summary": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS.",
      "upvotes": 0,
      "discussionId": "679069d57b150be8ddef06ef"
    },
    "publishedAt": "2025-01-22T00:57:27.738Z",
    "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  }
]