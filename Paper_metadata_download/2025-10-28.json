[
  {
    "paper": {
      "id": "2510.23564",
      "authors": [
        {
          "_id": "69003a0022d452aac6dd438f",
          "name": "Zhaoyang Yu",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4390",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4391",
          "name": "Huixue Su",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4392",
          "name": "Yufan Zhao",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4393",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4394",
          "name": "Mingyi Deng",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4395",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4396",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4397",
          "name": "Lingxiao Tang",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4398",
          "name": "Yingchao Li",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd4399",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd439a",
          "name": "Bang Liu",
          "hidden": false
        },
        {
          "_id": "69003a0022d452aac6dd439b",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:35:15.000Z",
      "submittedOnDailyAt": "2025-10-28T02:26:28.802Z",
      "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
      "submittedOnDailyBy": {
        "_id": "640dc84b474aa6f89554d518",
        "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
        "isPro": false,
        "fullname": "Zhaoyang Yu",
        "user": "MoshiQAQ",
        "type": "user"
      },
      "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.",
      "upvotes": 72,
      "discussionId": "69003a0122d452aac6dd439c",
      "githubRepo": "https://github.com/FoundationAgents/ReCode",
      "ai_summary": "ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.",
      "ai_keywords": [
        "Large Language Model",
        "ReCode",
        "Recursive Code Generation",
        "high-level planning",
        "low-level action",
        "decision granularity",
        "abstract placeholder functions",
        "primitive actions",
        "hierarchical decision-making processes",
        "inference performance",
        "data efficiency"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-10-27T13:35:15.000Z",
    "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23564.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640dc84b474aa6f89554d518",
      "avatarUrl": "/avatars/9fcee1023ed5c6cddb3c342e19f18295.svg",
      "fullname": "Zhaoyang Yu",
      "name": "MoshiQAQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23607",
      "authors": [
        {
          "_id": "690034a222d452aac6dd4346",
          "name": "Yujia Zhang",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd4347",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd4348",
          "name": "Yixing Lao",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd4349",
          "name": "Chengyao Wang",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd434a",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd434b",
          "name": "Naiyan Wang",
          "hidden": false
        },
        {
          "_id": "690034a222d452aac6dd434c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:59:59.000Z",
      "submittedOnDailyAt": "2025-10-28T01:46:45.129Z",
      "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
      "submittedOnDailyBy": {
        "_id": "643e5d6a1d0e956d94bb3608",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
        "isPro": false,
        "fullname": "Xiaoyang Wu",
        "user": "Gofinge",
        "type": "user"
      },
      "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
      "upvotes": 60,
      "discussionId": "690034a222d452aac6dd434d",
      "projectPage": "https://pointcept.github.io/Concerto/",
      "githubRepo": "https://github.com/Pointcept/Concerto",
      "ai_summary": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.",
      "ai_keywords": [
        "3D intra-modal self-distillation",
        "2D-3D cross-modal joint embedding",
        "zero-shot visualizations",
        "linear probing",
        "3D scene perception",
        "ScanNet",
        "mIoU",
        "video-lifted point cloud",
        "CLIP's language space",
        "open-world perception",
        "fine-grained geometric and semantic consistency"
      ],
      "githubStars": 61,
      "organization": {
        "_id": "643e5de3c0ed86c416e8eb25",
        "name": "Pointcept",
        "fullname": "Pointcept",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643e5d6a1d0e956d94bb3608/dtbQ44h-xIjwQlHT9Nbbv.png"
      }
    },
    "publishedAt": "2025-10-27T13:59:59.000Z",
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
    "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23607.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643e5d6a1d0e956d94bb3608",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
      "fullname": "Xiaoyang Wu",
      "name": "Gofinge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "643e5de3c0ed86c416e8eb25",
      "name": "Pointcept",
      "fullname": "Pointcept",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643e5d6a1d0e956d94bb3608/dtbQ44h-xIjwQlHT9Nbbv.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23588",
      "authors": [
        {
          "_id": "6900348b22d452aac6dd433b",
          "name": "Guangting Zheng",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd433c",
          "name": "Qinyu Zhao",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd433d",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd433e",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd433f",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd4340",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd4341",
          "name": "Jiajun Deng",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd4342",
          "name": "Yanyong Zhang",
          "hidden": false
        },
        {
          "_id": "6900348b22d452aac6dd4343",
          "name": "Rui Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:54:08.000Z",
      "submittedOnDailyAt": "2025-10-28T02:09:47.863Z",
      "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
      "upvotes": 34,
      "discussionId": "6900348b22d452aac6dd4344",
      "ai_summary": "FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.",
      "ai_keywords": [
        "Normalizing Flows",
        "Autoregressive models",
        "invertible autoregressive flow",
        "self-supervised dimension reduction",
        "one-step distillation",
        "resampling-based classifier-free guidance"
      ]
    },
    "publishedAt": "2025-10-27T13:54:08.000Z",
    "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
    "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.21817",
      "authors": [
        {
          "_id": "69002d2622d452aac6dd42d4",
          "name": "Xiaoyu Liu",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42d5",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42d6",
          "name": "Chi Yan",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42d7",
          "name": "Chu Wu",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42d8",
          "name": "Haihan Gao",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42d9",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42da",
          "name": "Shaoqi Dong",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42db",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42dc",
          "name": "Bin Luo",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42dd",
          "name": "Xiuyong Yang",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42de",
          "name": "Guanwu Li",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42df",
          "name": "Yusheng Cai",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e0",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e1",
          "name": "Deqiang Jiang",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e2",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e3",
          "name": "Xing Sun",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e4",
          "name": "Caifeng Shan",
          "hidden": false
        },
        {
          "_id": "69002d2622d452aac6dd42e5",
          "name": "Ran He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/YqWLnrC3Cf4cOxO20icQF.png"
      ],
      "publishedAt": "2025-10-21T17:59:56.000Z",
      "submittedOnDailyAt": "2025-10-28T01:13:48.742Z",
      "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants.",
      "upvotes": 34,
      "discussionId": "69002d2722d452aac6dd42e6",
      "projectPage": "https://lxysl.github.io/VITA-E/",
      "githubRepo": "https://github.com/Tencent/VITA/tree/VITA-E",
      "ai_summary": "VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.",
      "ai_keywords": [
        "embodied interaction framework",
        "dual-model architecture",
        "Active Model",
        "Standby Model",
        "model-as-controller",
        "VLM",
        "special tokens",
        "system-level commands",
        "emergency stops",
        "speech interruptions",
        "concurrent speech and action"
      ],
      "githubStars": 86
    },
    "publishedAt": "2025-10-21T13:59:56.000Z",
    "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
    "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/YqWLnrC3Cf4cOxO20icQF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21817.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23581",
      "authors": [
        {
          "_id": "690043b122d452aac6dd43db",
          "name": "Junyoung Seo",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43dc",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43dd",
          "name": "Alexandros Haliassos",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43de",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43df",
          "name": "Honglie Chen",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43e0",
          "name": "Linh Tran",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43e1",
          "name": "Seungryong Kim",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43e2",
          "name": "Zoe Landgraf",
          "hidden": false
        },
        {
          "_id": "690043b122d452aac6dd43e3",
          "name": "Jie Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:50:19.000Z",
      "submittedOnDailyAt": "2025-10-28T02:48:23.987Z",
      "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
      "submittedOnDailyBy": {
        "_id": "635bcc3086f52b24c1e6422d",
        "avatarUrl": "/avatars/078710e49e849e2d890b4496bdc72d0d.svg",
        "isPro": true,
        "fullname": "Junyoung Seo",
        "user": "jyseo",
        "type": "user"
      },
      "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
      "upvotes": 30,
      "discussionId": "690043b222d452aac6dd43e4",
      "projectPage": "https://lookahead-anchoring.github.io/",
      "ai_summary": "Lookahead Anchoring improves audio-driven human animation by using future keyframes as dynamic guides, enhancing lip synchronization, identity preservation, and visual quality.",
      "ai_keywords": [
        "temporal autoregressive generation",
        "identity drift",
        "keyframes",
        "lookahead anchoring",
        "directional beacons",
        "self-keyframing",
        "temporal lookahead distance",
        "temporal conditioning"
      ]
    },
    "publishedAt": "2025-10-27T13:50:19.000Z",
    "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
    "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635bcc3086f52b24c1e6422d",
      "avatarUrl": "/avatars/078710e49e849e2d890b4496bdc72d0d.svg",
      "fullname": "Junyoung Seo",
      "name": "jyseo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23587",
      "authors": [
        {
          "_id": "690065ad22d452aac6dd4454",
          "name": "Yizhang Zhu",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4455",
          "name": "Liangwei Wang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4456",
          "name": "Chenyu Yang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4457",
          "name": "Xiaotian Lin",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4458",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4459",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445a",
          "name": "Xinyu Liu",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445b",
          "name": "Zhangyang Peng",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445c",
          "name": "Tianqi Luo",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445d",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445e",
          "name": "Chengliang Chai",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd445f",
          "name": "Chong Chen",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4460",
          "name": "Shimin Di",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4461",
          "name": "Ju Fan",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4462",
          "name": "Ji Sun",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4463",
          "name": "Nan Tang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4464",
          "name": "Fugee Tsung",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4465",
          "name": "Jiannan Wang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4466",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4467",
          "name": "Yanwei Xu",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4468",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd4469",
          "name": "Yong Zhang",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd446a",
          "name": "Xuanhe Zhou",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd446b",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "690065ad22d452aac6dd446c",
          "name": "Yuyu Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:54:07.000Z",
      "submittedOnDailyAt": "2025-10-28T05:18:46.268Z",
      "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
      "submittedOnDailyBy": {
        "_id": "65dd77bfcb021a4a9ebdc62f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65dd77bfcb021a4a9ebdc62f/o58j3T670xByIjJhnNLj9.png",
        "isPro": false,
        "fullname": "Derrick ZHU",
        "user": "derrickzhu",
        "type": "user"
      },
      "summary": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents.",
      "upvotes": 28,
      "discussionId": "690065ae22d452aac6dd446d",
      "githubRepo": "https://github.com/HKUSTDial/awesome-data-agents",
      "ai_summary": "A systematic taxonomy for data agents is introduced to clarify their autonomy levels and capabilities, addressing terminological ambiguity and guiding future research and development.",
      "ai_keywords": [
        "large language models",
        "data agents",
        "SAE J3016 standard",
        "driving automation",
        "hierarchical taxonomy",
        "autonomy levels",
        "data management",
        "data preparation",
        "data analysis",
        "generative data agents"
      ],
      "githubStars": 41
    },
    "publishedAt": "2025-10-27T13:54:07.000Z",
    "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
    "summary": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65dd77bfcb021a4a9ebdc62f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65dd77bfcb021a4a9ebdc62f/o58j3T670xByIjJhnNLj9.png",
      "fullname": "Derrick ZHU",
      "name": "derrickzhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22201",
      "authors": [
        {
          "_id": "69002ae322d452aac6dd42aa",
          "name": "Minho Park",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42ab",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42ac",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42ad",
          "name": "Hyojin Jang",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42ae",
          "name": "Hoiyeong Jin",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42af",
          "name": "Jooyeol Yun",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42b0",
          "name": "Hojoon Lee",
          "hidden": false
        },
        {
          "_id": "69002ae322d452aac6dd42b1",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/MDwVqWBQkIBhBlLgwciwW.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/terac32nyczWudLi39iRt.mp4"
      ],
      "publishedAt": "2025-10-25T07:44:33.000Z",
      "submittedOnDailyAt": "2025-10-28T01:06:06.608Z",
      "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
      "submittedOnDailyBy": {
        "_id": "630461624ec2dfa82a5ad7e7",
        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
        "isPro": false,
        "fullname": "Minho Park",
        "user": "mpark",
        "type": "user"
      },
      "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.",
      "upvotes": 24,
      "discussionId": "69002ae322d452aac6dd42b2",
      "projectPage": "https://davian-robotics.github.io/ACG",
      "githubRepo": "https://github.com/DAVIAN-Robotics/ACG",
      "ai_summary": "Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.",
      "ai_keywords": [
        "diffusion models",
        "flow matching models",
        "Vision-Language-Action (VLA) models",
        "imitation learning",
        "action coherence",
        "trajectory drift",
        "RoboCasa",
        "DexMimicGen",
        "SO-101 tasks",
        "Action Coherence Guidance (ACG)"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "68f61ab10a6265597402e1b1",
        "name": "DAVIAN-Robotics",
        "fullname": "DAVIAN Robotics",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
      }
    },
    "publishedAt": "2025-10-25T03:44:33.000Z",
    "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
    "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/MDwVqWBQkIBhBlLgwciwW.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/terac32nyczWudLi39iRt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630461624ec2dfa82a5ad7e7",
      "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
      "fullname": "Minho Park",
      "name": "mpark",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "68f61ab10a6265597402e1b1",
      "name": "DAVIAN-Robotics",
      "fullname": "DAVIAN Robotics",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22733",
      "authors": [
        {
          "_id": "6900329b22d452aac6dd431c",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "6900329b22d452aac6dd431d",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6900329b22d452aac6dd431e",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6900329b22d452aac6dd431f",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "6900329b22d452aac6dd4320",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6900329b22d452aac6dd4321",
          "name": "Jiaxin Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-26T16:04:48.000Z",
      "submittedOnDailyAt": "2025-10-28T01:38:21.829Z",
      "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
      "submittedOnDailyBy": {
        "_id": "630b167317bbe470568db9b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b167317bbe470568db9b5/pEc1mwlfX1uq28jzbqCaP.png",
        "isPro": false,
        "fullname": "Qi Liu",
        "user": "liuqi6777",
        "type": "user"
      },
      "summary": "Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework E^2Rank, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\nE^2Rank achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.",
      "upvotes": 20,
      "discussionId": "6900329c22d452aac6dd4322",
      "projectPage": "https://alibaba-nlp.github.io/E2Rank/",
      "githubRepo": "https://github.com/Alibaba-NLP/E2Rank",
      "ai_summary": "A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.",
      "ai_keywords": [
        "text embedding models",
        "shared embedding space",
        "ranking fidelity",
        "LLM-based listwise rerankers",
        "listwise ranking objective",
        "cosine similarity",
        "unified ranking function",
        "listwise ranking prompt",
        "pseudo-relevance feedback",
        "BEIR reranking benchmark",
        "BRIGHT benchmark",
        "MTEB benchmark",
        "embedding performance"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "661f98de142a51d630dbbcc4",
        "name": "Alibaba-NLP",
        "fullname": "Alibaba-NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
      }
    },
    "publishedAt": "2025-10-26T12:04:48.000Z",
    "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
    "summary": "Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework E^2Rank, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\nE^2Rank achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b167317bbe470568db9b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b167317bbe470568db9b5/pEc1mwlfX1uq28jzbqCaP.png",
      "fullname": "Qi Liu",
      "name": "liuqi6777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "organization": {
      "_id": "661f98de142a51d630dbbcc4",
      "name": "Alibaba-NLP",
      "fullname": "Alibaba-NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23451",
      "authors": [
        {
          "_id": "69002f8622d452aac6dd42f3",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f4",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f5",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f6",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f7",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f8",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42f9",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "69002f8622d452aac6dd42fa",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T15:53:20.000Z",
      "submittedOnDailyAt": "2025-10-28T01:30:06.348Z",
      "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences",
      "submittedOnDailyBy": {
        "_id": "643379416c6ecd58798421b3",
        "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
        "isPro": false,
        "fullname": "Zhuoran Jin",
        "user": "jinzhuoran",
        "type": "user"
      },
      "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
      "upvotes": 18,
      "discussionId": "69002f8622d452aac6dd42fb",
      "githubRepo": "https://github.com/HongbangYuan/OmniReward",
      "ai_summary": "Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.",
      "ai_keywords": [
        "reward models",
        "modality imbalance",
        "preference rigidity",
        "Omni-Reward",
        "Omni-RewardBench",
        "Omni-RewardData",
        "Omni-RewardModel",
        "discriminative RMs",
        "generative RMs"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "640a887796aae649741a586f",
        "name": "CASIA",
        "fullname": "Chinese Academic of Science Institute of Automation",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
      }
    },
    "publishedAt": "2025-10-27T11:53:20.000Z",
    "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences",
    "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643379416c6ecd58798421b3",
      "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
      "fullname": "Zhuoran Jin",
      "name": "jinzhuoran",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "640a887796aae649741a586f",
      "name": "CASIA",
      "fullname": "Chinese Academic of Science Institute of Automation",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22706",
      "authors": [
        {
          "_id": "69003b1c22d452aac6dd43a3",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a4",
          "name": "Zhengyu Zou",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a5",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a6",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a7",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a8",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43a9",
          "name": "Yushi Lan",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43aa",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43ab",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43ac",
          "name": "Dingwen Zhang",
          "hidden": false
        },
        {
          "_id": "69003b1c22d452aac6dd43ad",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-26T14:57:44.000Z",
      "submittedOnDailyAt": "2025-10-28T02:20:52.943Z",
      "title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.",
      "upvotes": 16,
      "discussionId": "69003b1d22d452aac6dd43ae",
      "githubRepo": "https://github.com/lifuguan/IGGT_official",
      "ai_summary": "InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.",
      "ai_keywords": [
        "InstanceGrounded Geometry Transformer",
        "IGGT",
        "3D-Consistent Contrastive Learning",
        "3D reconstruction",
        "instance-level contextual understanding",
        "3D scene",
        "object instances",
        "InsScene-15K"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-10-26T10:57:44.000Z",
    "title": "IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction",
    "summary": "Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22706.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23603",
      "authors": [
        {
          "_id": "6900320522d452aac6dd4309",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430a",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430b",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430c",
          "name": "Shihao Wang",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430d",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430e",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd430f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd4310",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6900320522d452aac6dd4311",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:59:32.000Z",
      "submittedOnDailyAt": "2025-10-28T01:31:39.211Z",
      "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
      "upvotes": 14,
      "discussionId": "6900320622d452aac6dd4312",
      "projectPage": "https://circleradon.github.io/PixelRefer/",
      "ai_summary": "PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "open-world visual comprehension",
        "fine-grained reasoning",
        "PixelRefer",
        "region-level",
        "Scale-Adaptive Object Tokenizer",
        "SAOT",
        "object-level tokens",
        "global visual tokens",
        "PixelRefer-Lite",
        "Object-Centric Infusion module",
        "object-centric instruction dataset",
        "PixelRefer-2.2M"
      ]
    },
    "publishedAt": "2025-10-27T13:59:32.000Z",
    "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
    "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 147
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23052",
      "authors": [
        {
          "_id": "69003b6822d452aac6dd43b8",
          "name": "Zhanchao Zhou",
          "hidden": false
        },
        {
          "_id": "69003b6822d452aac6dd43b9",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "69003b6822d452aac6dd43ba",
          "name": "Haoxing Chen",
          "hidden": false
        },
        {
          "_id": "69003b6822d452aac6dd43bb",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "69003b6822d452aac6dd43bc",
          "name": "Jianguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T06:28:58.000Z",
      "submittedOnDailyAt": "2025-10-28T02:18:36.150Z",
      "title": "Knocking-Heads Attention",
      "submittedOnDailyBy": {
        "_id": "671b3d82101c0d9f78749256",
        "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
        "isPro": false,
        "fullname": "Zhanchao Zhou",
        "user": "Zcchill",
        "type": "user"
      },
      "summary": "Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.",
      "upvotes": 11,
      "discussionId": "69003b6922d452aac6dd43bd",
      "ai_summary": "Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.",
      "ai_keywords": [
        "multi-head attention",
        "MHA",
        "grouped-query attention",
        "GQA",
        "grouped-tied attention",
        "GTA",
        "knocking-heads attention",
        "KHA",
        "scaled dot-product attention",
        "parameter-efficient",
        "MoE",
        "high-quality tokens",
        "training dynamics",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-10-27T02:28:58.000Z",
    "title": "Knocking-Heads Attention",
    "summary": "Multi-head attention (MHA) has become the cornerstone of modern large\nlanguage models, enhancing representational capacity through parallel attention\nheads. However, increasing the number of heads inherently weakens individual\nhead capacity, and existing attention mechanisms - whether standard MHA or its\nvariants like grouped-query attention (GQA) and grouped-tied attention (GTA) -\nsimply concatenate outputs from isolated heads without strong interaction. To\naddress this limitation, we propose knocking-heads attention (KHA), which\nenables attention heads to \"knock\" on each other - facilitating cross-head\nfeature-level interactions before the scaled dot-product attention. This is\nachieved by applying a shared, diagonally-initialized projection matrix across\nall heads. The diagonal initialization preserves head-specific specialization\nat the start of training while allowing the model to progressively learn\nintegrated cross-head representations. KHA adds only minimal parameters and\nFLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention\nvariants. We validate KHA by training a 6.1B parameter MoE model (1.01B\nactivated) on 1T high-quality tokens. Compared to baseline attention\nmechanisms, KHA brings superior and more stable training dynamics, achieving\nbetter performance across downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23052.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671b3d82101c0d9f78749256",
      "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
      "fullname": "Zhanchao Zhou",
      "name": "Zcchill",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22946",
      "authors": [
        {
          "_id": "6900334b22d452aac6dd4324",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd4325",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd4326",
          "name": "Chenhui Gou",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd4327",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd4328",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd4329",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd432a",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd432b",
          "name": "Weihao Yu",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd432c",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd432d",
          "name": "Haoqi Fan",
          "hidden": false
        },
        {
          "_id": "6900334b22d452aac6dd432e",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T02:59:57.000Z",
      "submittedOnDailyAt": "2025-10-28T01:37:01.506Z",
      "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
      "upvotes": 10,
      "discussionId": "6900334b22d452aac6dd432f",
      "projectPage": "https://ucsc-vlaa.github.io/LightBagel/",
      "ai_summary": "A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.",
      "ai_keywords": [
        "multimodal models",
        "self-attention blocks",
        "compositional text-to-image generation",
        "complex text-to-image generation",
        "image editing"
      ]
    },
    "publishedAt": "2025-10-26T22:59:57.000Z",
    "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation",
    "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22946.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 147
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23544",
      "authors": [
        {
          "_id": "6900350c22d452aac6dd434f",
          "name": "Tingyu Song",
          "hidden": false
        },
        {
          "_id": "6900350c22d452aac6dd4350",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6900350c22d452aac6dd4351",
          "name": "Siyue Zhang",
          "hidden": false
        },
        {
          "_id": "6900350c22d452aac6dd4352",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "6900350c22d452aac6dd4353",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:19:37.000Z",
      "submittedOnDailyAt": "2025-10-28T01:44:45.429Z",
      "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.",
      "upvotes": 6,
      "discussionId": "6900350c22d452aac6dd4354",
      "ai_summary": "LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.",
      "ai_keywords": [
        "LIMRANK-SYNTHESIZER",
        "reranker model",
        "LIMRANK",
        "BRIGHT",
        "FollowIR",
        "scientific literature search",
        "retrieval-augmented generation"
      ]
    },
    "publishedAt": "2025-10-27T13:19:37.000Z",
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23272",
      "authors": [
        {
          "_id": "690061a922d452aac6dd444a",
          "name": "Bang Xiao",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd444b",
          "name": "Lingjie Jiang",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd444c",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd444d",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd444e",
          "name": "Yupan Huang",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd444f",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd4450",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "690061a922d452aac6dd4451",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T12:32:33.000Z",
      "submittedOnDailyAt": "2025-10-28T04:55:20.255Z",
      "title": "Code Aesthetics with Agentic Reward Feedback",
      "submittedOnDailyBy": {
        "_id": "66ab80e9bfb7d73a56bc293c",
        "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
        "isPro": false,
        "fullname": "Jack",
        "user": "lingjie23",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.",
      "upvotes": 5,
      "discussionId": "690061aa22d452aac6dd4452",
      "projectPage": "https://bangx7.github.io/code-aesthetics/",
      "ai_summary": "A new pipeline enhances the aesthetic quality of LLM-generated code through instruction-tuning, agentic reward feedback, and joint optimization, outperforming existing models.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "code aesthetics",
        "AesCode-358K",
        "agentic reward feedback",
        "multi-agent system",
        "GRPO-AR",
        "GRPO algorithm",
        "OpenDesign",
        "supervised fine-tuning",
        "reinforcement learning",
        "PandasPlotBench",
        "AesCoder-4B",
        "GPT-4o",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-10-27T08:32:33.000Z",
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "summary": "Large Language Models (LLMs) have become valuable assistants for developers\nin code-related tasks. While LLMs excel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstruct AesCode-358K, a large-scale instruction-tuning dataset focused on\ncode aesthetics. Next, we propose agentic reward feedback, a multi-agent system\nthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we develop GRPO-AR, which integrates these signals into the\nGRPO algorithm for joint optimization of functionality and code aesthetics.\nFinally, we develop OpenDesign, a benchmark for assessing code aesthetics.\nExperimental results show that combining supervised fine-tuning on AesCode-358K\nwith reinforcement learning using agentic reward feedback significantly\nimproves performance on OpenDesign and also enhances results on existing\nbenchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o\nand GPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23272.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ab80e9bfb7d73a56bc293c",
      "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
      "fullname": "Jack",
      "name": "lingjie23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22200",
      "authors": [
        {
          "_id": "690036a622d452aac6dd4356",
          "name": "Meituan LongCat Team",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd4357",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd4358",
          "name": "Qilong Huang",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd4359",
          "name": "Zhuoliang Kang",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435a",
          "name": "Hongyu Li",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435b",
          "name": "Shijun Liang",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435c",
          "name": "Liya Ma",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435d",
          "name": "Siyu Ren",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435e",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd435f",
          "name": "Rixu Xie",
          "hidden": false
        },
        {
          "_id": "690036a622d452aac6dd4360",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-25T07:41:02.000Z",
      "submittedOnDailyAt": "2025-10-28T01:51:30.094Z",
      "title": "LongCat-Video Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
      "upvotes": 5,
      "discussionId": "690036a722d452aac6dd4361",
      "githubRepo": "https://github.com/meituan-longcat/LongCat-Video",
      "ai_summary": "LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.",
      "ai_keywords": [
        "Diffusion Transformer",
        "Text-to-Video",
        "Image-to-Video",
        "Video-Continuation",
        "temporal coherence",
        "coarse-to-fine generation",
        "block sparse attention",
        "multi-reward RLHF"
      ],
      "githubStars": 612
    },
    "publishedAt": "2025-10-25T03:41:02.000Z",
    "title": "LongCat-Video Technical Report",
    "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22200.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 147
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.21003",
      "authors": [
        {
          "_id": "68fff75322d452aac6dd4260",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4261",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4262",
          "name": "Xuefei Ning",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4263",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4264",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4265",
          "name": "Zinan Lin",
          "hidden": false
        },
        {
          "_id": "68fff75322d452aac6dd4266",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T21:21:38.000Z",
      "submittedOnDailyAt": "2025-10-28T02:02:01.208Z",
      "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation",
      "submittedOnDailyBy": {
        "_id": "64c832a8c547ed5243d29630",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c832a8c547ed5243d29630/a46V0xOVRVknM9mUjKkTf.jpeg",
        "isPro": false,
        "fullname": "Zinan Lin",
        "user": "fjxmlzn",
        "type": "user"
      },
      "summary": "Image Auto-regressive (AR) models have emerged as a powerful paradigm of\nvisual generative models. Despite their promising performance, they suffer from\nslow generation speed due to the large number of sampling steps required.\nAlthough Distilled Decoding 1 (DD1) was recently proposed to enable few-step\nsampling for image AR models, it still incurs significant performance\ndegradation in the one-step setting, and relies on a pre-defined mapping that\nlimits its flexibility. In this work, we propose a new method, Distilled\nDecoding 2 (DD2), to further advances the feasibility of one-step sampling for\nimage AR models. Unlike DD1, DD2 does not without rely on a pre-defined\nmapping. We view the original AR model as a teacher model which provides the\nground truth conditional score in the latent embedding space at each token\nposition. Based on this, we propose a novel conditional score\ndistillation loss to train a one-step generator. Specifically, we train a\nseparate network to predict the conditional score of the generated distribution\nand apply score distillation at every token position conditioned on previous\ntokens. Experimental results show that DD2 enables one-step sampling for image\nAR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256.\nCompared to the strongest baseline DD1, DD2 reduces the gap between the\none-step sampling and original AR model by 67%, with up to 12.3times\ntraining speed-up simultaneously. DD2 takes a significant step toward the goal\nof one-step AR generation, opening up new possibilities for fast and\nhigh-quality AR modeling. Code is available at\nhttps://github.com/imagination-research/Distilled-Decoding-2.",
      "upvotes": 4,
      "discussionId": "68fff75422d452aac6dd4267",
      "ai_summary": "A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.",
      "ai_keywords": [
        "image auto-regressive models",
        "Distilled Decoding 1 (DD1)",
        "Distilled Decoding 2 (DD2)",
        "conditional score distillation loss",
        "latent embedding space",
        "token position",
        "one-step sampling",
        "FID",
        "ImageNet-256"
      ]
    },
    "publishedAt": "2025-10-23T17:21:38.000Z",
    "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation",
    "summary": "Image Auto-regressive (AR) models have emerged as a powerful paradigm of\nvisual generative models. Despite their promising performance, they suffer from\nslow generation speed due to the large number of sampling steps required.\nAlthough Distilled Decoding 1 (DD1) was recently proposed to enable few-step\nsampling for image AR models, it still incurs significant performance\ndegradation in the one-step setting, and relies on a pre-defined mapping that\nlimits its flexibility. In this work, we propose a new method, Distilled\nDecoding 2 (DD2), to further advances the feasibility of one-step sampling for\nimage AR models. Unlike DD1, DD2 does not without rely on a pre-defined\nmapping. We view the original AR model as a teacher model which provides the\nground truth conditional score in the latent embedding space at each token\nposition. Based on this, we propose a novel conditional score\ndistillation loss to train a one-step generator. Specifically, we train a\nseparate network to predict the conditional score of the generated distribution\nand apply score distillation at every token position conditioned on previous\ntokens. Experimental results show that DD2 enables one-step sampling for image\nAR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256.\nCompared to the strongest baseline DD1, DD2 reduces the gap between the\none-step sampling and original AR model by 67%, with up to 12.3times\ntraining speed-up simultaneously. DD2 takes a significant step toward the goal\nof one-step AR generation, opening up new possibilities for fast and\nhigh-quality AR modeling. Code is available at\nhttps://github.com/imagination-research/Distilled-Decoding-2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c832a8c547ed5243d29630",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c832a8c547ed5243d29630/a46V0xOVRVknM9mUjKkTf.jpeg",
      "fullname": "Zinan Lin",
      "name": "fjxmlzn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23571",
      "authors": [
        {
          "_id": "6900399b22d452aac6dd4384",
          "name": "Yash Jangir",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd4385",
          "name": "Yidi Zhang",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd4386",
          "name": "Kashu Yamazaki",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd4387",
          "name": "Chenyu Zhang",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd4388",
          "name": "Kuan-Hsun Tu",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd4389",
          "name": "Tsung-Wei Ke",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd438a",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd438b",
          "name": "Yonatan Bisk",
          "hidden": false
        },
        {
          "_id": "6900399b22d452aac6dd438c",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zpl9k6gQ2jfwjzzubtCbd.mp4"
      ],
      "publishedAt": "2025-10-27T17:41:38.000Z",
      "submittedOnDailyAt": "2025-10-28T02:03:56.227Z",
      "title": "RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
      "upvotes": 3,
      "discussionId": "6900399c22d452aac6dd438d",
      "ai_summary": "A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.",
      "ai_keywords": [
        "robot generalists",
        "instructable agents",
        "rigorous evaluation",
        "real-world testing",
        "simulation benchmarks",
        "VLA evaluation",
        "vision-language models",
        "2D-to-3D generative modeling",
        "differentiable rendering",
        "video demonstrations",
        "digital twins",
        "automated VLM-guided scoring",
        "human preference judgments",
        "crowdworkers",
        "policy generalization",
        "robustness",
        "simulated environments"
      ]
    },
    "publishedAt": "2025-10-27T13:41:38.000Z",
    "title": "RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
    "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/zpl9k6gQ2jfwjzzubtCbd.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23571.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 147
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22907",
      "authors": [
        {
          "_id": "6900310d22d452aac6dd4305",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6900310d22d452aac6dd4306",
          "name": "Lanser Contributors",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T01:25:20.000Z",
      "submittedOnDailyAt": "2025-10-28T01:28:47.140Z",
      "title": "Language Server CLI Empowers Language Agents with Process Rewards",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli",
      "upvotes": 3,
      "discussionId": "6900310d22d452aac6dd4307",
      "ai_summary": "Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.",
      "ai_keywords": [
        "Language Server Protocol",
        "LSP",
        "coding agents",
        "CI",
        "Selector DSL",
        "AST-path",
        "content-anchored selectors",
        "Analysis Bundles",
        "safety envelope",
        "preview",
        "workspace jails",
        "Git-aware",
        "transactional apply",
        "diagnostic deltas",
        "disambiguation confidence",
        "safe-apply checks",
        "determinism",
        "frozen snapshots",
        "monotonicity property",
        "process supervision",
        "counterfactual analysis"
      ]
    },
    "publishedAt": "2025-10-26T21:25:20.000Z",
    "title": "Language Server CLI Empowers Language Agents with Process Rewards",
    "summary": "Large language models routinely hallucinate APIs and mislocalize edits, while\nlanguage servers compute verified, IDE-grade facts about real code. We present\nLanser-CLI, a CLI-first orchestration layer that pins and mediates a Language\nServer Protocol (LSP) server for coding agents and CI, exposing deterministic,\nreplayable workflows. Our position is that language servers provide not only\nstructural information (definitions, references, types, diagnostics) but also\nan actionable process reward: machine-checked, step-wise signals that align an\nagent's planning loop with program reality. In this work, Lanser-CLI\ncontributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via\na Selector DSL (symbolic, AST-path, and content-anchored selectors) with a\nprincipled relocation algorithm; (ii) deterministic Analysis Bundles that\nnormalize Language Server responses and capture environment/capability metadata\nwith stable content hashes; (iii) a safety envelope for mutating operations\n(rename, code actions) with preview, workspace jails, and Git-aware,\ntransactional apply; and (iv) a process-reward functional derived from Language\nServer facts (diagnostic deltas, disambiguation confidence, and safe-apply\nchecks) that is computable online and replayable offline. We formalize\ndeterminism under frozen snapshots and establish a monotonicity property for\nthe process reward, making it suitable for process supervision and\ncounterfactual analysis. Project Page:\nhttps://github.com/yifanzhang-pro/lanser-cli",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22907.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23605",
      "authors": [
        {
          "_id": "69003a8622d452aac6dd439e",
          "name": "Shuhong Zheng",
          "hidden": false
        },
        {
          "_id": "69003a8622d452aac6dd439f",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "69003a8622d452aac6dd43a0",
          "name": "Igor Gilitschenski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-28T03:51:15.069Z",
      "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
      "submittedOnDailyBy": {
        "_id": "64361417a4bd75c62cc1e534",
        "avatarUrl": "/avatars/edf6ac3fb9bd63ceb78f3b95254e6b19.svg",
        "isPro": true,
        "fullname": "Shuhong Zheng",
        "user": "ShuhongZheng",
        "type": "user"
      },
      "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
      "upvotes": 1,
      "discussionId": "69003a8622d452aac6dd43a1",
      "ai_summary": "TIRE method enhances identity preservation in 3D/4D generation by tracking, inpainting, and resplatting regions of a 3D asset using video tracking and a subject-driven 2D inpainting model.",
      "ai_keywords": [
        "3D generative model",
        "video tracking",
        "subject-driven 2D inpainting model",
        "identity preservation",
        "3D/4D generation",
        "TIRE"
      ]
    },
    "publishedAt": "2025-10-27T13:59:51.000Z",
    "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
    "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64361417a4bd75c62cc1e534",
      "avatarUrl": "/avatars/edf6ac3fb9bd63ceb78f3b95254e6b19.svg",
      "fullname": "Shuhong Zheng",
      "name": "ShuhongZheng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23594",
      "authors": [
        {
          "_id": "6900326e22d452aac6dd4314",
          "name": "Yusu Qian",
          "hidden": false
        },
        {
          "_id": "6900326e22d452aac6dd4315",
          "name": "Cheng Wan",
          "hidden": false
        },
        {
          "_id": "6900326e22d452aac6dd4316",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "6900326e22d452aac6dd4317",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "6900326e22d452aac6dd4318",
          "name": "Qingyu Zhao",
          "hidden": false
        },
        {
          "_id": "6900326e22d452aac6dd4319",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T17:57:52.000Z",
      "submittedOnDailyAt": "2025-10-28T01:33:31.648Z",
      "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
      "upvotes": 1,
      "discussionId": "6900326e22d452aac6dd431a",
      "githubRepo": "https://github.com/JornyWan/PRISM-Bench",
      "ai_summary": "PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.",
      "ai_keywords": [
        "PRISM-Bench",
        "chain-of-thought",
        "logical consistency",
        "error detection",
        "visual reasoning",
        "multi-step reasoning",
        "symbolic reasoning",
        "geometric reasoning",
        "analogical reasoning",
        "MLLMs",
        "multimodal reasoning"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-10-27T13:57:52.000Z",
    "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
    "summary": "We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 147
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22975",
      "authors": [
        {
          "_id": "690051e222d452aac6dd4400",
          "name": "Rishit Dagli",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4401",
          "name": "Donglai Xiang",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4402",
          "name": "Vismay Modi",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4403",
          "name": "Charles Loop",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4404",
          "name": "Clement Fuji Tsang",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4405",
          "name": "Anka He Chen",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4406",
          "name": "Anita Hu",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4407",
          "name": "Gavriel State",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4408",
          "name": "David I. W. Levin",
          "hidden": false
        },
        {
          "_id": "690051e222d452aac6dd4409",
          "name": "Maria Shugrina",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T03:56:25.000Z",
      "submittedOnDailyAt": "2025-10-28T04:02:14.537Z",
      "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
      "submittedOnDailyBy": {
        "_id": "60796959c59d9e1697fa2324",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
        "isPro": false,
        "fullname": "Rishit Dagli",
        "user": "rishitdagli",
        "type": "user"
      },
      "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus (E), Poisson's ratio (nu), and density (rho) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
      "upvotes": 1,
      "discussionId": "690051e222d452aac6dd440a",
      "projectPage": "https://research.nvidia.com/labs/sil/projects/vomp",
      "ai_summary": "VoMP uses a feed-forward method with a Geometry Transformer to predict accurate volumetric material properties from 3D objects, outperforming existing methods in both accuracy and speed.",
      "ai_keywords": [
        "feed-forward method",
        "Young's modulus",
        "Poisson's ratio",
        "density",
        "3D objects",
        "voxelization",
        "per-voxel multi-view features",
        "Geometry Transformer",
        "material latent codes",
        "manifold of physically plausible materials",
        "annotation pipeline",
        "segmented 3D datasets",
        "material databases",
        "vision-language model",
        "benchmark"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-26T23:56:25.000Z",
    "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
    "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus (E), Poisson's ratio (nu), and density (rho) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22975.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60796959c59d9e1697fa2324",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
      "fullname": "Rishit Dagli",
      "name": "rishitdagli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.21800",
      "authors": [
        {
          "_id": "69002c5e22d452aac6dd42bb",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "69002c5e22d452aac6dd42bc",
          "name": "Angela Yuan",
          "hidden": false
        },
        {
          "_id": "69002c5e22d452aac6dd42bd",
          "name": "Quanquan Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T16:49:22.000Z",
      "submittedOnDailyAt": "2025-10-28T01:09:31.082Z",
      "title": "MARS-M: When Variance Reduction Meets Matrices",
      "submittedOnDailyBy": {
        "_id": "653d276681f52ceb4d12bd85",
        "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
        "isPro": false,
        "fullname": "Yifeng Liu",
        "user": "Lewis-Lau",
        "type": "user"
      },
      "summary": "Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\nmathcal{O}(T^{-1/3}), which improves upon\nmathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/MARS_M.",
      "upvotes": 1,
      "discussionId": "69002c5e22d452aac6dd42be",
      "projectPage": "https://github.com/AGI-Arena/MARS/tree/main/MARS_M",
      "githubRepo": "https://github.com/AGI-Arena/MARS/tree/main/MARS_M",
      "ai_summary": "MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.",
      "ai_keywords": [
        "Muon",
        "MARS",
        "variance reduction",
        "first-order stationary point",
        "language modeling",
        "computer vision"
      ],
      "githubStars": 711,
      "organization": {
        "_id": "67784c39dac147922d8d09f0",
        "name": "UCLA",
        "fullname": "University of California, Los Angeles",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
      }
    },
    "publishedAt": "2025-10-20T12:49:22.000Z",
    "title": "MARS-M: When Variance Reduction Meets Matrices",
    "summary": "Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\nmathcal{O}(T^{-1/3}), which improves upon\nmathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/MARS_M.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653d276681f52ceb4d12bd85",
      "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
      "fullname": "Yifeng Liu",
      "name": "Lewis-Lau",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67784c39dac147922d8d09f0",
      "name": "UCLA",
      "fullname": "University of California, Los Angeles",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
    },
    "isAuthorParticipating": false
  }
]