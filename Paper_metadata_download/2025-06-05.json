[
  {
    "paper": {
      "id": "2506.03569",
      "authors": [
        {
          "_id": "6841003e45e7d8a890731765",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731767",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731768",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731769",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176b",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176c",
          "user": {
            "_id": "642e72cec1b0f8e4e76af16d",
            "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
            "isPro": false,
            "fullname": "shuhao gu",
            "user": "gsh33",
            "type": "user"
          },
          "name": "Shuhao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176d",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176e",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731770",
          "user": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "isPro": false,
            "fullname": "Lei Li",
            "user": "tobiaslee",
            "type": "user"
          },
          "name": "Lei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731771",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731772",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731773",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731774",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731775",
          "user": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "isPro": false,
            "fullname": "Dawei Zhu",
            "user": "dwzhu",
            "type": "user"
          },
          "name": "Dawei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731776",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731777",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731778",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731779",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177a",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177b",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177c",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177d",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177e",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177f",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731780",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731781",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731782",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731783",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731784",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731785",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731786",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731787",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731788",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731789",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178a",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178b",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178d",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178e",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178f",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731790",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731791",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731792",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731793",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731794",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731795",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731796",
          "name": "Shande Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731797",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731798",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731799",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179b",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179c",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179e",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179f",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a0",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a1",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a2",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a3",
          "user": {
            "_id": "680e9a219e529f779991be0c",
            "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
            "isPro": false,
            "fullname": "Huaqiu Liu",
            "user": "Prestonprom",
            "type": "user"
          },
          "name": "Huaqiu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a4",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a5",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a6",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a7",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a8",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a9",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317aa",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ab",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ac",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ad",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ae",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317af",
          "name": "Bingquan Xia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T04:32:54.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
      "title": "MiMo-VL Technical Report",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
      "upvotes": 36,
      "discussionId": "6841004145e7d8a890731853",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
      "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
      "ai_keywords": [
        "vision-language models",
        "multimodal reasoning",
        "four-stage pre-training",
        "Mixed On-policy Reinforcement Learning",
        "MORL",
        "Chain-of-Thought",
        "reproducibility"
      ]
    },
    "publishedAt": "2025-06-04T00:32:54.000Z",
    "title": "MiMo-VL Technical Report",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04207",
      "authors": [
        {
          "_id": "684117e22db29aa7b403af8d",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8e",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8f",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af90",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af91",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af92",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af93",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af94",
          "name": "Weijie Wang",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af95",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af96",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:08.000Z",
      "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
      "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "65352acb7139c5dd8d9a8590",
        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
        "isPro": false,
        "fullname": "JiachengChen",
        "user": "JC-Chen",
        "type": "user"
      },
      "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
      "upvotes": 25,
      "discussionId": "684117e32db29aa7b403afc2",
      "githubRepo": "https://github.com/CSfufu/Revisual-R1"
    },
    "publishedAt": "2025-06-04T13:51:08.000Z",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65352acb7139c5dd8d9a8590",
      "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
      "fullname": "JiachengChen",
      "name": "JC-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02921",
      "authors": [
        {
          "_id": "683ff4dcfbc9041ef7274c51",
          "user": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "isPro": false,
            "fullname": "Yijun YANG",
            "user": "thomasyyj",
            "type": "user"
          },
          "name": "Yijun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c52",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c53",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c54",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c55",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c56",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c57",
          "name": "Ivan Titov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:23:06.000Z",
      "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
      "title": "A Controllable Examination for Long-Context Language Models",
      "submittedOnDailyBy": {
        "_id": "657eea68f4f72f2c4c44640d",
        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
        "isPro": false,
        "fullname": "Yijun YANG",
        "user": "thomasyyj",
        "type": "user"
      },
      "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
      "upvotes": 23,
      "discussionId": "683ff4ddfbc9041ef7274c73",
      "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
      "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
      "ai_keywords": [
        "long-context language models (LCLM)",
        "real-world tasks",
        "synthetic tasks",
        "needle-in-the-haystack (NIAH)",
        "seamless context",
        "controllable setting",
        "sound evaluation",
        "LongBioBench",
        "semantic understanding",
        "elementary reasoning",
        "trustworthiness",
        "long-context continual pretraining",
        "RoPE embedding"
      ]
    },
    "publishedAt": "2025-06-03T10:23:06.000Z",
    "title": "A Controllable Examination for Long-Context Language Models",
    "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657eea68f4f72f2c4c44640d",
      "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
      "fullname": "Yijun YANG",
      "name": "thomasyyj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04180",
      "authors": [
        {
          "_id": "6840fefb3098ab525906d852",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d853",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d854",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d855",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d856",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:27:42.000Z",
      "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
      "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
      "upvotes": 18,
      "discussionId": "6840fefc3098ab525906d89c",
      "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
      "ai_keywords": [
        "agent-based framework",
        "structured thinking-through planning",
        "refinement stages",
        "SuperWriter-Agent",
        "SuperWriter-LM",
        "hierarchical Direct Preference Optimization",
        "Monte Carlo Tree Search",
        "DPO",
        "automatic evaluation",
        "human evaluation",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-04T13:27:42.000Z",
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
    "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01320",
      "authors": [
        {
          "_id": "684124368cb0edba3ab8f738",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f739",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73a",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73b",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T05:02:33.000Z",
      "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
      "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
      "submittedOnDailyBy": {
        "_id": "66ee81b676a8038cb42c8caa",
        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
        "isPro": false,
        "fullname": "Yunhong Min",
        "user": "myhong",
        "type": "user"
      },
      "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
      "upvotes": 15,
      "discussionId": "6841243c8cb0edba3ab8f8bf",
      "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
      "ai_keywords": [
        "SMC-based framework",
        "pCNL-based initial particle sampling",
        "inference-time reward alignment",
        "score-based generative model",
        "Sequential Monte Carlo",
        "denoising process",
        "Gaussian prior",
        "reward-aware posterior",
        "preconditioned Crank-Nicolson Langevin",
        "layout-to-image generation",
        "quantity-aware generation",
        "aesthetic-preference generation"
      ]
    },
    "publishedAt": "2025-06-02T01:02:33.000Z",
    "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
    "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee81b676a8038cb42c8caa",
      "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
      "fullname": "Yunhong Min",
      "name": "myhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03139",
      "authors": [
        {
          "_id": "683fb0a7be8421eda3152283",
          "user": {
            "_id": "676b86e79ff0244316f7202f",
            "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
            "isPro": false,
            "fullname": "chensiqi",
            "user": "xiaoooobai",
            "type": "user"
          },
          "name": "Siqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152284",
          "name": "Xinyu Dong",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152285",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152286",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152287",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152288",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152289",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228a",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228b",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228c",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228d",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228e",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228f",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:57.000Z",
      "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
      "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
      "upvotes": 11,
      "discussionId": "683fb0a7be8421eda31522ca",
      "projectPage": "https://zju-real.github.io/SVGenius/",
      "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
      "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "SVG processing",
        "SVGenius",
        "complexity stratification",
        "reasoning-enhanced training",
        "style transfer"
      ]
    },
    "publishedAt": "2025-06-03T13:58:57.000Z",
    "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
    "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04228",
      "authors": [
        {
          "_id": "684103aed45a1fc5540ddc10",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc11",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc12",
          "user": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "isPro": false,
            "fullname": "xichen",
            "user": "xichenhku",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc13",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc14",
          "name": "Yiyang Wang",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc15",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
      "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
      "upvotes": 10,
      "discussionId": "684103b0d45a1fc5540ddca8",
      "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
      "ai_keywords": [
        "LayerFlow",
        "text-to-video diffusion transformer",
        "layer embeddings",
        "sub-clips",
        "multi-stage training strategy",
        "motion LoRA",
        "content LoRA",
        "layered images",
        "copy-pasted video data",
        "smooth videos"
      ]
    },
    "publishedAt": "2025-06-04T13:59:58.000Z",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24500",
      "authors": [
        {
          "_id": "683fb063ef97de05eb2a44cc",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cd",
          "name": "Xing Gao",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44ce",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cf",
          "name": "Xiang Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d0",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d1",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d2",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d3",
          "name": "Jialu Du",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d5",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d6",
          "name": "Weiming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:01:06.000Z",
      "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
      "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
      "submittedOnDailyBy": {
        "_id": "67c03110e8c7d56a8e135ac8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
        "isPro": false,
        "fullname": "Hou",
        "user": "Guiyang1001",
        "type": "user"
      },
      "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
      "upvotes": 10,
      "discussionId": "683fb064ef97de05eb2a452b",
      "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
      "ai_keywords": [
        "Large Language Models",
        "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
        "TimeHC-RL",
        "System 1",
        "System 2",
        "RL",
        "DeepSeek-R1",
        "OpenAI-O3"
      ]
    },
    "publishedAt": "2025-05-30T08:01:06.000Z",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c03110e8c7d56a8e135ac8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
      "fullname": "Hou",
      "name": "Guiyang1001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03295",
      "authors": [
        {
          "_id": "6840e7d81fadbc85ae3bdc0f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc10",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc11",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc12",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc13",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T18:35:52.000Z",
      "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
      "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
      "submittedOnDailyBy": {
        "_id": "636a35eff8d9af4aea181608",
        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
        "isPro": false,
        "fullname": "yubo",
        "user": "ubowang",
        "type": "user"
      },
      "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
      "upvotes": 8,
      "discussionId": "6840e7d81fadbc85ae3bdc45",
      "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
      "ai_keywords": [
        "Critique Fine-Tuning",
        "teacher LLMs",
        "Qwen-Math",
        "Llama family models",
        "reasoning tasks",
        "one-shot CFT",
        "performance gains",
        "logic reasoning benchmarks",
        "math benchmarks",
        "prompt problems"
      ]
    },
    "publishedAt": "2025-06-03T14:35:52.000Z",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04158",
      "authors": [
        {
          "_id": "6840fb71d4e16ff5f95108aa",
          "name": "Yujia Hu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ab",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ac",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ad",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ae",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
      ],
      "publishedAt": "2025-06-04T16:57:24.000Z",
      "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
      "title": "Image Editing As Programs with Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
      "upvotes": 7,
      "discussionId": "6840fb73d4e16ff5f9510950",
      "projectPage": "https://yujiahu1109.github.io/IEAP/",
      "githubRepo": "https://github.com/YujiaHu1109/IEAP",
      "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
      "ai_keywords": [
        "diffusion models",
        "text-to-image generation",
        "instruction-driven image editing",
        "structurally inconsistent edits",
        "Image Editing As Programs (IEAP)",
        "Diffusion Transformer (DiT)",
        "atomic operations",
        "lightweight adapter",
        "vision-language model (VLM)",
        "modularizing edits"
      ]
    },
    "publishedAt": "2025-06-04T12:57:24.000Z",
    "title": "Image Editing As Programs with Diffusion Models",
    "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03517",
      "authors": [
        {
          "_id": "68412f853c22997c7329f3a0",
          "user": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "isPro": false,
            "fullname": "Ziyi Wu",
            "user": "Dazitu616",
            "type": "user"
          },
          "name": "Ziyi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a1",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a2",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a3",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a4",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a5",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a6",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a7",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:06:08.000Z",
      "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
      "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
      "upvotes": 6,
      "discussionId": "68412f8a3c22997c7329f4ff",
      "projectPage": "https://snap-research.github.io/DenseDPO/"
    },
    "publishedAt": "2025-06-03T23:06:08.000Z",
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03106",
      "authors": [
        {
          "_id": "684104d9ee7646c073776b2e",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b2f",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b30",
          "user": {
            "_id": "666e91b1623133f1ce35acc5",
            "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
            "isPro": false,
            "fullname": "YipengZhang",
            "user": "YipengZhang",
            "type": "user"
          },
          "name": "Yipeng Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b31",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b32",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b33",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b34",
          "name": "Helen Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:39:02.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
      "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
      "upvotes": 5,
      "discussionId": "684104daee7646c073776b88",
      "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
      "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "large language models",
        "LLMs",
        "scalar rewards",
        "performance plateaus",
        "self-reflection",
        "persistent failures",
        "natural language feedback",
        "critiques",
        "policy optimization",
        "Qwen2.5-7B-Base",
        "Qwen3-8B-Base",
        "pass@1",
        "policy exploration",
        "entropy",
        "response length"
      ]
    },
    "publishedAt": "2025-06-03T13:39:02.000Z",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03099",
      "authors": [
        {
          "_id": "684107c6142b5c0b4226025f",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "684107c6142b5c0b42260260",
          "name": "Weimin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:29:28.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang ",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
      "upvotes": 5,
      "discussionId": "684107c8142b5c0b42260293",
      "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
      "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
      "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
      "ai_keywords": [
        "DiT",
        "audio large language model",
        "asymmetric knowledge distillation",
        "bidirectional teacher model",
        "sparse causal",
        "autoregressive student model",
        "inference pipeline",
        "CUDA streams",
        "frame-generation throughput"
      ]
    },
    "publishedAt": "2025-06-03T13:29:28.000Z",
    "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
    "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang ",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02592",
      "authors": [
        {
          "_id": "684104c89ec96d9991484c24",
          "user": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "isPro": false,
            "fullname": "Zhi-Yuan Chen",
            "user": "JaxChen",
            "type": "user"
          },
          "name": "Zhi-Yuan Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c25",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c26",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c27",
          "name": "Enrui Hu",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c28",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:12:47.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
      "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
      "submittedOnDailyBy": {
        "_id": "65309a1d657ae56cdb65e0e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
        "isPro": false,
        "fullname": "Zhi-Yuan Chen",
        "user": "JaxChen",
        "type": "user"
      },
      "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
      "upvotes": 5,
      "discussionId": "684104c99ec96d9991484c5e",
      "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
      "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
      "ai_keywords": [
        "large language models",
        "self-preference bias",
        "judge model",
        "gold judgments",
        "DBG score",
        "response quality",
        "attention-based perspective"
      ]
    },
    "publishedAt": "2025-06-03T04:12:47.000Z",
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65309a1d657ae56cdb65e0e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
      "fullname": "Zhi-Yuan Chen",
      "name": "JaxChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04141",
      "authors": [
        {
          "_id": "684106fc8cb0edba3ab212bb",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bc",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bd",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212be",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bf",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c0",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c1",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c2",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c3",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
      ],
      "publishedAt": "2025-06-04T16:33:41.000Z",
      "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
      "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
      "upvotes": 4,
      "discussionId": "684106ff8cb0edba3ab21374",
      "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
      "ai_keywords": [
        "multimodal large language models",
        "MMR-V",
        "long-range",
        "multi-frame reasoning",
        "multimodal reasoning",
        "manual annotation",
        "distractor annotation strategy",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-06-04T12:33:41.000Z",
    "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
    "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04108",
      "authors": [
        {
          "_id": "684104a16b106ae42f5acc1a",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1b",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1c",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1d",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1e",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1f",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc20",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc21",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc22",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:01:48.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
      "title": "Rectified Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6300ef4779c5ddbc6cf83e1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
        "isPro": false,
        "fullname": "Yutao Sun",
        "user": "sunyt32",
        "type": "user"
      },
      "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
      "upvotes": 4,
      "discussionId": "684104a26b106ae42f5acc50",
      "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
      "ai_keywords": [
        "sparse decoding",
        "KV cache misalignment",
        "Rectified Sparse Attention",
        "ReSA",
        "block-sparse attention",
        "dense rectification",
        "pretraining distribution",
        "long-context inference"
      ]
    },
    "publishedAt": "2025-06-04T12:01:48.000Z",
    "title": "Rectified Sparse Attention",
    "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6300ef4779c5ddbc6cf83e1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
      "fullname": "Yutao Sun",
      "name": "sunyt32",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03956",
      "authors": [
        {
          "_id": "6841396eee27975702b57e87",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e88",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e89",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8a",
          "name": "Chunhui Ding",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8b",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:46:33.000Z",
      "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
      "title": "Adapt before Continual Learning",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
      "upvotes": 4,
      "discussionId": "6841396eee27975702b57eb7",
      "projectPage": "https://github.com/byyx666/ACL_code",
      "githubRepo": "https://github.com/byyx666/ACL_code",
      "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
      "ai_keywords": [
        "Continual Learning",
        "CL",
        "Pre-trained models",
        "PTMs",
        "plasticity",
        "stability",
        "domain gaps",
        "catastrophic forgetting",
        "prompt tuning",
        "embeddings",
        "class prototypes"
      ]
    },
    "publishedAt": "2025-06-04T09:46:33.000Z",
    "title": "Adapt before Continual Learning",
    "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02945",
      "authors": [
        {
          "_id": "6841008f2f66f731bf010feb",
          "name": "Aishwarya Sahoo",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fec",
          "name": "Jeevana Kruthi Karnuthala",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fed",
          "name": "Tushar Parmanand Budhwani",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fee",
          "name": "Pranchal Agarwal",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fef",
          "name": "Sankaran Vaidyanathan",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff0",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff1",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff2",
          "name": "Jennifer Healey",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff3",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff4",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff5",
          "name": "Uttaran Bhattacharya",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff6",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:44:23.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
      "title": "Quantitative LLM Judges",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
      "upvotes": 4,
      "discussionId": "684100902f66f731bf01101e",
      "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "large language model",
        "quantitative LLM judges",
        "regression models",
        "score alignment",
        "predictive power",
        "post-hoc modeling"
      ]
    },
    "publishedAt": "2025-06-03T10:44:23.000Z",
    "title": "Quantitative LLM Judges",
    "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21541",
      "authors": [
        {
          "_id": "6840f79ceb249b555b244efc",
          "name": "Zitong Wang",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efd",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efe",
          "name": "Qianyu Zhou",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244eff",
          "name": "Xuequan Lu",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f00",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f01",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:08:04.000Z",
      "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
      "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
      "upvotes": 4,
      "discussionId": "6840f7a1eb249b555b244ffe",
      "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
      "ai_keywords": [
        "diffusion models",
        "diffusion Transformer",
        "posterior",
        "semantic prompts",
        "blending type",
        "In-Context Decomposition",
        "Layer Position Encoding Cloning",
        "AlphaBlend dataset",
        "translucent flare removal",
        "semi-transparent cell decomposition",
        "glassware decomposition",
        "LOGO dataset"
      ]
    },
    "publishedAt": "2025-05-24T12:08:04.000Z",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
    "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04225",
      "authors": [
        {
          "_id": "68413366adeec0116d071af2",
          "user": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "isPro": false,
            "fullname": "Tianyu Huang",
            "user": "tyhuang",
            "type": "user"
          },
          "name": "Tianyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af3",
          "name": "Wangguandong Zheng",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af4",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af5",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af6",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af7",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af8",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af9",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afa",
          "name": "Rynson W. H. Lau",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afb",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afc",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
      ],
      "publishedAt": "2025-06-04T17:59:04.000Z",
      "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
      "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
      "submittedOnDailyBy": {
        "_id": "63425d394c9a81858b36aeb5",
        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
        "isPro": false,
        "fullname": "Tianyu Huang",
        "user": "tyhuang",
        "type": "user"
      },
      "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
      "upvotes": 3,
      "discussionId": "6841336badeec0116d071c2b",
      "projectPage": "https://voyager-world.github.io",
      "githubRepo": "https://github.com/Voyager-World/Voyager",
      "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
      "ai_keywords": [
        "video diffusion",
        "world-consistent video diffusion",
        "3D point-cloud sequences",
        "camera path",
        "end-to-end scene generation",
        "consistent frames",
        "unified architecture",
        "RGB and depth video sequences",
        "world observation",
        "global coherence",
        "long-range world exploration",
        "world cache",
        "point culling",
        "auto-regressive inference",
        "smooth video sampling",
        "scene extension",
        "context-aware consistency",
        "scalable data engine",
        "camera pose estimation",
        "metric depth prediction",
        "large-scale",
        "diverse training data"
      ]
    },
    "publishedAt": "2025-06-04T13:59:04.000Z",
    "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
    "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63425d394c9a81858b36aeb5",
      "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
      "fullname": "Tianyu Huang",
      "name": "tyhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04142",
      "authors": [
        {
          "_id": "684132cb725b7fb67f68ffb8",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffb9",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffba",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbb",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbc",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbd",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:33:44.000Z",
      "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
      "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
      "upvotes": 3,
      "discussionId": "684132cc725b7fb67f68fff5",
      "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
      "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "trustworthy evaluation",
        "data contamination",
        "benchmarks",
        "dynamic benchmarks",
        "shortcut solutions",
        "shortcut neurons",
        "comparative analysis",
        "causal analysis",
        "shortcut neuron patching",
        "MixEval",
        "Spearman coefficient"
      ]
    },
    "publishedAt": "2025-06-04T12:33:44.000Z",
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03930",
      "authors": [
        {
          "_id": "6841090145662bb7d322ecc6",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc7",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc8",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc9",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecca",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:24:44.000Z",
      "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
      "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
      "upvotes": 3,
      "discussionId": "6841090245662bb7d322ed1f",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
      "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "visualization tasks",
        "plot generation",
        "execution-grounded supervision",
        "iterative code correction",
        "VisCode-200K",
        "Python-based visualization",
        "validated plotting code",
        "natural language instructions",
        "rendered plots",
        "correction dialogues",
        "Qwen2.5-Coder-Instruct",
        "VisCoder",
        "PandasPlotBench",
        "self-debug evaluation",
        "feedback-driven learning"
      ]
    },
    "publishedAt": "2025-06-04T09:24:44.000Z",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03448",
      "authors": [
        {
          "_id": "684105479060432bf302b432",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b433",
          "user": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "isPro": false,
            "fullname": "Maitreya Patel",
            "user": "mpatel57",
            "type": "user"
          },
          "name": "Maitreya Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b434",
          "name": "Shivam Singh",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b435",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b436",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T23:20:24.000Z",
      "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
      "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
      "submittedOnDailyBy": {
        "_id": "622d2ff38d04fd29a9ccf1a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
        "isPro": false,
        "fullname": "Maitreya Patel",
        "user": "mpatel57",
        "type": "user"
      },
      "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
      "upvotes": 3,
      "discussionId": "6841054b9060432bf302b559",
      "projectPage": "https://refedit.vercel.app",
      "githubRepo": "https://github.com/bimsarapathiraja/refedit",
      "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
      "ai_keywords": [
        "RefEdit-Bench",
        "RefCOCO",
        "instruction-based editing model",
        "scalable synthetic data generation pipeline",
        "Flux/SD3",
        "state-of-the-art results"
      ]
    },
    "publishedAt": "2025-06-03T19:20:24.000Z",
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
    "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622d2ff38d04fd29a9ccf1a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
      "fullname": "Maitreya Patel",
      "name": "mpatel57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00482",
      "authors": [
        {
          "_id": "68402986a50b67f983749710",
          "user": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "isPro": false,
            "fullname": "Eunsu Kim",
            "user": "EunsuKim",
            "type": "user"
          },
          "name": "Eunsu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749711",
          "name": "Haneul Yoo",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749712",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749713",
          "name": "Hitesh Patel",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749714",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749715",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T09:24:32.000Z",
      "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
      "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
      "upvotes": 3,
      "discussionId": "68402987a50b67f983749746",
      "projectPage": "https://huggingface.co/BenchHub",
      "githubRepo": "https://github.com/rladmstn1714/BenchHub",
      "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "BenchHub",
        "benchmark repository",
        "domain-specific models",
        "benchmark datasets",
        "continuous updates",
        "scalable data management",
        "model performance",
        "domain-aware benchmarking"
      ]
    },
    "publishedAt": "2025-05-31T05:24:32.000Z",
    "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
    "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23807",
      "authors": [
        {
          "_id": "683fec0a9f37285365be6142",
          "user": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "isPro": false,
            "fullname": "Yuli chen",
            "user": "yulichen",
            "type": "user"
          },
          "name": "Yuli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6143",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6144",
          "name": "Jiale Han",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6145",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6146",
          "name": "Yingting Li",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6147",
          "name": "Shuhao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T07:35:00.000Z",
      "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
      "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "656201912d309fa7e27ddf40",
        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
        "isPro": false,
        "fullname": "Yuli chen",
        "user": "yulichen",
        "type": "user"
      },
      "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
      "upvotes": 3,
      "discussionId": "683fec0a9f37285365be617f",
      "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
      "ai_keywords": [
        "pruning",
        "Large Language Models (LLMs)",
        "uniform layerwise pruning",
        "non-uniform layerwise pruning",
        "Dynamic Layerwise Pruning (DLP)",
        "perplexity",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    "publishedAt": "2025-05-27T03:35:00.000Z",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656201912d309fa7e27ddf40",
      "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
      "fullname": "Yuli chen",
      "name": "yulichen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04133",
      "authors": [
        {
          "_id": "6840f32dda736de98e843831",
          "user": {
            "_id": "64d3c16a0553a2522f1aa792",
            "avatarUrl": "/avatars/951e272ffccf2388f138b248e5ef7142.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "shainar",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T01:39:24.184Z",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843832",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843833",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843834",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
      ],
      "publishedAt": "2025-06-04T16:26:11.000Z",
      "submittedOnDailyAt": "2025-06-05T00:02:27.010Z",
      "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
      "upvotes": 2,
      "discussionId": "6840f32eda736de98e843858",
      "ai_summary": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.",
      "ai_keywords": [
        "LLMs",
        "agentic AI",
        "multi-agent systems",
        "TRiSM",
        "governance",
        "explainability",
        "ModelOps",
        "privacy",
        "security",
        "encryption",
        "adversarial defense",
        "compliance",
        "AI regulations",
        "trust-building mechanisms",
        "transparency",
        "oversight",
        "interpretability",
        "human-centered performance",
        "benchmarking",
        "responsible AI",
        "research directions"
      ]
    },
    "publishedAt": "2025-06-04T12:26:11.000Z",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04034",
      "authors": [
        {
          "_id": "6840ff0b535bfb4942b31576",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31577",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31578",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31579",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b3157a",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
      ],
      "publishedAt": "2025-06-04T14:56:57.000Z",
      "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
      "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
      "submittedOnDailyBy": {
        "_id": "647f46b6838ac3601fc89852",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
        "isPro": true,
        "fullname": "Qing Jiang",
        "user": "Mountchicken",
        "type": "user"
      },
      "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
      "upvotes": 2,
      "discussionId": "6840ff0e535bfb4942b3165f",
      "projectPage": "https://rexthinker.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
      "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
      "ai_keywords": [
        "CoT reasoning",
        "HumanRef-CoT",
        "GPT-4o",
        "structured reasoning",
        "cold-start supervised fine-tuning",
        "GRPO-based RL learning"
      ]
    },
    "publishedAt": "2025-06-04T10:56:57.000Z",
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f46b6838ac3601fc89852",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
      "fullname": "Qing Jiang",
      "name": "Mountchicken",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03614",
      "authors": [
        {
          "_id": "684134ca20ff8abcccb11302",
          "name": "Zhanhui Zhou",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11303",
          "name": "Lingjie Chen",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11304",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11305",
          "name": "Chaochao Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T06:46:06.000Z",
      "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
      "title": "VLMs Can Aggregate Scattered Training Patches",
      "submittedOnDailyBy": {
        "_id": "642e5a7ba0b65dce1f87a7a2",
        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
        "isPro": false,
        "fullname": "Zhanhui Zhou",
        "user": "ZHZisZZ",
        "type": "user"
      },
      "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
      "upvotes": 1,
      "discussionId": "684134cb20ff8abcccb11334",
      "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
      "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "visual stitching",
        "data moderation",
        "adversarial data poisoning",
        "image patches",
        "textual descriptions",
        "inference"
      ]
    },
    "publishedAt": "2025-06-04T02:46:06.000Z",
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e5a7ba0b65dce1f87a7a2",
      "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
      "fullname": "Zhanhui Zhou",
      "name": "ZHZisZZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02294",
      "authors": [
        {
          "_id": "6840cd169241913d43af9d28",
          "name": "Niclas Popp",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d29",
          "name": "Kevin Alexander Laube",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2a",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2b",
          "name": "Lukas Schott",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T22:15:59.000Z",
      "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
      "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
      "submittedOnDailyBy": {
        "_id": "655646baf8a2d3c020546ec8",
        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
        "isPro": false,
        "fullname": "Niclas P",
        "user": "NPBP26",
        "type": "user"
      },
      "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
      "upvotes": 1,
      "discussionId": "6840cd199241913d43af9dac",
      "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
      "ai_keywords": [
        "knowledge distillation",
        "diffusion-based data augmentation",
        "covariate shift",
        "teacher-student model",
        "CelebA",
        "SpuCo Birds",
        "spurious ImageNet",
        "mean group accuracy",
        "worst group accuracy",
        "spurious mAUC"
      ]
    },
    "publishedAt": "2025-06-02T18:15:59.000Z",
    "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
    "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655646baf8a2d3c020546ec8",
      "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
      "fullname": "Niclas P",
      "name": "NPBP26",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01344",
      "authors": [
        {
          "_id": "6841009bdf863485e04879c8",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879c9",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ca",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cb",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cc",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cd",
          "name": "Vivek Gupta",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ce",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T06:02:41.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
      "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
      "upvotes": 1,
      "discussionId": "6841009ddf863485e0487a38",
      "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
      "ai_keywords": [
        "Flowcharts",
        "Fine-grained Flowchart Attribution",
        "FlowPathAgent",
        "graph-based reasoning",
        "symbolic graph",
        "neurosymbolic agent",
        "flowExplainBench",
        "flowchart QA"
      ]
    },
    "publishedAt": "2025-06-02T02:02:41.000Z",
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
    "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  }
]