[
    {
        "paper": {
            "id": "2410.04199",
            "authors": [
                {
                    "_id": "6705ecde7583ffc1e02f01f8",
                    "user": {
                        "_id": "63024676056ec3a2a8714b24",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Liu",
                        "user": "Dominic789654",
                        "type": "user"
                    },
                    "name": "Xiang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T07:37:24.402Z",
                    "hidden": false
                },
                {
                    "_id": "6705ecde7583ffc1e02f01f9",
                    "name": "Peijie Dong",
                    "hidden": false
                },
                {
                    "_id": "6705ecde7583ffc1e02f01fa",
                    "user": {
                        "_id": "66545bbcc2e9c65dcc1ee57b",
                        "avatarUrl": "/avatars/120834f04f86e2f7be33c9eb6d99b1fa.svg",
                        "isPro": false,
                        "fullname": "minghui",
                        "user": "xuminghui",
                        "type": "user"
                    },
                    "name": "Xuming Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T08:59:09.897Z",
                    "hidden": false
                },
                {
                    "_id": "6705ecde7583ffc1e02f01fb",
                    "user": {
                        "_id": "6676935fcd0b89a0115174b0",
                        "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
                        "isPro": false,
                        "fullname": "Xiaowen Chu",
                        "user": "wenxinsiju",
                        "type": "user"
                    },
                    "name": "Xiaowen Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T08:59:03.917Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-05T15:33:25.000Z",
            "title": "LongGenBench: Long-context Generation Benchmark",
            "summary": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models.",
            "upvotes": 10,
            "discussionId": "6705ecdf7583ffc1e02f0257"
        },
        "publishedAt": "2024-10-09T01:11:06.253Z",
        "title": "LongGenBench: Long-context Generation Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04199.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "fullname": "Xiang Liu",
            "name": "Dominic789654",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.04717",
            "authors": [
                {
                    "_id": "6705e93f01c631bb8a88a9f7",
                    "user": {
                        "_id": "642b8add48f67b6f21d4eb20",
                        "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
                        "isPro": false,
                        "fullname": "Dylan",
                        "user": "shizhuo2",
                        "type": "user"
                    },
                    "name": "Dylan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T07:37:26.824Z",
                    "hidden": false
                },
                {
                    "_id": "6705e93f01c631bb8a88a9f8",
                    "name": "Justin Wang",
                    "hidden": false
                },
                {
                    "_id": "6705e93f01c631bb8a88a9f9",
                    "name": "Francois Charton",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T03:15:11.000Z",
            "title": "Only-IF:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization",
            "summary": "Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\nonly emerges when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$textbf{specialist} and textbf{generalist}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.",
            "upvotes": 9,
            "discussionId": "6705e94001c631bb8a88aa32"
        },
        "publishedAt": "2024-10-09T00:56:19.082Z",
        "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04717.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
            "fullname": "Dylan",
            "name": "shizhuo2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.01912",
            "authors": [
                {
                    "_id": "67052598e2d3403eaf1fc3bc",
                    "user": {
                        "_id": "61b0a4ce1b3d95b3d1ed9251",
                        "avatarUrl": "/avatars/b5f8c68801829b5653ee1d55244dbe16.svg",
                        "isPro": false,
                        "fullname": "Liang Chen",
                        "user": "leonardPKU",
                        "type": "user"
                    },
                    "name": "Liang Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-08T12:29:20.810Z",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3bd",
                    "name": "Sinan Tan",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3be",
                    "user": {
                        "_id": "64b15284372d4340772a3dca",
                        "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
                        "isPro": false,
                        "fullname": "Zefan Cai",
                        "user": "ZefanCai",
                        "type": "user"
                    },
                    "name": "Zefan Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T11:21:21.768Z",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3bf",
                    "name": "Weichu Xie",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c0",
                    "name": "Haozhe Zhao",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c1",
                    "name": "Yichi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c2",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T11:43:10.132Z",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c3",
                    "user": {
                        "_id": "60113fad51e116b62cd0a30e",
                        "avatarUrl": "/avatars/469357d0a4a5d2e104ae5e32801b395d.svg",
                        "isPro": false,
                        "fullname": "Jinze Bai",
                        "user": "Jinze",
                        "type": "user"
                    },
                    "name": "Jinze Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T11:43:20.221Z",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c4",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67052598e2d3403eaf1fc3c5",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T18:10:05.000Z",
            "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive\n  Transformer for Efficient Finegrained Image Generation",
            "summary": "This work tackles the information loss bottleneck of vector-quantization (VQ)\nautoregressive image generation by introducing a novel model architecture\ncalled the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer\npredicts more codes for an image by introducing a new autoregression direction,\nmodel depth, along with the sequence length direction. Compared to\ntraditional 1D autoregression and previous work utilizing similar 2D image\ndecomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end\nmodel that can generate higher quality images with the same backbone model size\nand sequence length, opening a new optimization perspective for autoregressive\nimage generation. Furthermore, our experiments reveal that the\nDnD-Transformer's potential extends beyond generating natural images. It can\neven generate images with rich text and graphical elements in a self-supervised\nmanner, demonstrating an understanding of these combined modalities. This has\nnot been previously demonstrated for popular vision generative models such as\ndiffusion models, showing a spark of vision-language intelligence when trained\nsolely on images. Code, datasets and models are open at\nhttps://github.com/chenllliang/DnD-Transformer.",
            "upvotes": 8,
            "discussionId": "670525a0e2d3403eaf1fc73c"
        },
        "publishedAt": "2024-10-09T06:40:12.133Z",
        "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/GWGFvMPxqHrNRsmDEV2O1.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/ZBJU7b9u_nhJlxdxJ7CVC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/bzIdeCcL7kYabNfLVILOz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/sxH7AqIemv_iNk45wR9EP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/eS1m39rBj2VTb_KJPQvUN.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/MKv3q1GrciZD0sbqXBSdh.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61b0a4ce1b3d95b3d1ed9251/8IE1cbjRSlKMde79e1QyB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01912.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b5f8c68801829b5653ee1d55244dbe16.svg",
            "fullname": "Liang Chen",
            "name": "leonardPKU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05193",
            "authors": [
                {
                    "_id": "6704e69412e582928046c613",
                    "user": {
                        "_id": "62a42f22c683d02f5b63320c",
                        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
                        "isPro": false,
                        "fullname": "Qiyuan Zhang",
                        "user": "DonJoey",
                        "type": "user"
                    },
                    "name": "Qiyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-08T15:16:43.381Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c614",
                    "user": {
                        "_id": "667c1c0b18aae25ed39b96c1",
                        "avatarUrl": "/avatars/3513e7ae5d2cc8767e09893605123ee2.svg",
                        "isPro": false,
                        "fullname": "Yufei Wang",
                        "user": "yufeiwang201217a",
                        "type": "user"
                    },
                    "name": "Yufei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T09:17:57.837Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c615",
                    "user": {
                        "_id": "61bd88aa94bd578f7286181a",
                        "avatarUrl": "/avatars/0f40dfd7accc558f9201103e3e4ec007.svg",
                        "isPro": false,
                        "fullname": "Tiezheng Yu",
                        "user": "Tiezheng",
                        "type": "user"
                    },
                    "name": "Tiezheng YU",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T09:18:04.918Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c616",
                    "user": {
                        "_id": "63c20105726f62e411fbe882",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
                        "isPro": false,
                        "fullname": "Yuxin Jiang",
                        "user": "YuxinJiang",
                        "type": "user"
                    },
                    "name": "Yuxin Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T09:18:38.205Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c617",
                    "name": "Chuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c618",
                    "name": "Liangyou Li",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c619",
                    "name": "Yasheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c61a",
                    "name": "Xin Jiang",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c61b",
                    "user": {
                        "_id": "655b1360c11dee7f7e7cf794",
                        "avatarUrl": "/avatars/efb4b91e9bb8ab531331c8e4296f754c.svg",
                        "isPro": false,
                        "fullname": "lifengshang",
                        "user": "lifengshang",
                        "type": "user"
                    },
                    "name": "Lifeng Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T09:20:33.974Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c61c",
                    "name": "Ruiming Tang",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c61d",
                    "user": {
                        "_id": "65d2bb5c6130ef7be012d235",
                        "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
                        "isPro": false,
                        "fullname": "Fuyuan Lyu",
                        "user": "silentspring2",
                        "type": "user"
                    },
                    "name": "Fuyuan Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T12:54:01.875Z",
                    "hidden": false
                },
                {
                    "_id": "6704e69412e582928046c61e",
                    "name": "Chen Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T16:50:47.000Z",
            "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
            "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
            "upvotes": 7,
            "discussionId": "6704e69512e582928046c644"
        },
        "publishedAt": "2024-10-09T00:03:21.715Z",
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05193.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "fullname": "Qiyuan Zhang",
            "name": "DonJoey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03290",
            "authors": [
                {
                    "_id": "6703aeb7400ad7197be3fcdf",
                    "user": {
                        "_id": "63fee47352441fe3e87b5088",
                        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
                        "isPro": false,
                        "fullname": "WANG HAIBO",
                        "user": "WHB139426",
                        "type": "user"
                    },
                    "name": "Haibo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T09:08:43.190Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce0",
                    "user": {
                        "_id": "64b6c686cf5117d7962d8f62",
                        "avatarUrl": "/avatars/96ed7a9602aa4c21b3a3d89608e76dc8.svg",
                        "isPro": false,
                        "fullname": "Zhiyang Xu",
                        "user": "Zhiyang03",
                        "type": "user"
                    },
                    "name": "Zhiyang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T13:07:21.689Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce1",
                    "user": {
                        "_id": "67017abfe4d49b157ac534d9",
                        "avatarUrl": "/avatars/997e1b9f54b27a7728a9d4abfee4ba91.svg",
                        "isPro": false,
                        "fullname": "Yu Cheng",
                        "user": "ych133",
                        "type": "user"
                    },
                    "name": "Yu Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T16:26:25.964Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce2",
                    "user": {
                        "_id": "633bd54b00732349209a18fe",
                        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                        "isPro": false,
                        "fullname": "Shizhe Diao",
                        "user": "shizhediao",
                        "type": "user"
                    },
                    "name": "Shizhe Diao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T13:07:42.547Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce3",
                    "user": {
                        "_id": "635c2c2a7a165601151d3f85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666984965757-noauth.png",
                        "isPro": false,
                        "fullname": "Yufan Zhou",
                        "user": "YfZ",
                        "type": "user"
                    },
                    "name": "Yufan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T13:08:13.416Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce4",
                    "name": "Yixin Cao",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce5",
                    "user": {
                        "_id": "64b65c84c991daa61404abd7",
                        "avatarUrl": "/avatars/3592aa8774ea4e2836099e0701b4b78d.svg",
                        "isPro": false,
                        "fullname": "WangQifan",
                        "user": "Bayees",
                        "type": "user"
                    },
                    "name": "Qifan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T13:09:34.090Z",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce6",
                    "name": "Weifeng Ge",
                    "hidden": false
                },
                {
                    "_id": "6703aeb7400ad7197be3fce7",
                    "name": "Lifu Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-04T10:04:37.000Z",
            "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video\n  Large Language Models",
            "summary": "Video Large Language Models (Video-LLMs) have demonstrated remarkable\ncapabilities in coarse-grained video understanding, however, they struggle with\nfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,\na novel Video-LLM adept at perceiving and reasoning over specific video moments\nin a fine-grained manner. We identify that current Video-LLMs have limitations\nfor fine-grained video understanding since they lack effective temporal\nmodeling and timestamp representation. In light of this, we sharpen our model\nby incorporating (1) an additional temporal stream to encode the relationships\nbetween frames and (2) discrete temporal tokens enriched with specific time\nknowledge to represent timestamps. To optimize the training of\nGrounded-VideoLLM, we employ a multi-stage training scheme, beginning with\nsimple video-captioning tasks and progressively introducing video temporal\ngrounding tasks of increasing complexity. To further enhance\nGrounded-VideoLLM's temporal reasoning capability, we also curate a grounded\nVideoQA dataset by an automatic annotation pipeline. Extensive experiments\ndemonstrate that Grounded-VideoLLM not only excels in fine-grained grounding\ntasks such as temporal sentence grounding, dense video captioning, and grounded\nVideoQA, but also shows great potential as a versatile video assistant for\ngeneral video understanding.",
            "upvotes": 5,
            "discussionId": "6703aeb9400ad7197be3fd5e"
        },
        "publishedAt": "2024-10-09T07:40:30.699Z",
        "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03290.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "fullname": "WANG HAIBO",
            "name": "WHB139426",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02705",
            "authors": [
                {
                    "_id": "670635fa5178a313ef69ba21",
                    "name": "Zongming Li",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba22",
                    "user": {
                        "_id": "646b3db131968a60a01e4cf5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                        "isPro": true,
                        "fullname": "Tianheng Cheng",
                        "user": "wondervictor",
                        "type": "user"
                    },
                    "name": "Tianheng Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:54:40.586Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba23",
                    "user": {
                        "_id": "6412a33900634c4fe9873652",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
                        "isPro": false,
                        "fullname": "Shoufa Chen",
                        "user": "ShoufaChen",
                        "type": "user"
                    },
                    "name": "Shoufa Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:54:47.669Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba24",
                    "user": {
                        "_id": "640dc9bf8512ec51d7f0ac1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dc9bf8512ec51d7f0ac1a/sT4rdEoQbzfW6D3xDVdqt.jpeg",
                        "isPro": false,
                        "fullname": "peizesun",
                        "user": "peizesun",
                        "type": "user"
                    },
                    "name": "Peize Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:54:55.199Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba25",
                    "user": {
                        "_id": "66504b391ca90edc544280d2",
                        "avatarUrl": "/avatars/81ac0cab23802346ae154ca5918f64cc.svg",
                        "isPro": false,
                        "fullname": "Haocheng Shen",
                        "user": "HCKodiak",
                        "type": "user"
                    },
                    "name": "Haocheng Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:55:02.914Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba26",
                    "name": "Longjin Ran",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba27",
                    "user": {
                        "_id": "65389a669c474315d7425f96",
                        "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
                        "isPro": false,
                        "fullname": "chenxiaoxin",
                        "user": "steelozazala",
                        "type": "user"
                    },
                    "name": "Xiaoxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:55:23.910Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba28",
                    "user": {
                        "_id": "66c2e7fc934e2f07753542ac",
                        "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
                        "isPro": false,
                        "fullname": "LWY",
                        "user": "wenyuliu",
                        "type": "user"
                    },
                    "name": "Wenyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:55:29.707Z",
                    "hidden": false
                },
                {
                    "_id": "670635fa5178a313ef69ba29",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:55:35.614Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:28:07.000Z",
            "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
            "summary": "Autoregressive (AR) models have reformulated image generation as next-token\nprediction, demonstrating remarkable potential and emerging as strong\ncompetitors to diffusion models. However, control-to-image generation, akin to\nControlNet, remains largely unexplored within AR models. Although a natural\napproach, inspired by advancements in Large Language Models, is to tokenize\ncontrol images into tokens and prefill them into the autoregressive model\nbefore decoding image tokens, it still falls short in generation quality\ncompared to ControlNet and suffers from inefficiency. To this end, we introduce\nControlAR, an efficient and effective framework for integrating spatial\ncontrols into autoregressive image generation models. Firstly, we explore\ncontrol encoding for AR models and propose a lightweight control encoder to\ntransform spatial inputs (e.g., canny edges or depth maps) into control tokens.\nThen ControlAR exploits the conditional decoding method to generate the next\nimage token conditioned on the per-token fusion between control and image\ntokens, similar to positional encodings. Compared to prefilling tokens, using\nconditional decoding significantly strengthens the control capability of AR\nmodels but also maintains the model's efficiency. Furthermore, the proposed\nControlAR surprisingly empowers AR models with arbitrary-resolution image\ngeneration via conditional decoding and specific controls. Extensive\nexperiments can demonstrate the controllability of the proposed ControlAR for\nthe autoregressive control-to-image generation across diverse inputs, including\nedges, depths, and segmentation masks. Furthermore, both quantitative and\nqualitative results indicate that ControlAR surpasses previous state-of-the-art\ncontrollable diffusion models, e.g., ControlNet++. Code, models, and demo will\nsoon be available at https://github.com/hustvl/ControlAR.",
            "upvotes": 4,
            "discussionId": "670636005178a313ef69bd70"
        },
        "publishedAt": "2024-10-09T06:24:02.189Z",
        "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02705.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "fullname": "Tianheng Cheng",
            "name": "wondervictor",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.04422",
            "authors": [
                {
                    "_id": "6706260a18b86c7e905baa92",
                    "user": {
                        "_id": "6374c494958cd71fa7ea0a9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png",
                        "isPro": false,
                        "fullname": "yuyijiong",
                        "user": "yuyijiong",
                        "type": "user"
                    },
                    "name": "Yijiong Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T07:37:08.228Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-06T09:29:19.000Z",
            "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
            "summary": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.",
            "upvotes": 4,
            "discussionId": "6706260b18b86c7e905bab0e"
        },
        "publishedAt": "2024-10-09T06:14:32.227Z",
        "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04422.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png",
            "fullname": "yuyijiong",
            "name": "yuyijiong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02743",
            "authors": [
                {
                    "_id": "6705ff67f6d7020cce5daaab",
                    "user": {
                        "_id": "626a6ce8adf559c88de9ace0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651142275247-626a6ce8adf559c88de9ace0.jpeg",
                        "isPro": false,
                        "fullname": "CYK",
                        "user": "cyk1337",
                        "type": "user"
                    },
                    "name": "Yekun Chai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T13:32:24.626Z",
                    "hidden": false
                },
                {
                    "_id": "6705ff67f6d7020cce5daaac",
                    "name": "Haoran Sun",
                    "hidden": false
                },
                {
                    "_id": "6705ff67f6d7020cce5daaad",
                    "name": "Huang Fang",
                    "hidden": false
                },
                {
                    "_id": "6705ff67f6d7020cce5daaae",
                    "user": {
                        "_id": "63202a4af7db36538c9fe3ba",
                        "avatarUrl": "/avatars/d270cfb9c22183e4edc3a91bd12ce66a.svg",
                        "isPro": false,
                        "fullname": "Shuohuan Wang",
                        "user": "Fyzal",
                        "type": "user"
                    },
                    "name": "Shuohuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T12:58:15.521Z",
                    "hidden": false
                },
                {
                    "_id": "6705ff67f6d7020cce5daaaf",
                    "name": "Yu Sun",
                    "hidden": false
                },
                {
                    "_id": "6705ff67f6d7020cce5daab0",
                    "name": "Hua Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:55:13.000Z",
            "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
            "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .",
            "upvotes": 4,
            "discussionId": "6705ff68f6d7020cce5dab15"
        },
        "publishedAt": "2024-10-09T02:29:33.851Z",
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02743.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651142275247-626a6ce8adf559c88de9ace0.jpeg",
            "fullname": "CYK",
            "name": "cyk1337",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03399",
            "authors": [
                {
                    "_id": "670673a010de8dee36ac7069",
                    "name": "Dmitry Osin",
                    "hidden": false
                },
                {
                    "_id": "670673a010de8dee36ac706a",
                    "name": "Igor Udovichenko",
                    "hidden": false
                },
                {
                    "_id": "670673a010de8dee36ac706b",
                    "user": {
                        "_id": "63bbfd74141c7d395c471768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Viktor Moskvoretskii",
                        "user": "VityaVitalich",
                        "type": "user"
                    },
                    "name": "Viktor Moskvoretskii",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-09T13:12:12.389Z",
                    "hidden": false
                },
                {
                    "_id": "670673a010de8dee36ac706c",
                    "user": {
                        "_id": "65afde6ba0b4bf3b0e95b4e8",
                        "avatarUrl": "/avatars/e9b97040b0a619bf6609465d1678705c.svg",
                        "isPro": false,
                        "fullname": "Egor Shvetsov",
                        "user": "dalime",
                        "type": "user"
                    },
                    "name": "Egor Shvetsov",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-09T12:14:25.738Z",
                    "hidden": false
                },
                {
                    "_id": "670673a010de8dee36ac706d",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-04T13:03:43.000Z",
            "title": "EBES: Easy Benchmarking for Event Sequences",
            "summary": "Event sequences, characterized by irregular sampling intervals and a mix of\ncategorical and numerical features, are common data structures in various\nreal-world domains such as healthcare, finance, and user interaction logs.\nDespite advances in temporal data modeling techniques, there is no standardized\nbenchmarks for evaluating their performance on event sequences. This\ncomplicates result comparison across different papers due to varying evaluation\nprotocols, potentially misleading progress in this field. We introduce EBES, a\ncomprehensive benchmarking tool with standardized evaluation scenarios and\nprotocols, focusing on regression and classification problems with\nsequence-level targets. Our library simplifies benchmarking, dataset addition,\nand method integration through a unified interface. It includes a novel\nsynthetic dataset and provides preprocessed real-world datasets, including the\nlargest publicly available banking dataset. Our results provide an in-depth\nanalysis of datasets, identifying some as unsuitable for model comparison. We\ninvestigate the importance of modeling temporal and sequential components, as\nwell as the robustness and scaling properties of the models. These findings\nhighlight potential directions for future research. Our benchmark aim is to\nfacilitate reproducible research, expediting progress and increasing real-world\nimpacts.",
            "upvotes": 2,
            "discussionId": "670673a110de8dee36ac708e"
        },
        "publishedAt": "2024-10-09T10:45:38.936Z",
        "title": "EBES: Easy Benchmarking for Event Sequences",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65afde6ba0b4bf3b0e95b4e8/YjkOpb4u92aiuYiHEzvw7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03399.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e9b97040b0a619bf6609465d1678705c.svg",
            "fullname": "Egor Shvetsov",
            "name": "dalime",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05076",
            "authors": [
                {
                    "_id": "6706ad084c73cea935952f99",
                    "name": "Lijie Yang",
                    "hidden": false
                },
                {
                    "_id": "6706ad084c73cea935952f9a",
                    "user": {
                        "_id": "6480ec2d856901b0edbbd47a",
                        "avatarUrl": "/avatars/dd8c22589c46fc19d7bf4a1329692d91.svg",
                        "isPro": false,
                        "fullname": "Zhihao Jia",
                        "user": "zhihaojia",
                        "type": "user"
                    },
                    "name": "Zhihao Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-09T16:19:21.988Z",
                    "hidden": false
                },
                {
                    "_id": "6706ad084c73cea935952f9b",
                    "name": "Zhuofu Chen",
                    "hidden": false
                },
                {
                    "_id": "6706ad084c73cea935952f9c",
                    "name": "Zikun Li",
                    "hidden": false
                },
                {
                    "_id": "6706ad084c73cea935952f9d",
                    "name": "Zhihao Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T14:30:27.000Z",
            "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
            "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
            "upvotes": 1,
            "discussionId": "6706ad094c73cea93595300a"
        },
        "publishedAt": "2024-10-09T14:56:52.982Z",
        "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05076.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ae2ac17a08566186be99fd72d356da3b.svg",
            "fullname": "Zhihao Zhang",
            "name": "JackFram",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]