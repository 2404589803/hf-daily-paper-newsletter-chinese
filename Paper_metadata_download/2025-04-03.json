[
  {
    "paper": {
      "id": "2504.00999",
      "authors": [
        {
          "_id": "67ecc3973d267d266649e075",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e076",
          "name": "Luyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e077",
          "name": "Zedong Wang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e078",
          "name": "Juanxi Tian",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e079",
          "name": "Cheng Tan",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07a",
          "name": "Zicheng Liu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07b",
          "name": "Chang Yu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07c",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07d",
          "name": "Haonan Lu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07e",
          "name": "Haoqian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07f",
          "name": "Zhen Lei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:39:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:15:32.614Z",
      "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
      "submittedOnDailyBy": {
        "_id": "670880950e79a8b46f7ff9dd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
        "isPro": false,
        "fullname": "Juanxi Tian",
        "user": "Juanxi",
        "type": "user"
      },
      "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
      "upvotes": 46,
      "discussionId": "67ecc3993d267d266649e10c",
      "projectPage": "https://apexgen-x.github.io/MergeVQ/",
      "githubRepo": "https://github.com/ApexGen-X/MergeVQ",
      "ai_keywords": [
        "Masked Image Modeling (MIM)",
        "Vector Quantization (VQ)",
        "shared latent space",
        "generation quality",
        "representation learning",
        "token merging",
        "generative models",
        "token merge module",
        "self-attention blocks",
        "encoder",
        "Look-up Free Quantization (LFQ)",
        "global alignment",
        "cross-attention",
        "decoder",
        "reconstruction",
        "MergeAR",
        "KV Cache compression",
        "raster-order prediction",
        "AR generative model",
        "ImageNet",
        "token efficiency",
        "inference speed"
      ]
    },
    "publishedAt": "2025-04-01T13:39:19.000Z",
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
    "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00999.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "670880950e79a8b46f7ff9dd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
      "fullname": "Juanxi Tian",
      "name": "Juanxi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00883",
      "authors": [
        {
          "_id": "67edf28e042e8ba3e95d1960",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1961",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1962",
          "name": "Yanhao Zhang",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1963",
          "name": "Zijian Kong",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1964",
          "name": "Haonan Lu",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1965",
          "name": "Zhenyu Yang",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1966",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:11:11.000Z",
      "submittedOnDailyAt": "2025-04-03T01:03:18.798Z",
      "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
      "submittedOnDailyBy": {
        "_id": "64bba541da140e461924dfed",
        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
        "isPro": false,
        "fullname": "zhijie deng",
        "user": "zhijie3",
        "type": "user"
      },
      "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
      "upvotes": 38,
      "discussionId": "67edf28f042e8ba3e95d1a60",
      "githubRepo": "https://github.com/zhijie-group/R1-Zero-VSI",
      "ai_keywords": [
        "multi-modal large language models (MLLMs)",
        "video-based visual-spatial intelligence (VSI)",
        "Chain of Thought (CoT)",
        "GRPO training",
        "VSI-100k dataset",
        "DeepSeek-R1-Zero",
        "KL penalty",
        "vsGRPO-2B model",
        "Qwen2-VL-2B",
        "vsGRPO-7B model",
        "Qwen2-VL-7B",
        "LLaVA-NeXT-Video-72B",
        "supervised fine-tuning",
        "direct preference optimization"
      ]
    },
    "publishedAt": "2025-04-01T11:11:11.000Z",
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00883.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01014",
      "authors": [
        {
          "_id": "67eca389e14049f5ff064ea6",
          "user": {
            "_id": "6506b77a773ceaa8d52ecea1",
            "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
            "isPro": false,
            "fullname": "CJH",
            "user": "Howe666",
            "type": "user"
          },
          "name": "Junhao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:27.228Z",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea7",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea8",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea9",
          "name": "Jing Liao",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064eaa",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:57:18.000Z",
      "submittedOnDailyAt": "2025-04-03T01:15:35.152Z",
      "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
      "submittedOnDailyBy": {
        "_id": "6506b77a773ceaa8d52ecea1",
        "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
        "isPro": false,
        "fullname": "CJH",
        "user": "Howe666",
        "type": "user"
      },
      "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
      "upvotes": 20,
      "discussionId": "67eca39ce14049f5ff06535b",
      "projectPage": "https://howe125.github.io/AnimeGamer.github.io/",
      "githubRepo": "https://github.com/TencentARC/AnimeGamer",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "video diffusion model",
        "action-aware multimodal representations",
        "automated metrics",
        "human evaluations"
      ]
    },
    "publishedAt": "2025-04-01T13:57:18.000Z",
    "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
    "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01014.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6506b77a773ceaa8d52ecea1",
      "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
      "fullname": "CJH",
      "name": "Howe666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20783",
      "authors": [
        {
          "_id": "67e97f581cb6fc648f642a05",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a06",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a07",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a08",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a09",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0b",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0c",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:14.000Z",
      "submittedOnDailyAt": "2025-04-03T03:47:54.547Z",
      "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
      "submittedOnDailyBy": {
        "_id": "65f5392c68b8e0cb3c9977a2",
        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
        "isPro": false,
        "fullname": "Zichen",
        "user": "lkevinzc",
        "type": "user"
      },
      "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
      "upvotes": 18,
      "discussionId": "67e97f591cb6fc648f642a38",
      "githubRepo": "https://github.com/sail-sg/understand-r1-zero",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning capabilities",
        "LLMs",
        "base models",
        "DeepSeek-V3-Base",
        "pretraining characteristics",
        "Qwen2.5",
        "prompt templates",
        "pretraining biases",
        "Group Relative Policy Optimization (GRPO)",
        "optimization bias",
        "response length",
        "Dr. GRPO",
        "token efficiency",
        "minimalist R1-Zero recipe",
        "AIME 2024",
        "7B base model"
      ]
    },
    "publishedAt": "2025-03-26T13:59:14.000Z",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f5392c68b8e0cb3c9977a2",
      "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
      "fullname": "Zichen",
      "name": "lkevinzc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01956",
      "authors": [
        {
          "_id": "67ee01265839c8a023344aee",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344aef",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344af0",
          "name": "Jiawei Chi",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344af1",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
      ],
      "publishedAt": "2025-04-02T17:59:21.000Z",
      "submittedOnDailyAt": "2025-04-03T02:07:36.716Z",
      "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
      "submittedOnDailyBy": {
        "_id": "65c38f6c137aba2aee524989",
        "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
        "isPro": false,
        "fullname": "Hanyang Wang",
        "user": "hanyang-21",
        "type": "user"
      },
      "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
      "upvotes": 16,
      "discussionId": "67ee012a5839c8a023344bdb",
      "projectPage": "https://hanyang-21.github.io/VideoScene",
      "githubRepo": "https://github.com/hanyang-21/VideoScene",
      "ai_keywords": [
        "video generative models",
        "video diffusion models",
        "3D scenes",
        "sparse views",
        "geometry regularization",
        "feed-forward model",
        "video generative prior",
        "inference time",
        "3D constraint",
        "reconstruction artifacts",
        "VideoScene",
        "3D-aware leap flow distillation",
        "dynamic denoising policy network"
      ]
    },
    "publishedAt": "2025-04-02T13:59:21.000Z",
    "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
    "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c38f6c137aba2aee524989",
      "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
      "fullname": "Hanyang Wang",
      "name": "hanyang-21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01724",
      "authors": [
        {
          "_id": "67edf7b6d277de0ec2aa5b6b",
          "name": "Yuxuan Luo",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6c",
          "name": "Zhengkun Rong",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6d",
          "name": "Lizhen Wang",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6e",
          "name": "Longhao Zhang",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6f",
          "name": "Tianshu Hu",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b70",
          "name": "Yongming Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T13:30:32.000Z",
      "submittedOnDailyAt": "2025-04-03T01:22:04.548Z",
      "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
      "upvotes": 14,
      "discussionId": "67edf7bcd277de0ec2aa5d7b",
      "ai_keywords": [
        "diffusion transformer (DiT)",
        "hybrid guidance",
        "implicit facial representations",
        "3D head spheres",
        "3D body skeletons",
        "facial expressions",
        "body movements",
        "expressive animations",
        "identity-preserving animations",
        "progressive training strategy",
        "varying resolutions",
        "varying scales",
        "motion patterns",
        "sequential frames",
        "visual references",
        "long-term temporal coherence",
        "long-term consistency",
        "expressive results",
        "upper-body generation",
        "full-body generation"
      ]
    },
    "publishedAt": "2025-04-02T09:30:32.000Z",
    "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
    "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00824",
      "authors": [
        {
          "_id": "67ede79d21d7e74ee3e2832a",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832b",
          "name": "Xueguang Ma",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832c",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832d",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832e",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28330",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28331",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28332",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28333",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
      ],
      "publishedAt": "2025-04-01T14:12:14.000Z",
      "submittedOnDailyAt": "2025-04-03T00:13:20.491Z",
      "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
      "upvotes": 13,
      "discussionId": "67ede79e21d7e74ee3e2838c",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ScholarCopilot",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "ScholarCopilot",
        "large language models",
        "retrieval token [RET]",
        "scholarly references",
        "top-1 retrieval accuracy",
        "arXiv",
        "generation quality",
        "relevance",
        "coherence",
        "academic rigor",
        "completeness",
        "innovation",
        "citation recall",
        "writing efficiency",
        "user experience"
      ]
    },
    "publishedAt": "2025-04-01T10:12:14.000Z",
    "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
    "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01934",
      "authors": [
        {
          "_id": "67edfe07f5d1509d1a990178",
          "name": "Runhui Huang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990179",
          "name": "Chunwei Wang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017a",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017b",
          "name": "Guansong Lu",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017c",
          "name": "Yunlong Yuan",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017d",
          "name": "Jianhua Han",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017e",
          "name": "Lu Hou",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017f",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990180",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990181",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990182",
          "name": "Hang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T17:45:00.000Z",
      "submittedOnDailyAt": "2025-04-03T01:58:02.658Z",
      "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
      "submittedOnDailyBy": {
        "_id": "630f0542cc8ed75decb03b68",
        "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
        "isPro": false,
        "fullname": "huangrh9",
        "user": "huangrh9",
        "type": "user"
      },
      "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/.",
      "upvotes": 11,
      "discussionId": "67edfe09f5d1509d1a990214",
      "projectPage": "https://illume-unified-mllm.github.io/",
      "githubRepo": "https://github.com/illume-unified-mllm/ILLUME_plus",
      "ai_keywords": [
        "dual visual tokenization",
        "diffusion decoder",
        "deep semantic understanding",
        "high-fidelity image generation",
        "VQGAN",
        "LaViT",
        "semantic encoders",
        "DualViTok",
        "texture preservation",
        "multimodal understanding",
        "continuous-input, discrete-output scheme",
        "MLLM",
        "progressive training procedure",
        "dynamic resolution",
        "context-aware image editing"
      ]
    },
    "publishedAt": "2025-04-02T13:45:00.000Z",
    "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
    "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01934.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "630f0542cc8ed75decb03b68",
      "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
      "fullname": "huangrh9",
      "name": "huangrh9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01848",
      "authors": [
        {
          "_id": "67edf3d579018bf61e050435",
          "name": "Giulio Starace",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050436",
          "name": "Oliver Jaffe",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050437",
          "name": "Dane Sherburn",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050438",
          "name": "James Aung",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050439",
          "name": "Jun Shern Chan",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043a",
          "name": "Leon Maksin",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043b",
          "name": "Rachel Dias",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043c",
          "name": "Evan Mays",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043d",
          "name": "Benjamin Kinsella",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043e",
          "name": "Wyatt Thompson",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043f",
          "name": "Johannes Heidecke",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050440",
          "name": "Amelia Glaese",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050441",
          "name": "Tejal Patwardhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T15:55:24.000Z",
      "submittedOnDailyAt": "2025-04-03T01:05:22.442Z",
      "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\nhttps://github.com/openai/preparedness{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
      "upvotes": 11,
      "discussionId": "67edf3d679018bf61e0504c0",
      "ai_keywords": [
        "anLM-based judge",
        "replication attempts"
      ]
    },
    "publishedAt": "2025-04-02T11:55:24.000Z",
    "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
    "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\nhttps://github.com/openai/preparedness{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01308",
      "authors": [
        {
          "_id": "67ede544ed9c94861b82b29f",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a0",
          "name": "Yushen Zuo",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a1",
          "name": "Yuanjun Chai",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a2",
          "name": "Zhendong Liu",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a3",
          "user": {
            "_id": "6528ce81598467feb33d992d",
            "avatarUrl": "/avatars/e98a7bf16e6fd5118e861d562f93bb9b.svg",
            "isPro": false,
            "fullname": "Yicheng Fu",
            "user": "sofyc",
            "type": "user"
          },
          "name": "Yichen Fu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T01:32:57.886Z",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a4",
          "name": "Yichun Feng",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a5",
          "name": "Kin-man Lam",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T02:35:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:10:56.307Z",
      "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
      "upvotes": 10,
      "discussionId": "67ede549ed9c94861b82b433",
      "githubRepo": "https://github.com/JarvisUSTC/DiffPure-RobustVLM",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "Large Language Models (LLMs)",
        "noise-augmented training",
        "Gaussian noise",
        "Robust-VLGuard",
        "multimodal safety dataset",
        "aligned / misaligned image-text pairs",
        "noise-augmented fine-tuning",
        "diffusion models",
        "DiffPure-VLM",
        "diffusion model",
        "distribution-shifting property",
        "adversarial perturbations"
      ]
    },
    "publishedAt": "2025-04-01T22:35:19.000Z",
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
    "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01308.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01204",
      "authors": [
        {
          "_id": "67edf4bf5e87fcaa485a0ad9",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0ada",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adb",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adc",
          "name": "Yongxin Chen",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0add",
          "name": "Chenfanfu Jiang",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0ade",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adf",
          "name": "Donglai Xiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T21:37:57.000Z",
      "submittedOnDailyAt": "2025-04-03T01:09:40.312Z",
      "title": "Articulated Kinematics Distillation from Video Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
      "upvotes": 9,
      "discussionId": "67edf4c65e87fcaa485a0cb7",
      "ai_keywords": [
        "skeleton-based representation",
        "Degrees of Freedom (DoFs)",
        "joint-level control",
        "Score Distillation Sampling (SDS)",
        "video diffusion models",
        "articulated motions",
        "structural integrity",
        "physics-based simulation",
        "text-to-4D generation"
      ]
    },
    "publishedAt": "2025-04-01T17:37:57.000Z",
    "title": "Articulated Kinematics Distillation from Video Diffusion Models",
    "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23135",
      "authors": [
        {
          "_id": "67eb3d2110032c28d1ea109f",
          "user": {
            "_id": "628ece6054698ce61d1e7be3",
            "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
            "isPro": false,
            "fullname": "Ao Wang",
            "user": "jameslahm",
            "type": "user"
          },
          "name": "Ao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T16:11:10.373Z",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a0",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a1",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a2",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a3",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
      ],
      "publishedAt": "2025-03-29T16:00:54.000Z",
      "submittedOnDailyAt": "2025-04-03T00:26:03.944Z",
      "title": "LSNet: See Large, Focus Small",
      "submittedOnDailyBy": {
        "_id": "628ece6054698ce61d1e7be3",
        "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
        "isPro": false,
        "fullname": "Ao Wang",
        "user": "jameslahm",
        "type": "user"
      },
      "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
      "upvotes": 3,
      "discussionId": "67eb3d2310032c28d1ea1108",
      "projectPage": "https://github.com/THU-MIG/lsnet",
      "githubRepo": "https://github.com/THU-MIG/lsnet",
      "ai_keywords": [
        "Convolutional Neural Networks",
        "Vision Transformers",
        "lightweight and efficient network designs",
        "self-attention mechanisms",
        "token mixing",
        "small-kernel aggregation",
        "dynamic heteroscale vision ability",
        "human vision system",
        "``See Large, Focus Small'' strategy",
        "LS (\\textbf{L}arge-\\textbf{S}mall) convolution",
        "large-kernel perception",
        "precise feature aggregation",
        "visual representations",
        "efficient processing of visual information",
        "LSNet",
        "superior performance",
        "efficiency"
      ]
    },
    "publishedAt": "2025-03-29T12:00:54.000Z",
    "title": "LSNet: See Large, Focus Small",
    "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23135.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628ece6054698ce61d1e7be3",
      "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
      "fullname": "Ao Wang",
      "name": "jameslahm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  }
]