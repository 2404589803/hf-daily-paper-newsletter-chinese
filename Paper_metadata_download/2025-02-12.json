[
  {
    "paper": {
      "id": "2502.06807",
      "authors": [
        {
          "_id": "67ac1b080686a1e0690741ce",
          "name": "OpenAI",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d0",
          "name": "Ahmed El-Kishky",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d1",
          "name": "Alexander Wei",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d2",
          "name": "Andre Saraiva",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d3",
          "name": "Borys Minaev",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d4",
          "name": "Daniel Selsam",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d5",
          "name": "David Dohan",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d6",
          "name": "Francis Song",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d7",
          "name": "Hunter Lightman",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d8",
          "name": "Ignasi Clavera",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d9",
          "name": "Jakub Pachocki",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741da",
          "name": "Jerry Tworek",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741db",
          "name": "Lorenz Kuhn",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dc",
          "name": "Lukasz Kaiser",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dd",
          "name": "Mark Chen",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741de",
          "name": "Max Schwarzer",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741df",
          "name": "Mostafa Rohaninejad",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e0",
          "name": "Nat McAleese",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e1",
          "name": "o3 contributors",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e2",
          "name": "Oleg MÃ¼rk",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e3",
          "name": "Rhythm Garg",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e4",
          "name": "Rui Shu",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e5",
          "name": "Szymon Sidor",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e6",
          "name": "Vineet Kosaraju",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e7",
          "name": "Wenda Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T23:00:15.000Z",
      "title": "Competitive Programming with Large Reasoning Models",
      "summary": "We show that reinforcement learning applied to large language models (LLMs)\nsignificantly boosts performance on complex coding and reasoning tasks.\nAdditionally, we compare two general-purpose reasoning models - OpenAI o1 and\nan early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses\nhand-engineered inference strategies designed for competing in the 2024\nInternational Olympiad in Informatics (IOI). We competed live at IOI 2024 with\no1-ioi and, using hand-crafted test-time strategies, placed in the 49th\npercentile. Under relaxed competition constraints, o1-ioi achieved a gold\nmedal. However, when evaluating later models such as o3, we find that o3\nachieves gold without hand-crafted domain-specific strategies or relaxed\nconstraints. Our findings show that although specialized pipelines such as\no1-ioi yield solid improvements, the scaled-up, general-purpose o3 model\nsurpasses those results without relying on hand-crafted inference heuristics.\nNotably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces\nrating on par with elite human competitors. Overall, these results indicate\nthat scaling general-purpose reinforcement learning, rather than relying on\ndomain-specific techniques, offers a robust path toward state-of-the-art AI in\nreasoning domains, such as competitive programming.",
      "upvotes": 22,
      "discussionId": "67ac1b090686a1e069074208"
    },
    "publishedAt": "2025-02-11T22:53:19.310Z",
    "title": "Competitive Programming with Large Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07316",
      "authors": [
        {
          "_id": "67ac0ab720e98bddc5c19fed",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fee",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fef",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff0",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff1",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff2",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T07:26:50.000Z",
      "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
      "summary": "Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.",
      "upvotes": 11,
      "discussionId": "67ac0ab820e98bddc5c1a039"
    },
    "publishedAt": "2025-02-11T23:00:20.080Z",
    "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621e40ac944c7e36aaec2369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
      "fullname": "Junlong Li",
      "name": "lockon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03492",
      "authors": [
        {
          "_id": "67a5a8e595df68b0a167c298",
          "name": "Zhihui Xie",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c299",
          "name": "Jie chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29a",
          "name": "Liyu Chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29b",
          "name": "Weichao Mao",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29c",
          "name": "Jingjing Xu",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29d",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T02:18:46.000Z",
      "title": "Teaching Language Models to Critique via Reinforcement Learning",
      "summary": "Teaching large language models (LLMs) to critique and refine their outputs is\ncrucial for building systems that can iteratively improve, yet it is\nfundamentally limited by the ability to provide accurate judgments and\nactionable suggestions. In this work, we study LLM critics for code generation\nand propose CTRL, a framework for Critic\nTraining via Reinforcement Learning, which\ntrains a critic model to generate feedback that maximizes correction\nperformance for a fixed generator model without human supervision. Our results\ndemonstrate that critics trained with CTRL significantly enhance\npass rates and mitigate compounding errors across both base and stronger\ngenerator models. Furthermore, we show that these critic models act as accurate\ngenerative reward models and enable test-time scaling through iterative\ncritique-revision, achieving up to 106.1% relative improvements across\nchallenging code generation benchmarks.",
      "upvotes": 7,
      "discussionId": "67a5a8e695df68b0a167c2c6"
    },
    "publishedAt": "2025-02-11T23:55:37.671Z",
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f103fc78da4c7ebd7c887",
      "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
      "fullname": "Xie",
      "name": "Zhihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07617",
      "authors": [
        {
          "_id": "67ac1d68c29356f92ed772c5",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c6",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c7",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c8",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c9",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772ca",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T15:05:33.000Z",
      "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language\n  Models",
      "summary": "We provide an empirical investigation of the potential of pre-training\nvision-language models on an unprecedented scale: 100 billion examples. We find\nthat model performance tends to saturate at this scale on many common\nWestern-centric classification and retrieval benchmarks, such as COCO Captions.\nNevertheless, tasks of cultural diversity achieve more substantial gains from\nthe 100-billion scale web data, thanks to its coverage of long-tail concepts.\nFurthermore, we analyze the model's multilinguality and show gains in\nlow-resource languages as well. In addition, we observe that reducing the size\nof the pretraining dataset via quality filters like using CLIP, typically used\nto enhance performance, may inadvertently reduce the cultural diversity\nrepresented even in large-scale datasets. Our results highlight that while\ntraditional benchmarks may not benefit significantly from scaling noisy, raw\nweb data to 100 billion examples, this data scale is vital for building truly\ninclusive multimodal systems.",
      "upvotes": 7,
      "discussionId": "67ac1d6ac29356f92ed77354"
    },
    "publishedAt": "2025-02-11T23:03:08.578Z",
    "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07701",
      "authors": [
        {
          "_id": "67ac23166def89f9aae56abd",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abe",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abf",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac0",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac1",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac2",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac3",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac4",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac5",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac6",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T16:58:15.000Z",
      "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
      "summary": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
      "upvotes": 5,
      "discussionId": "67ac23186def89f9aae56b69"
    },
    "publishedAt": "2025-02-11T23:27:13.769Z",
    "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07527",
      "authors": [
        {
          "_id": "67ac1eaac61306b0ac95d2c6",
          "name": "Yingce Xia",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c7",
          "name": "Peiran Jin",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c8",
          "name": "Shufang Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c9",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ca",
          "name": "Chuan Cao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cb",
          "name": "Renqian Luo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cc",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cd",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ce",
          "name": "Zequn Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cf",
          "name": "Yuan-Jyue Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d0",
          "name": "Zekun Guo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d1",
          "name": "Yeqi Bai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d2",
          "name": "Pan Deng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d3",
          "name": "Yaosen Min",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d4",
          "name": "Ziheng Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d5",
          "name": "Hongxia Hao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d6",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d7",
          "name": "Jielan Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d8",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d9",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2da",
          "name": "Jianwei Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2db",
          "name": "Kehan Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dc",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dd",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2de",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2df",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e0",
          "name": "Xixian Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e1",
          "name": "Yanting Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e2",
          "name": "Houtian Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e3",
          "name": "Yeqing Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e4",
          "name": "Mingqian Ma",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e5",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e6",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e7",
          "name": "Krzysztof Maziarz",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e8",
          "name": "Marwin Segler",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e9",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ea",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2eb",
          "name": "Yu Shi",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ec",
          "name": "Shuxin Zheng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ed",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ee",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ef",
          "name": "Peggy Dai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f0",
          "name": "Tie-Yan Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f1",
          "name": "Haiguang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f2",
          "name": "Tao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:08:03.000Z",
      "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
      "summary": "Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.",
      "upvotes": 5,
      "discussionId": "67ac1eabc61306b0ac95d346"
    },
    "publishedAt": "2025-02-11T23:10:26.895Z",
    "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06589",
      "authors": [
        {
          "_id": "67ac1d45e6f1e95ccf6de3b7",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b8",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b9",
          "name": "Haoming Jiang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bb",
          "name": "Kewei Cheng",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bc",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bd",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3be",
          "name": "Qing Ping",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bf",
          "name": "Tianyi Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c0",
          "name": "Binxuan Huang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c1",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c2",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c3",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c4",
          "name": "Ruijie Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c5",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c6",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c7",
          "name": "Priyanka Nigam",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c8",
          "name": "Bing Yin",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c9",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T15:54:34.000Z",
      "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training",
      "summary": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.",
      "upvotes": 5,
      "discussionId": "67ac1d46e6f1e95ccf6de419"
    },
    "publishedAt": "2025-02-11T23:04:08.153Z",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.03997",
      "authors": [
        {
          "_id": "67ac206214d5fe7767e7ec4e",
          "name": "Yu Yuan",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec4f",
          "name": "Shizhao Sun",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec50",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec51",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:57:14.000Z",
      "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
      "summary": "Computer Aided Design (CAD) is indispensable across various industries.\nText-based CAD editing, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce CAD-Editor, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively.",
      "upvotes": 4,
      "discussionId": "67ac206314d5fe7767e7ec98"
    },
    "publishedAt": "2025-02-11T23:16:28.213Z",
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb00a191a1b8ec4fbba2a9",
      "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
      "fullname": "ShizhaoSun",
      "name": "ShizhaoSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07374",
      "authors": [
        {
          "_id": "67ac1c6436464325ebe3c6e3",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e4",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e5",
          "name": "Tyler Griggs",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e6",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e7",
          "name": "Xiangxi Mo",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e8",
          "name": "Shishir G. Patil",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e9",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6ea",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6eb",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:48:48.000Z",
      "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
      "summary": "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 4,
      "discussionId": "67ac1c6536464325ebe3c723"
    },
    "publishedAt": "2025-02-11T22:58:37.585Z",
    "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07445",
      "authors": [
        {
          "_id": "67ac216d602eb9ca8a517be6",
          "name": "Nurit Cohen-Inger",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be7",
          "name": "Yehonatan Elisha",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be8",
          "name": "Bracha Shapira",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be9",
          "name": "Lior Rokach",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517bea",
          "name": "Seffi Cohen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T10:43:36.000Z",
      "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
      "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.",
      "upvotes": 2,
      "discussionId": "67ac216e602eb9ca8a517c1d"
    },
    "publishedAt": "2025-02-11T23:22:50.454Z",
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6731e56a07cf693a1104d2cb",
      "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
      "fullname": "Seffi Cohen",
      "name": "seffico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07508",
      "authors": [
        {
          "_id": "67ac2006a6b5a26040fc94f7",
          "name": "Yang Luo",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f8",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f9",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fa",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fb",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fc",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fd",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fe",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T12:22:35.000Z",
      "title": "Enhance-A-Video: Better Generated Video for Free",
      "summary": "DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.",
      "upvotes": 2,
      "discussionId": "67ac200ea6b5a26040fc9709"
    },
    "publishedAt": "2025-02-11T23:14:10.293Z",
    "title": "Enhance-A-Video: Better Generated Video for Free",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04465",
      "authors": [
        {
          "_id": "67a953844ea315a67e02461d",
          "user": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "isPro": false,
            "fullname": "Luca Della Libera",
            "user": "lucadellalib",
            "type": "user"
          },
          "name": "Luca Della Libera",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461e",
          "name": "Francesco Paissan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461f",
          "name": "Cem Subakan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e024620",
          "name": "Mirco Ravanelli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T19:24:50.000Z",
      "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
      "summary": "Large language models have revolutionized natural language processing through\nself-supervised pretraining on massive datasets. Inspired by this success,\nresearchers have explored adapting these methods to speech by discretizing\ncontinuous audio into tokens using neural audio codecs. However, existing\napproaches face limitations, including high bitrates, the loss of either\nsemantic or acoustic information, and the reliance on multi-codebook designs\nwhen trying to capture both, which increases architectural complexity for\ndownstream tasks. To address these challenges, we introduce FocalCodec, an\nefficient low-bitrate codec based on focal modulation that utilizes a single\nbinary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec\ndelivers competitive performance in speech resynthesis and voice conversion at\nlower bitrates than the current state-of-the-art, while effectively handling\nmultilingual speech and noisy environments. Evaluation on downstream tasks\nshows that FocalCodec successfully preserves sufficient semantic and acoustic\ninformation, while also being well-suited for generative modeling. Demo\nsamples, code and checkpoints are available at\nhttps://lucadellalib.github.io/focalcodec-web/.",
      "upvotes": 1,
      "discussionId": "67a953854ea315a67e024659"
    },
    "publishedAt": "2025-02-12T01:31:44.368Z",
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63195d0582e7eec0eac040e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
      "fullname": "Luca Della Libera",
      "name": "lucadellalib",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07531",
      "authors": [
        {
          "_id": "67ac21acaa680a0f8782d273",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d274",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d275",
          "name": "Yanpeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d276",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d277",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d278",
          "name": "Xiangru Huang",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d279",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:11:59.000Z",
      "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation",
      "summary": "Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.",
      "upvotes": 1,
      "discussionId": "67ac21b2aa680a0f8782d3bd"
    },
    "publishedAt": "2025-02-11T23:21:13.452Z",
    "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07776",
      "authors": [
        {
          "_id": "67ac1f7851c7f3b53ffc4def",
          "name": "Chenchen Gu",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df0",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df1",
          "name": "Rohith Kuditipudi",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df2",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df3",
          "user": {
            "_id": "661595d1b3d0b21da55cde7d",
            "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
            "isPro": false,
            "fullname": "Tatsu Hashimoto",
            "user": "thashim",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:58:04.000Z",
      "title": "Auditing Prompt Caching in Language Model APIs",
      "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
      "upvotes": 1,
      "discussionId": "67ac1f7851c7f3b53ffc4e1b"
    },
    "publishedAt": "2025-02-11T23:11:49.993Z",
    "title": "Auditing Prompt Caching in Language Model APIs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6040
    },
    "isAuthorParticipating": false
  }
]