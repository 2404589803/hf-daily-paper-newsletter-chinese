[
  {
    "paper": {
      "id": "2505.10554",
      "authors": [
        {
          "_id": "6826a569ea77771e3880f793",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f794",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f795",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f796",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f797",
          "name": "Amrita Saha",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f798",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f799",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f79a",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:58:33.000Z",
      "submittedOnDailyAt": "2025-05-16T01:09:52.437Z",
      "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "upvotes": 33,
      "discussionId": "6826a56aea77771e3880f7c8",
      "ai_keywords": [
        "large reasoning models",
        "long chain-of-thought reasoning",
        "outcome-based reinforcement learning",
        "RL",
        "self-correction",
        "backtracking",
        "verification phenomena",
        "aha moment",
        "meta-abilities",
        "deduction",
        "induction",
        "abduction",
        "automatically generated tasks",
        "self-verifiable tasks",
        "parameter-space merging",
        "domain-specific reinforcement learning",
        "performance ceiling",
        "math benchmarks",
        "coding benchmarks",
        "science benchmarks",
        "Meta-Ability-Alignment"
      ]
    },
    "publishedAt": "2025-05-15T13:58:33.000Z",
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09666",
      "authors": [
        {
          "_id": "68269a1eaa8aded616d280a0",
          "user": {
            "_id": "64cfa0b9749587dbe01d0079",
            "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
            "isPro": false,
            "fullname": "Yumin Choi",
            "user": "YuminChoi",
            "type": "user"
          },
          "name": "Yumin Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:06.309Z",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a1",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a2",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:15.000Z",
      "submittedOnDailyAt": "2025-05-16T01:05:44.315Z",
      "title": "System Prompt Optimization with Meta-Learning",
      "submittedOnDailyBy": {
        "_id": "63036b6c5c70c21d0ea79d48",
        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
        "isPro": false,
        "fullname": "Jinheon Baek",
        "user": "jinheon",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
      "upvotes": 27,
      "discussionId": "68269a1eaa8aded616d280d1",
      "githubRepo": "https://github.com/Dozi01/MetaSPO",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "bilevel system prompt optimization",
        "meta-learning framework",
        "system prompts",
        "user prompts",
        "unseen datasets",
        "domains",
        "rapid adaptation",
        "test-time user prompts"
      ]
    },
    "publishedAt": "2025-05-14T12:46:15.000Z",
    "title": "System Prompt Optimization with Meta-Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09723",
      "authors": [
        {
          "_id": "6826b00c251d26fc0cd035cc",
          "name": "Yuxin Jiang",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cd",
          "name": "Shengcong Chen",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035ce",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cf",
          "name": "Liliang Chen",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d0",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d1",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d2",
          "name": "Xindong He",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d3",
          "name": "Chiming Liu",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d4",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d5",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d6",
          "name": "Guanghui Ren",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
      ],
      "publishedAt": "2025-05-14T18:30:53.000Z",
      "submittedOnDailyAt": "2025-05-16T02:11:01.174Z",
      "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
      "upvotes": 14,
      "discussionId": "6826b013251d26fc0cd037ba",
      "githubRepo": "https://github.com/AgibotTech/EnerVerse-AC",
      "ai_keywords": [
        "action-conditional world model",
        "future visual observations",
        "multi-level action-conditioning mechanism",
        "ray map encoding",
        "dynamic multi-view image generation",
        "diverse failure trajectories",
        "data engine",
        "evaluator",
        "human-collected trajectories",
        "diverse datasets",
        "action-conditioned video observations",
        "robotic manipulation evaluation"
      ]
    },
    "publishedAt": "2025-05-14T14:30:53.000Z",
    "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
    "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10185",
      "authors": [
        {
          "_id": "68269f67a47cb2b87646b98c",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98d",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98e",
          "name": "Minju Seo",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98f",
          "name": "Yongrae Jo",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b990",
          "name": "Dongyoung Go",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b991",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b992",
          "name": "Jinho Park",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b993",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b994",
          "name": "Sean Welleck",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b995",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b996",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b997",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T11:31:02.000Z",
      "submittedOnDailyAt": "2025-05-16T00:44:19.223Z",
      "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
      "upvotes": 13,
      "discussionId": "68269f68a47cb2b87646b9ed",
      "ai_keywords": [
        "long chain-of-thought (CoT)",
        "large language models",
        "reasoning strategies",
        "predefined strategy types",
        "CoT Encyclopedia",
        "bottom-up framework",
        "reasoning criteria",
        "semantic space",
        "contrastive rubrics",
        "reasoning behavior",
        "interpretability",
        "performance gains"
      ]
    },
    "publishedAt": "2025-05-15T07:31:02.000Z",
    "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
    "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09694",
      "authors": [
        {
          "_id": "6826ae4611765454f5757d7c",
          "name": "Hu Yue",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7d",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7e",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7f",
          "name": "Shengcong Chen",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d80",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d81",
          "name": "Liliang Chen",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d82",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d83",
          "name": "Guanghui Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T18:00:19.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:39.761Z",
      "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
      "upvotes": 13,
      "discussionId": "6826ae4911765454f5757e32",
      "ai_keywords": [
        "text-to-video diffusion models",
        "embodied world models",
        "physically plausible scenes",
        "language commands",
        "perceptual metrics",
        "visual scene consistency",
        "motion correctness",
        "semantic alignment",
        "Embodied World Model Benchmark (EWMBench)",
        "multi-dimensional evaluation toolkit",
        "video generation models"
      ]
    },
    "publishedAt": "2025-05-14T14:00:19.000Z",
    "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
    "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10562",
      "authors": [
        {
          "_id": "6826a5d8154611642ada50da",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50db",
          "user": {
            "_id": "640ed40dc025ddf618950af7",
            "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
            "isPro": false,
            "fullname": "Fan Zhang",
            "user": "ryanzhangfan",
            "type": "user"
          },
          "name": "Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:04.101Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dc",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dd",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50de",
          "name": "Zhuoyan Luo",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50df",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e0",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e1",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
      ],
      "publishedAt": "2025-05-15T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-16T01:26:23.739Z",
      "title": "End-to-End Vision Tokenizer Tuning",
      "submittedOnDailyBy": {
        "_id": "640ed40dc025ddf618950af7",
        "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
        "isPro": false,
        "fullname": "Fan Zhang",
        "user": "ryanzhangfan",
        "type": "user"
      },
      "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
      "upvotes": 9,
      "discussionId": "6826a5d9154611642ada5122",
      "ai_keywords": [
        "vision tokenization",
        "end-to-end vision tokenizer tuning (ETT)",
        "autoregressive tasks",
        "visual embeddings",
        "tokenizer codebook",
        "multimodal understanding",
        "visual generation tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:59:39.000Z",
    "title": "End-to-End Vision Tokenizer Tuning",
    "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640ed40dc025ddf618950af7",
      "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
      "fullname": "Fan Zhang",
      "name": "ryanzhangfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07782",
      "authors": [
        {
          "_id": "6822b3c8c10ac9c466c63e01",
          "user": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "isPro": true,
            "fullname": "Rushi Qiang",
            "user": "Jerrycool",
            "type": "user"
          },
          "name": "Rushi Qiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:59.704Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e02",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e03",
          "name": "Yinghao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e04",
          "name": "Dingu Sagar V K",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e05",
          "user": {
            "_id": "644a2c4e9a1c5faef7a5dbd8",
            "avatarUrl": "/avatars/fbbbc1347f8e423b2477e2506fdb43d9.svg",
            "isPro": false,
            "fullname": "Rongzhi Zhang",
            "user": "Solute",
            "type": "user"
          },
          "name": "Rongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:47.628Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e06",
          "name": "Changhao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e07",
          "name": "Ian Shu-Hei Wong",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e08",
          "name": "Sherry Yang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e09",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0a",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0b",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:35:43.000Z",
      "submittedOnDailyAt": "2025-05-16T00:26:20.796Z",
      "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
      "submittedOnDailyBy": {
        "_id": "6466e31a14e059dde8bbe4be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
        "isPro": true,
        "fullname": "Rushi Qiang",
        "user": "Jerrycool",
        "type": "user"
      },
      "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
      "upvotes": 9,
      "discussionId": "6822b3c9c10ac9c466c63e8a",
      "projectPage": "https://mle-dojo.github.io/MLE-Dojo-page/",
      "githubRepo": "https://github.com/MLE-Dojo/MLE-Dojo",
      "ai_keywords": [
        "reinforcement learning",
        "autonomous large language model (LLM) agents",
        "iterative machine learning engineering (MLE) workflows",
        "Kaggle challenges",
        "MLE tasks",
        "data processing",
        "architecture search",
        "hyperparameter tuning",
        "code debugging",
        "supervised fine-tuning",
        "model-based agent tuning"
      ]
    },
    "publishedAt": "2025-05-12T13:35:43.000Z",
    "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
    "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466e31a14e059dde8bbe4be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
      "fullname": "Rushi Qiang",
      "name": "Jerrycool",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10558",
      "authors": [
        {
          "_id": "6826bc3cf032d8147549ac6b",
          "user": {
            "_id": "635eac5ea81c7f7424a23b8c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
            "isPro": false,
            "fullname": "intchous",
            "user": "intchous",
            "type": "user"
          },
          "name": "Peiying Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6c",
          "user": {
            "_id": "6412b90e6e51a8e21887ff30",
            "avatarUrl": "/avatars/b0c3a8624a686481b9f609be96b1307c.svg",
            "isPro": false,
            "fullname": "Z",
            "user": "CHERRY-Z",
            "type": "user"
          },
          "name": "Nanxuan Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6d",
          "name": "Jing Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:21.000Z",
      "submittedOnDailyAt": "2025-05-16T02:49:19.469Z",
      "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
      "submittedOnDailyBy": {
        "_id": "635eac5ea81c7f7424a23b8c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
        "isPro": false,
        "fullname": "intchous",
        "user": "intchous",
        "type": "user"
      },
      "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
      "upvotes": 7,
      "discussionId": "6826bc3df032d8147549acac",
      "ai_keywords": [
        "T2V (text-to-vector)",
        "SVG (Scalable Vector Graphics)",
        "T2I (text-to-image)",
        "diffusion model",
        "path-level representation",
        "structural regularity",
        "expressive capabilities"
      ]
    },
    "publishedAt": "2025-05-15T13:59:21.000Z",
    "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
    "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10558.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635eac5ea81c7f7424a23b8c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
      "fullname": "intchous",
      "name": "intchous",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10320",
      "authors": [
        {
          "_id": "6826a180cad9000ebc70f038",
          "name": "Chenxi Whitehouse",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f039",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03a",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03b",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03c",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03d",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03e",
          "user": {
            "_id": "64b75f4b037d6452a30f71aa",
            "avatarUrl": "/avatars/5a0322e7ecda05164e45526d605e3619.svg",
            "isPro": false,
            "fullname": "Swarnadeep Saha",
            "user": "swarna92",
            "type": "user"
          },
          "name": "Swarnadeep Saha",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T02:22:57.318Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T14:05:15.000Z",
      "submittedOnDailyAt": "2025-05-16T00:58:47.493Z",
      "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64b6feee17681d64b19b112b",
        "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
        "isPro": false,
        "fullname": "Swarnadeep Saha",
        "user": "swarnaNLP",
        "type": "user"
      },
      "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
      "upvotes": 7,
      "discussionId": "6826a181cad9000ebc70f0a3",
      "ai_keywords": [
        "reinforcement learning",
        "chain-of-thought reasoning",
        "judgment tasks",
        "judgment bias",
        "Pairwise-J1",
        "Pointwise-J1",
        "offline training",
        "online training",
        "reward strategies",
        "seed prompts",
        "evaluation criteria",
        "reference answers",
        "correctness of model responses"
      ]
    },
    "publishedAt": "2025-05-15T10:05:15.000Z",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6feee17681d64b19b112b",
      "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
      "fullname": "Swarnadeep Saha",
      "name": "swarnaNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09738",
      "authors": [
        {
          "_id": "6826c755f032d814754cadfe",
          "name": "Shaurya Sharthak",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cadff",
          "name": "Vinayak Pahalwan",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae00",
          "user": {
            "_id": "6523d85b27d1f3d84ab3a0a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6523d85b27d1f3d84ab3a0a4/GF159gtSvxSWR9TAJIQby.jpeg",
            "isPro": false,
            "fullname": "Adithya Kamath",
            "user": "adi-kmt",
            "type": "user"
          },
          "name": "Adithya Kamath",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:11:49.739Z",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae01",
          "user": {
            "_id": "644bf6ef778ecbfb977e8e84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
            "isPro": true,
            "fullname": "Adarsh AS",
            "user": "adarshxs",
            "type": "user"
          },
          "name": "Adarsh Shirawalmath",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-16T05:19:45.785Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
      ],
      "publishedAt": "2025-05-14T19:00:27.000Z",
      "submittedOnDailyAt": "2025-05-16T03:53:49.621Z",
      "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
      "submittedOnDailyBy": {
        "_id": "644bf6ef778ecbfb977e8e84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
        "isPro": true,
        "fullname": "Adarsh AS",
        "user": "adarshxs",
        "type": "user"
      },
      "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
      "upvotes": 7,
      "discussionId": "6826c756f032d814754cae4d",
      "githubRepo": "https://github.com/Tinycompany-AI/tokenadapt",
      "ai_keywords": [
        "Tokenadapt",
        "Supertokens",
        "subword decomposition",
        "semantically similar tokens",
        "zero-shot perplexity",
        "perplexity ratios"
      ]
    },
    "publishedAt": "2025-05-14T15:00:27.000Z",
    "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
    "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644bf6ef778ecbfb977e8e84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
      "fullname": "Adarsh AS",
      "name": "adarshxs",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10527",
      "authors": [
        {
          "_id": "682699da19c4a596dbcea4f5",
          "name": "Binghai Wang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f6",
          "name": "Runji Lin",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f7",
          "name": "Keming Lu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f9",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fa",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fb",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fc",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fd",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fe",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4ff",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea500",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea501",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea502",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea503",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea504",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea505",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea506",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea507",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea508",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:38:37.000Z",
      "submittedOnDailyAt": "2025-05-16T00:22:55.977Z",
      "title": "WorldPM: Scaling Human Preference Modeling",
      "submittedOnDailyBy": {
        "_id": "63d9d68c1cae35c27bf7a6a7",
        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
        "isPro": false,
        "fullname": "Bowen Yu",
        "user": "Tigerph",
        "type": "user"
      },
      "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
      "upvotes": 6,
      "discussionId": "682699dd19c4a596dbcea604",
      "ai_keywords": [
        "preference modeling",
        "World Preference Modeling (WorldPM)",
        "World Preference",
        "unified representation",
        "human preferences",
        "preference data",
        "public forums",
        "user communities",
        "extensive training",
        "15M-scale data",
        "parameter-efficient fine-tuning",
        "Adversarial metrics",
        "deceptive features",
        "Objective metrics",
        "Subjective metrics",
        "preference fine-tuning",
        "generalization performance",
        "human preference datasets",
        "RLHF (Reinforcement Learning from Human Feedback)",
        "in-house evaluations",
        "public evaluation sets"
      ]
    },
    "publishedAt": "2025-05-15T13:38:37.000Z",
    "title": "WorldPM: Scaling Human Preference Modeling",
    "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9d68c1cae35c27bf7a6a7",
      "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
      "fullname": "Bowen Yu",
      "name": "Tigerph",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10565",
      "authors": [
        {
          "_id": "6826bda8caf89edf94b736dc",
          "user": {
            "_id": "6425761a175bd295228311a0",
            "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
            "isPro": false,
            "fullname": "zehan wang",
            "user": "sleetwang6",
            "type": "user"
          },
          "name": "Zehan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:01.498Z",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736dd",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736de",
          "name": "Lihe Yang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736df",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e0",
          "name": "Ziang Zhang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e1",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e2",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-16T02:55:45.280Z",
      "title": "Depth Anything with Any Prior",
      "submittedOnDailyBy": {
        "_id": "663b4d6aa55b0634634cd302",
        "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
        "isPro": false,
        "fullname": "ZehanWang",
        "user": "ZehanWang",
        "type": "user"
      },
      "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
      "upvotes": 5,
      "discussionId": "6826bdaacaf89edf94b73753",
      "projectPage": "https://prior-depth-anything.github.io/",
      "githubRepo": "https://github.com/SpatialVision/Prior-Depth-Anything",
      "ai_keywords": [
        "metric depth maps",
        "coarse-to-fine pipeline",
        "pixel-level metric alignment",
        "distance-aware weighting",
        "metric priors",
        "domain gap",
        "generalization",
        "conditioned monocular depth estimation (MDE)",
        "zero-shot generalization",
        "depth completion",
        "super-resolution",
        "inpainting",
        "test-time improvements",
        "accuracy-efficiency trade-off"
      ]
    },
    "publishedAt": "2025-05-15T13:59:50.000Z",
    "title": "Depth Anything with Any Prior",
    "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663b4d6aa55b0634634cd302",
      "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
      "fullname": "ZehanWang",
      "name": "ZehanWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09990",
      "authors": [
        {
          "_id": "6826dce4682e62074d0ed686",
          "name": "Long Cheng",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed687",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed688",
          "name": "Yi Ru Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed689",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68a",
          "name": "Boyang Li",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68b",
          "name": "Yushan Huang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68c",
          "name": "Elvis Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68d",
          "name": "Ainaz Eftekhar",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68e",
          "name": "Jason Lee",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68f",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed690",
          "name": "Rose Hendrix",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed691",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed692",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed693",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed694",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
      ],
      "publishedAt": "2025-05-15T06:04:42.000Z",
      "submittedOnDailyAt": "2025-05-16T05:07:15.117Z",
      "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
      "upvotes": 5,
      "discussionId": "6826dce8682e62074d0ed7eb",
      "projectPage": "https://pointarena.github.io/",
      "githubRepo": "https://github.com/pointarena/pointarena",
      "ai_keywords": [
        "multimodal models",
        "referential object localization tasks",
        "PointArena",
        "Point-Bench",
        "reasoning categories",
        "Point-Battle",
        "Point-Act",
        "blind, pairwise model comparisons",
        "Molmo-72B",
        "supervised training",
        "precise pointing capabilities"
      ]
    },
    "publishedAt": "2025-05-15T02:04:42.000Z",
    "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
    "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09926",
      "authors": [
        {
          "_id": "68268c3408f7cb26defd82fc",
          "name": "Bin-Bin Gao",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fd",
          "name": "Yue Zhu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fe",
          "name": "Jiangtao Yan",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82ff",
          "name": "Yuezhi Cai",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8300",
          "name": "Weixi Zhang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8301",
          "name": "Meng Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8302",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8303",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8304",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8305",
          "name": "Chengjie Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T03:24:28.000Z",
      "submittedOnDailyAt": "2025-05-16T00:07:29.541Z",
      "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
      "upvotes": 4,
      "discussionId": "68268c3808f7cb26defd83bf",
      "ai_keywords": [
        "CLIP",
        "pre-trained vision-language models",
        "prompt templates",
        "token interactions",
        "fine-tuning",
        "adaptive visual and textual representations",
        "comparative learning",
        "query and normal image prompt",
        "contextual and aligned residual features",
        "residual features",
        "visual adapter",
        "textual adapter",
        "prompt-query adapter",
        "zero-/few-shot generalization",
        "training-free",
        "anomaly detection benchmarks",
        "industrial domains",
        "medical domains"
      ]
    },
    "publishedAt": "2025-05-14T23:24:28.000Z",
    "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08617",
      "authors": [
        {
          "_id": "6826aa068caf98415c50897f",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508980",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508981",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508982",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508983",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508984",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508985",
          "name": "Guanjie Chen",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508986",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508987",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508988",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508989",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T14:35:51.000Z",
      "submittedOnDailyAt": "2025-05-16T01:29:32.761Z",
      "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64264095ba51f8a2136946a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
        "isPro": false,
        "fullname": "Zhaochen Su",
        "user": "Warrieryes",
        "type": "user"
      },
      "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
      "upvotes": 4,
      "discussionId": "6826aa078caf98415c5089d6",
      "githubRepo": "https://github.com/zhaochen0110/OpenThinkIMG",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "OpenThinkIMG",
        "vision tool interfaces",
        "scalable trajectory generation",
        "policy initialization",
        "flexible training environment",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "V-ToolRL",
        "adaptive policies",
        "external vision tools",
        "optimal tool-usage strategies",
        "task success",
        "feedback from tool interactions",
        "chart reasoning tasks",
        "Qwen2-VL-2B",
        "Taco",
        "CogCom",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-05-13T10:35:51.000Z",
    "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
    "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64264095ba51f8a2136946a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
      "fullname": "Zhaochen Su",
      "name": "Warrieryes",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10046",
      "authors": [
        {
          "_id": "6826b00ed4c8b864e5ed1c0f",
          "name": "Bingda Tang",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c10",
          "name": "Boyang Zheng",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c11",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c12",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c13",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T07:43:23.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:37.654Z",
      "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
      "upvotes": 2,
      "discussionId": "6826b00ed4c8b864e5ed1c4f",
      "githubRepo": "https://github.com/tang-bd/fuse-dit",
      "ai_keywords": [
        "large language models (LLMs)",
        "diffusion transformers (DiTs)",
        "multi-modal generation",
        "text-to-image synthesis",
        "controlled comparisons",
        "established baselines",
        "important design choices",
        "training recipes",
        "reproducible recipe",
        "training at scale"
      ]
    },
    "publishedAt": "2025-05-15T03:43:23.000Z",
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
    "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 619
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09265",
      "authors": [
        {
          "_id": "682547e6501a31b392e78f6a",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:50.125Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:26.000Z",
      "submittedOnDailyAt": "2025-05-16T00:08:33.750Z",
      "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
      "upvotes": 2,
      "discussionId": "682547eb501a31b392e79038",
      "githubRepo": "https://github.com/gaobb/MetaUAS",
      "ai_keywords": [
        "Meta-learning",
        "Universal Anomaly Segmentation (MetaUAS)",
        "synthetic image pairs",
        "object-level changes",
        "local region changes",
        "prompt",
        "query images",
        "soft feature alignment module",
        "paired-image change perception",
        "single-image semantic segmentation",
        "universal anomaly segmentation",
        "pure vision model",
        "zero-shot",
        "few-shot",
        "full-shot anomaly segmentation"
      ]
    },
    "publishedAt": "2025-05-14T06:25:26.000Z",
    "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09264",
      "authors": [
        {
          "_id": "682548bff4997d78fe92cd57",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:51.290Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:14.000Z",
      "submittedOnDailyAt": "2025-05-16T00:10:20.034Z",
      "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
      "upvotes": 2,
      "discussionId": "682548c1f4997d78fe92cdbc",
      "githubRepo": "https://github.com/gaobb/OneNIP",
      "ai_keywords": [
        "self-attention transformers",
        "multi-class anomaly detection",
        "self-attention reconstruction models",
        "target features",
        "context",
        "latent space",
        "One Normal Image Prompt (OneNIP)",
        "supervised refiner",
        "reconstruction errors",
        "MVTec",
        "BTAD",
        "VisA"
      ]
    },
    "publishedAt": "2025-05-14T06:25:14.000Z",
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
    "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09263",
      "authors": [
        {
          "_id": "682549a75b5784e023ed7d8a",
          "name": "Guan Gui",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8b",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:31:36.508Z",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8d",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8e",
          "name": "Yunsheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:06.000Z",
      "submittedOnDailyAt": "2025-05-16T00:12:32.405Z",
      "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
      "upvotes": 2,
      "discussionId": "682549ac5b5784e023ed7e72",
      "ai_keywords": [
        "Anomaly-driven Generation (AnoGen)",
        "diffusion model",
        "anomaly distribution",
        "embedding",
        "bounding boxes",
        "weakly-supervised anomaly detection",
        "DRAEM",
        "DesTSeg",
        "MVTec",
        "AU-PR metric"
      ]
    },
    "publishedAt": "2025-05-14T06:25:06.000Z",
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
    "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.08581",
      "authors": [
        {
          "_id": "6825c47fbe0e3c3bfbf4d285",
          "user": {
            "_id": "66bf75432777c05070bf49dc",
            "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
            "isPro": false,
            "fullname": "Haofeng Liu",
            "user": "HeverLaw",
            "type": "user"
          },
          "name": "Haofeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:36.358Z",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d286",
          "name": "Mingqi Gao",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d287",
          "name": "Xuxiao Luo",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d288",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d289",
          "name": "Guanyi Qin",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28a",
          "name": "Junde Wu",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28b",
          "name": "Yueming Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
      ],
      "publishedAt": "2025-05-13T13:56:10.000Z",
      "submittedOnDailyAt": "2025-05-16T02:23:20.718Z",
      "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
      "submittedOnDailyBy": {
        "_id": "66bf75432777c05070bf49dc",
        "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
        "isPro": false,
        "fullname": "Haofeng Liu",
        "user": "HeverLaw",
        "type": "user"
      },
      "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
      "upvotes": 1,
      "discussionId": "6825c480be0e3c3bfbf4d2c0",
      "githubRepo": "https://github.com/jinlab-imvr/ReSurgSAM2",
      "ai_keywords": [
        "Segment Anything Model 2",
        "text-referred target detection",
        "cross-modal spatial-temporal Mamba",
        "initial frame identification",
        "diversity-driven long-term memory",
        "diversity-driven memory mechanism",
        "memory bank",
        "real-time tracking",
        "real-time operation"
      ]
    },
    "publishedAt": "2025-05-13T09:56:10.000Z",
    "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
    "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bf75432777c05070bf49dc",
      "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
      "fullname": "Haofeng Liu",
      "name": "HeverLaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]