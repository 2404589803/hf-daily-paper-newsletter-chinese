[
    "{'paper': {'id': '2412.09645', 'authors': [{'_id': '67610b5eb04baaf63514fcc9', 'user': {'_id': '61f24cbb88b9b5abbe184a85', 'avatarUrl': '/avatars/17c806013092ef18cbb1304cf9c5312e.svg', 'isPro': False, 'fullname': 'zhangfan', 'user': 'Fan-s', 'type': 'user'}, 'name': 'Fan Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:36.826Z', 'hidden': False}, {'_id': '67610b5eb04baaf63514fcca', 'user': {'_id': '6658d01c6f1a71ba56d6c273', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tc4nZrMuZQLfgt5aVxtH4.jpeg', 'isPro': False, 'fullname': 'Tian Shulin', 'user': 'shulin16', 'type': 'user'}, 'name': 'Shulin Tian', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:23:16.414Z', 'hidden': False}, {'_id': '67610b5eb04baaf63514fccb', 'user': {'_id': '60efe7fa0d920bc7805cada5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png', 'isPro': False, 'fullname': 'Ziqi Huang', 'user': 'Ziqi', 'type': 'user'}, 'name': 'Ziqi Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:23:04.978Z', 'hidden': False}, {'_id': '67610b5eb04baaf63514fccc', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '67610b5eb04baaf63514fccd', 'user': {'_id': '62ab1ac1d48b4d8b048a3473', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png', 'isPro': False, 'fullname': 'Ziwei Liu', 'user': 'liuziwei7', 'type': 'user'}, 'name': 'Ziwei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:22:56.546Z', 'hidden': False}], 'publishedAt': '2024-12-10T18:52:39.000Z', 'title': 'Evaluation Agent: Efficient and Promptable Evaluation Framework for\\n  Visual Generative Models', 'summary': \"Recent advancements in visual generative models have enabled high-quality\\nimage and video generation, opening diverse applications. However, evaluating\\nthese models often demands sampling hundreds or thousands of images or videos,\\nmaking the process computationally expensive, especially for diffusion-based\\nmodels with inherently slow sampling. Moreover, existing evaluation methods\\nrely on rigid pipelines that overlook specific user needs and provide numerical\\nresults without clear explanations. In contrast, humans can quickly form\\nimpressions of a model's capabilities by observing only a few samples. To mimic\\nthis, we propose the Evaluation Agent framework, which employs human-like\\nstrategies for efficient, dynamic, multi-round evaluations using only a few\\nsamples per round, while offering detailed, user-tailored analyses. It offers\\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\\nscalability across various models and tools. Experiments show that Evaluation\\nAgent reduces evaluation time to 10% of traditional methods while delivering\\ncomparable results. The Evaluation Agent framework is fully open-sourced to\\nadvance research in visual generative models and their efficient evaluation.\", 'upvotes': 23, 'discussionId': '67610b60b04baaf63514fd5d'}, 'publishedAt': '2024-12-17T00:34:04.556Z', 'title': 'Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09645.png', 'numComments': 1, 'submittedBy': {'_id': '60efe7fa0d920bc7805cada5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png', 'fullname': 'Ziqi Huang', 'name': 'Ziqi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}}",
    "{'paper': {'id': '2412.11919', 'authors': [{'_id': '6760ec669e797e610c5979e0', 'user': {'_id': '66e03eace17fb5ff054b7686', 'avatarUrl': '/avatars/2b739ff11e43dd9e701c647a92617f20.svg', 'isPro': False, 'fullname': 'Xiaoxi Li', 'user': 'lixiaoxi45', 'type': 'user'}, 'name': 'Xiaoxi Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:22:41.868Z', 'hidden': False}, {'_id': '6760ec669e797e610c5979e1', 'user': {'_id': '6695f14df0ffd8e3a379ad61', 'avatarUrl': '/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg', 'isPro': False, 'fullname': 'Jiajie Jin', 'user': 'jinjiajie', 'type': 'user'}, 'name': 'Jiajie Jin', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:21:52.824Z', 'hidden': False}, {'_id': '6760ec669e797e610c5979e2', 'name': 'Yujia Zhou', 'hidden': False}, {'_id': '6760ec669e797e610c5979e3', 'user': {'_id': '62f3a590261bc5fb2e072a5f', 'avatarUrl': '/avatars/d65d362ddc32aca3d6c564252d81e109.svg', 'isPro': False, 'fullname': 'YongkangWu', 'user': 'wuyongkang', 'type': 'user'}, 'name': 'Yongkang Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:21:03.390Z', 'hidden': False}, {'_id': '6760ec669e797e610c5979e4', 'user': {'_id': '67145068485b75297d6a5ec5', 'avatarUrl': '/avatars/67cb59ec072783ee5e4d909c4615bd44.svg', 'isPro': False, 'fullname': 'LiZhongHua', 'user': 'Benen2024', 'type': 'user'}, 'name': 'Zhonghua Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:20:44.760Z', 'hidden': False}, {'_id': '6760ec669e797e610c5979e5', 'name': 'Qi Ye', 'hidden': False}, {'_id': '6760ec669e797e610c5979e6', 'user': {'_id': '66f0bf59e9d50ec57febf751', 'avatarUrl': '/avatars/be97941e60064e5dd806c6fe9db3c537.svg', 'isPro': False, 'fullname': 'Zhicheng Dou', 'user': 'douzc', 'type': 'user'}, 'name': 'Zhicheng Dou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:20:11.013Z', 'hidden': False}], 'publishedAt': '2024-12-16T16:03:25.000Z', 'title': 'RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\\n  Evidence within Generation', 'summary': \"Large language models (LLMs) exhibit remarkable generative capabilities but\\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\\nan effective solution by incorporating external knowledge, but existing methods\\nstill face several limitations: additional deployment costs of separate\\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\\njoint optimization of retrieval and generation. To address these issues, we\\npropose RetroLLM, a unified framework that integrates retrieval and\\ngeneration into a single, cohesive process, enabling LLMs to directly generate\\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\\nmitigate false pruning in the process of constrained evidence generation, we\\nintroduce (1) hierarchical FM-Index constraints, which generate\\ncorpus-constrained clues to identify a subset of relevant documents before\\nevidence generation, reducing irrelevant decoding space; and (2) a\\nforward-looking constrained decoding strategy, which considers the relevance of\\nfuture sequences to improve evidence accuracy. Extensive experiments on five\\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\\nin-domain and out-of-domain tasks. The code is available at\\nhttps://github.com/sunnynexus/RetroLLM.\", 'upvotes': 23, 'discussionId': '6760ec679e797e610c597a17'}, 'publishedAt': '2024-12-16T22:14:50.259Z', 'title': 'RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11919.png', 'numComments': 1, 'submittedBy': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'fullname': 'KABI', 'name': 'dongguanting', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}}",
    "{'paper': {'id': '2412.09871', 'authors': [{'_id': '675ffc2a79e2036a7090d4ff', 'user': {'_id': '6279d059812ee439d9c8057c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6279d059812ee439d9c8057c/2E3-rFQganxuz3jDL-Esc.jpeg', 'isPro': False, 'fullname': 'Artidoro Pagnoni', 'user': 'artidoro', 'type': 'user'}, 'name': 'Artidoro Pagnoni', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:04:12.348Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d500', 'name': 'Ram Pasunuru', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d501', 'name': 'Pedro Rodriguez', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d502', 'name': 'John Nguyen', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d503', 'user': {'_id': '5e6b7153d4cd9779932a7600', 'avatarUrl': '/avatars/c1304f100758c5e2a54fd4d47f638841.svg', 'isPro': False, 'fullname': 'Benjamin Muller', 'user': 'benjamin-mlr', 'type': 'user'}, 'name': 'Benjamin Muller', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:24:59.088Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d504', 'user': {'_id': '66edd75d5718c003b9c66c09', 'avatarUrl': '/avatars/2bc2067b0c2a61092e401c3237921f19.svg', 'isPro': False, 'fullname': 'Margaret Li', 'user': 'marg33', 'type': 'user'}, 'name': 'Margaret Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:24:38.264Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d505', 'user': {'_id': '64f8d07dd493d8b0d2c04ecd', 'avatarUrl': '/avatars/7112232dcadcd2a7c12ec635d781b3ea.svg', 'isPro': False, 'fullname': 'Zhou', 'user': 'Chunting', 'type': 'user'}, 'name': 'Chunting Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:24:30.461Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d506', 'name': 'Lili Yu', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d507', 'user': {'_id': '62f023a36a027498eaa2f9cc', 'avatarUrl': '/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg', 'isPro': False, 'fullname': 'Jason Weston', 'user': 'spermwhale', 'type': 'user'}, 'name': 'Jason Weston', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:24:09.875Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d508', 'name': 'Luke Zettlemoyer', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d509', 'user': {'_id': '664767349e82dfd4a36db460', 'avatarUrl': '/avatars/df9af7bdc1d5117571d1b2349feade44.svg', 'isPro': False, 'fullname': 'Gargi Ghosh', 'user': 'gargighosh', 'type': 'user'}, 'name': 'Gargi Ghosh', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:24:01.190Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d50a', 'user': {'_id': '5e67eda71009063689407484', 'avatarUrl': '/avatars/6d7688faba888d463584eaa02ad011e3.svg', 'isPro': False, 'fullname': 'Mike Lewis', 'user': 'mikelewis0', 'type': 'user'}, 'name': 'Mike Lewis', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:23:55.718Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d50b', 'user': {'_id': '6508c81db44445e9b36f3a1e', 'avatarUrl': '/avatars/7d4b257398c601b482b9655f046021f3.svg', 'isPro': False, 'fullname': 'Ari Holtzman', 'user': 'ariholtzman', 'type': 'user'}, 'name': 'Ari Holtzman', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:23:40.460Z', 'hidden': False}, {'_id': '675ffc2a79e2036a7090d50c', 'name': 'Srinivasan Iyer', 'hidden': False}], 'publishedAt': '2024-12-13T05:33:32.000Z', 'title': 'Byte Latent Transformer: Patches Scale Better Than Tokens', 'summary': 'We introduce the Byte Latent Transformer (BLT), a new byte-level LLM\\narchitecture that, for the first time, matches tokenization-based LLM\\nperformance at scale with significant improvements in inference efficiency and\\nrobustness. BLT encodes bytes into dynamically sized patches, which serve as\\nthe primary units of computation. Patches are segmented based on the entropy of\\nthe next byte, allocating more compute and model capacity where increased data\\ncomplexity demands it. We present the first FLOP controlled scaling study of\\nbyte-level models up to 8B parameters and 4T training bytes. Our results\\ndemonstrate the feasibility of scaling models trained on raw bytes without a\\nfixed vocabulary. Both training and inference efficiency improve due to\\ndynamically selecting long patches when data is predictable, along with\\nqualitative improvements on reasoning and long tail generalization. Overall,\\nfor fixed inference costs, BLT shows significantly better scaling than\\ntokenization-based models, by simultaneously growing both patch and model size.', 'upvotes': 21, 'discussionId': '675ffc2b79e2036a7090d5aa'}, 'publishedAt': '2024-12-17T03:31:23.583Z', 'title': 'Byte Latent Transformer: Patches Scale Better Than Tokens', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6279d059812ee439d9c8057c/35ZLbFfPrqyq5nYPXPk2b.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09871.png', 'numComments': 2, 'submittedBy': {'_id': '6279d059812ee439d9c8057c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6279d059812ee439d9c8057c/2E3-rFQganxuz3jDL-Esc.jpeg', 'fullname': 'Artidoro Pagnoni', 'name': 'artidoro', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 20}}",
    "{'paper': {'id': '2412.10316', 'authors': [{'_id': '676044eedd3e9eeed65d9a23', 'user': {'_id': '6362801380c1a705a6ea54ac', 'avatarUrl': '/avatars/041ad5abf9be42e336938f51ebb8746c.svg', 'isPro': False, 'fullname': 'Yaowei Li', 'user': 'Yw22', 'type': 'user'}, 'name': 'Yaowei Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:41:12.208Z', 'hidden': False}, {'_id': '676044eedd3e9eeed65d9a24', 'user': {'_id': '650447dd52ca06fef957f05d', 'avatarUrl': '/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg', 'isPro': False, 'fullname': 'Yuxuan BIAN', 'user': 'BianYx', 'type': 'user'}, 'name': 'Yuxuan Bian', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:41:18.245Z', 'hidden': False}, {'_id': '676044eedd3e9eeed65d9a25', 'user': {'_id': '62d4577bc85b0fcf7fde39bb', 'avatarUrl': '/avatars/a3a5729e33ae89ce9ba408830db3c835.svg', 'isPro': False, 'fullname': 'Xuan Ju', 'user': 'juxuan27', 'type': 'user'}, 'name': 'Xuan Ju', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:41:23.731Z', 'hidden': False}, {'_id': '676044eedd3e9eeed65d9a26', 'user': {'_id': '658409ceca19ccf6d9989add', 'avatarUrl': '/avatars/3ac1dbd25e52435185babdeb3da28875.svg', 'isPro': False, 'fullname': 'Zhaoyang Zhang', 'user': 'ZyZcuhk', 'type': 'user'}, 'name': 'Zhaoyang Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:04:08.717Z', 'hidden': False}, {'_id': '676044eedd3e9eeed65d9a27', 'user': {'_id': '63ca3ddc04c979828310bfcb', 'avatarUrl': '/avatars/615e0d8622950b4408b40d550f02a894.svg', 'isPro': False, 'fullname': 'Ying Shan', 'user': 'yshan2u', 'type': 'user'}, 'name': 'Ying Shan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:41:29.223Z', 'hidden': False}, {'_id': '676044eedd3e9eeed65d9a28', 'name': 'Qiang Xu', 'hidden': False}], 'publishedAt': '2024-12-13T17:58:06.000Z', 'title': 'BrushEdit: All-In-One Image Inpainting and Editing', 'summary': 'Image editing has advanced significantly with the development of diffusion\\nmodels using both inversion-based and instruction-based methods. However,\\ncurrent inversion-based approaches struggle with big modifications (e.g.,\\nadding or removing objects) due to the structured nature of inversion noise,\\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\\nconstrain users to black-box operations, limiting direct interaction for\\nspecifying editing regions and intensity. To address these limitations, we\\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\\nparadigm, which leverages multimodal large language models (MLLMs) and image\\ninpainting models to enable autonomous, user-friendly, and interactive\\nfree-form instruction editing. Specifically, we devise a system enabling\\nfree-form instruction editing by integrating MLLMs and a dual-branch image\\ninpainting model in an agent-cooperative framework to perform editing category\\nclassification, main object identification, mask acquisition, and editing area\\ninpainting. Extensive experiments show that our framework effectively combines\\nMLLMs and inpainting models, achieving superior performance across seven\\nmetrics including mask region preservation and editing effect coherence.', 'upvotes': 19, 'discussionId': '67604505dd3e9eeed65da8b6'}, 'publishedAt': '2024-12-17T02:10:09.389Z', 'title': 'BrushEdit: All-In-One Image Inpainting and Editing', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fvw43H9zR5VRbxTpoYvz-.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10316.png', 'numComments': 2, 'submittedBy': {'_id': '658409ceca19ccf6d9989add', 'avatarUrl': '/avatars/3ac1dbd25e52435185babdeb3da28875.svg', 'fullname': 'Zhaoyang Zhang', 'name': 'ZyZcuhk', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.11815', 'authors': [{'_id': '6760ea776fc1fd2b8ed9c802', 'user': {'_id': '64970d3d9c3b29dca8633f87', 'avatarUrl': '/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg', 'isPro': False, 'fullname': 'JunhaoZhuang', 'user': 'JunhaoZhuang', 'type': 'user'}, 'name': 'Junhao Zhuang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:56.429Z', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c803', 'user': {'_id': '62d4577bc85b0fcf7fde39bb', 'avatarUrl': '/avatars/a3a5729e33ae89ce9ba408830db3c835.svg', 'isPro': False, 'fullname': 'Xuan Ju', 'user': 'juxuan27', 'type': 'user'}, 'name': 'Xuan Ju', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:25:47.793Z', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c804', 'user': {'_id': '658409ceca19ccf6d9989add', 'avatarUrl': '/avatars/3ac1dbd25e52435185babdeb3da28875.svg', 'isPro': False, 'fullname': 'Zhaoyang Zhang', 'user': 'ZyZcuhk', 'type': 'user'}, 'name': 'Zhaoyang Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:54.477Z', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c805', 'name': 'Yong Liu', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c806', 'name': 'Shiyi Zhang', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c807', 'name': 'Chun Yuan', 'hidden': False}, {'_id': '6760ea776fc1fd2b8ed9c808', 'user': {'_id': '63ca3ddc04c979828310bfcb', 'avatarUrl': '/avatars/615e0d8622950b4408b40d550f02a894.svg', 'isPro': False, 'fullname': 'Ying Shan', 'user': 'yshan2u', 'type': 'user'}, 'name': 'Ying Shan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:25:16.323Z', 'hidden': False}], 'publishedAt': '2024-12-16T14:32:49.000Z', 'title': 'ColorFlow: Retrieval-Augmented Image Sequence Colorization', 'summary': 'Automatic black-and-white image sequence colorization while preserving\\ncharacter and object identity (ID) is a complex task with significant market\\ndemand, such as in cartoon or comic series colorization. Despite advancements\\nin visual colorization using large-scale generative models like diffusion\\nmodels, challenges with controllability and identity consistency persist,\\nmaking current solutions unsuitable for industrial application.To address this,\\nwe propose ColorFlow, a three-stage diffusion-based framework tailored for\\nimage sequence colorization in industrial applications. Unlike existing methods\\nthat require per-ID finetuning or explicit ID embedding extraction, we propose\\na novel robust and generalizable Retrieval Augmented Colorization pipeline for\\ncolorizing images with relevant color references. Our pipeline also features a\\ndual-branch design: one branch for color identity extraction and the other for\\ncolorization, leveraging the strengths of diffusion models. We utilize the\\nself-attention mechanism in diffusion models for strong in-context learning and\\ncolor identity matching. To evaluate our model, we introduce ColorFlow-Bench, a\\ncomprehensive benchmark for reference-based colorization. Results show that\\nColorFlow outperforms existing models across multiple metrics, setting a new\\nstandard in sequential image colorization and potentially benefiting the art\\nindustry. We release our codes and models on our project page:\\nhttps://zhuang2002.github.io/ColorFlow/.', 'upvotes': 18, 'discussionId': '6760ea7c6fc1fd2b8ed9c987'}, 'publishedAt': '2024-12-17T00:19:04.238Z', 'title': 'ColorFlow: Retrieval-Augmented Image Sequence Colorization', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/OfpEDLPXziooneMYnNcDF.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11815.png', 'numComments': 3, 'submittedBy': {'_id': '658409ceca19ccf6d9989add', 'avatarUrl': '/avatars/3ac1dbd25e52435185babdeb3da28875.svg', 'fullname': 'Zhaoyang Zhang', 'name': 'ZyZcuhk', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.12095', 'authors': [{'_id': '67610887df97bdafe87c37aa', 'name': 'Chaorui Deng', 'hidden': False}, {'_id': '67610887df97bdafe87c37ab', 'name': 'Deyao Zh', 'hidden': False}, {'_id': '67610887df97bdafe87c37ac', 'user': {'_id': '61fb81006374891646732f37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg', 'isPro': False, 'fullname': 'Kunchang Li', 'user': 'Andy1621', 'type': 'user'}, 'name': 'Kunchang Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:39.007Z', 'hidden': False}, {'_id': '67610887df97bdafe87c37ad', 'name': 'Shi Guan', 'hidden': False}, {'_id': '67610887df97bdafe87c37ae', 'name': 'Haoqi Fan', 'hidden': False}], 'publishedAt': '2024-12-16T18:59:29.000Z', 'title': 'Causal Diffusion Transformers for Generative Modeling', 'summary': \"We introduce Causal Diffusion as the autoregressive (AR) counterpart of\\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\\nto both discrete and continuous modalities and compatible with existing\\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\\ncombine diffusion with AR models, we show that introducing sequential\\nfactorization to a diffusion model can substantially improve its performance\\nand enables a smooth transition between AR and diffusion generation modes.\\nHence, we propose CausalFusion - a decoder-only transformer that\\ndual-factorizes data across sequential tokens and diffusion noise levels,\\nleading to state-of-the-art results on the ImageNet generation benchmark while\\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\\nin-context reasoning. We further demonstrate CausalFusion's multimodal\\ncapabilities through a joint image generation and captioning model, and\\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\\nWe hope that this work could provide the community with a fresh perspective on\\ntraining multimodal models over discrete and continuous data.\", 'upvotes': 15, 'discussionId': '6761088ddf97bdafe87c3b4e'}, 'publishedAt': '2024-12-17T00:14:39.699Z', 'title': 'Causal Diffusion Transformers for Generative Modeling', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12095.png', 'numComments': 1, 'submittedBy': {'_id': '61fb81006374891646732f37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg', 'fullname': 'Kunchang Li', 'name': 'Andy1621', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 13}}",
    "{'paper': {'id': '2412.11231', 'authors': [{'_id': '6760eccda4962b8ccefc461e', 'user': {'_id': '66fa30dee6210a5175235a3c', 'avatarUrl': '/avatars/dff82a480fe61789bbb59285fb5c4ddb.svg', 'isPro': False, 'fullname': 'Tingfeng Hui', 'user': 'Chaox72', 'type': 'user'}, 'name': 'Tingfeng Hui', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:52.567Z', 'hidden': False}, {'_id': '6760eccda4962b8ccefc461f', 'name': 'Lulu Zhao', 'hidden': False}, {'_id': '6760eccda4962b8ccefc4620', 'user': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'isPro': False, 'fullname': 'KABI', 'user': 'dongguanting', 'type': 'user'}, 'name': 'Guanting Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:34:16.450Z', 'hidden': False}, {'_id': '6760eccda4962b8ccefc4621', 'name': 'Yaqi Zhang', 'hidden': False}, {'_id': '6760eccda4962b8ccefc4622', 'name': 'Hua Zhou', 'hidden': False}, {'_id': '6760eccda4962b8ccefc4623', 'name': 'Sen Su', 'hidden': False}], 'publishedAt': '2024-12-15T16:07:48.000Z', 'title': 'Smaller Language Models Are Better Instruction Evolvers', 'summary': 'Instruction tuning has been widely used to unleash the complete potential of\\nlarge language models. Notably, complex and diverse instructions are of\\nsignificant importance as they can effectively align models with various\\ndownstream tasks. However, current approaches to constructing large-scale\\ninstructions predominantly favour powerful models such as GPT-4 or those with\\nover 70 billion parameters, under the empirical presumption that such larger\\nlanguage models (LLMs) inherently possess enhanced capabilities. In this study,\\nwe question this prevalent assumption and conduct an in-depth exploration into\\nthe potential of smaller language models (SLMs) in the context of instruction\\nevolution. Extensive experiments across three scenarios of instruction\\nevolution reveal that smaller language models (SLMs) can synthesize more\\neffective instructions than LLMs. Further analysis demonstrates that SLMs\\npossess a broader output space during instruction evolution, resulting in more\\ncomplex and diverse variants. We also observe that the existing metrics fail to\\nfocus on the impact of the instructions. Thus, we propose Instruction\\nComplex-Aware IFD (IC-IFD), which introduces instruction complexity in the\\noriginal IFD score to evaluate the effectiveness of instruction data more\\naccurately. Our source code is available at:\\nhttps://github.com/HypherX/Evolution-Analysis{https://github.com/HypherX/Evolution-Analysis}', 'upvotes': 13, 'discussionId': '6760eccea4962b8ccefc4690'}, 'publishedAt': '2024-12-16T22:35:06.109Z', 'title': 'Smaller Language Models Are Better Instruction Evolvers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11231.png', 'numComments': 1, 'submittedBy': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'fullname': 'KABI', 'name': 'dongguanting', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}}",
    "{'paper': {'id': '2412.12083', 'authors': [{'_id': '6760ee8b8cd4d1c2b6f45f23', 'user': {'_id': '632a80706813868fa4a649e3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632a80706813868fa4a649e3/MbTsAYGadNwS3-G5vxEnm.jpeg', 'isPro': False, 'fullname': 'Zhibing LI', 'user': 'lizb6626', 'type': 'user'}, 'name': 'Zhibing Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:46.852Z', 'hidden': False}, {'_id': '6760ee8b8cd4d1c2b6f45f24', 'user': {'_id': '62fae9328e137d7c4b896498', 'avatarUrl': '/avatars/1bda39dec585c099417cc9daa9f53c42.svg', 'isPro': False, 'fullname': 'Tong Wu', 'user': 'tongwu2020', 'type': 'user'}, 'name': 'Tong Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:44:25.382Z', 'hidden': False}, {'_id': '6760ee8b8cd4d1c2b6f45f25', 'user': {'_id': '65367c40061949598892dbdc', 'avatarUrl': '/avatars/4baf27263841471cbd5f629a8b99424d.svg', 'isPro': False, 'fullname': 'Jing Tan', 'user': 'jingtan', 'type': 'user'}, 'name': 'Jing Tan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:44:07.182Z', 'hidden': False}, {'_id': '6760ee8b8cd4d1c2b6f45f26', 'user': {'_id': '64de20c5808492ba6e65d124', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg', 'isPro': False, 'fullname': 'Zhang Mengchen', 'user': 'Dubhe-zmc', 'type': 'user'}, 'name': 'Mengchen Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:43:09.687Z', 'hidden': False}, {'_id': '6760ee8b8cd4d1c2b6f45f27', 'user': {'_id': '64638c4d51fa6e63060521b5', 'avatarUrl': '/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg', 'isPro': False, 'fullname': 'JIaqi', 'user': 'Jiaqiwang', 'type': 'user'}, 'name': 'Jiaqi Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:42:54.901Z', 'hidden': False}, {'_id': '6760ee8b8cd4d1c2b6f45f28', 'user': {'_id': '636317ed80c1a705a6eff396', 'avatarUrl': '/avatars/3db090e101b916d9256d0d3e043db71d.svg', 'isPro': False, 'fullname': 'Dahua Lin', 'user': 'lindahua', 'type': 'user'}, 'name': 'Dahua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T09:42:07.153Z', 'hidden': False}], 'publishedAt': '2024-12-16T18:52:56.000Z', 'title': 'IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\\n  Illuminations', 'summary': 'Capturing geometric and material information from images remains a\\nfundamental challenge in computer vision and graphics. Traditional\\noptimization-based methods often require hours of computational time to\\nreconstruct geometry, material properties, and environmental lighting from\\ndense multi-view inputs, while still struggling with inherent ambiguities\\nbetween lighting and material. On the other hand, learning-based approaches\\nleverage rich material priors from existing 3D object datasets but face\\nchallenges with maintaining multi-view consistency. In this paper, we introduce\\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\\nan arbitrary number of images under varying illuminations. Our method achieves\\naccurate and multi-view consistent estimation on surface normals and material\\nproperties. This is made possible through a novel cross-view, cross-domain\\nattention module and an illumination-augmented, view-adaptive training\\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\\nconditions, supporting robust training. Extensive experiments demonstrate that\\nIDArb outperforms state-of-the-art methods both qualitatively and\\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\\nhighlighting its broad applications in realistic 3D content creation.', 'upvotes': 10, 'discussionId': '6760ee8e8cd4d1c2b6f4600d'}, 'publishedAt': '2024-12-16T22:28:04.285Z', 'title': 'IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/632a80706813868fa4a649e3/6DhlMJpW3hCJ76o25ndAo.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12083.png', 'numComments': 1, 'submittedBy': {'_id': '632a80706813868fa4a649e3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632a80706813868fa4a649e3/MbTsAYGadNwS3-G5vxEnm.jpeg', 'fullname': 'Zhibing LI', 'name': 'lizb6626', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.11605', 'authors': [{'_id': '6760f48fa38bf4e171b85bb8', 'user': {'_id': '627626d42d26ac639e56f565', 'avatarUrl': '/avatars/805c5f909f52656345b8bde486c9fa8f.svg', 'isPro': False, 'fullname': 'Jiale Cheng', 'user': 'CCCCCC', 'type': 'user'}, 'name': 'Jiale Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:44.419Z', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bb9', 'name': 'Xiao Liu', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bba', 'user': {'_id': '65eaf755ab0a6a90da55ab58', 'avatarUrl': '/avatars/a46890a9d067a913513edf3759f12c85.svg', 'isPro': False, 'fullname': 'Cunxiang Wang', 'user': 'wangcunxiang', 'type': 'user'}, 'name': 'Cunxiang Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:21:56.020Z', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bbb', 'name': 'Xiaotao Gu', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bbc', 'user': {'_id': '6445e2e83a0fa0e98cd0c995', 'avatarUrl': '/avatars/5ab8e5fbfaa9e5b456fb94eaa7e62832.svg', 'isPro': False, 'fullname': 'Yida Lu', 'user': 'lrxl', 'type': 'user'}, 'name': 'Yida Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:22:10.606Z', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bbd', 'name': 'Dan Zhang', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bbe', 'user': {'_id': '640e73bdfdeaae1390857b62', 'avatarUrl': '/avatars/cd6779e30f716002a7838ed93d5c0754.svg', 'isPro': False, 'fullname': 'Yuxiao Dong', 'user': 'yuxiaod', 'type': 'user'}, 'name': 'Yuxiao Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:22:40.968Z', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bbf', 'name': 'Jie Tang', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bc0', 'user': {'_id': '62ec23fcbd19e355478fc584', 'avatarUrl': '/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg', 'isPro': False, 'fullname': 'Hongning Wang', 'user': 'howang', 'type': 'user'}, 'name': 'Hongning Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:22:48.091Z', 'hidden': False}, {'_id': '6760f48fa38bf4e171b85bc1', 'user': {'_id': '650d3b9ee65b12d3b97ccfbd', 'avatarUrl': '/avatars/1e0ecb1c4175fe1240b0eaad3fcc61a6.svg', 'isPro': False, 'fullname': 'huangminlie', 'user': 'huangminlie', 'type': 'user'}, 'name': 'Minlie Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:23:16.966Z', 'hidden': False}], 'publishedAt': '2024-12-16T09:47:43.000Z', 'title': 'SPaR: Self-Play with Tree-Search Refinement to Improve\\n  Instruction-Following in Large Language Models', 'summary': 'Instruction-following is a fundamental capability of language models,\\nrequiring the model to recognize even the most subtle requirements in the\\ninstructions and accurately reflect them in its output. Such an ability is\\nwell-suited for and often optimized by preference learning. However, existing\\nmethods often directly sample multiple independent responses from the model\\nwhen creating preference pairs. Such practice can introduce content variations\\nirrelevant to whether the instruction is precisely followed (e.g., different\\nexpressions about the same semantic), interfering with the goal of teaching\\nmodels to recognize the key differences that lead to improved instruction\\nfollowing. In light of this, we introduce SPaR, a self-play framework\\nintegrating tree-search self-refinement to yield valid and comparable\\npreference pairs free from distractions. By playing against itself, an LLM\\nemploys a tree-search strategy to refine its previous responses with respect to\\nthe instruction while minimizing unnecessary variations. Our experiments show\\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\\nFurthermore, SPaR demonstrates promising scalability and transferability,\\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\\ninference scaling in tree search would impact model performance. Our code and\\ndata are publicly available at https://github.com/thu-coai/SPaR.', 'upvotes': 9, 'discussionId': '6760f490a38bf4e171b85c10'}, 'publishedAt': '2024-12-16T23:00:32.367Z', 'title': 'SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11605.png', 'numComments': 1, 'submittedBy': {'_id': '627626d42d26ac639e56f565', 'avatarUrl': '/avatars/805c5f909f52656345b8bde486c9fa8f.svg', 'fullname': 'Jiale Cheng', 'name': 'CCCCCC', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.12091', 'authors': [{'_id': '67610a192504d8a0fbf6a83a', 'name': 'Hanwen Liang', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a83b', 'user': {'_id': '63f54aa73aa49d8cb97b84bc', 'avatarUrl': '/avatars/c73c5870039611ab9162daad46a1ba20.svg', 'isPro': False, 'fullname': 'junli cao', 'user': 'jlcao2', 'type': 'user'}, 'name': 'Junli Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:19:05.096Z', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a83c', 'user': {'_id': '636c0c1a15cd58e915bb8139', 'avatarUrl': '/avatars/7c675ac6a7d303d3425e498c4e939eb0.svg', 'isPro': False, 'fullname': 'Vidit Goel', 'user': 'vidit98', 'type': 'user'}, 'name': 'Vidit Goel', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:19:00.007Z', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a83d', 'user': {'_id': '645fed74335c21d19f3bf76c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645fed74335c21d19f3bf76c/gwVsllRWtSHbg4a1erkdF.jpeg', 'isPro': False, 'fullname': 'Guocheng Qian', 'user': 'guochengqian', 'type': 'user'}, 'name': 'Guocheng Qian', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:18:51.075Z', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a83e', 'user': {'_id': '6338047fe1b78345008dcd22', 'avatarUrl': '/avatars/7306138e83ca642f1638a39efa169c7f.svg', 'isPro': False, 'fullname': 'Sergei Korolev', 'user': 'ZanyRumata', 'type': 'user'}, 'name': 'Sergei Korolev', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:18:45.439Z', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a83f', 'name': 'Demetri Terzopoulos', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a840', 'name': 'Konstantinos N. Plataniotis', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a841', 'name': 'Sergey Tulyakov', 'hidden': False}, {'_id': '67610a192504d8a0fbf6a842', 'name': 'Jian Ren', 'hidden': False}], 'publishedAt': '2024-12-16T18:58:17.000Z', 'title': 'Wonderland: Navigating 3D Scenes from a Single Image', 'summary': 'This paper addresses a challenging question: How can we efficiently create\\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\\nmethods face several constraints, such as requiring multi-view data,\\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\\novercome these limitations. Specifically, we introduce a large-scale\\nreconstruction model that uses latents from a video diffusion model to predict\\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\\ndiffusion model is designed to create videos precisely following specified\\ncamera trajectories, allowing it to generate compressed video latents that\\ncontain multi-view information while maintaining 3D consistency. We train the\\n3D reconstruction model to operate on the video latent space with a progressive\\ntraining strategy, enabling the efficient generation of high-quality,\\nwide-scope, and generic 3D scenes. Extensive evaluations across various\\ndatasets demonstrate that our model significantly outperforms existing methods\\nfor single-view 3D scene generation, particularly with out-of-domain images.\\nFor the first time, we demonstrate that a 3D reconstruction model can be\\neffectively built upon the latent space of a diffusion model to realize\\nefficient 3D scene generation.', 'upvotes': 8, 'discussionId': '67610a1a2504d8a0fbf6a87a'}, 'publishedAt': '2024-12-17T00:28:02.049Z', 'title': 'Wonderland: Navigating 3D Scenes from a Single Image', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12091.png', 'numComments': 1, 'submittedBy': {'_id': '63f54aa73aa49d8cb97b84bc', 'avatarUrl': '/avatars/c73c5870039611ab9162daad46a1ba20.svg', 'fullname': 'junli cao', 'name': 'jlcao2', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.11279', 'authors': [{'_id': '676196b4ea3649f2ad086a49', 'name': 'Hao Shao', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4a', 'name': 'Shulun Wang', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4b', 'name': 'Yang Zhou', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4c', 'name': 'Guanglu Song', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4d', 'name': 'Dailan He', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4e', 'name': 'Shuo Qin', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a4f', 'name': 'Zhuofan Zong', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a50', 'name': 'Bingqi Ma', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a51', 'name': 'Yu Liu', 'hidden': False}, {'_id': '676196b4ea3649f2ad086a52', 'name': 'Hongsheng Li', 'hidden': False}], 'publishedAt': '2024-12-15T18:58:32.000Z', 'title': 'VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video\\n  Face Swapping', 'summary': 'Video face swapping is becoming increasingly popular across various\\napplications, yet existing methods primarily focus on static images and\\nstruggle with video face swapping because of temporal consistency and complex\\nscenarios. In this paper, we present the first diffusion-based framework\\nspecifically designed for video face swapping. Our approach introduces a novel\\nimage-video hybrid training framework that leverages both abundant static image\\ndata and temporal video sequences, addressing the inherent limitations of\\nvideo-only training. The framework incorporates a specially designed diffusion\\nmodel coupled with a VidFaceVAE that effectively processes both types of data\\nto better maintain temporal coherence of the generated videos. To further\\ndisentangle identity and pose features, we construct the Attribute-Identity\\nDisentanglement Triplet (AIDT) Dataset, where each triplet has three face\\nimages, with two images sharing the same pose and two sharing the same\\nidentity. Enhanced with a comprehensive occlusion augmentation, this dataset\\nalso improves robustness against occlusions. Additionally, we integrate 3D\\nreconstruction techniques as input conditioning to our network for handling\\nlarge pose variations. Extensive experiments demonstrate that our framework\\nachieves superior performance in identity preservation, temporal consistency,\\nand visual quality compared to existing methods, while requiring fewer\\ninference steps. Our approach effectively mitigates key challenges in video\\nface swapping, including temporal flickering, identity preservation, and\\nrobustness to occlusions and pose variations.', 'upvotes': 7, 'discussionId': '676196b7ea3649f2ad086b4a'}, 'publishedAt': '2024-12-17T10:59:42.454Z', 'title': 'VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11279.png', 'numComments': 1, 'submittedBy': {'_id': '651bda4337fecec1fe8bb09f', 'avatarUrl': '/avatars/f1b3d2b9874022e64d8e5dfeac0e7120.svg', 'fullname': 'Hao Shao', 'name': 'deepcs233', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.12094', 'authors': [{'_id': '67615d54402071c9e07c0c42', 'name': 'Guoxuan Chen', 'hidden': False}, {'_id': '67615d54402071c9e07c0c43', 'user': {'_id': '637751ad7169ed981703d26a', 'avatarUrl': '/avatars/9823f115040a3d6b0240038010e46cb6.svg', 'isPro': False, 'fullname': 'Han Shi', 'user': 'shihan96', 'type': 'user'}, 'name': 'Han Shi', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T11:18:01.539Z', 'hidden': False}, {'_id': '67615d54402071c9e07c0c44', 'name': 'Jiawei Li', 'hidden': False}, {'_id': '67615d54402071c9e07c0c45', 'name': 'Yihang Gao', 'hidden': False}, {'_id': '67615d54402071c9e07c0c46', 'name': 'Xiaozhe Ren', 'hidden': False}, {'_id': '67615d54402071c9e07c0c47', 'name': 'Yimeng Chen', 'hidden': False}, {'_id': '67615d54402071c9e07c0c48', 'name': 'Xin Jiang', 'hidden': False}, {'_id': '67615d54402071c9e07c0c49', 'name': 'Zhenguo Li', 'hidden': False}, {'_id': '67615d54402071c9e07c0c4a', 'name': 'Weiyang Liu', 'hidden': False}, {'_id': '67615d54402071c9e07c0c4b', 'name': 'Chao Huang', 'hidden': False}], 'publishedAt': '2024-12-16T18:58:57.000Z', 'title': 'SepLLM: Accelerate Large Language Models by Compressing One Segment into\\n  One Separator', 'summary': \"Large Language Models (LLMs) have exhibited exceptional performance across a\\nspectrum of natural language processing tasks. However, their substantial sizes\\npose considerable challenges, particularly in computational demands and\\ninference speed, due to their quadratic complexity. In this work, we have\\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\\nseparators) contribute disproportionately to attention scores compared to\\nsemantically meaningful tokens. This observation suggests that information of\\nthe segments between these separator tokens can be effectively condensed into\\nthe separator tokens themselves without significant information loss. Guided by\\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\\ninference by compressing these segments and eliminating redundant tokens.\\nAdditionally, we implement efficient kernels for training acceleration.\\nExperimental results across training-free, training-from-scratch, and\\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\\ntokens or more while maintaining consistent language modeling capabilities.\", 'upvotes': 6, 'discussionId': '67615d59402071c9e07c0d91'}, 'publishedAt': '2024-12-17T06:16:29.864Z', 'title': 'SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12094.png', 'numComments': 1, 'submittedBy': {'_id': '637751ad7169ed981703d26a', 'avatarUrl': '/avatars/9823f115040a3d6b0240038010e46cb6.svg', 'fullname': 'Han Shi', 'name': 'shihan96', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.11258', 'authors': [{'_id': '6760e4c35b8363d75f3e7241', 'user': {'_id': '64b4ab62eec33e27dcd733b5', 'avatarUrl': '/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg', 'isPro': False, 'fullname': 'Xinli XU', 'user': 'Xxlbigbrother', 'type': 'user'}, 'name': 'Xinli Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:58.140Z', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7242', 'user': {'_id': '64758ee7d815855e4efa206b', 'avatarUrl': '/avatars/105ada08a9a982fc7b723bdc678f7e72.svg', 'isPro': False, 'fullname': 'wenhang ge', 'user': 'spongy', 'type': 'user'}, 'name': 'Wenhang Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:17:45.208Z', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7243', 'name': 'Dicong Qiu', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7244', 'user': {'_id': '634a91c53a0cd2d4985dc98a', 'avatarUrl': '/avatars/fe680e683761c347cfa4bc9273b67b0d.svg', 'isPro': False, 'fullname': 'ZhiFei Chen', 'user': 'zhifeichen097', 'type': 'user'}, 'name': 'ZhiFei Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:17:31.856Z', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7245', 'user': {'_id': '64049ae20ab5e22719f35103', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg', 'isPro': False, 'fullname': 'Dongyu Yan', 'user': 'StarYDY', 'type': 'user'}, 'name': 'Dongyu Yan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:17:25.961Z', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7246', 'name': 'Zhuoyun Liu', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7247', 'name': 'Haoyu Zhao', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7248', 'name': 'Hanfeng Zhao', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e7249', 'name': 'Shunsi Zhang', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e724a', 'user': {'_id': '656eccea896aeb1168a2bade', 'avatarUrl': '/avatars/83298bc28c9a5de3d600b5d889c1dd82.svg', 'isPro': False, 'fullname': 'Junwei Liang', 'user': 'junweiliang', 'type': 'user'}, 'name': 'Junwei Liang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:16:24.900Z', 'hidden': False}, {'_id': '6760e4c35b8363d75f3e724b', 'user': {'_id': '655cba1d87b67834000590e8', 'avatarUrl': '/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg', 'isPro': False, 'fullname': 'Yingcong Chen', 'user': 'yingcongchen', 'type': 'user'}, 'name': 'Ying-Cong Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:16:19.064Z', 'hidden': False}], 'publishedAt': '2024-12-15T17:44:10.000Z', 'title': 'GaussianProperty: Integrating Physical Properties to 3D Gaussians with\\n  LMMs', 'summary': 'Estimating physical properties for visual data is a crucial task in computer\\nvision, graphics, and robotics, underpinning applications such as augmented\\nreality, physical simulation, and robotic grasping. However, this area remains\\nunder-explored due to the inherent ambiguities in physical property estimation.\\nTo address these challenges, we introduce GaussianProperty, a training-free\\nframework that assigns physical properties of materials to 3D Gaussians.\\nSpecifically, we integrate the segmentation capability of SAM with the\\nrecognition capability of GPT-4V(ision) to formulate a global-local physical\\nproperty reasoning module for 2D images. Then we project the physical\\nproperties from multi-view 2D images to 3D Gaussians using a voting strategy.\\nWe demonstrate that 3D Gaussians with physical property annotations enable\\napplications in physics-based dynamic simulation and robotic grasping. For\\nphysics-based dynamic simulation, we leverage the Material Point Method (MPM)\\nfor realistic dynamic simulation. For robot grasping, we develop a grasping\\nforce prediction strategy that estimates a safe force range required for object\\ngrasping based on the estimated physical properties. Extensive experiments on\\nmaterial segmentation, physics-based dynamic simulation, and robotic grasping\\nvalidate the effectiveness of our proposed method, highlighting its crucial\\nrole in understanding physical properties from visual data. Online demo, code,\\nmore cases and annotated datasets are available on\\nhttps://Gaussian-Property.github.io{this https URL}.', 'upvotes': 6, 'discussionId': '6760e4c85b8363d75f3e730d'}, 'publishedAt': '2024-12-16T21:41:12.675Z', 'title': 'GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11258.png', 'numComments': 1, 'submittedBy': {'_id': '64b4ab62eec33e27dcd733b5', 'avatarUrl': '/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg', 'fullname': 'Xinli XU', 'name': 'Xxlbigbrother', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.11834', 'authors': [{'_id': '6760f7212504d8a0fbf21ac9', 'user': {'_id': '673ab3647afcea17eb4378fd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/bGxG4c7zp4NHEim0MCF7h.jpeg', 'isPro': False, 'fullname': 'Jingze Shi', 'user': 'JingzeShi', 'type': 'user'}, 'name': 'Jingze Shi', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-17T03:59:29.876Z', 'hidden': False}, {'_id': '6760f7212504d8a0fbf21aca', 'user': {'_id': '66a0a9e7e81290f90f93b0a6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66a0a9e7e81290f90f93b0a6/_HAsUv1eh0Im-aYZaa1C0.png', 'isPro': False, 'fullname': 'BinghengWu', 'user': 'wubingheng', 'type': 'user'}, 'name': 'Bingheng Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:41.783Z', 'hidden': False}], 'publishedAt': '2024-12-16T14:56:28.000Z', 'title': 'Wonderful Matrices: Combining for a More Efficient and Effective\\n  Foundation Model Architecture', 'summary': 'In order to make the foundation model more efficient and effective, our idea\\nis combining sequence transformation and state transformation. First, we prove\\nthe availability of rotary position embedding in the state space duality\\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\\nself-attention and state space duality by more than 4%, to ensure that the\\ncombining sequence transformation unifies position encoding. Second, we propose\\ndynamic mask attention, which maintains 100% accuracy in the more challenging\\nmulti-query associative recall task, improving by more than 150% compared to\\nquadratic causal self-attention and state space duality, to ensure that the\\ncombining sequence transformation selectively filters relevant information.\\nThird, we design cross domain mixture of experts, which makes the computational\\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\\nthe mixture of experts, to ensure that the combining state transformation\\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\\npopular model architectures.', 'upvotes': 4, 'discussionId': '6760f7212504d8a0fbf21afb'}, 'publishedAt': '2024-12-16T23:02:58.609Z', 'title': 'Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11834.png', 'numComments': 1, 'submittedBy': {'_id': '673ab3647afcea17eb4378fd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/bGxG4c7zp4NHEim0MCF7h.jpeg', 'fullname': 'Jingze Shi', 'name': 'JingzeShi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.11586', 'authors': [{'_id': '6760edcb125981709b9f2e00', 'user': {'_id': '65813fbeabafd960c84fdf2f', 'avatarUrl': '/avatars/d8f9cb56a2e44c3ca4472099437e0b50.svg', 'isPro': False, 'fullname': 'Xiaokun Sun', 'user': 'XiaokunSun', 'type': 'user'}, 'name': 'Xiaokun Sun', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:50.594Z', 'hidden': False}, {'_id': '6760edcb125981709b9f2e01', 'name': 'Zeyu Cai', 'hidden': False}, {'_id': '6760edcb125981709b9f2e02', 'user': {'_id': '642e6cc9be01b88c9446fe3d', 'avatarUrl': '/avatars/43a8f9bf35bea54bcda547174d921aa4.svg', 'isPro': False, 'fullname': 'Zhenyu Zhang', 'user': 'JesseZhang', 'type': 'user'}, 'name': 'Zhenyu Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:15:41.589Z', 'hidden': False}, {'_id': '6760edcb125981709b9f2e03', 'user': {'_id': '65734004769f3ee9bde1af10', 'avatarUrl': '/avatars/d6310ed861972fd691687d8f47413f33.svg', 'isPro': False, 'fullname': 'Ying Tai', 'user': 'yingtai', 'type': 'user'}, 'name': 'Ying Tai', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T08:03:48.616Z', 'hidden': False}, {'_id': '6760edcb125981709b9f2e04', 'name': 'Jian Yang', 'hidden': False}], 'publishedAt': '2024-12-16T09:17:36.000Z', 'title': 'StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair\\n  Geometric Priors', 'summary': 'While haircut indicates distinct personality, existing avatar generation\\nmethods fail to model practical hair due to the general or entangled\\nrepresentation. We propose StrandHead, a novel text to 3D head avatar\\ngeneration method capable of generating disentangled 3D hair with strand\\nrepresentation. Without using 3D data for supervision, we demonstrate that\\nrealistic hair strands can be generated from prompts by distilling 2D\\ngenerative diffusion models. To this end, we propose a series of reliable\\npriors on shape initialization, geometric primitives, and statistical haircut\\nfeatures, leading to a stable optimization and text-aligned performance.\\nExtensive experiments show that StrandHead achieves the state-of-the-art\\nreality and diversity of generated 3D head and hair. The generated 3D hair can\\nalso be easily implemented in the Unreal Engine for physical simulation and\\nother applications. The code will be available at\\nhttps://xiaokunsun.github.io/StrandHead.github.io.', 'upvotes': 4, 'discussionId': '6760edd2125981709b9f2f0c'}, 'publishedAt': '2024-12-16T22:35:55.797Z', 'title': 'StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65813fbeabafd960c84fdf2f/5-NL_C13nXCS1rRWurkGE.png', 'https://cdn-uploads.huggingface.co/production/uploads/65813fbeabafd960c84fdf2f/Zk84UX9QulneFo8Sfzio1.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11586.png', 'numComments': 1, 'submittedBy': {'_id': '65813fbeabafd960c84fdf2f', 'avatarUrl': '/avatars/d8f9cb56a2e44c3ca4472099437e0b50.svg', 'fullname': 'Xiaokun Sun', 'name': 'XiaokunSun', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.11974', 'authors': [{'_id': '676196e778b836c285b3bc20', 'name': 'Qi Sun', 'hidden': False}, {'_id': '676196e778b836c285b3bc21', 'name': 'Pengfei Hong', 'hidden': False}, {'_id': '676196e778b836c285b3bc22', 'name': 'Tej Deep Pala', 'hidden': False}, {'_id': '676196e778b836c285b3bc23', 'name': 'Vernon Toh', 'hidden': False}, {'_id': '676196e778b836c285b3bc24', 'name': 'U-Xuan Tan', 'hidden': False}, {'_id': '676196e778b836c285b3bc25', 'name': 'Deepanway Ghosal', 'hidden': False}, {'_id': '676196e778b836c285b3bc26', 'name': 'Soujanya Poria', 'hidden': False}], 'publishedAt': '2024-12-16T16:58:28.000Z', 'title': 'Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\\n  Thought and Look-ahead Spatial Reasoning', 'summary': 'Traditional reinforcement learning-based robotic control methods are often\\ntask-specific and fail to generalize across diverse environments or unseen\\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\\nscene understanding and planning capabilities but lack the ability to generate\\nactionable policies tailored to specific robotic embodiments. To address this,\\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\\nlong-horizon spatial reasoning and grounded task planning. In this work, we\\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\\nmanipulation trajectories auto-annotated with grounded task reasoning and\\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\\nbased on gripper states and motion trajectories, which can help mitigate\\nhallucination in grounding subtask reasoning generation. Experimental results\\ndemonstrate that Emma-X achieves superior performance over competitive\\nbaselines, particularly in real-world robotic tasks requiring spatial\\nreasoning.', 'upvotes': 2, 'discussionId': '676196ea78b836c285b3bd03'}, 'publishedAt': '2024-12-17T10:23:24.545Z', 'title': 'Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11974.png', 'numComments': 1, 'submittedBy': {'_id': '62c3f44617c780a4f2ed603c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1657009213654-noauth.jpeg', 'fullname': 'Hong Pengfei', 'name': 'emrys-hong', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.12004', 'authors': [{'_id': '67618c51f088f81ece9d0543', 'name': 'Jiya Manchanda', 'hidden': False}, {'_id': '67618c51f088f81ece9d0544', 'name': 'Laura Boettcher', 'hidden': False}, {'_id': '67618c51f088f81ece9d0545', 'name': 'Matheus Westphalen', 'hidden': False}, {'_id': '67618c51f088f81ece9d0546', 'name': 'Jasser Jasser', 'hidden': False}], 'publishedAt': '2024-12-16T17:32:11.000Z', 'title': 'The Open Source Advantage in Large Language Models (LLMs)', 'summary': 'Large language models (LLMs) mark a key shift in natural language processing\\n(NLP), having advanced text generation, translation, and domain-specific\\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\\nextensive computational resources, lead with state-of-the-art performance\\ntoday. However, they face criticism for their \"black box\" nature and for\\nlimiting accessibility in a manner that hinders reproducibility and equitable\\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\\nprioritize democratization through community-driven development and\\ncomputational efficiency. These models have significantly reduced performance\\ngaps, particularly in linguistic diversity and domain-specific applications,\\nwhile providing accessible tools for global researchers and developers.\\nNotably, both paradigms rely on foundational architectural innovations, such as\\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\\nby scaling effectively, while open-source models adapt to real-world\\napplications in underrepresented languages and domains. Techniques like\\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\\nmodels to achieve competitive results despite limited resources. To be sure,\\nthe tension between closed-source and open-source approaches underscores a\\nbroader debate on transparency versus proprietary control in AI. Ethical\\nconsiderations further highlight this divide. Closed-source systems restrict\\nexternal scrutiny, while open-source models promote reproducibility and\\ncollaboration but lack standardized auditing documentation frameworks to\\nmitigate biases. Hybrid approaches that leverage the strengths of both\\nparadigms are likely to shape the future of LLM innovation, ensuring\\naccessibility, competitive technical performance, and ethical deployment.', 'upvotes': 2, 'discussionId': '67618c52f088f81ece9d0581'}, 'publishedAt': '2024-12-17T09:36:09.553Z', 'title': 'The Open Source Advantage in Large Language Models (LLMs)', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12004.png', 'numComments': 1, 'submittedBy': {'_id': '6032802e1f993496bc14d9e3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png', 'fullname': 'Omar Sanseviero', 'name': 'osanseviero', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2931}}",
    "{'paper': {'id': '2412.11449', 'authors': [{'_id': '67615ccf42d2c5bcb831b56e', 'user': {'_id': '62d7f1119b629105a5d84aad', 'avatarUrl': '/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg', 'isPro': False, 'fullname': 'Prateek Verma', 'user': 'prateekv', 'type': 'user'}, 'name': 'Prateek Verma', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:26:33.703Z', 'hidden': False}], 'publishedAt': '2024-12-16T05:03:48.000Z', 'title': 'Whisper-GPT: A Hybrid Representation Audio Large Language Model', 'summary': 'We propose WHISPER-GPT: A generative large language model (LLM) for speech\\nand music that allows us to work with continuous audio representations and\\ndiscrete tokens simultaneously as part of a single architecture. There has been\\na huge surge in generative audio, speech, and music models that utilize\\ndiscrete audio tokens derived from neural compression algorithms, e.g. ENCODEC.\\nHowever, one of the major drawbacks of this approach is handling the context\\nlength. It blows up for high-fidelity generative architecture if one has to\\naccount for all the audio contents at various frequencies for the next token\\nprediction. By combining continuous audio representation like the spectrogram\\nand discrete acoustic tokens, we retain the best of both worlds: Have all the\\ninformation needed from the audio at a specific time instance in a single\\ntoken, yet allow LLM to predict the future token to allow for sampling and\\nother benefits discrete space provides. We show how our architecture improves\\nthe perplexity and negative log-likelihood scores for the next token prediction\\ncompared to a token-based LLM for speech and music.', 'upvotes': 2, 'discussionId': '67615cd042d2c5bcb831b5e5'}, 'publishedAt': '2024-12-17T06:13:52.861Z', 'title': 'Whisper-GPT: A Hybrid Representation Audio Large Language Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11449.png', 'numComments': 1, 'submittedBy': {'_id': '62d7f1119b629105a5d84aad', 'avatarUrl': '/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg', 'fullname': 'Prateek Verma', 'name': 'prateekv', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.10447', 'authors': [{'_id': '67614ef841245764e08a2812', 'user': {'_id': '62faae5686456e533453422c', 'avatarUrl': '/avatars/78e175e131fe472257c2f93787e463a1.svg', 'isPro': False, 'fullname': 'Jimmy Wu', 'user': 'jimmyyhwu', 'type': 'user'}, 'name': 'Jimmy Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-17T10:34:45.672Z', 'hidden': False}, {'_id': '67614ef841245764e08a2813', 'name': 'William Chong', 'hidden': False}, {'_id': '67614ef841245764e08a2814', 'name': 'Robert Holmberg', 'hidden': False}, {'_id': '67614ef841245764e08a2815', 'name': 'Aaditya Prasad', 'hidden': False}, {'_id': '67614ef841245764e08a2816', 'name': 'Yihuai Gao', 'hidden': False}, {'_id': '67614ef841245764e08a2817', 'name': 'Oussama Khatib', 'hidden': False}, {'_id': '67614ef841245764e08a2818', 'name': 'Shuran Song', 'hidden': False}, {'_id': '67614ef841245764e08a2819', 'name': 'Szymon Rusinkiewicz', 'hidden': False}, {'_id': '67614ef841245764e08a281a', 'name': 'Jeannette Bohg', 'hidden': False}], 'publishedAt': '2024-12-11T18:54:22.000Z', 'title': 'TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot\\n  Learning', 'summary': 'Exploiting the promise of recent advances in imitation learning for mobile\\nmanipulation will require the collection of large numbers of human-guided\\ndemonstrations. This paper proposes an open-source design for an inexpensive,\\nrobust, and flexible mobile manipulator that can support arbitrary arms,\\nenabling a wide range of real-world household mobile manipulation tasks.\\nCrucially, our design uses powered casters to enable the mobile base to be\\nfully holonomic, able to control all planar degrees of freedom independently\\nand simultaneously. This feature makes the base more maneuverable and\\nsimplifies many mobile manipulation tasks, eliminating the kinematic\\nconstraints that create complex and time-consuming motions in nonholonomic\\nbases. We equip our robot with an intuitive mobile phone teleoperation\\ninterface to enable easy data acquisition for imitation learning. In our\\nexperiments, we use this interface to collect data and show that the resulting\\nlearned policies can successfully perform a variety of common household mobile\\nmanipulation tasks.', 'upvotes': 2, 'discussionId': '67614ef941245764e08a2885'}, 'publishedAt': '2024-12-17T05:21:11.731Z', 'title': 'TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62faae5686456e533453422c/V8O1ij-f9XCLh-LaJlnVU.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10447.png', 'numComments': 1, 'submittedBy': {'_id': '62faae5686456e533453422c', 'avatarUrl': '/avatars/78e175e131fe472257c2f93787e463a1.svg', 'fullname': 'Jimmy Wu', 'name': 'jimmyyhwu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.11457', 'authors': [{'_id': '6760ef0da38bf4e171b6eb31', 'user': {'_id': '64ab8cb76324705e6a65f7c4', 'avatarUrl': '/avatars/15dcae6c345d31ea6e17c11108a7deb7.svg', 'isPro': False, 'fullname': 'Ruijie Lu', 'user': 'JasonAplp', 'type': 'user'}, 'name': 'Ruijie Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:14:33.511Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb32', 'user': {'_id': '64c396def6fe448b1ad553d6', 'avatarUrl': '/avatars/b2ce4739f42dc00ee974fff7ee1cb301.svg', 'isPro': False, 'fullname': 'Yixin Chen', 'user': 'YixinChen', 'type': 'user'}, 'name': 'Yixin Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:14:27.475Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb33', 'user': {'_id': '65ae5edddacd99fd58277620', 'avatarUrl': '/avatars/5ba35c984d54eef4eacf11ebebafa3a0.svg', 'isPro': False, 'fullname': 'Junfeng Ni', 'user': 'JunfengNi', 'type': 'user'}, 'name': 'Junfeng Ni', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:14:22.016Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb34', 'user': {'_id': '6304b389bad6ce7fc02691d5', 'avatarUrl': '/avatars/a762ca59624ce409650165f36b973488.svg', 'isPro': False, 'fullname': 'Baoxiong Jia', 'user': 'BuzzBeater', 'type': 'user'}, 'name': 'Baoxiong Jia', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:14:15.760Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb35', 'user': {'_id': '636de85cc4a7a729c164d2b5', 'avatarUrl': '/avatars/3e281e547e1697e1c06805e7e63f3918.svg', 'isPro': False, 'fullname': 'Yu Liu', 'user': 'YuLiu', 'type': 'user'}, 'name': 'Yu Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:14:09.545Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb36', 'user': {'_id': '63fef01cc0ec83fda43d5c40', 'avatarUrl': '/avatars/363efd5c58e4e810267b666240e3f6b3.svg', 'isPro': False, 'fullname': 'Diwen Wan', 'user': 'dnvtmf', 'type': 'user'}, 'name': 'Diwen Wan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:13:55.269Z', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb37', 'name': 'Gang Zeng', 'hidden': False}, {'_id': '6760ef0da38bf4e171b6eb38', 'user': {'_id': '63c7a33121bd95f80ed74652', 'avatarUrl': '/avatars/7dd59afea785a2bff0ec2b757abd474e.svg', 'isPro': False, 'fullname': 'Siyuan Huang', 'user': 'thuhsy', 'type': 'user'}, 'name': 'Siyuan Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-17T11:13:36.914Z', 'hidden': False}], 'publishedAt': '2024-12-16T05:23:45.000Z', 'title': 'MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes', 'summary': \"Repurposing pre-trained diffusion models has been proven to be effective for\\nNVS. However, these methods are mostly limited to a single object; directly\\napplying such methods to compositional multi-object scenarios yields inferior\\nresults, especially incorrect object placement and inconsistent shape and\\nappearance under novel views. How to enhance and systematically evaluate the\\ncross-view consistency of such models remains under-explored. To address this\\nissue, we propose MOVIS to enhance the structural awareness of the\\nview-conditioned diffusion model for multi-object NVS in terms of model inputs,\\nauxiliary tasks, and training strategy. First, we inject structure-aware\\nfeatures, including depth and object mask, into the denoising U-Net to enhance\\nthe model's comprehension of object instances and their spatial relationships.\\nSecond, we introduce an auxiliary task requiring the model to simultaneously\\npredict novel view object masks, further improving the model's capability in\\ndifferentiating and placing objects. Finally, we conduct an in-depth analysis\\nof the diffusion sampling process and carefully devise a structure-guided\\ntimestep sampling scheduler during training, which balances the learning of\\nglobal object placement and fine-grained detail recovery. To systematically\\nevaluate the plausibility of synthesized images, we propose to assess\\ncross-view consistency and novel view object placement alongside existing\\nimage-level NVS metrics. Extensive experiments on challenging synthetic and\\nrealistic datasets demonstrate that our method exhibits strong generalization\\ncapabilities and produces consistent novel view synthesis, highlighting its\\npotential to guide future 3D-aware multi-object NVS tasks.\", 'upvotes': 2, 'discussionId': '6760ef12a38bf4e171b6ec4b'}, 'publishedAt': '2024-12-16T22:26:35.562Z', 'title': 'MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11457.png', 'numComments': 1, 'submittedBy': {'_id': '63c7a33121bd95f80ed74652', 'avatarUrl': '/avatars/7dd59afea785a2bff0ec2b757abd474e.svg', 'fullname': 'Siyuan Huang', 'name': 'thuhsy', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.11314', 'authors': [{'_id': '67613d66896c29e2a598e378', 'user': {'_id': '60ef34ff8432bc401cd0abbf', 'avatarUrl': '/avatars/4992b27a8230b80e9bb9a4f92bd5638c.svg', 'isPro': False, 'fullname': 'Dmitry Ustalov', 'user': 'dustalov', 'type': 'user'}, 'name': 'Dmitry Ustalov', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-17T08:59:57.545Z', 'hidden': False}], 'publishedAt': '2024-12-15T21:22:46.000Z', 'title': 'Reliable, Reproducible, and Really Fast Leaderboards with Evalica', 'summary': 'The rapid advancement of natural language processing (NLP) technologies, such\\nas instruction-tuned large language models (LLMs), urges the development of\\nmodern evaluation protocols with human and machine feedback. We introduce\\nEvalica, an open-source toolkit that facilitates the creation of reliable and\\nreproducible model leaderboards. This paper presents its design, evaluates its\\nperformance, and demonstrates its usability through its Web interface,\\ncommand-line interface, and Python API.', 'upvotes': 1, 'discussionId': '67613d67896c29e2a598e3b9'}, 'publishedAt': '2024-12-17T09:16:12.071Z', 'title': 'Reliable, Reproducible, and Really Fast Leaderboards with Evalica', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11314.png', 'numComments': 1, 'submittedBy': {'_id': '60ef34ff8432bc401cd0abbf', 'avatarUrl': '/avatars/4992b27a8230b80e9bb9a4f92bd5638c.svg', 'fullname': 'Dmitry Ustalov', 'name': 'dustalov', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.11689', 'authors': [{'_id': '67614df02541e5e5d18f0ebf', 'user': {'_id': '6479f8335f3450e1ded40774', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/iOjE2dcSUS-XYE0WJe6U8.jpeg', 'isPro': False, 'fullname': 'Andrei Semenov', 'user': 'Andron00e', 'type': 'user'}, 'name': 'Andrei Semenov', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-17T10:27:49.157Z', 'hidden': False}, {'_id': '67614df02541e5e5d18f0ec0', 'name': 'Philip Zmushko', 'hidden': False}, {'_id': '67614df02541e5e5d18f0ec1', 'user': {'_id': '65edd2cdcae0146ec561e52c', 'avatarUrl': '/avatars/b983767f48affb39a39dc57eb7674af2.svg', 'isPro': False, 'fullname': 'Alex Pichugin', 'user': 'pichuginad', 'type': 'user'}, 'name': 'Alexander Pichugin', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-17T10:09:53.189Z', 'hidden': False}, {'_id': '67614df02541e5e5d18f0ec2', 'name': 'Aleksandr Beznosikov', 'hidden': False}], 'publishedAt': '2024-12-16T12:02:12.000Z', 'title': 'Just a Simple Transformation is Enough for Data Protection in Vertical\\n  Federated Learning', 'summary': 'Vertical Federated Learning (VFL) aims to enable collaborative training of\\ndeep learning models while maintaining privacy protection. However, the VFL\\nprocedure still has components that are vulnerable to attacks by malicious\\nparties. In our work, we consider feature reconstruction attacks, a common risk\\ntargeting input data compromise. We theoretically claim that feature\\nreconstruction attacks cannot succeed without knowledge of the prior\\ndistribution on data. Consequently, we demonstrate that even simple model\\narchitecture transformations can significantly impact the protection of input\\ndata during VFL. Confirming these findings with experimental results, we show\\nthat MLP-based models are resistant to state-of-the-art feature reconstruction\\nattacks.', 'upvotes': 1, 'discussionId': '67614df12541e5e5d18f0f43'}, 'publishedAt': '2024-12-17T05:18:39.833Z', 'title': 'Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11689.png', 'numComments': 1, 'submittedBy': {'_id': '6479f8335f3450e1ded40774', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/iOjE2dcSUS-XYE0WJe6U8.jpeg', 'fullname': 'Andrei Semenov', 'name': 'Andron00e', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.11100', 'authors': [{'_id': '67617735a0a946bd33fd708c', 'name': 'Jinxiu Liu', 'hidden': False}, {'_id': '67617735a0a946bd33fd708d', 'name': 'Shaoheng Lin', 'hidden': False}, {'_id': '67617735a0a946bd33fd708e', 'name': 'Yinxiao Li', 'hidden': False}, {'_id': '67617735a0a946bd33fd708f', 'name': 'Ming-Hsuan Yang', 'hidden': False}], 'publishedAt': '2024-12-15T07:42:26.000Z', 'title': 'DynamicScaler: Seamless and Scalable Video Generation for Panoramic\\n  Scenes', 'summary': 'The increasing demand for immersive AR/VR applications and spatial\\nintelligence has heightened the need to generate high-quality scene-level and\\n360{\\\\deg} panoramic video. However, most video diffusion models are constrained\\nby limited resolution and aspect ratio, which restricts their applicability to\\nscene-level dynamic content synthesis. In this work, we propose the\\nDynamicScaler, addressing these challenges by enabling spatially scalable and\\npanoramic dynamic scene synthesis that preserves coherence across panoramic\\nscenes of arbitrary size. Specifically, we introduce a Offset Shifting\\nDenoiser, facilitating efficient, synchronous, and coherent denoising panoramic\\ndynamic scenes via a diffusion model with fixed resolution through a seamless\\nrotating Window, which ensures seamless boundary transitions and consistency\\nacross the entire panoramic space, accommodating varying resolutions and aspect\\nratios. Additionally, we employ a Global Motion Guidance mechanism to ensure\\nboth local detail fidelity and global motion continuity. Extensive experiments\\ndemonstrate our method achieves superior content and motion quality in\\npanoramic scene-level video generation, offering a training-free, efficient,\\nand scalable solution for immersive dynamic scene creation with constant VRAM\\nconsumption regardless of the output video resolution. Our project page is\\navailable at https://dynamic-scaler.pages.dev/.', 'upvotes': 0, 'discussionId': '67617738a0a946bd33fd71e2'}, 'publishedAt': '2024-12-17T10:18:56.637Z', 'title': 'DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6456785cbfdf9c63ce301bf2/4-vjCVzFzUs-6YyLtkk9f.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11100.png', 'numComments': 1, 'submittedBy': {'_id': '6456785cbfdf9c63ce301bf2', 'avatarUrl': '/avatars/db1b71e547d901cc6ed3c7f96a50b7b1.svg', 'fullname': 'Jinxiu', 'name': 'BrandonLiu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}"
]