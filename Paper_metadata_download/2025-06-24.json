[
  {
    "paper": {
      "id": "2506.18882",
      "authors": [
        {
          "_id": "685a163c0e4ad7e219758569",
          "name": "Hong Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856a",
          "name": "Houyuan Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856b",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856c",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856d",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856e",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856f",
          "name": "Xianda Guo",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758570",
          "name": "Xuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758571",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758572",
          "name": "Baochang Zhang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758573",
          "name": "Satoshi Ikehata",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758574",
          "name": "Boxin Shi",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758575",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758576",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
      ],
      "publishedAt": "2025-06-23T17:53:11.000Z",
      "submittedOnDailyAt": "2025-06-24T02:59:45.489Z",
      "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
      "submittedOnDailyBy": {
        "_id": "643a1f5b58cb07c2a3745116",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
        "isPro": false,
        "fullname": "Hugo",
        "user": "chongjie",
        "type": "user"
      },
      "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
      "upvotes": 54,
      "discussionId": "685a163c0e4ad7e219758577",
      "githubRepo": "https://github.com/houyuanchen111/LINO_UniPS",
      "ai_summary": "Photometric stereo aims to recover high-quality surface normals under arbitrary lighting conditions, addressing challenges related to illumination-surface normal coupling and high-frequency geometric detail preservation.",
      "ai_keywords": [
        "photometric stereo",
        "deep coupling",
        "surface normals",
        "illumination conditions",
        "intensity variations",
        "self-shadowing",
        "inter-reflections",
        "subtle normal variations"
      ]
    },
    "publishedAt": "2025-06-23T13:53:11.000Z",
    "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
    "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643a1f5b58cb07c2a3745116",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
      "fullname": "Hugo",
      "name": "chongjie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18871",
      "authors": [
        {
          "_id": "685a0be90e4ad7e2197584f4",
          "name": "Chenyuan Wu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f5",
          "name": "Pengfei Zheng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f6",
          "name": "Ruiran Yan",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f7",
          "name": "Shitao Xiao",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f8",
          "name": "Xin Luo",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f9",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fa",
          "name": "Wanli Li",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fb",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fc",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fd",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fe",
          "name": "Ze Liu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584ff",
          "name": "Ziyi Xia",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758500",
          "name": "Chaofan Li",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758501",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758502",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758503",
          "name": "Kun Luo",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758504",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758505",
          "name": "Defu Lian",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758506",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758507",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758508",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758509",
          "name": "Zheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:38:54.000Z",
      "submittedOnDailyAt": "2025-06-24T01:06:04.763Z",
      "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
      "submittedOnDailyBy": {
        "_id": "6564a2ceedae9c33b7654a1f",
        "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
        "isPro": false,
        "fullname": "JUNJIE ZHOU",
        "user": "JUNJIE99",
        "type": "user"
      },
      "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
      "upvotes": 30,
      "discussionId": "685a0be90e4ad7e21975850a",
      "projectPage": "https://vectorspacelab.github.io/OmniGen2/",
      "githubRepo": "https://github.com/VectorSpaceLab/OmniGen2",
      "ai_summary": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.",
      "ai_keywords": [
        "decoding pathways",
        "unshared parameters",
        "decoupled image tokenizer",
        "multimodal understanding models",
        "reflection mechanism",
        "reflection dataset",
        "OmniContext",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-23T13:38:54.000Z",
    "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6564a2ceedae9c33b7654a1f",
      "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
      "fullname": "JUNJIE ZHOU",
      "name": "JUNJIE99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18841",
      "authors": [
        {
          "_id": "685a0f330e4ad7e219758514",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758515",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758516",
          "name": "Zhiqiang Hu",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758517",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758518",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:08:07.123Z",
      "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
      "upvotes": 29,
      "discussionId": "685a0f340e4ad7e219758519",
      "ai_summary": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.",
      "ai_keywords": [
        "reinforcement learning",
        "reward models",
        "long-form text generation",
        "ultra-long generation",
        "large language models",
        "synthetic fine-tuning",
        "length control",
        "writing quality",
        "structural formatting",
        "WritingBench",
        "Arena-Write"
      ]
    },
    "publishedAt": "2025-06-23T12:59:02.000Z",
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18851",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758528",
          "name": "Zhuowei Chen",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758529",
          "name": "Bingchuan Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852a",
          "name": "Tianxiang Ma",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852b",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852c",
          "name": "Mingcong Liu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852d",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852e",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852f",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758530",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758531",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758532",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:11:56.000Z",
      "submittedOnDailyAt": "2025-06-24T01:12:12.451Z",
      "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
      "submittedOnDailyBy": {
        "_id": "6304e2dabad6ce7fc0287d57",
        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
        "isPro": false,
        "fullname": "Zhuowei_Chen",
        "user": "ZhuoweiChen",
        "type": "user"
      },
      "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
      "upvotes": 19,
      "discussionId": "685a0fb40e4ad7e219758535",
      "projectPage": "https://phantom-video.github.io/Phantom-Data/",
      "githubRepo": "https://github.com/Phantom-video/Phantom-Data",
      "ai_summary": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.",
      "ai_keywords": [
        "Phantom-Data",
        "subject-to-video generation",
        "copy-paste problem",
        "in-pair training paradigm",
        "subject detection",
        "cross-context subject retrieval",
        "prior-guided identity verification"
      ]
    },
    "publishedAt": "2025-06-23T13:11:56.000Z",
    "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
    "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18851.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6304e2dabad6ce7fc0287d57",
      "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
      "fullname": "Zhuowei_Chen",
      "name": "ZhuoweiChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18896",
      "authors": [
        {
          "_id": "685a02790e4ad7e2197584b2",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b4",
          "name": "Jingwen Gu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b5",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b6",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b7",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b8",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:03:32.146Z",
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 17,
      "discussionId": "685a027a0e4ad7e2197584b9",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-prm-68463c73cf1c6a0ec6fafeb5",
      "githubRepo": "https://github.com/Gen-Verse/ReasonFlux",
      "ai_summary": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.",
      "ai_keywords": [
        "Process Reward Models",
        "trajectory-aware PRM",
        "trajectory-response outputs",
        "step-level supervision",
        "trajectory-level supervision",
        "chain-of-thought data",
        "model distillation",
        "policy optimization",
        "reinforcement learning",
        "Best-of-N test-time scaling",
        "AIME",
        "MATH500",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-06-23T13:59:02.000Z",
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18898",
      "authors": [
        {
          "_id": "685a07510e4ad7e2197584c6",
          "name": "Jiaming Han",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c7",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c8",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c9",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ca",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cb",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cc",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cd",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ce",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-24T01:02:20.046Z",
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
      "submittedOnDailyBy": {
        "_id": "62318c0386753f5f41d0e261",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
        "isPro": false,
        "fullname": "Jiaming Han",
        "user": "csuhan",
        "type": "user"
      },
      "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
      "upvotes": 15,
      "discussionId": "685a07520e4ad7e2197584cf",
      "projectPage": "https://tar.csuhan.com",
      "githubRepo": "https://github.com/csuhan/Tar",
      "ai_summary": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.",
      "ai_keywords": [
        "Text-Aligned Tokenizer (TA-Tok)",
        "multimodal LLM",
        "Tar",
        "scale-adaptive encoding",
        "diffusion-based model",
        "autoregressive model",
        "modality fusion",
        "pre-training tasks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:14.000Z",
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
    "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18898.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62318c0386753f5f41d0e261",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
      "fullname": "Jiaming Han",
      "name": "csuhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18254",
      "authors": [
        {
          "_id": "685a39d60e4ad7e219758622",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758623",
          "name": "Bo Ji",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758624",
          "name": "Shouli Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758625",
          "name": "Shu Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758626",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758627",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758628",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758629",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862a",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862c",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862d",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T02:56:36.000Z",
      "submittedOnDailyAt": "2025-06-24T04:14:54.081Z",
      "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
      "submittedOnDailyBy": {
        "_id": "64abc4aa6cadc7aca585dddf",
        "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
        "isPro": false,
        "fullname": "Tianyu Yu",
        "user": "Yirany",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
      "upvotes": 13,
      "discussionId": "685a39d80e4ad7e21975862e",
      "projectPage": "https://github.com/OpenBMB/RLPR",
      "githubRepo": "https://github.com/OpenBMB/RLPR",
      "ai_summary": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "reasoning capabilities",
        "LLM",
        "RLPR",
        "token probability scores",
        "prob-to-reward",
        "stabilizing methods",
        "TheoremQA",
        "Minerva",
        "General-Reasoner"
      ]
    },
    "publishedAt": "2025-06-22T22:56:36.000Z",
    "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18254.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64abc4aa6cadc7aca585dddf",
      "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
      "fullname": "Tianyu Yu",
      "name": "Yirany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15741",
      "authors": [
        {
          "_id": "685a48970e4ad7e219758662",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758663",
          "user": {
            "_id": "64301abe450c0de9a1d3d18e",
            "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
            "isPro": false,
            "fullname": "tianrui",
            "user": "tianyue818",
            "type": "user"
          },
          "name": "Tianrui Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T07:14:35.823Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758664",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758665",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758666",
          "name": "Yeyi Guan",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758667",
          "name": "Jinxiang Xia",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758668",
          "name": "Yi Yao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758669",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866a",
          "name": "Ningning Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866b",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866c",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866d",
          "name": "Xin Gui",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866e",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866f",
          "name": "Yuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758670",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758671",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758672",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758673",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758674",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758675",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758676",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758677",
          "name": "Xitong Gao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758678",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758679",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T05:17:03.413Z",
      "title": "OAgents: An Empirical Study of Building Effective Agents",
      "submittedOnDailyBy": {
        "_id": "628c8598ef14f971b698107f",
        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
        "isPro": false,
        "fullname": "Zhou",
        "user": "Wangchunshu",
        "type": "user"
      },
      "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
      "upvotes": 13,
      "discussionId": "685a48980e4ad7e21975867a"
    },
    "publishedAt": "2025-06-17T13:59:02.000Z",
    "title": "OAgents: An Empirical Study of Building Effective Agents",
    "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c8598ef14f971b698107f",
      "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
      "fullname": "Zhou",
      "name": "Wangchunshu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18903",
      "authors": [
        {
          "_id": "685a1ce80e4ad7e2197585b7",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b8",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b9",
          "name": "Andrea Vedaldi",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585ba",
          "name": "Tomas Jakab",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
      ],
      "publishedAt": "2025-06-23T17:59:56.000Z",
      "submittedOnDailyAt": "2025-06-24T02:12:37.233Z",
      "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
      "submittedOnDailyBy": {
        "_id": "638e29cf319f9c746b87ad4b",
        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
        "isPro": true,
        "fullname": "Runjia Li",
        "user": "liguang0115",
        "type": "user"
      },
      "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
      "upvotes": 4,
      "discussionId": "685a1ce80e4ad7e2197585bb",
      "projectPage": "https://v-mem.github.io/",
      "githubRepo": "https://github.com/runjiali-rl/vmem",
      "ai_summary": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.",
      "ai_keywords": [
        "memory mechanism",
        "video generators",
        "out-painting",
        "3D geometry",
        "context window",
        "Surfel-Indexed View Memory",
        "surfels",
        "scene coherence",
        "camera control",
        "long-term scene synthesis benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:56.000Z",
    "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
    "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e29cf319f9c746b87ad4b",
      "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
      "fullname": "Runjia Li",
      "name": "liguang0115",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18631",
      "authors": [
        {
          "_id": "685a1b390e4ad7e2197585a9",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585aa",
          "name": "Jiarui Yu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ab",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ac",
          "name": "Hande Dong",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ad",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ae",
          "name": "Fei Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
      ],
      "publishedAt": "2025-06-23T13:36:24.000Z",
      "submittedOnDailyAt": "2025-06-24T02:05:49.825Z",
      "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "65ed3051492a7f35db21fea2",
        "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
        "isPro": false,
        "fullname": "Chenxing Wei",
        "user": "kittttttt",
        "type": "user"
      },
      "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
      "upvotes": 4,
      "discussionId": "685a1b390e4ad7e2197585af",
      "githubRepo": "https://github.com/kithib/ReDit",
      "ai_summary": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.",
      "ai_keywords": [
        "rule-based reward system",
        "reward hacking",
        "discrete rewards",
        "gradient anomaly",
        "unstable optimization",
        "slow convergence",
        "random noise",
        "exploratory gradients",
        "flat reward regions",
        "local optima",
        "vanilla GRPO",
        "performance improvement",
        "gradient issues"
      ]
    },
    "publishedAt": "2025-06-23T09:36:24.000Z",
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18309",
      "authors": [
        {
          "_id": "685a2df10e4ad7e219758604",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758605",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758606",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758607",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758608",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758609",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860a",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860c",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860d",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860e",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860f",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T05:51:52.000Z",
      "submittedOnDailyAt": "2025-06-24T03:21:15.103Z",
      "title": "LettinGo: Explore User Profile Generation for Recommendation System",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
      "upvotes": 4,
      "discussionId": "685a2df20e4ad7e219758610",
      "ai_summary": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.",
      "ai_keywords": [
        "large language models (LLMs)",
        "semantically richer",
        "more transparent",
        "supervised fine-tuning (SFT)",
        "Direct Preference Optimization (DPO)",
        "profile generator",
        "pairwise preference data",
        "recommendation systems",
        "recommendation accuracy",
        "flexibility",
        "contextual awareness"
      ]
    },
    "publishedAt": "2025-06-23T01:51:52.000Z",
    "title": "LettinGo: Explore User Profile Generation for Recommendation System",
    "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16962",
      "authors": [
        {
          "_id": "6858d907c0c8e29df8ea3ce2",
          "user": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
            "isPro": false,
            "fullname": "manglu",
            "user": "manglu3935",
            "type": "user"
          },
          "name": "Haoran Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-23T15:52:05.938Z",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce3",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce4",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce5",
          "name": "Yujie Zhang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce7",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce8",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce9",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3cea",
          "name": "Xiaosong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T12:51:19.000Z",
      "submittedOnDailyAt": "2025-06-24T02:20:40.889Z",
      "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
      "submittedOnDailyBy": {
        "_id": "67547707f168984215451697",
        "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
        "isPro": false,
        "fullname": "manglu",
        "user": "manglu3935",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
      "upvotes": 4,
      "discussionId": "6858d907c0c8e29df8ea3ceb",
      "githubRepo": "https://github.com/manglu097/Chiron-o1",
      "ai_summary": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chain-of-thought",
        "Mentor-Intern Collaborative Search",
        "MICS",
        "mentor models",
        "intern models",
        "MICS-Score",
        "multi-task medical reasoning dataset",
        "MMRP",
        "curriculum learning",
        "medical visual question answering",
        "reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-20T08:51:19.000Z",
    "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67547707f168984215451697",
      "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
      "fullname": "manglu",
      "name": "manglu3935",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17538",
      "authors": [
        {
          "_id": "685a33c50e4ad7e219758612",
          "name": "Yile Gu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758613",
          "name": "Rohan Kadekodi",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758614",
          "name": "Hoang Nguyen",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758615",
          "name": "Keisuke Kamahori",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758616",
          "name": "Yiyu Liu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758617",
          "name": "Baris Kasikci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T01:32:22.000Z",
      "submittedOnDailyAt": "2025-06-24T03:49:17.409Z",
      "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
      "submittedOnDailyBy": {
        "_id": "6304ac1a412a1b9d381ca378",
        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
        "isPro": false,
        "fullname": "Keisuke Kamahori",
        "user": "kamahori",
        "type": "user"
      },
      "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
      "upvotes": 3,
      "discussionId": "685a33c70e4ad7e219758618",
      "githubRepo": "https://github.com/efeslab/ConsumerBench",
      "ai_summary": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.",
      "ai_keywords": [
        "Generative AI",
        "ConsumerBench",
        "system efficiency",
        "response time",
        "benchmarking framework",
        "multi-application scenarios",
        "application-level metrics",
        "latency",
        "Service Level Objective",
        "SLO",
        "system-level metrics",
        "CPU utilization",
        "GPU utilization",
        "memory bandwidth",
        "greedy allocation",
        "static model server configurations",
        "custom kernels",
        "SLO-aware scheduling strategies"
      ]
    },
    "publishedAt": "2025-06-20T21:32:22.000Z",
    "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
    "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18879",
      "authors": [
        {
          "_id": "685a1a090e4ad7e21975859c",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859d",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859e",
          "name": "Muhammad Yusuf Hassan",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859f",
          "name": "Talha Chafekar",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a0",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a1",
          "name": "Zhile Ren",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a2",
          "name": "Pengsheng Guo",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a3",
          "name": "Foroozan Karimzadeh",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a4",
          "name": "Colorado Reed",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a5",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a6",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:50:11.000Z",
      "submittedOnDailyAt": "2025-06-24T01:53:55.727Z",
      "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
      "submittedOnDailyBy": {
        "_id": "62d09eb86a61a88ea0d83918",
        "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
        "isPro": false,
        "fullname": "Junyan Li",
        "user": "senfu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
      "upvotes": 2,
      "discussionId": "685a1a090e4ad7e2197585a7",
      "ai_summary": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).",
      "ai_keywords": [
        "Commutative Vector Quantization",
        "CommVQ",
        "additive quantization",
        "codebook",
        "Rotary Position Embedding",
        "RoPE",
        "Expectation-Maximization",
        "self-attention",
        "FP16",
        "KV cache quantization",
        "GSM8K",
        "LLaMA-3.1 8B model",
        "RTX 4090 GPU"
      ]
    },
    "publishedAt": "2025-06-23T13:50:11.000Z",
    "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d09eb86a61a88ea0d83918",
      "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
      "fullname": "Junyan Li",
      "name": "senfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18527",
      "authors": [
        {
          "_id": "685a0cba0e4ad7e21975850c",
          "name": "JiaKui Hu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850d",
          "name": "Yuxiao Yang",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850e",
          "name": "Jialun Liu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850f",
          "name": "Jinbo Wu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758510",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758511",
          "name": "Yanye Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T11:28:37.000Z",
      "submittedOnDailyAt": "2025-06-24T03:17:08.044Z",
      "title": "Auto-Regressively Generating Multi-View Consistent Images",
      "submittedOnDailyBy": {
        "_id": "64ccd5cc4726a3f833831087",
        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
        "isPro": false,
        "fullname": "Hu",
        "user": "Jiakui",
        "type": "user"
      },
      "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
      "upvotes": 2,
      "discussionId": "685a0cbb0e4ad7e219758512",
      "ai_summary": "The Multi-View Auto-Regressive (MV-AR) method uses an auto-regressive model to generate consistent multi-view images from prompts, addressing challenges in shape and texture synthesis across diverse conditions.",
      "ai_keywords": [
        "Multi-View Auto-Regressive",
        "MV-AR",
        "auto-regressive model",
        "next-token-prediction",
        "condition injection modules",
        "text-to-multi-view",
        "X-to-multi-view",
        "progressive training strategy",
        "Shuffle View",
        "data augmentation"
      ]
    },
    "publishedAt": "2025-06-23T07:28:37.000Z",
    "title": "Auto-Regressively Generating Multi-View Consistent Images",
    "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccd5cc4726a3f833831087",
      "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
      "fullname": "Hu",
      "name": "Jiakui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18349",
      "authors": [
        {
          "_id": "685a02460e4ad7e2197584a9",
          "name": "Zichong Li",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584aa",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ab",
          "name": "Zixuan Zhang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ac",
          "name": "Ilgee Hong",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ad",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ae",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584af",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T07:15:59.000Z",
      "submittedOnDailyAt": "2025-06-24T02:19:52.155Z",
      "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
      "submittedOnDailyBy": {
        "_id": "63e6b5e22d2c508de9001afd",
        "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
        "isPro": false,
        "fullname": "Chen Liang",
        "user": "cliang1453",
        "type": "user"
      },
      "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
      "upvotes": 2,
      "discussionId": "685a02460e4ad7e2197584b0",
      "ai_summary": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "large language models (LLMs)",
        "parameter counts",
        "knowledge transfer",
        "one-shot pruning",
        "Phi 3.5-MoE",
        "Phi-mini-MoE",
        "Phi-tiny-MoE",
        "structured pruning",
        "staged distillation",
        "MMLU scores",
        "latency"
      ]
    },
    "publishedAt": "2025-06-23T03:15:59.000Z",
    "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
    "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6b5e22d2c508de9001afd",
      "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
      "fullname": "Chen Liang",
      "name": "cliang1453",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10597",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758522",
          "name": "Xunguang Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758523",
          "name": "Zhenlan Ji",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758524",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758525",
          "name": "Zongjie Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758526",
          "name": "Daoyuan Wu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758527",
          "name": "Shuai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T11:42:40.000Z",
      "submittedOnDailyAt": "2025-06-24T01:16:03.239Z",
      "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6601853162471e0981261241",
        "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
        "isPro": false,
        "fullname": "XunguangWang",
        "user": "xunguangwang",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
      "upvotes": 0,
      "discussionId": "685a0fb40e4ad7e219758533",
      "ai_summary": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "jailbreak attacks",
        "guardrails",
        "Security-Efficiency-Utility framework",
        "multi-dimensional taxonomy"
      ]
    },
    "publishedAt": "2025-06-12T07:42:40.000Z",
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10597.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601853162471e0981261241",
      "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
      "fullname": "XunguangWang",
      "name": "xunguangwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]