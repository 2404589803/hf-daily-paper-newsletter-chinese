[
  {
    "paper": {
      "id": "2505.17667",
      "authors": [
        {
          "_id": "6833d7c5a3262d6b1e4d358e",
          "name": "Fanqi Wan",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d358f",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3590",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3591",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3592",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3593",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3594",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3595",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3596",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3597",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:31:55.000Z",
      "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
      "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "62ecbffd99112e99c5f7fded",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
        "isPro": false,
        "fullname": "Fanqi Wan",
        "user": "Wanfq",
        "type": "user"
      },
      "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
      "upvotes": 34,
      "discussionId": "6833d7c6a3262d6b1e4d35c5",
      "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
      "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ]
    },
    "publishedAt": "2025-05-23T05:31:55.000Z",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18125",
      "authors": [
        {
          "_id": "6833f8b419852283c4b3bbd6",
          "name": "Alan Arazi",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd7",
          "name": "Eilam Shapira",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd8",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:34:28.000Z",
      "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
      "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
      "upvotes": 33,
      "discussionId": "6833f8b419852283c4b3bc02",
      "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
      "ai_keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ]
    },
    "publishedAt": "2025-05-23T13:34:28.000Z",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
    "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15929",
      "authors": [
        {
          "_id": "6830404effb59afb6569273a",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273b",
          "name": "Taiqiang Wu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273c",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273d",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273e",
          "name": "Jizhou Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273f",
          "name": "Yuyue Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692740",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692741",
          "name": "Zijian Hao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692742",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692743",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692744",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692745",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692746",
          "name": "Wendong Xu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692747",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692748",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692749",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274b",
          "name": "Zhuoqing Mao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274c",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T18:33:50.000Z",
      "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
      "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
      "upvotes": 32,
      "discussionId": "68304052ffb59afb6569282f",
      "projectPage": "https://phyx-bench.github.io/",
      "githubRepo": "https://github.com/NastyMarcus/PhyX",
      "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
      "ai_keywords": [
        "multimodal questions",
        "reasoning types",
        "sub-domains",
        "core physics domains",
        "thermodynamics",
        "electromagnetism",
        "mechanics",
        "modern physics",
        "optics",
        "wave\\&acoustics",
        "fine-grained statistics",
        "case studies",
        "evaluation paradigms",
        "VLMEvalKit"
      ]
    },
    "publishedAt": "2025-05-21T14:33:50.000Z",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17612",
      "authors": [
        {
          "_id": "6833c9fd298a7bec9c3da3b0",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b1",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b2",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b3",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b4",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:20:15.000Z",
      "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
      "upvotes": 29,
      "discussionId": "6833ca00298a7bec9c3da444",
      "githubRepo": "https://github.com/Nardien/agent-distillation",
      "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
      "ai_keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ]
    },
    "publishedAt": "2025-05-23T04:20:15.000Z",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17225",
      "authors": [
        {
          "_id": "6833c65d49b9e903d3ddbd11",
          "name": "Doohyuk Jang",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd12",
          "name": "Yoonjeon Kim",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd13",
          "name": "Chanjae Park",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd14",
          "name": "Hyun Ryu",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd15",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T19:00:01.000Z",
      "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
      "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "61b15ce1a5dd7dc7024406dc",
        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
        "isPro": false,
        "fullname": "Yoonjeon Kim",
        "user": "yjyjyj98",
        "type": "user"
      },
      "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
      "upvotes": 28,
      "discussionId": "6833c65e49b9e903d3ddbd6a",
      "projectPage": "https://reasoningtrap.github.io/",
      "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
      "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
      "ai_keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ]
    },
    "publishedAt": "2025-05-22T15:00:01.000Z",
    "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
    "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b15ce1a5dd7dc7024406dc",
      "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
      "fullname": "Yoonjeon Kim",
      "name": "yjyjyj98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18092",
      "authors": [
        {
          "_id": "6833ea049f968fc5c6b64486",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64487",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64488",
          "name": "Fanqi Wan",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64489",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448a",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448b",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448c",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448d",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448e",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448f",
          "name": "Zhansheng Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64490",
          "name": "Bin Yang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64491",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64492",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64493",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64494",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:47:00.000Z",
      "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
      "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
      "submittedOnDailyBy": {
        "_id": "64777a346e6c7ac608c1e9bf",
        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
        "isPro": false,
        "fullname": "Weizhou Shen",
        "user": "shenwzh3",
        "type": "user"
      },
      "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
      "upvotes": 27,
      "discussionId": "6833ea059f968fc5c6b644c1",
      "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
      "ai_keywords": [
        "context compression",
        "dynamic context optimization",
        "bidirectional reasoning layers",
        "token critic mechanisms",
        "window-parallel inference",
        "Qwen",
        "RAG",
        "sparse attention",
        "large language models",
        "SOTA performance"
      ]
    },
    "publishedAt": "2025-05-23T12:47:00.000Z",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64777a346e6c7ac608c1e9bf",
      "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
      "fullname": "Weizhou Shen",
      "name": "shenwzh3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17941",
      "authors": [
        {
          "_id": "6833cc35015eb19058ed83d9",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83da",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83db",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dc",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dd",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T14:17:56.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
      "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
      "upvotes": 20,
      "discussionId": "6833cc36015eb19058ed8419",
      "githubRepo": "https://github.com/czg1225/VeriThinker",
      "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "CoT compression",
        "verification task",
        "reasoning chain lengths",
        "reasoning tokens",
        "accuracy",
        "DeepSeek-R1-Distill-Qwen-7B",
        "MATH500",
        "AIME25",
        "speculative reasoning"
      ]
    },
    "publishedAt": "2025-05-23T10:17:56.000Z",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17561",
      "authors": [
        {
          "_id": "6833cb9030cd9df52a117557",
          "name": "Kwanyoung Kim",
          "hidden": false
        },
        {
          "_id": "6833cb9030cd9df52a117558",
          "name": "Sanghyun Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:09:10.000Z",
      "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
      "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
      "upvotes": 17,
      "discussionId": "6833cb9430cd9df52a11765d",
      "projectPage": "https://anse-project.github.io/anse-project/",
      "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
      "ai_keywords": [
        "video diffusion models",
        "noise seeds",
        "prompt alignment",
        "external priors",
        "frequency filters",
        "inter-frame smoothing",
        "ANSE",
        "Active Noise Selection for Generation",
        "BANSA",
        "Bayesian Active Noise Selection via Attention",
        "acquisition function",
        "entropy disagreement",
        "stochastic attention samples",
        "score estimation",
        "diffusion step",
        "temporal coherence"
      ]
    },
    "publishedAt": "2025-05-23T03:09:10.000Z",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
    "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16211",
      "authors": [
        {
          "_id": "6833d9cfdf7cbb5c087cb9cd",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ce",
          "name": "Can Shen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9cf",
          "name": "Yile Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d0",
          "name": "Jirui Han",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d1",
          "name": "Kelong Zheng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d2",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d3",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d4",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d5",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d6",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d7",
          "name": "Yingbin Jin",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d8",
          "name": "Xinxin Xing",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d9",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9da",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9db",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dc",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dd",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9de",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9df",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e0",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e1",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e2",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e4",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e6",
          "name": "Zhizheng Wu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e7",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e8",
          "name": "Eng-Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e9",
          "name": "XiaoFeng Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ea",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9eb",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ec",
          "name": "Xinfeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T04:27:46.000Z",
      "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
      "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
      "upvotes": 14,
      "discussionId": "6833d9d1df7cbb5c087cba85",
      "githubRepo": "https://github.com/JusperLee/AudioTrust",
      "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
      "ai_keywords": [
        "Audio Large Language Models",
        "ALLMs",
        "trustworthiness",
        "fairness",
        "hallucination",
        "safety",
        "privacy",
        "robustness",
        "authentication",
        "AudioTrust",
        "experimental setups",
        "audio-specific evaluation metrics",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-05-22T00:27:46.000Z",
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
    "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18129",
      "authors": [
        {
          "_id": "6833cf89df7cbb5c087a4caa",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cab",
          "name": "Linge Du",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cac",
          "name": "Xuyang Shen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cad",
          "name": "Shaoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cae",
          "name": "Pengfei Li",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4caf",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb0",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb1",
          "name": "Yuchao Dai",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb2",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb3",
          "name": "Junjie Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:41:14.000Z",
      "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
      "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "642e4d4d6748dd4f8eeb7732",
        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
        "isPro": false,
        "fullname": "Xuyang Shen",
        "user": "Ryan1122",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
      "upvotes": 13,
      "discussionId": "6833cf8adf7cbb5c087a4d0c",
      "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
      "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
      "ai_keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ]
    },
    "publishedAt": "2025-05-23T13:41:14.000Z",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e4d4d6748dd4f8eeb7732",
      "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
      "fullname": "Xuyang Shen",
      "name": "Ryan1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15692",
      "authors": [
        {
          "_id": "68306ffdff038ca6400a153a",
          "name": "Jinyang Wu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153b",
          "name": "Chonghua Liao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153c",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153d",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153e",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153f",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1540",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1541",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T16:06:10.000Z",
      "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
      "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
      "upvotes": 11,
      "discussionId": "68306ffeff038ca6400a1569",
      "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "TAPO",
        "Thought-Augmented Policy Optimization",
        "high-level guidance",
        "thought patterns",
        "model exploration",
        "AIME",
        "AMC",
        "Minerva Math",
        "reasoning models",
        "explainability",
        "output readability"
      ]
    },
    "publishedAt": "2025-05-21T12:06:10.000Z",
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17618",
      "authors": [
        {
          "_id": "6833eeaf98515618764fc204",
          "name": "Haoran He",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc205",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc206",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc207",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc208",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc209",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc20a",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:25:46.000Z",
      "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
      "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
      "upvotes": 9,
      "discussionId": "6833eeb198515618764fc277",
      "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "image generation",
        "video generation",
        "diffusion models",
        "flow-based models",
        "denoising trajectory",
        "stochastic differential equation",
        "selection",
        "mutation",
        "EvoSearch"
      ]
    },
    "publishedAt": "2025-05-23T04:25:46.000Z",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17558",
      "authors": [
        {
          "_id": "6833c8af029c4a53a60a5dfa",
          "user": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "isPro": false,
            "fullname": "Shrey Pandit",
            "user": "SP2001",
            "type": "user"
          },
          "name": "Shrey Pandit",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfb",
          "name": "Ashwin Vinod",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfc",
          "name": "Liu Leqi",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfd",
          "name": "Ying Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:05:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
      "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
      "submittedOnDailyBy": {
        "_id": "648749094dea003c6dae810f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
        "isPro": false,
        "fullname": "Shrey Pandit",
        "user": "SP2001",
        "type": "user"
      },
      "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
      "upvotes": 9,
      "discussionId": "6833c8b0029c4a53a60a5e3a",
      "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
      "ai_keywords": [
        "LLMs",
        "hallucinations",
        "DPO alignment procedure",
        "curriculum learning",
        "probability scores",
        "fact checking models",
        "HaluCheck models",
        "MedHallu",
        "HaluEval",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-05-23T03:05:09.000Z",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648749094dea003c6dae810f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
      "fullname": "Shrey Pandit",
      "name": "SP2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16479",
      "authors": [
        {
          "_id": "682fdc63bf762029ddcad451",
          "name": "Yuetong Liu",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad452",
          "user": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "isPro": false,
            "fullname": "Yunqiu Xu",
            "user": "Yunqiu",
            "type": "user"
          },
          "name": "Yunqiu Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad453",
          "name": "Yang Wei",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad454",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad455",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:06:35.000Z",
      "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
      "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
      "upvotes": 9,
      "discussionId": "682fdc67bf762029ddcad58c",
      "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
      "githubRepo": "https://github.com/henlyta/ClearNight",
      "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
      "ai_keywords": [
        "Retinex-based dual priors",
        "illumination-aware degradation generation",
        "weather-aware dynamic specific-commonality collaboration",
        "nighttime image restoration"
      ]
    },
    "publishedAt": "2025-05-22T06:06:35.000Z",
    "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16483",
      "authors": [
        {
          "_id": "6833cb27e10e89e250a6d9ae",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9af",
          "name": "Haozhe Zhao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b0",
          "name": "Cheng Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b1",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b2",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b3",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b4",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b5",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b6",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b7",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b8",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b9",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9ba",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9bb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:10:07.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
      "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
      "upvotes": 7,
      "discussionId": "6833cb28e10e89e250a6da0a",
      "projectPage": "https://github.com/S1s-Z/CANOE",
      "githubRepo": "https://github.com/S1s-Z/CANOE",
      "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
      "ai_keywords": [
        "teaching large language models",
        "faithfulness",
        "context",
        "CANOE",
        "short-form generation",
        "long-form generation",
        "question-answering",
        "Dual-GRPO",
        "rule-based reinforcement learning",
        "preference data",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-22T06:10:07.000Z",
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15389",
      "authors": [
        {
          "_id": "682f518184a99219c4b3090c",
          "name": "DongGeon Lee",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090d",
          "name": "Joonwon Jang",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090e",
          "name": "Jihae Jeong",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090f",
          "name": "Hwanjo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:26:40.000Z",
      "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
      "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
      "upvotes": 6,
      "discussionId": "682f518184a99219c4b30956",
      "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "MemeSafetyBench",
        "safety taxonomy",
        "LLM-based instruction generation",
        "single and multi-turn interactions"
      ]
    },
    "publishedAt": "2025-05-21T07:26:40.000Z",
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17826",
      "authors": [
        {
          "_id": "6833ce1bd5c438959f750d57",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d58",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d59",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5a",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5b",
          "name": "Daoyuan Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5c",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5d",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5e",
          "name": "Yilun Huang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5f",
          "name": "Yilei Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d60",
          "name": "Dawei Gao",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d61",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d62",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d63",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
      ],
      "publishedAt": "2025-05-23T12:41:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
      "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
      "upvotes": 5,
      "discussionId": "6833ce1cd5c438959f750dab",
      "projectPage": "https://github.com/modelscope/Trinity-RFT",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT",
      "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "RFT-core",
        "synchronous/asynchronous",
        "on-policy/off-policy",
        "online/offline",
        "agent-environment interaction",
        "data pipelines",
        "reinforcement learning paradigms"
      ]
    },
    "publishedAt": "2025-05-23T08:41:09.000Z",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17417",
      "authors": [
        {
          "_id": "6833d2df73bebebe5cd6604e",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd6604f",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66050",
          "name": "Huy Hoang Ha",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66051",
          "name": "Tuan Le Duc Anh",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66052",
          "name": "Shreyas Gopal",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66053",
          "name": "Yue Heng Yeo",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66054",
          "name": "Warren Keng Hoong Low",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66055",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66056",
          "name": "Jia Qi Yip",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T03:05:47.000Z",
      "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
      "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
      "upvotes": 5,
      "discussionId": "6833d2df73bebebe5cd66074",
      "githubRepo": "https://github.com/menloresearch/ichigo",
      "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "speech instruction data",
        "TTS",
        "semantic representation",
        "Whisper encoder",
        "fine-tuning",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-22T23:05:47.000Z",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
    "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17508",
      "authors": [
        {
          "_id": "6833cf5a2d728e2330d572e3",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e4",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e5",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e6",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e7",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e8",
          "name": "Andrew C Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
      ],
      "publishedAt": "2025-05-23T06:01:21.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
      "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
      "upvotes": 4,
      "discussionId": "6833cf5b2d728e2330d57313",
      "projectPage": "https://complex-reasoning.github.io/RPG",
      "githubRepo": "https://github.com/complex-reasoning/RPG",
      "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
      "ai_keywords": [
        "policy gradient algorithms",
        "KL regularization",
        "KL divergence",
        "surrogate loss functions",
        "online reinforcement learning",
        "full differentiable loss functions",
        "REINFORCE-style gradient estimators",
        "GRPO",
        "REINFORCE++",
        "DAPO",
        "regularized policy gradient (RPG)",
        "forward KL divergence",
        "reverse KL divergence",
        "normalized policy distributions",
        "unnormalized policy distributions"
      ]
    },
    "publishedAt": "2025-05-23T02:01:21.000Z",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17412",
      "authors": [
        {
          "_id": "6833e93697966d18e7c1e4d7",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d8",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d9",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4da",
          "name": "Yifei Zeng",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4db",
          "name": "Yikang Yang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dc",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dd",
          "name": "Jiachen Qian",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4de",
          "name": "Siyu Zhu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4df",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e0",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e1",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:58:01.000Z",
      "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
      "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
      "submittedOnDailyBy": {
        "_id": "645a24779f06c5897254d14b",
        "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
        "isPro": false,
        "fullname": "Youtian Lin",
        "user": "LoYoT",
        "type": "user"
      },
      "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
      "upvotes": 4,
      "discussionId": "6833e93b97966d18e7c1e676",
      "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
      "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
      "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
      "ai_keywords": [
        "Signed Distance Functions",
        "sparse volumes",
        "Spatial Sparse Attention",
        "Diffusion Transformer",
        "variational autoencoder",
        "gigascale 3D generation"
      ]
    },
    "publishedAt": "2025-05-22T22:58:01.000Z",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
    "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a24779f06c5897254d14b",
      "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
      "fullname": "Youtian Lin",
      "name": "LoYoT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17091",
      "authors": [
        {
          "_id": "6833cb25fe87d9433dfd2b1c",
          "name": "Prateek Verma",
          "hidden": false
        },
        {
          "_id": "6833cb25fe87d9433dfd2b1d",
          "name": "Mert Pilanci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T22:20:16.000Z",
      "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
      "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
      "submittedOnDailyBy": {
        "_id": "62d7f1119b629105a5d84aad",
        "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
        "isPro": false,
        "fullname": "Prateek Verma",
        "user": "prateekv",
        "type": "user"
      },
      "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
      "upvotes": 4,
      "discussionId": "6833cb26fe87d9433dfd2b64",
      "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
      "ai_keywords": [
        "auto-regressive",
        "LLM",
        "text tokens",
        "audio",
        "visual",
        "embeddings",
        "category labels",
        "classification",
        "FSD-50K",
        "GTZAN",
        "CIFAR-10",
        "Fashion-MNIST",
        "image patches"
      ]
    },
    "publishedAt": "2025-05-20T18:20:16.000Z",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d7f1119b629105a5d84aad",
      "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
      "fullname": "Prateek Verma",
      "name": "prateekv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16270",
      "authors": [
        {
          "_id": "6833d08edf7cbb5c087a8bf1",
          "user": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
            "isPro": false,
            "fullname": "Jiaru Zou",
            "user": "jiaruz2",
            "type": "user"
          },
          "name": "Jiaru Zou",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf2",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf3",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf4",
          "name": "Yunzhe Qi",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf5",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf7",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:00:45.000Z",
      "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
      "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
      "upvotes": 3,
      "discussionId": "6833d08fdf7cbb5c087a8c29",
      "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
      "ai_keywords": [
        "large language models",
        "supervised fine-tuning",
        "domain-specific data",
        "generation loss",
        "model parameters",
        "learning signals",
        "Mistake Log",
        "transformer-based model",
        "Copilot model",
        "logits rectification",
        "joint training paradigm",
        "fused inference paradigm",
        "performance improvements",
        "computational overhead",
        "scalability",
        "transferability"
      ]
    },
    "publishedAt": "2025-05-22T02:00:45.000Z",
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17063",
      "authors": [
        {
          "_id": "6833e65bf9ae3819ea4c568e",
          "name": "Yiduo Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c568f",
          "name": "Zhen Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5690",
          "name": "Chuanwei Huang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5691",
          "name": "Zi-Ang Wang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5692",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5693",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5694",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5695",
          "name": "Yikang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T05:35:13.000Z",
      "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
      "title": "Synthetic Data RL: Task Definition Is All You Need",
      "submittedOnDailyBy": {
        "_id": "638e4e66629b4d0a62ce1bf3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
        "isPro": false,
        "fullname": "Zhen Guo",
        "user": "zguo0525",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
      "upvotes": 3,
      "discussionId": "6833e65cf9ae3819ea4c56c9",
      "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
      "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
      "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "synthetic data",
        "reinforcement fine-tuning",
        "question and answer pairs",
        "model solvability",
        "average pass rate",
        "data budget",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-18T01:35:13.000Z",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e4e66629b4d0a62ce1bf3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
      "fullname": "Zhen Guo",
      "name": "zguo0525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17016",
      "authors": [
        {
          "_id": "6833f7847e0c637c71de0ec6",
          "name": "Shuhan Tan",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec7",
          "name": "Kairan Dou",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec8",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec9",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:45.000Z",
      "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
      "title": "Interactive Post-Training for Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "64b64debeb9a833e08d079fd",
        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
        "isPro": false,
        "fullname": "Shuhan Tan",
        "user": "tanshh97",
        "type": "user"
      },
      "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
      "upvotes": 1,
      "discussionId": "6833f7857e0c637c71de0f07",
      "projectPage": "https://ariostgx.github.io/ript_vla/",
      "githubRepo": "https://github.com/Ariostgx/ript-vla",
      "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
      "ai_keywords": [
        "reinforcement-learning-based",
        "interactive post-training",
        "Vision-Language-Action (VLA) models",
        "sparse binary success rewards",
        "offline expert demonstration",
        "supervised imitation",
        "dynamic rollout sampling",
        "leave-one-out advantage estimation",
        "policy optimization",
        "lightweight QueST model",
        "OpenVLA-OFT model",
        "success rate",
        "computational efficiency",
        "data-efficient",
        "policy learned",
        "generalization",
        "initial state context"
      ]
    },
    "publishedAt": "2025-05-22T13:59:45.000Z",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b64debeb9a833e08d079fd",
      "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
      "fullname": "Shuhan Tan",
      "name": "tanshh97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12891",
      "authors": [
        {
          "_id": "683059e8e2f446ed653e8512",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8513",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8514",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8515",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8516",
          "name": "Tianyi Zhuang",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8517",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8518",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8519",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:22:02.000Z",
      "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
      "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
      "upvotes": 1,
      "discussionId": "683059eae2f446ed653e85d7",
      "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
      "ai_keywords": [
        "Temporal reasoning",
        "Large Language Models (LLMs)",
        "QA pairs",
        "benchmark",
        "TIME-Wiki",
        "TIME-News",
        "TIME-Dial",
        "reasoning models",
        "non-reasoning models",
        "TIME-Lite"
      ]
    },
    "publishedAt": "2025-05-19T05:22:02.000Z",
    "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
    "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17540",
      "authors": [
        {
          "_id": "683409de1869c47bd0c423a4",
          "name": "Mingrui Wu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a6",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a8",
          "name": "Jianjin Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a9",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423aa",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ab",
          "name": "Weihao Han",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ac",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ad",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ae",
          "name": "Xiaoshuai Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423af",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b0",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b1",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b2",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b4",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:44:26.000Z",
      "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
      "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6416d0b2058f65de43191027",
        "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
        "isPro": false,
        "fullname": "Mingrui Wu",
        "user": "mrwu",
        "type": "user"
      },
      "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
      "upvotes": 0,
      "discussionId": "683409e21869c47bd0c4248e",
      "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "large language models",
        "LLMs",
        "reinforcement learning",
        "structured prompts",
        "self-reflective prompts",
        "reward models",
        "human preference",
        "semantic alignment",
        "visual composition",
        "GenEval",
        "T2I-Compbench",
        "spatial layout fidelity",
        "compositional generalization"
      ]
    },
    "publishedAt": "2025-05-23T02:44:26.000Z",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6416d0b2058f65de43191027",
      "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
      "fullname": "Mingrui Wu",
      "name": "mrwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16293",
      "authors": [
        {
          "_id": "683400b5231225ee202c20b7",
          "name": "Rishabh Maheshwary",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b8",
          "name": "Masoud Hashemi",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b9",
          "name": "Khyati Mahajan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20ba",
          "name": "Shiva Krishna Reddy Malay",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bb",
          "name": "Sai Rajeswar",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bc",
          "name": "Sathwik Tejaswi Madhusudhan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bd",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20be",
          "name": "Vikas Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:45:05.000Z",
      "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
      "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
      "submittedOnDailyBy": {
        "_id": "645c26d423ed9b7788d5e24b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
        "isPro": false,
        "fullname": "Rishabh Maheshwary",
        "user": "rmahesh",
        "type": "user"
      },
      "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
      "upvotes": 0,
      "discussionId": "683400b6231225ee202c20e3",
      "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
      "ai_keywords": [
        "Iterative RAG",
        "multi-hop question answering",
        "context length",
        "irrelevant information",
        "dimensionality reduction",
        "Notes Writing",
        "Large Language Models",
        "LLMs",
        "framework agnostic",
        "evaluation datasets"
      ]
    },
    "publishedAt": "2025-05-22T02:45:05.000Z",
    "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
    "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c26d423ed9b7788d5e24b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
      "fullname": "Rishabh Maheshwary",
      "name": "rmahesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11881",
      "authors": [
        {
          "_id": "6833ebdb142b0e50399413d3",
          "name": "Giyeong Oh",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d4",
          "name": "Woohyun Cho",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d5",
          "name": "Siyeol Kim",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d6",
          "name": "Suhwan Choi",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d7",
          "name": "Younjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T07:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
      "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
      "submittedOnDailyBy": {
        "_id": "63d93667255ef6add20f9272",
        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
        "isPro": false,
        "fullname": "Giyeong Oh",
        "user": "BootsofLagrangian",
        "type": "user"
      },
      "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
      "upvotes": 0,
      "discussionId": "6833ebdc142b0e5039941420",
      "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
      "ai_keywords": [
        "residual connections",
        "vanishing gradients",
        "orthogonal update",
        "ResNetV2",
        "Vision Transformers",
        "CIFARs",
        "TinyImageNet",
        "ImageNet-1k",
        "top-1 accuracy"
      ]
    },
    "publishedAt": "2025-05-17T03:16:11.000Z",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
    "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d93667255ef6add20f9272",
      "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
      "fullname": "Giyeong Oh",
      "name": "BootsofLagrangian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]