[
  {
    "paper": {
      "id": "2510.02283",
      "authors": [
        {
          "_id": "68df2b55df49fb0df1e03be2",
          "name": "Justin Cui",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be3",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be5",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be6",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be7",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be8",
          "name": "Andrew Bai",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03be9",
          "name": "Yuanhao Ban",
          "hidden": false
        },
        {
          "_id": "68df2b55df49fb0df1e03bea",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/Fb3ffDmXrslDcQSJH51MW.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/I0qCOZqF-XJo2Xg3i__RJ.mp4"
      ],
      "publishedAt": "2025-10-02T17:55:42.000Z",
      "submittedOnDailyAt": "2025-10-03T00:21:50.292Z",
      "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
      "submittedOnDailyBy": {
        "_id": "65862671e878be571bf9fc52",
        "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
        "isPro": false,
        "fullname": "cuijiaxing",
        "user": "cuijiaxing",
        "type": "user"
      },
      "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
      "upvotes": 43,
      "discussionId": "68df2b56df49fb0df1e03beb",
      "projectPage": "https://self-forcing-plus-plus.github.io/",
      "githubRepo": "https://github.com/justincui03/Self-Forcing-Plus-Plus",
      "ai_summary": "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.",
      "ai_keywords": [
        "diffusion models",
        "transformer architectures",
        "autoregressive formulations",
        "bidirectional teachers",
        "latent space",
        "quality degradation",
        "temporal consistency",
        "position embedding"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-02T13:55:42.000Z",
    "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
    "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/Fb3ffDmXrslDcQSJH51MW.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/I0qCOZqF-XJo2Xg3i__RJ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02283.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65862671e878be571bf9fc52",
      "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
      "fullname": "cuijiaxing",
      "name": "cuijiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00446",
      "authors": [
        {
          "_id": "68ddef306024653e8a3ed0e9",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-02T13:55:10.013Z",
          "hidden": false
        },
        {
          "_id": "68ddef306024653e8a3ed0ea",
          "name": "Yichun Qian",
          "hidden": false
        },
        {
          "_id": "68ddef306024653e8a3ed0eb",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "68ddef306024653e8a3ed0ec",
          "name": "Beijun Shen",
          "hidden": false
        },
        {
          "_id": "68ddef306024653e8a3ed0ed",
          "name": "Xiaodong Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T02:54:57.000Z",
      "submittedOnDailyAt": "2025-10-03T00:37:13.283Z",
      "title": "LongCodeZip: Compress Long Context for Code Language Models",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
      "upvotes": 31,
      "discussionId": "68ddef316024653e8a3ed0ee",
      "githubRepo": "https://github.com/YerbaPage/LongCodeZip",
      "ai_summary": "LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.",
      "ai_keywords": [
        "Large Language Models",
        "code LLMs",
        "context pruning",
        "LLMLingua",
        "conditional perplexity",
        "function-level chunks",
        "fine-grained compression",
        "token budget",
        "code completion",
        "code summarization",
        "question answering",
        "code intelligence applications"
      ],
      "githubStars": 24,
      "organization": {
        "_id": "6724d0b84c0a2bf36e39a226",
        "name": "Stanford-University",
        "fullname": "Stanford University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png"
      }
    },
    "publishedAt": "2025-09-30T22:54:57.000Z",
    "title": "LongCodeZip: Compress Long Context for Code Language Models",
    "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00446.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 247
    },
    "organization": {
      "_id": "6724d0b84c0a2bf36e39a226",
      "name": "Stanford-University",
      "fullname": "Stanford University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.02297",
      "authors": [
        {
          "_id": "68df29e3df49fb0df1e03bca",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "68df29e3df49fb0df1e03bcb",
          "name": "Yang Young Lu",
          "hidden": false
        },
        {
          "_id": "68df29e3df49fb0df1e03bcc",
          "name": "Yuntian Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/eATqBkbxrVSqsBzcMJ__-.png"
      ],
      "publishedAt": "2025-10-02T17:59:00.000Z",
      "submittedOnDailyAt": "2025-10-03T00:14:16.738Z",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "submittedOnDailyBy": {
        "_id": "63081e15a670ed10f9d44229",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
        "isPro": true,
        "fullname": "Yuntian Deng",
        "user": "yuntian-deng",
        "type": "user"
      },
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "upvotes": 20,
      "discussionId": "68df29e3df49fb0df1e03bcd",
      "projectPage": "https://interactivetraining.ai/",
      "githubRepo": "https://github.com/yuntian-group/interactive-training",
      "ai_summary": "Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.",
      "ai_keywords": [
        "Interactive Training",
        "control server",
        "optimizer hyperparameters",
        "training data",
        "model checkpoints",
        "training stability",
        "sensitivity to initial hyperparameters",
        "adaptability",
        "AI agents",
        "training logs",
        "training dynamics"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "66e4ffb6caa240e5c2fa39e9",
        "name": "yuntian-group",
        "fullname": "Yuntian Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/6xB3f3moklDS6qrZAit3I.png"
      }
    },
    "publishedAt": "2025-10-02T13:59:00.000Z",
    "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
    "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/eATqBkbxrVSqsBzcMJ__-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02297.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63081e15a670ed10f9d44229",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
      "fullname": "Yuntian Deng",
      "name": "yuntian-deng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 258
    },
    "organization": {
      "_id": "66e4ffb6caa240e5c2fa39e9",
      "name": "yuntian-group",
      "fullname": "Yuntian Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/6xB3f3moklDS6qrZAit3I.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02209",
      "authors": [
        {
          "_id": "68df3031df49fb0df1e03c3a",
          "name": "Yanxu Chen",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c3b",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c3c",
          "name": "Yantao Liu",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c3d",
          "name": "Jin Ye",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c3e",
          "name": "Jianing Yu",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c3f",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "68df3031df49fb0df1e03c40",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T16:54:57.000Z",
      "submittedOnDailyAt": "2025-10-03T00:38:58.116Z",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
      "upvotes": 19,
      "discussionId": "68df3031df49fb0df1e03c41",
      "ai_summary": "StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.",
      "ai_keywords": [
        "large language models",
        "LLM agents",
        "reasoning",
        "tool use",
        "sequential decision-making",
        "software engineering",
        "scientific discovery",
        "finance domain",
        "financial benchmarks",
        "question answering",
        "dynamic trading",
        "contamination-free benchmark",
        "daily market signals",
        "prices",
        "fundamentals",
        "news",
        "sequential buy",
        "sell",
        "hold decisions",
        "cumulative return",
        "maximum drawdown",
        "Sortino ratio",
        "GPT-5",
        "Claude-4",
        "Qwen3",
        "Kimi-K2",
        "GLM-4.5",
        "buy-and-hold baseline",
        "static financial knowledge tasks",
        "trading strategies"
      ]
    },
    "publishedAt": "2025-10-02T12:54:57.000Z",
    "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
    "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01444",
      "authors": [
        {
          "_id": "68df2e3edf49fb0df1e03c19",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1a",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1b",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1c",
          "name": "Runpeng Dai",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1d",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c1f",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c20",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c21",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c22",
          "name": "Pratap Tokekar",
          "hidden": false
        },
        {
          "_id": "68df2e3edf49fb0df1e03c23",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T20:32:08.000Z",
      "submittedOnDailyAt": "2025-10-03T00:33:45.725Z",
      "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6656bf615b203a05a1f0968c",
        "avatarUrl": "/avatars/1ee0b0099c10dd76c8e3b7d312221b15.svg",
        "isPro": false,
        "fullname": "Rui Liu",
        "user": "lr10260",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce VOGUE (Visual Uncertainty Guided\nExploration), a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
      "upvotes": 17,
      "discussionId": "68df2e3edf49fb0df1e03c24",
      "ai_summary": "VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "multimodal LLMs",
        "visual uncertainty",
        "stochastic context",
        "symmetric KL divergence",
        "uncertainty-aware exploration",
        "token-entropy bonus",
        "annealed sampling schedule",
        "GRPO",
        "pass@1 accuracy",
        "pass@4 performance",
        "exploration decay"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-10-01T16:32:08.000Z",
    "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce VOGUE (Visual Uncertainty Guided\nExploration), a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6656bf615b203a05a1f0968c",
      "avatarUrl": "/avatars/1ee0b0099c10dd76c8e3b7d312221b15.svg",
      "fullname": "Rui Liu",
      "name": "lr10260",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02314",
      "authors": [
        {
          "_id": "68df49acdf49fb0df1e03d51",
          "name": "Bo-Hsu Ke",
          "hidden": false
        },
        {
          "_id": "68df49acdf49fb0df1e03d52",
          "name": "You-Zhe Xie",
          "hidden": false
        },
        {
          "_id": "68df49acdf49fb0df1e03d53",
          "name": "Yu-Lun Liu",
          "hidden": false
        },
        {
          "_id": "68df49acdf49fb0df1e03d54",
          "name": "Wei-Chen Chiu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/N5QZmwAl2bgkGqsFR0liN.mp4"
      ],
      "publishedAt": "2025-10-02T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-03T02:30:49.018Z",
      "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/",
      "upvotes": 16,
      "discussionId": "68df49addf49fb0df1e03d55",
      "projectPage": "https://hentci.github.io/stealthattack/",
      "githubRepo": "https://github.com/Hentci/StealthAttack_official",
      "ai_summary": "A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.",
      "ai_keywords": [
        "Neural Radiance Fields",
        "3D Gaussian Splatting",
        "image-level poisoning attacks",
        "density-guided poisoning",
        "Gaussian points",
        "Kernel Density Estimation",
        "viewpoint-dependent illusory objects",
        "multi-view consistency",
        "KDE-based evaluation protocol"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "63e39e6499a032b1c950403d",
        "name": "NYCU",
        "fullname": "National Yang Ming Chiao Tung University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
      }
    },
    "publishedAt": "2025-10-02T13:59:57.000Z",
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
    "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/N5QZmwAl2bgkGqsFR0liN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "63e39e6499a032b1c950403d",
      "name": "NYCU",
      "fullname": "National Yang Ming Chiao Tung University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01591",
      "authors": [
        {
          "_id": "68df27e9df49fb0df1e03bb7",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bb8",
          "name": "Ruosen Li",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bb9",
          "name": "Yujun Zhou",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bba",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bbb",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bbc",
          "name": "Xinya Du",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bbd",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68df27e9df49fb0df1e03bbe",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T02:14:33.000Z",
      "submittedOnDailyAt": "2025-10-03T00:04:05.892Z",
      "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
      "submittedOnDailyBy": {
        "_id": "62ffa3f8311cad266f9af236",
        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
        "isPro": false,
        "fullname": "Zhenwen Liang",
        "user": "invokerliang",
        "type": "user"
      },
      "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
      "upvotes": 16,
      "discussionId": "68df27e9df49fb0df1e03bbf",
      "ai_summary": "Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.",
      "ai_keywords": [
        "Large Language Model",
        "hidden states",
        "token embeddings",
        "semantic features",
        "lexical features",
        "output logits",
        "confidence-related information",
        "Clue",
        "Clustering and Experience-based Verification",
        "nearest-centroid distance",
        "AIME",
        "GPQA"
      ]
    },
    "publishedAt": "2025-10-01T22:14:33.000Z",
    "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
    "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01591.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "62ffa3f8311cad266f9af236",
      "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
      "fullname": "Zhenwen Liang",
      "name": "invokerliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02240",
      "authors": [
        {
          "_id": "68df4813df49fb0df1e03d44",
          "name": "Sicheng Feng",
          "hidden": false
        },
        {
          "_id": "68df4813df49fb0df1e03d45",
          "name": "Kaiwen Tuo",
          "hidden": false
        },
        {
          "_id": "68df4813df49fb0df1e03d46",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "68df4813df49fb0df1e03d47",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "68df4813df49fb0df1e03d48",
          "name": "Jianke Zhu",
          "hidden": false
        },
        {
          "_id": "68df4813df49fb0df1e03d49",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:29:46.000Z",
      "submittedOnDailyAt": "2025-10-03T02:28:45.311Z",
      "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "66863d26e2b71e3d09189ae9",
        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
        "isPro": false,
        "fullname": "Song Wang",
        "user": "songw-zju",
        "type": "user"
      },
      "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
      "upvotes": 11,
      "discussionId": "68df4814df49fb0df1e03d4a",
      "projectPage": "https://fscdc.github.io/RewardMap",
      "githubRepo": "https://github.com/fscdc/RewardMap",
      "ai_summary": "RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.",
      "ai_keywords": [
        "ReasonMap",
        "ReasonMap-Plus",
        "Visual Question Answering (VQA)",
        "reinforcement learning (RL)",
        "sparse rewards",
        "unstable optimization",
        "difficulty-aware reward design",
        "multi-stage RL",
        "cold-start training",
        "Supervised Fine-Tuning (SFT)",
        "spatial reasoning",
        "fine-grained visual reasoning"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "67d81e5e25ad2831362ec592",
        "name": "WestlakeUniversity",
        "fullname": "Westlake University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
      }
    },
    "publishedAt": "2025-10-02T13:29:46.000Z",
    "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
    "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863d26e2b71e3d09189ae9",
      "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
      "fullname": "Song Wang",
      "name": "songw-zju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67d81e5e25ad2831362ec592",
      "name": "WestlakeUniversity",
      "fullname": "Westlake University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62b624f3b52bef716e248fd7/80xTZ-_peWYICJr1JDkOw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01265",
      "authors": [
        {
          "_id": "68df26d8df49fb0df1e03ba8",
          "name": "Ali Hatamizadeh",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03ba9",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03baa",
          "name": "Shrimai Prabhumoye",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03bab",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03bac",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03bad",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03bae",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "68df26d8df49fb0df1e03baf",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T17:53:54.000Z",
      "submittedOnDailyAt": "2025-10-03T00:55:02.431Z",
      "title": "RLP: Reinforcement as a Pretraining Objective",
      "submittedOnDailyBy": {
        "_id": "64414b62603214724ebd2636",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg",
        "isPro": false,
        "fullname": "Ali",
        "user": "ahatamiz",
        "type": "user"
      },
      "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.",
      "upvotes": 9,
      "discussionId": "68df26d8df49fb0df1e03bb0",
      "projectPage": "https://t.co/6PNJYfiAoJ",
      "githubRepo": "https://github.com/NVlabs/RLP",
      "ai_summary": "RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "pre-training",
        "next-token prediction",
        "chain-of-thought",
        "information gain",
        "log-likelihood",
        "dense reward signal",
        "Qwen3-1.7B-Base",
        "AIME25",
        "MMLU-Pro",
        "Nemotron-Nano-12B-v2",
        "scientific reasoning"
      ],
      "githubStars": 70,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-09-26T13:53:54.000Z",
    "title": "RLP: Reinforcement as a Pretraining Objective",
    "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64414b62603214724ebd2636",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg",
      "fullname": "Ali",
      "name": "ahatamiz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02253",
      "authors": [
        {
          "_id": "68df2c09df49fb0df1e03bf4",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bf5",
          "name": "Shilin Lu",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bf6",
          "name": "Shuli Leng",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bf7",
          "name": "Shaocong Zhang",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bf8",
          "name": "Zhuming Lian",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bf9",
          "name": "Xinlei Yu",
          "hidden": false
        },
        {
          "_id": "68df2c09df49fb0df1e03bfa",
          "name": "Adams Wai-Kin Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:39:13.000Z",
      "submittedOnDailyAt": "2025-10-03T00:22:00.581Z",
      "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
      "submittedOnDailyBy": {
        "_id": "631c4a23aa346997917bcb89",
        "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
        "isPro": false,
        "fullname": "Shilin Lu",
        "user": "Shilin-LU",
        "type": "user"
      },
      "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
      "upvotes": 8,
      "discussionId": "68df2c0adf49fb0df1e03bfb",
      "ai_summary": "DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.",
      "ai_keywords": [
        "DragFlow",
        "FLUX",
        "flow matching",
        "DiT",
        "UNet",
        "DDPMs",
        "affine transformations",
        "pretrained open-domain personalization adapters",
        "IP-Adapter",
        "gradient mask-based hard constraints",
        "multimodal large language models",
        "MLLMs",
        "Region-based Dragging benchmark",
        "ReD Bench",
        "DragBench-DR"
      ]
    },
    "publishedAt": "2025-10-02T13:39:13.000Z",
    "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
    "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c4a23aa346997917bcb89",
      "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
      "fullname": "Shilin Lu",
      "name": "Shilin-LU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02250",
      "authors": [
        {
          "_id": "68df34efdf49fb0df1e03ca7",
          "name": "Gonzalo Gonzalez-Pumariega",
          "hidden": false
        },
        {
          "_id": "68df34efdf49fb0df1e03ca8",
          "name": "Vincent Tu",
          "hidden": false
        },
        {
          "_id": "68df34efdf49fb0df1e03ca9",
          "name": "Chih-Lun Lee",
          "hidden": false
        },
        {
          "_id": "68df34efdf49fb0df1e03caa",
          "name": "Jiachen Yang",
          "hidden": false
        },
        {
          "_id": "68df34efdf49fb0df1e03cab",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "68df34efdf49fb0df1e03cac",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:37:08.000Z",
      "submittedOnDailyAt": "2025-10-03T01:00:35.455Z",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "upvotes": 8,
      "discussionId": "68df34efdf49fb0df1e03cad",
      "projectPage": "https://www.simular.ai/articles/agent-s3",
      "githubRepo": "https://github.com/simular-ai/Agent-S/",
      "ai_summary": "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.",
      "ai_keywords": [
        "Behavior Best-of-N",
        "bBoN",
        "rollouts",
        "behavior narratives",
        "trajectory selection",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld"
      ],
      "githubStars": 6317,
      "organization": {
        "_id": "63f9b97fac8368a4dce39668",
        "name": "simular-ai",
        "fullname": "Simular",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f9b921aa034447f0ded85f/cwz8V-HNY3JjSdhGdSFkY.png"
      }
    },
    "publishedAt": "2025-10-02T13:37:08.000Z",
    "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
    "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02250.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "63f9b97fac8368a4dce39668",
      "name": "simular-ai",
      "fullname": "Simular",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f9b921aa034447f0ded85f/cwz8V-HNY3JjSdhGdSFkY.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01284",
      "authors": [
        {
          "_id": "68df3d06df49fb0df1e03d05",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "68df3d06df49fb0df1e03d06",
          "name": "Weimin Wang",
          "hidden": false
        },
        {
          "_id": "68df3d06df49fb0df1e03d07",
          "name": "Calder Katyal",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b43ffec624a43b1a1ada46/djeLgQC3e6--D6ou913st.mp4"
      ],
      "publishedAt": "2025-09-30T21:03:50.000Z",
      "submittedOnDailyAt": "2025-10-03T01:53:05.749Z",
      "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
      "upvotes": 8,
      "discussionId": "68df3d07df49fb0df1e03d08",
      "projectPage": "https://aaxwaz.github.io/Ovi",
      "githubRepo": "https://github.com/character-ai/Ovi",
      "ai_summary": "Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.",
      "ai_keywords": [
        "twin-DiT modules",
        "blockwise cross-modal fusion",
        "scaled-RoPE embeddings",
        "bidirectional cross-attention",
        "cinematic storytelling",
        "movie-grade video clips"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "641f40f26d51620635e2f5ba",
        "name": "characterai",
        "fullname": "Character.AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/UVDEosgcp2kBUfuAMW7aU.png"
      }
    },
    "publishedAt": "2025-09-30T17:03:50.000Z",
    "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
    "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b43ffec624a43b1a1ada46/djeLgQC3e6--D6ou913st.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01284.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "641f40f26d51620635e2f5ba",
      "name": "characterai",
      "fullname": "Character.AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/UVDEosgcp2kBUfuAMW7aU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01179",
      "authors": [
        {
          "_id": "68de23376024653e8a3ed1e2",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e3",
          "name": "Adriana Meza Soria",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e4",
          "name": "Shawn Tan",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e5",
          "name": "Anurag Roy",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e6",
          "name": "Ashish Sunil Agrawal",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e7",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "68de23376024653e8a3ed1e8",
          "name": "Rameswar Panda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T17:58:03.000Z",
      "submittedOnDailyAt": "2025-10-03T00:25:08.684Z",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
      "upvotes": 6,
      "discussionId": "68de23386024653e8a3ed1e9",
      "githubRepo": "https://github.com/TheAgentArk/Toucan",
      "ai_summary": "Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "tool-agentic training data",
        "Model Context Protocols (MCPs)",
        "tool-use queries",
        "model-based quality filtering",
        "agentic trajectories",
        "teacher models",
        "agentic frameworks",
        "BFCL V3 benchmark",
        "MCP-Universe Bench"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "616e7b1d75754a5d5fa455cf",
        "name": "ibm",
        "fullname": "IBM",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/9228luWRoGbZwKGxkOOsj.png"
      }
    },
    "publishedAt": "2025-10-01T13:58:03.000Z",
    "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
    "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "organization": {
      "_id": "616e7b1d75754a5d5fa455cf",
      "name": "ibm",
      "fullname": "IBM",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/9228luWRoGbZwKGxkOOsj.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00428",
      "authors": [
        {
          "_id": "68de602e70ada21878c74fff",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "68de602e70ada21878c75000",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "68de602e70ada21878c75001",
          "name": "Juho Jung",
          "hidden": false
        },
        {
          "_id": "68de602e70ada21878c75002",
          "name": "Dongseop Kim",
          "hidden": false
        },
        {
          "_id": "68de602e70ada21878c75003",
          "name": "Won Hwa Kim",
          "hidden": false
        },
        {
          "_id": "68de602e70ada21878c75004",
          "name": "Sunghoon Joo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T02:14:23.000Z",
      "submittedOnDailyAt": "2025-10-03T00:40:30.421Z",
      "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
      "submittedOnDailyBy": {
        "_id": "6357a08f8ed056fa1ccd3b38",
        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
        "isPro": false,
        "fullname": "Seongjae Kang",
        "user": "erjui",
        "type": "user"
      },
      "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
      "upvotes": 6,
      "discussionId": "68de602e70ada21878c75005",
      "githubRepo": "https://github.com/vuno/contextualized-srrg",
      "ai_summary": "Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.",
      "ai_keywords": [
        "structured radiology report generation",
        "chest X-ray images",
        "clinical context",
        "temporal hallucinations",
        "multimodal large language models",
        "multi-view X-ray images",
        "clinical indication",
        "imaging techniques",
        "prior studies",
        "patient histories"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-30T22:14:23.000Z",
    "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
    "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00428.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6357a08f8ed056fa1ccd3b38",
      "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
      "fullname": "Seongjae Kang",
      "name": "erjui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02173",
      "authors": [
        {
          "_id": "68df5d52df49fb0df1e03d7a",
          "name": "Hsuan Su",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d7b",
          "name": "Ting-Yao Hu",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d7c",
          "name": "Hema Swetha Koppula",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d7d",
          "name": "Kundan Krishna",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d7e",
          "name": "Hadi Pouransari",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d7f",
          "name": "Cheng-Yu Hsieh",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d80",
          "name": "Cem Koc",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d81",
          "name": "Joseph Yitan Cheng",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d82",
          "name": "Oncel Tuzel",
          "hidden": false
        },
        {
          "_id": "68df5d52df49fb0df1e03d83",
          "name": "Raviteja Vemulapalli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T16:24:28.000Z",
      "submittedOnDailyAt": "2025-10-03T03:58:53.282Z",
      "title": "Learning to Reason for Hallucination Span Detection",
      "submittedOnDailyBy": {
        "_id": "608abf1272b50b02c4b02865",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
        "isPro": false,
        "fullname": "Hsuan Su",
        "user": "jacksukk",
        "type": "user"
      },
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "upvotes": 5,
      "discussionId": "68df5d52df49fb0df1e03d84",
      "ai_summary": "A reinforcement learning framework with span-level rewards improves hallucination span detection in large language models by incentivizing reasoning.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reinforcement learning",
        "span-level reward function",
        "Group Relative Policy Optimization",
        "Class-Aware Policy Optimization",
        "RAGTruth benchmark",
        "summarization",
        "question answering",
        "data-to-text",
        "hallucination span detection"
      ]
    },
    "publishedAt": "2025-10-02T12:24:28.000Z",
    "title": "Learning to Reason for Hallucination Span Detection",
    "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608abf1272b50b02c4b02865",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
      "fullname": "Hsuan Su",
      "name": "jacksukk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26376",
      "authors": [
        {
          "_id": "68df3bbfdf49fb0df1e03cdc",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03cdd",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03cde",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03cdf",
          "name": "Rongjin Guo",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03ce0",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03ce1",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "68df3bbfdf49fb0df1e03ce2",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T15:08:25.000Z",
      "submittedOnDailyAt": "2025-10-03T01:30:06.294Z",
      "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
      "submittedOnDailyBy": {
        "_id": "6570450a78d7aca0c361a177",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
        "isPro": false,
        "fullname": "Harold Chen",
        "user": "Harold328",
        "type": "user"
      },
      "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing\nlarge language models, yet its application to next-token prediction (NTP)\nautoregressive (AR) image generation remains largely uncharted. Existing TTS\napproaches for visual AR (VAR), which rely on frequent partial decoding and\nexternal reward models, are ill-suited for NTP-based image generation due to\nthe inherent incompleteness of intermediate decoding results. To bridge this\ngap, we introduce ScalingAR, the first TTS framework specifically designed for\nNTP-based AR image generation that eliminates the need for early decoding or\nauxiliary rewards. ScalingAR leverages token entropy as a novel signal in\nvisual token generation and operates at two complementary scaling levels: (i)\nProfile Level, which streams a calibrated confidence state by fusing intrinsic\nand conditional signals; and (ii) Policy Level, which utilizes this state to\nadaptively terminate low-confidence trajectories and dynamically schedule\nguidance for phase-appropriate conditioning strength. Experiments on both\ngeneral and compositional benchmarks show that ScalingAR (1) improves base\nmodels by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces\nvisual token consumption by 62.0% while outperforming baselines, and (3)\nsuccessfully enhances robustness, mitigating performance drops by 26.0% in\nchallenging scenarios.",
      "upvotes": 5,
      "discussionId": "68df3bc0df49fb0df1e03ce3",
      "githubRepo": "https://github.com/EnVision-Research/ScalingAR",
      "ai_summary": "ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.",
      "ai_keywords": [
        "test-time scaling",
        "next-token prediction",
        "autoregressive image generation",
        "token entropy",
        "profile level",
        "policy level",
        "GenEval",
        "TIIF-Bench",
        "visual token consumption",
        "robustness"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-09-30T11:08:25.000Z",
    "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
    "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing\nlarge language models, yet its application to next-token prediction (NTP)\nautoregressive (AR) image generation remains largely uncharted. Existing TTS\napproaches for visual AR (VAR), which rely on frequent partial decoding and\nexternal reward models, are ill-suited for NTP-based image generation due to\nthe inherent incompleteness of intermediate decoding results. To bridge this\ngap, we introduce ScalingAR, the first TTS framework specifically designed for\nNTP-based AR image generation that eliminates the need for early decoding or\nauxiliary rewards. ScalingAR leverages token entropy as a novel signal in\nvisual token generation and operates at two complementary scaling levels: (i)\nProfile Level, which streams a calibrated confidence state by fusing intrinsic\nand conditional signals; and (ii) Policy Level, which utilizes this state to\nadaptively terminate low-confidence trajectories and dynamically schedule\nguidance for phase-appropriate conditioning strength. Experiments on both\ngeneral and compositional benchmarks show that ScalingAR (1) improves base\nmodels by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces\nvisual token consumption by 62.0% while outperforming baselines, and (3)\nsuccessfully enhances robustness, mitigating performance drops by 26.0% in\nchallenging scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6570450a78d7aca0c361a177",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
      "fullname": "Harold Chen",
      "name": "Harold328",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.21789",
      "authors": [
        {
          "_id": "68df677ddf49fb0df1e03dab",
          "name": "Xinlei Yu",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03dac",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03dad",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03dae",
          "name": "Yongbo He",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03daf",
          "name": "Zhangquan Chen",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db0",
          "name": "Zhucun Xue",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db1",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db2",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db3",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db4",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "68df677ddf49fb0df1e03db5",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T02:43:24.000Z",
      "submittedOnDailyAt": "2025-10-03T04:38:13.318Z",
      "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
      "submittedOnDailyBy": {
        "_id": "67d63e228d5c7a132cbcf39b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
        "isPro": false,
        "fullname": "neil yu",
        "user": "yxl66666",
        "type": "user"
      },
      "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git.",
      "upvotes": 5,
      "discussionId": "68df677edf49fb0df1e03db6",
      "githubRepo": "https://github.com/YU-deep/ViF",
      "ai_summary": "ViF mitigates visual hallucination snowballing in Multi-Agent Systems by enhancing visual attention and message relay through selected visual tokens.",
      "ai_keywords": [
        "Multi-Agent System",
        "Visual Language Models",
        "visual hallucination snowballing",
        "turn-wise attention",
        "layer-wise attention",
        "token-wise attention",
        "visual attention allocation",
        "vision tokens",
        "unimodal attention peak",
        "Visual Flow",
        "attention reallocation"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-25T22:43:24.000Z",
    "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow",
    "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21789.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "67d63e228d5c7a132cbcf39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ynwA3Sya5irwMRCmSeLiC.png",
      "fullname": "neil yu",
      "name": "yxl66666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02295",
      "authors": [
        {
          "_id": "68df252adf49fb0df1e03b9e",
          "name": "Enxin Song",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03b9f",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba0",
          "name": "Shusheng Yang",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba1",
          "name": "Ethan Armand",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba2",
          "name": "Xiaojun Shan",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba3",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba4",
          "name": "Jianwen Xie",
          "hidden": false
        },
        {
          "_id": "68df252adf49fb0df1e03ba5",
          "name": "Zhuowen Tu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:58:54.000Z",
      "submittedOnDailyAt": "2025-10-03T04:56:26.524Z",
      "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
      "submittedOnDailyBy": {
        "_id": "63a7b196e27a6dbd4861e275",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a7b196e27a6dbd4861e275/QGn4hdDPNuSRgZVKxzDpq.jpeg",
        "isPro": false,
        "fullname": "EnxinSong",
        "user": "Enxin",
        "type": "user"
      },
      "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
      "upvotes": 4,
      "discussionId": "68df252adf49fb0df1e03ba6",
      "projectPage": "https://enxinsong.com/VideoNSA-web/",
      "githubRepo": "https://github.com/Espere-1119-Song/VideoNSA",
      "ai_summary": "VideoNSA, an adaptation of Native Sparse Attention to video-language models, enhances long-video understanding and temporal reasoning through end-to-end training and a hardware-aware hybrid attention approach.",
      "ai_keywords": [
        "Native Sparse Attention",
        "NSA",
        "VideoNSA",
        "Qwen2.5-VL",
        "token-compression",
        "sparse baselines",
        "long-video understanding",
        "temporal reasoning",
        "spatial benchmarks",
        "global-local attention",
        "dynamic attention sinks"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-10-02T13:58:54.000Z",
    "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a7b196e27a6dbd4861e275",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a7b196e27a6dbd4861e275/QGn4hdDPNuSRgZVKxzDpq.jpeg",
      "fullname": "EnxinSong",
      "name": "Enxin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02294",
      "authors": [
        {
          "_id": "68df2abedf49fb0df1e03bdb",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "68df2abedf49fb0df1e03bdc",
          "name": "Zihan Liao",
          "hidden": false
        },
        {
          "_id": "68df2abedf49fb0df1e03bdd",
          "name": "Hang Yu",
          "hidden": false
        },
        {
          "_id": "68df2abedf49fb0df1e03bde",
          "name": "Peng Di",
          "hidden": false
        },
        {
          "_id": "68df2abedf49fb0df1e03bdf",
          "name": "Rui Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:58:49.000Z",
      "submittedOnDailyAt": "2025-10-03T00:33:13.324Z",
      "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
      "submittedOnDailyBy": {
        "_id": "6430bdd8cd31d174a9f900fb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
        "isPro": false,
        "fullname": "Ziyin Zhang",
        "user": "Geralt-Targaryen",
        "type": "user"
      },
      "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
      "upvotes": 4,
      "discussionId": "68df2abedf49fb0df1e03be0",
      "githubRepo": "https://github.com/codefuse-ai/CodeFuse-Embeddings/tree/main/F2LLM",
      "ai_summary": "F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.",
      "ai_keywords": [
        "embedding models",
        "foundation models",
        "contrastive pretraining",
        "training pipelines",
        "synthetic training data",
        "query-document-negative tuples",
        "MTEB English leaderboard",
        "parameter-efficient fine-tuning"
      ],
      "githubStars": 25,
      "organization": {
        "_id": "64f97f402003abb61b3d68de",
        "name": "codefuse-ai",
        "fullname": "CodeFuse AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ed861f254de0e729dba8f1/IxbJCzRWm0Ov645jLyupL.png"
      }
    },
    "publishedAt": "2025-10-02T13:58:49.000Z",
    "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
    "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "64f97f402003abb61b3d68de",
      "name": "codefuse-ai",
      "fullname": "CodeFuse AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ed861f254de0e729dba8f1/IxbJCzRWm0Ov645jLyupL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02286",
      "authors": [
        {
          "_id": "68df64e7df49fb0df1e03da3",
          "name": "Ruohao Guo",
          "hidden": false
        },
        {
          "_id": "68df64e7df49fb0df1e03da4",
          "name": "Afshin Oroojlooy",
          "hidden": false
        },
        {
          "_id": "68df64e7df49fb0df1e03da5",
          "name": "Roshan Sridhar",
          "hidden": false
        },
        {
          "_id": "68df64e7df49fb0df1e03da6",
          "name": "Miguel Ballesteros",
          "hidden": false
        },
        {
          "_id": "68df64e7df49fb0df1e03da7",
          "name": "Alan Ritter",
          "hidden": false
        },
        {
          "_id": "68df64e7df49fb0df1e03da8",
          "name": "Dan Roth",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:57:05.000Z",
      "submittedOnDailyAt": "2025-10-03T04:40:07.960Z",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "submittedOnDailyBy": {
        "_id": "628711eb40423ef48fbc07af",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628711eb40423ef48fbc07af/QmW1ow6LFNxPa0TyYBAJY.jpeg",
        "isPro": false,
        "fullname": "Ruohao Guo",
        "user": "ruohao",
        "type": "user"
      },
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "upvotes": 4,
      "discussionId": "68df64e8df49fb0df1e03da9",
      "ai_summary": "DialTree-RPO, an on-policy reinforcement learning framework with tree search, autonomously discovers diverse multi-turn attack strategies against large language models, achieving higher attack success rates and uncovering new attack trajectories.",
      "ai_keywords": [
        "adversarial attacks",
        "multi-turn interaction",
        "reinforcement learning",
        "tree search",
        "sequential decision-making",
        "dialogue policies",
        "attack success rate"
      ]
    },
    "publishedAt": "2025-10-02T13:57:05.000Z",
    "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
    "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628711eb40423ef48fbc07af",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628711eb40423ef48fbc07af/QmW1ow6LFNxPa0TyYBAJY.jpeg",
      "fullname": "Ruohao Guo",
      "name": "ruohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01304",
      "authors": [
        {
          "_id": "68df4ea4df49fb0df1e03d57",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d58",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d59",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5a",
          "name": "Xikun Bao",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5b",
          "name": "Yukun Qi",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5c",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5d",
          "name": "Qiuchen Wang",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5e",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d5f",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d60",
          "name": "Huaian Chen",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d61",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68df4ea4df49fb0df1e03d62",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T17:58:05.000Z",
      "submittedOnDailyAt": "2025-10-03T02:57:49.307Z",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "665d652e0f35c005de892108",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
        "isPro": false,
        "fullname": "Yu Zeng",
        "user": "YuZeng260",
        "type": "user"
      },
      "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
      "upvotes": 4,
      "discussionId": "68df4ea4df49fb0df1e03d63",
      "projectPage": "https://yuzeng0-0.github.io/AGILE/",
      "githubRepo": "https://github.com/yuzeng0-0/AGILE",
      "ai_summary": "AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "jigsaw tasks",
        "perceptual capabilities",
        "reasoning capabilities",
        "AGILE",
        "Agentic jiGsaw Interaction Learning",
        "executable code",
        "fine-grained visual feedback",
        "multimodal understanding",
        "multimodal reasoning",
        "multimodal reinforcement learning"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-10-01T13:58:05.000Z",
    "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
    "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01304.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665d652e0f35c005de892108",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
      "fullname": "Yu Zeng",
      "name": "YuZeng260",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02259",
      "authors": [
        {
          "_id": "68df2a3fdf49fb0df1e03bd3",
          "name": "Tobias Kreiman",
          "hidden": false
        },
        {
          "_id": "68df2a3fdf49fb0df1e03bd4",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "68df2a3fdf49fb0df1e03bd5",
          "name": "Fadi Atieh",
          "hidden": false
        },
        {
          "_id": "68df2a3fdf49fb0df1e03bd6",
          "name": "Elizabeth Weaver",
          "hidden": false
        },
        {
          "_id": "68df2a3fdf49fb0df1e03bd7",
          "name": "Eric Qu",
          "hidden": false
        },
        {
          "_id": "68df2a3fdf49fb0df1e03bd8",
          "name": "Aditi S. Krishnapriyan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:42:10.000Z",
      "submittedOnDailyAt": "2025-10-03T00:14:38.936Z",
      "title": "Transformers Discover Molecular Structure Without Graph Priors",
      "submittedOnDailyBy": {
        "_id": "64bece3e01f1983a86a3e043",
        "avatarUrl": "/avatars/7f6372d409e8eb0474f7ff6282cfd4e9.svg",
        "isPro": false,
        "fullname": "Toby Kreiman",
        "user": "tkreiman",
        "type": "user"
      },
      "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinatesx2013without predefined graphs or physical\npriorsx2013can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatternsx2013such as attention weights that decay inversely with\ninteratomic distancex2013and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
      "upvotes": 3,
      "discussionId": "68df2a3fdf49fb0df1e03bd9",
      "ai_summary": "Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.",
      "ai_keywords": [
        "Graph Neural Networks",
        "GNNs",
        "molecular machine learning",
        "molecular property prediction",
        "machine learning interatomic potentials",
        "MLIPs",
        "message passing",
        "predefined graphs",
        "fixed radius cutoff",
        "k-nearest neighbor",
        "receptive field",
        "inference",
        "sparse graph operations",
        "Transformers",
        "Cartesian coordinates",
        "physical priors",
        "energy and force mean absolute errors",
        "OMol25 dataset",
        "attention weights",
        "interatomic distance",
        "molecular environments",
        "hard-coded biases",
        "scaling training resources",
        "empirical scaling laws",
        "standardized architectures",
        "molecular modeling"
      ],
      "organization": {
        "_id": "61f20a9ce108f2cba2dc0730",
        "name": "Berkeley",
        "fullname": "UC Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
      }
    },
    "publishedAt": "2025-10-02T13:42:10.000Z",
    "title": "Transformers Discover Molecular Structure Without Graph Priors",
    "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinatesx2013without predefined graphs or physical\npriorsx2013can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatternsx2013such as attention weights that decay inversely with\ninteratomic distancex2013and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bece3e01f1983a86a3e043",
      "avatarUrl": "/avatars/7f6372d409e8eb0474f7ff6282cfd4e9.svg",
      "fullname": "Toby Kreiman",
      "name": "tkreiman",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "61f20a9ce108f2cba2dc0730",
      "name": "Berkeley",
      "fullname": "UC Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02190",
      "authors": [
        {
          "_id": "68df3194df49fb0df1e03c4e",
          "name": "Yang Yao",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c4f",
          "name": "Yixu Wang",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c50",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c51",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c52",
          "name": "Tianle Gu",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c53",
          "name": "Lingyu Li",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c54",
          "name": "Dingyi Zhao",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c55",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c56",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c57",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c58",
          "name": "Yan Teng",
          "hidden": false
        },
        {
          "_id": "68df3194df49fb0df1e03c59",
          "name": "Yingchun Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T16:40:02.000Z",
      "submittedOnDailyAt": "2025-10-03T00:45:32.030Z",
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "upvotes": 3,
      "discussionId": "68df3195df49fb0df1e03c5a",
      "ai_summary": "A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.",
      "ai_keywords": [
        "Deep Research Agents",
        "task decomposition",
        "cross-source retrieval",
        "multi-stage reasoning",
        "structured output",
        "benchmark",
        "evaluation framework",
        "thematic domains",
        "reference bundles",
        "semantic quality",
        "topical focus",
        "retrieval trustworthiness"
      ]
    },
    "publishedAt": "2025-10-02T12:40:02.000Z",
    "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
    "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02190.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02315",
      "authors": [
        {
          "_id": "68df48ecdf49fb0df1e03d4c",
          "name": "Eric Tillmann Bill",
          "hidden": false
        },
        {
          "_id": "68df48ecdf49fb0df1e03d4d",
          "name": "Enis Simsar",
          "hidden": false
        },
        {
          "_id": "68df48ecdf49fb0df1e03d4e",
          "name": "Thomas Hofmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:59:58.000Z",
      "submittedOnDailyAt": "2025-10-03T02:25:08.105Z",
      "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
      "submittedOnDailyBy": {
        "_id": "63412f2add8853dc7e306a4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
        "isPro": false,
        "fullname": "Enis Simsar",
        "user": "enisimsar",
        "type": "user"
      },
      "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.",
      "upvotes": 2,
      "discussionId": "68df48eddf49fb0df1e03d4f",
      "githubRepo": "https://github.com/ericbill21/FOCUS/",
      "ai_summary": "A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.",
      "ai_keywords": [
        "flow matching",
        "stochastic optimal control",
        "subject disentanglement",
        "training-free test-time controller",
        "Adjoint Matching",
        "flow-diffusion correspondence",
        "fine-tuning",
        "multi-subject alignment",
        "Stable Diffusion",
        "FLUX",
        "Stable Diffusion XL",
        "FOCUS"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-02T13:59:58.000Z",
    "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
    "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63412f2add8853dc7e306a4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nqbteeQhS2dPA5LVy7dTD.png",
      "fullname": "Enis Simsar",
      "name": "enisimsar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01796",
      "authors": [
        {
          "_id": "68df2db5df49fb0df1e03c13",
          "name": "Meng-Hsi Chen",
          "hidden": false
        },
        {
          "_id": "68df2db5df49fb0df1e03c14",
          "name": "Yu-Ang Lee",
          "hidden": false
        },
        {
          "_id": "68df2db5df49fb0df1e03c15",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "68df2db5df49fb0df1e03c16",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T08:38:15.000Z",
      "submittedOnDailyAt": "2025-10-03T00:28:44.666Z",
      "title": "Rethinking the shape convention of an MLP",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
      "upvotes": 2,
      "discussionId": "68df2db5df49fb0df1e03c17",
      "ai_summary": "Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.",
      "ai_keywords": [
        "multi-layer perceptrons",
        "MLPs",
        "wide-narrow-wide",
        "Hourglass",
        "skip connections",
        "residual computation",
        "parameter-matched designs",
        "random initialization",
        "generative tasks",
        "Pareto frontiers",
        "architectural search",
        "Transformers",
        "residual networks"
      ],
      "organization": {
        "_id": "6388e08c5a3d2a335624705b",
        "name": "MediaTek-Research",
        "fullname": "MediaTek Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
      }
    },
    "publishedAt": "2025-10-02T04:38:15.000Z",
    "title": "Rethinking the shape convention of an MLP",
    "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "6388e08c5a3d2a335624705b",
      "name": "MediaTek-Research",
      "fullname": "MediaTek Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00523",
      "authors": [
        {
          "_id": "68df2bccdf49fb0df1e03bed",
          "name": "Wei-Yao Wang",
          "hidden": false
        },
        {
          "_id": "68df2bccdf49fb0df1e03bee",
          "name": "Kazuya Tateishi",
          "hidden": false
        },
        {
          "_id": "68df2bccdf49fb0df1e03bef",
          "name": "Qiyu Wu",
          "hidden": false
        },
        {
          "_id": "68df2bccdf49fb0df1e03bf0",
          "name": "Shusuke Takahashi",
          "hidden": false
        },
        {
          "_id": "68df2bccdf49fb0df1e03bf1",
          "name": "Yuki Mitsufuji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T05:11:54.000Z",
      "submittedOnDailyAt": "2025-10-03T00:22:23.038Z",
      "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
      "submittedOnDailyBy": {
        "_id": "66fbb4eac5e1b393cf3266d8",
        "avatarUrl": "/avatars/a9cff3c9466849dc19cad0bd37fb0459.svg",
        "isPro": false,
        "fullname": "Wei-Yao Wang",
        "user": "SwyWang",
        "type": "user"
      },
      "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
      "upvotes": 2,
      "discussionId": "68df2bcddf49fb0df1e03bf2",
      "ai_summary": "VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.",
      "ai_keywords": [
        "multimodal representation learning",
        "vision-language models",
        "visual-interactive capabilities",
        "segmentation model",
        "visual prompts",
        "entity-level information",
        "global representations",
        "Segmentation-and-Scene Caption Retrieval",
        "MMEB",
        "SCaR"
      ]
    },
    "publishedAt": "2025-10-01T01:11:54.000Z",
    "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
    "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fbb4eac5e1b393cf3266d8",
      "avatarUrl": "/avatars/a9cff3c9466849dc19cad0bd37fb0459.svg",
      "fullname": "Wei-Yao Wang",
      "name": "SwyWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24203",
      "authors": [
        {
          "_id": "68df35c1df49fb0df1e03cb3",
          "name": "Chaorui Yao",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb4",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb5",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb6",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb7",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb8",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cb9",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "68df35c1df49fb0df1e03cba",
          "name": "Bolin Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T02:34:54.000Z",
      "submittedOnDailyAt": "2025-10-03T01:08:33.784Z",
      "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
      "upvotes": 2,
      "discussionId": "68df35c2df49fb0df1e03cbb",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k",
      "ai_summary": "Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.",
      "ai_keywords": [
        "off-policy reinforcement learning",
        "large language models",
        "REINFORCE",
        "Group Relative Policy Optimization",
        "GRPO",
        "Online Policy Mirror Descent",
        "OPMD",
        "Asymmetric REINFORCE",
        "AsymRE",
        "importance sampling",
        "clipping",
        "data-weighting strategies"
      ],
      "githubStars": 357
    },
    "publishedAt": "2025-09-28T22:34:54.000Z",
    "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
    "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01241",
      "authors": [
        {
          "_id": "68df2f6edf49fb0df1e03c26",
          "name": "Hu Wei",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c27",
          "name": "Ze Xu",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c28",
          "name": "Boyu Yang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c29",
          "name": "Linlin Miao",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2a",
          "name": "Weiqi Zhai",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2b",
          "name": "Yihan Li",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2c",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2d",
          "name": "Zhijun Wang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2e",
          "name": "Boya Wang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c2f",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c30",
          "name": "Jialing Yuan",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c31",
          "name": "Xiaoyue Zhang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c32",
          "name": "Cheng He",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c33",
          "name": "Minglei Chen",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c34",
          "name": "Zifan Zhang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c35",
          "name": "Qianhui Li",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c36",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "68df2f6edf49fb0df1e03c37",
          "name": "Xiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T02:09:32.000Z",
      "submittedOnDailyAt": "2025-10-03T00:36:10.446Z",
      "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) now perform strongly on many public math suites,\nyet frontier separation within mathematics increasingly suffers from ceiling\neffects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a\n100-item, structure-aware diagnostic set with per-item metadata on length,\nnumeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item\ncontest-style suite spanning four stages from high school to doctoral under a\nseven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a\nsingle setup and analyze subject x model and grade x model performance. On the\ncontest suite, the strongest model reaches 44% while the runner-up reaches 37%;\naccuracy declines from high school to doctoral, and top systems exhibit a\ndoctoral-to-high-school retention near 79%. On the reasoning set, the best\nmodel attains 81% overall, and hardest-slice results reveal clear robustness\ngaps between leaders and the mid-tier. In summary, we release\nSKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;\ntogether, SKYLENAGE provides a hard, reasoning-centered and broadly covering\nmath benchmark with calibrated difficulty and rich metadata, serving as a\nreference benchmark for future evaluations of mathematical reasoning.",
      "upvotes": 2,
      "discussionId": "68df2f6edf49fb0df1e03c38",
      "ai_summary": "SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "SKYLENAGE-ReasoningMATH",
        "SKYLENAGE-MATH",
        "structure-aware diagnostic set",
        "numeric density",
        "symbolic complexity",
        "contest-style suite",
        "subject x model performance",
        "grade x model performance",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-09-23T22:09:32.000Z",
    "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
    "summary": "Large language models (LLMs) now perform strongly on many public math suites,\nyet frontier separation within mathematics increasingly suffers from ceiling\neffects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a\n100-item, structure-aware diagnostic set with per-item metadata on length,\nnumeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item\ncontest-style suite spanning four stages from high school to doctoral under a\nseven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a\nsingle setup and analyze subject x model and grade x model performance. On the\ncontest suite, the strongest model reaches 44% while the runner-up reaches 37%;\naccuracy declines from high school to doctoral, and top systems exhibit a\ndoctoral-to-high-school retention near 79%. On the reasoning set, the best\nmodel attains 81% overall, and hardest-slice results reveal clear robustness\ngaps between leaders and the mid-tier. In summary, we release\nSKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;\ntogether, SKYLENAGE provides a hard, reasoning-centered and broadly covering\nmath benchmark with calibrated difficulty and rich metadata, serving as a\nreference benchmark for future evaluations of mathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01241.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02272",
      "authors": [
        {
          "_id": "68df3a3bdf49fb0df1e03cbd",
          "name": "Wen Yang",
          "hidden": false
        },
        {
          "_id": "68df3a3bdf49fb0df1e03cbe",
          "name": "Junhong Wu",
          "hidden": false
        },
        {
          "_id": "68df3a3bdf49fb0df1e03cbf",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "68df3a3bdf49fb0df1e03cc0",
          "name": "Chengqing Zong",
          "hidden": false
        },
        {
          "_id": "68df3a3bdf49fb0df1e03cc1",
          "name": "Jiajun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:49:49.000Z",
      "submittedOnDailyAt": "2025-10-03T01:27:58.747Z",
      "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
      "submittedOnDailyBy": {
        "_id": "641fd72a73cfc036ddbf69c8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641fd72a73cfc036ddbf69c8/W4HG5HRm-OkzVpOotgu3m.jpeg",
        "isPro": false,
        "fullname": "WenYang",
        "user": "James-WYang",
        "type": "user"
      },
      "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\nDoes the reasoning capability achieved from English RPT effectively\ntransfer to other languages? We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: First-Parallel Leap, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable Parallel Scaling Law, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\nMonolingual Generalization Gap, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
      "upvotes": 1,
      "discussionId": "68df3a3bdf49fb0df1e03cc2",
      "ai_summary": "Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.",
      "ai_keywords": [
        "Reinforcement Post-Training",
        "Large Reasoning Models",
        "cross-linguistic perspective",
        "multilingual reasoning benchmarks",
        "cross-lingual transferability",
        "interventional studies",
        "parallel training",
        "Parallel Leap",
        "Parallel Scaling Law",
        "Monolingual Generalization Gap"
      ],
      "organization": {
        "_id": "640a887796aae649741a586f",
        "name": "CASIA",
        "fullname": "Chinese Academic of Science Institute of Automation",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
      }
    },
    "publishedAt": "2025-10-02T13:49:49.000Z",
    "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
    "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\nDoes the reasoning capability achieved from English RPT effectively\ntransfer to other languages? We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: First-Parallel Leap, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable Parallel Scaling Law, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\nMonolingual Generalization Gap, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02272.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641fd72a73cfc036ddbf69c8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641fd72a73cfc036ddbf69c8/W4HG5HRm-OkzVpOotgu3m.jpeg",
      "fullname": "WenYang",
      "name": "James-WYang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "640a887796aae649741a586f",
      "name": "CASIA",
      "fullname": "Chinese Academic of Science Institute of Automation",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02245",
      "authors": [
        {
          "_id": "68df3b51df49fb0df1e03cd2",
          "name": "Runzhe Zhan",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd3",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd4",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd5",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd6",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd7",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd8",
          "name": "Derek F. Wong",
          "hidden": false
        },
        {
          "_id": "68df3b51df49fb0df1e03cd9",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T17:31:30.000Z",
      "submittedOnDailyAt": "2025-10-03T01:31:39.660Z",
      "title": "ExGRPO: Learning to Reason from Experience",
      "submittedOnDailyBy": {
        "_id": "62495cb96ee7ee6b646db130",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
        "isPro": false,
        "fullname": "Runzhe Zhan",
        "user": "rzzhan",
        "type": "user"
      },
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "upvotes": 1,
      "discussionId": "68df3b52df49fb0df1e03cda",
      "ai_summary": "ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.",
      "ai_keywords": [
        "reinforcement learning from verifiable rewards",
        "RLVR",
        "on-policy training",
        "rollout experiences",
        "experience characteristics",
        "rollout correctness",
        "entropy",
        "ExGRPO",
        "Experiential Group Relative Policy Optimization",
        "mixed-policy objective",
        "exploration",
        "experience exploitation",
        "reasoning performance",
        "mathematical benchmarks",
        "general benchmarks"
      ]
    },
    "publishedAt": "2025-10-02T13:31:30.000Z",
    "title": "ExGRPO: Learning to Reason from Experience",
    "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62495cb96ee7ee6b646db130",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg",
      "fullname": "Runzhe Zhan",
      "name": "rzzhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01691",
      "authors": [
        {
          "_id": "68df2d8edf49fb0df1e03bfd",
          "name": "Jiyao Liu",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03bfe",
          "name": "Jinjie Wei",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03bff",
          "name": "Wanying Qu",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c00",
          "name": "Chenglong Ma",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c01",
          "name": "Junzhi Ning",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c02",
          "name": "Yunheng Li",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c03",
          "name": "Ying Chen",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c04",
          "name": "Xinzhe Luo",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c05",
          "name": "Pengcheng Chen",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c06",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c07",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c08",
          "name": "Huihui Xu",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c09",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0a",
          "name": "Shujian Gao",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0b",
          "name": "Dingkang Yang",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0c",
          "name": "Zhongying Deng",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0d",
          "name": "Jin Ye",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0e",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c0f",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "68df2d8edf49fb0df1e03c10",
          "name": "Ningsheng Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T05:42:00.000Z",
      "submittedOnDailyAt": "2025-10-03T00:27:45.114Z",
      "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
      "upvotes": 1,
      "discussionId": "68df2d8edf49fb0df1e03c11",
      "ai_summary": "MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.",
      "ai_keywords": [
        "Multi-modal Large Language Models",
        "MedQ-Bench",
        "MedQ-Perception",
        "MedQ-Reasoning",
        "perception-reasoning paradigm",
        "human-curated questions",
        "visual attributes",
        "no-reference reasoning",
        "comparison reasoning",
        "multi-dimensional judging protocol",
        "human-AI alignment validation"
      ]
    },
    "publishedAt": "2025-10-02T01:42:00.000Z",
    "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
    "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01691.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01670",
      "authors": [
        {
          "_id": "68df30a8df49fb0df1e03c43",
          "name": "Erfan Shayegani",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c44",
          "name": "Keegan Hines",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c45",
          "name": "Yue Dong",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c46",
          "name": "Nael Abu-Ghazaleh",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c47",
          "name": "Roman Lutz",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c48",
          "name": "Spencer Whitehead",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c49",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c4a",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "68df30a8df49fb0df1e03c4b",
          "name": "Vibhav Vineet",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T04:52:15.000Z",
      "submittedOnDailyAt": "2025-10-03T00:41:00.967Z",
      "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
      "upvotes": 1,
      "discussionId": "68df30a9df49fb0df1e03c4c",
      "ai_summary": "Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.",
      "ai_keywords": [
        "Blind Goal-Directedness",
        "contextual reasoning",
        "ambiguity",
        "contradictory goals",
        "benchmark",
        "OSWorld",
        "LLM-based judges",
        "execution-first bias",
        "thought-action disconnect",
        "request-primacy"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-02T00:52:15.000Z",
    "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
    "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01670.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 117
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01623",
      "authors": [
        {
          "_id": "68df33ebdf49fb0df1e03c92",
          "name": "Angen Ye",
          "hidden": false
        },
        {
          "_id": "68df33ebdf49fb0df1e03c93",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "68df33ebdf49fb0df1e03c94",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "68df33ebdf49fb0df1e03c95",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "68df33ebdf49fb0df1e03c96",
          "name": "Dapeng Zhang",
          "hidden": false
        },
        {
          "_id": "68df33ebdf49fb0df1e03c97",
          "name": "Zheng Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T02:54:03.000Z",
      "submittedOnDailyAt": "2025-10-03T00:55:59.842Z",
      "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": true,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
      "upvotes": 1,
      "discussionId": "68df33ebdf49fb0df1e03c98",
      "projectPage": "https://gigaai-research.github.io/VLA-R1",
      "githubRepo": "https://github.com/GigaAI-research/VLA-R1",
      "ai_summary": "VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.",
      "ai_keywords": [
        "VLA models",
        "Reinforcement Learning from Verifiable Rewards (RLVR)",
        "Group Relative Policy Optimization (GRPO)",
        "region alignment",
        "trajectory consistency",
        "output formatting",
        "chain-of-thought supervision",
        "affordance",
        "trajectory annotations"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-10-01T22:54:03.000Z",
    "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01623.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01538",
      "authors": [
        {
          "_id": "68df42f7df49fb0df1e03d25",
          "name": "Haokun Zhao",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d26",
          "name": "Xiang Zhang",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d27",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d28",
          "name": "Yiwei Xu",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d29",
          "name": "Yuting He",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d2a",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "68df42f7df49fb0df1e03d2b",
          "name": "Chenyu You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T00:18:59.000Z",
      "submittedOnDailyAt": "2025-10-03T01:59:16.170Z",
      "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.",
      "upvotes": 1,
      "discussionId": "68df42f7df49fb0df1e03d2c",
      "ai_summary": "TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.",
      "ai_keywords": [
        "LLM-driven",
        "agentic framework",
        "LLM-guided diagnostics",
        "multi-modal diagnostics",
        "self-planning",
        "model fitting",
        "validation",
        "ensemble strategy",
        "white-box system",
        "interpretable",
        "extensible"
      ]
    },
    "publishedAt": "2025-10-01T20:18:59.000Z",
    "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
    "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01143",
      "authors": [
        {
          "_id": "68df7fbadf49fb0df1e03e05",
          "name": "Harry Dong",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e06",
          "name": "David Brandfonbrener",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e07",
          "name": "Eryk Helenowski",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e08",
          "name": "Yun He",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e09",
          "name": "Mrinal Kumar",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e0a",
          "name": "Han Fang",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e0b",
          "name": "Yuejie Chi",
          "hidden": false
        },
        {
          "_id": "68df7fbadf49fb0df1e03e0c",
          "name": "Karthik Abinav Sankararaman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T17:33:35.000Z",
      "submittedOnDailyAt": "2025-10-03T06:18:45.587Z",
      "title": "Generalized Parallel Scaling with Interdependent Generations",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "Parallel LLM inference scaling involves sampling a set of N>1 responses for\na single input prompt. However, these N parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
      "upvotes": 1,
      "discussionId": "68df7fbadf49fb0df1e03e0d",
      "ai_summary": "Bridge enhances parallel LLM inference by generating interdependent responses, improving accuracy and consistency with minimal additional parameters.",
      "ai_keywords": [
        "LLM",
        "parallel inference",
        "response length scaling",
        "batched LLM hidden states",
        "holistic tensors",
        "reinforcement learning",
        "verifiable rewards",
        "generation width",
        "post-generation aggregation"
      ]
    },
    "publishedAt": "2025-10-01T13:33:35.000Z",
    "title": "Generalized Parallel Scaling with Interdependent Generations",
    "summary": "Parallel LLM inference scaling involves sampling a set of N>1 responses for\na single input prompt. However, these N parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24304",
      "authors": [
        {
          "_id": "68dc886d4159d1f2418f9a2e",
          "name": "Zefeng He",
          "hidden": false
        },
        {
          "_id": "68dc886d4159d1f2418f9a2f",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "68dc886d4159d1f2418f9a30",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "68dc886d4159d1f2418f9a31",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "68dc886d4159d1f2418f9a32",
          "name": "Daizong Liu",
          "hidden": false
        },
        {
          "_id": "68dc886d4159d1f2418f9a33",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T05:36:58.000Z",
      "submittedOnDailyAt": "2025-10-03T00:43:57.534Z",
      "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress\nin video understanding, their application to long video reasoning is hindered\nby uniform frame sampling and static textual reasoning, which are inefficient\nand struggle to handle visually intensive video tasks. To overcome these\nchallenges, in this paper, we introduce the concept of thinking with long\nvideos and propose a novel framework FrameThinker. Within this framework, LVLMs\nare able to iteratively interrogate video content. Developing such video\nreasoning capabilities in LVLMs presents notable challenges, particularly in\nadapting the model to new video actions (e.g. select frame), and designing\nreward functions to guide LVLMs to adopt the newly introduced action. To solve\nthese challenges, we propose a two-phase training strategy, first employing\nSupervised Fine-Tuning (SFT) to instill fundamental action capabilities,\nfollowed by Reinforcement Learning (RL) to optimize a strategic decision-making\npolicy. Notably, in this RL phase, we conduct an in-depth and comprehensive\nexploration of the reward design for each action and format reward. Extensive\nexperiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and\nlong-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and\nLVBench, demonstrate that FrameThinker achieves a significant average\nimprovement of +10.4% over baselines while drastically reducing the number of\nprocessed frames. Most notably, our 7B model, FrameThinker establishes a new\nstate-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average\nof only 20.6 frames. This not only outperforms the competitive LongVILA-R1\n(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating\nunparalleled efficiency and effectiveness.",
      "upvotes": 1,
      "discussionId": "68dc886d4159d1f2418f9a34",
      "projectPage": "https://github.com/lcqysl/FrameThinker-RL",
      "githubRepo": "https://github.com/lcqysl/FrameThinker-RL",
      "ai_summary": "FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "video understanding",
        "long video reasoning",
        "frame sampling",
        "textual reasoning",
        "FrameThinker",
        "iterative interrogation",
        "video reasoning capabilities",
        "Supervised Fine-Tuning",
        "SFT",
        "Reinforcement Learning",
        "RL",
        "reward functions",
        "Video-Holmes",
        "LongVideo-Reason",
        "LongVideoBench",
        "MLVU",
        "VideoMME",
        "LVBench",
        "LongVILA-R1"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-29T01:36:58.000Z",
    "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
    "summary": "While Large Vision-Language Models (LVLMs) have achieved substantial progress\nin video understanding, their application to long video reasoning is hindered\nby uniform frame sampling and static textual reasoning, which are inefficient\nand struggle to handle visually intensive video tasks. To overcome these\nchallenges, in this paper, we introduce the concept of thinking with long\nvideos and propose a novel framework FrameThinker. Within this framework, LVLMs\nare able to iteratively interrogate video content. Developing such video\nreasoning capabilities in LVLMs presents notable challenges, particularly in\nadapting the model to new video actions (e.g. select frame), and designing\nreward functions to guide LVLMs to adopt the newly introduced action. To solve\nthese challenges, we propose a two-phase training strategy, first employing\nSupervised Fine-Tuning (SFT) to instill fundamental action capabilities,\nfollowed by Reinforcement Learning (RL) to optimize a strategic decision-making\npolicy. Notably, in this RL phase, we conduct an in-depth and comprehensive\nexploration of the reward design for each action and format reward. Extensive\nexperiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and\nlong-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and\nLVBench, demonstrate that FrameThinker achieves a significant average\nimprovement of +10.4% over baselines while drastically reducing the number of\nprocessed frames. Most notably, our 7B model, FrameThinker establishes a new\nstate-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average\nof only 20.6 frames. This not only outperforms the competitive LongVILA-R1\n(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating\nunparalleled efficiency and effectiveness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24304.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00537",
      "authors": [
        {
          "_id": "68df3f9edf49fb0df1e03d1c",
          "name": "Nandan Kumar Jha",
          "hidden": false
        },
        {
          "_id": "68df3f9edf49fb0df1e03d1d",
          "name": "Brandon Reagen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T05:38:21.000Z",
      "submittedOnDailyAt": "2025-10-03T02:04:12.607Z",
      "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
      "submittedOnDailyBy": {
        "_id": "670ec3f6db1a6bcfe832e0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
        "isPro": false,
        "fullname": "Nandan Kumar Jha",
        "user": "nandan523",
        "type": "user"
      },
      "summary": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.",
      "upvotes": 0,
      "discussionId": "68df3f9edf49fb0df1e03d1e",
      "ai_summary": "Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.",
      "ai_keywords": [
        "feed-forward networks",
        "FFNs",
        "Hard Rank",
        "Soft Rank",
        "Spectral Concentration",
        "Spectral Utilization Index",
        "SUI",
        "latent space",
        "spectral scaling law",
        "dominant-mode subspaces",
        "tail capacity",
        "dominant-mode capacity",
        "inference-efficient LLM design"
      ],
      "organization": {
        "_id": "63f68bebb29015adc33fb06b",
        "name": "nyuniversity",
        "fullname": "New York University"
      }
    },
    "publishedAt": "2025-10-01T01:38:21.000Z",
    "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
    "summary": "As large language models (LLMs) scale, the question is not only how large\nthey become, but how much of their capacity is effectively utilized. Existing\nscaling laws relate model size to loss, yet overlook how components exploit\ntheir latent space. We study feed-forward networks (FFNs) and recast width\nselection as a spectral utilization problem. Using a lightweight diagnostic\nsuite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral\nConcentration, and the composite Spectral Utilization Index (SUI) -- we\nquantify how many latent directions are meaningfully activated across LLaMA,\nGPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling\nlaw: soft rank follows an almost perfect power law with FFN width, while hard\nrank grows only sublinearly and with high variance. This asymmetry suggests\nthat widening FFNs mostly adds low-energy tail directions, while dominant-mode\nsubspaces saturate early. Moreover, at larger widths, variance further\ncollapses into a narrow subspace, leaving much of the latent space\nunder-utilized. These results recast FFN width selection as a principled\ntrade-off between tail capacity and dominant-mode capacity, offering concrete\nguidance for inference-efficient LLM design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00537.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670ec3f6db1a6bcfe832e0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-mZNLeLJoXzkwPgYO38lF.png",
      "fullname": "Nandan Kumar Jha",
      "name": "nandan523",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "63f68bebb29015adc33fb06b",
      "name": "nyuniversity",
      "fullname": "New York University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00352",
      "authors": [
        {
          "_id": "68df2768df49fb0df1e03bb2",
          "name": "Tong Chen",
          "hidden": false
        },
        {
          "_id": "68df2768df49fb0df1e03bb3",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "68df2768df49fb0df1e03bb4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T23:33:33.000Z",
      "submittedOnDailyAt": "2025-10-03T00:01:57.991Z",
      "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Designing sequences that satisfy multiple, often conflicting, objectives is a\ncentral challenge in therapeutic and biomolecular engineering. Existing\ngenerative frameworks largely operate in continuous spaces with\nsingle-objective guidance, while discrete approaches lack guarantees for\nmulti-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified\nUpdates for Refining Discrete Flows), a discrete optimization algorithm with\ntheoretical guarantees of convergence to the Pareto front. Building on\nRectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization,\nlocally balanced proposals, and annealed Metropolis-Hastings updates to bias\nsampling toward Pareto-optimal states while preserving distributional\ninvariance. Applied to peptide and SMILES sequence design, AReUReDi\nsimultaneously optimizes up to five therapeutic properties (including affinity,\nsolubility, hemolysis, half-life, and non-fouling) and outperforms both\nevolutionary and diffusion-based baselines. These results establish AReUReDi as\na powerful, sequence-based framework for multi-property biomolecule generation.",
      "upvotes": 0,
      "discussionId": "68df2768df49fb0df1e03bb5",
      "ai_summary": "AReUReDi, a discrete optimization algorithm, achieves Pareto optimality in multi-objective biomolecule sequence design, outperforming evolutionary and diffusion-based methods.",
      "ai_keywords": [
        "Rectified Discrete Flows",
        "Tchebycheff scalarization",
        "locally balanced proposals",
        "annealed Metropolis-Hastings updates",
        "Pareto front",
        "peptide design",
        "SMILES sequence design",
        "affinity",
        "solubility",
        "hemolysis",
        "half-life",
        "non-fouling"
      ]
    },
    "publishedAt": "2025-09-30T19:33:33.000Z",
    "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance",
    "summary": "Designing sequences that satisfy multiple, often conflicting, objectives is a\ncentral challenge in therapeutic and biomolecular engineering. Existing\ngenerative frameworks largely operate in continuous spaces with\nsingle-objective guidance, while discrete approaches lack guarantees for\nmulti-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified\nUpdates for Refining Discrete Flows), a discrete optimization algorithm with\ntheoretical guarantees of convergence to the Pareto front. Building on\nRectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization,\nlocally balanced proposals, and annealed Metropolis-Hastings updates to bias\nsampling toward Pareto-optimal states while preserving distributional\ninvariance. Applied to peptide and SMILES sequence design, AReUReDi\nsimultaneously optimizes up to five therapeutic properties (including affinity,\nsolubility, hemolysis, half-life, and non-fouling) and outperforms both\nevolutionary and diffusion-based baselines. These results establish AReUReDi as\na powerful, sequence-based framework for multi-property biomolecule generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26330",
      "authors": [
        {
          "_id": "68dc8d234159d1f2418f9a90",
          "name": "Ren-Di Wu",
          "hidden": false
        },
        {
          "_id": "68dc8d234159d1f2418f9a91",
          "name": "Yu-Yen Lin",
          "hidden": false
        },
        {
          "_id": "68dc8d234159d1f2418f9a92",
          "name": "Huei-Fang Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/664ddfff6beee47c792f5836/dvSqb_9b1DAqROBFgsUf5.png"
      ],
      "publishedAt": "2025-09-30T14:41:24.000Z",
      "submittedOnDailyAt": "2025-10-03T05:32:50.141Z",
      "title": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking\n  for Training-free Zero-Shot Composed Image Retrieval",
      "submittedOnDailyBy": {
        "_id": "664ddfff6beee47c792f5836",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ddfff6beee47c792f5836/Wd0LIMCiHc9fq5iTslYgb.png",
        "isPro": false,
        "fullname": "Ren-Di Wu",
        "user": "whats2000",
        "type": "user"
      },
      "summary": "Composed Image Retrieval (CIR) aims to retrieve target images that preserve\nthe visual content of a reference image while incorporating user-specified\ntextual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which\nrequire no task-specific training or labeled data, are highly desirable, yet\naccurately capturing user intent remains challenging. In this paper, we present\nSQUARE, a novel two-stage training-free framework that leverages Multimodal\nLarge Language Models (MLLMs) to enhance ZS-CIR. In the Semantic\nQuery-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from\na vision-language model (VLM) such as CLIP with MLLM-generated captions of the\ntarget image. These captions provide high-level semantic guidance, enabling the\nquery to better capture the user's intent and improve global retrieval quality.\nIn the Efficient Batch Reranking (EBR) stage, top-ranked candidates are\npresented as an image grid with visual marks to the MLLM, which performs joint\nvisual-semantic reasoning across all candidates. Our reranking strategy\noperates in a single pass and yields more accurate rankings. Experiments show\nthat SQUARE, with its simplicity and effectiveness, delivers strong performance\non four standard CIR benchmarks. Notably, it maintains high performance even\nwith lightweight pre-trained, demonstrating its potential applicability.",
      "upvotes": 0,
      "discussionId": "68dc8d244159d1f2418f9a93",
      "ai_summary": "SQUARE is a two-stage training-free framework that uses Multimodal Large Language Models to enhance zero-shot Composed Image Retrieval by enriching query embeddings and performing efficient batch reranking.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "vision-language model",
        "VLM",
        "CLIP",
        "Semantic Query-Augmented Fusion",
        "SQAF",
        "Efficient Batch Reranking",
        "EBR",
        "joint visual-semantic reasoning",
        "Composed Image Retrieval",
        "CIR"
      ]
    },
    "publishedAt": "2025-09-30T10:41:24.000Z",
    "title": "SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking\n  for Training-free Zero-Shot Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) aims to retrieve target images that preserve\nthe visual content of a reference image while incorporating user-specified\ntextual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which\nrequire no task-specific training or labeled data, are highly desirable, yet\naccurately capturing user intent remains challenging. In this paper, we present\nSQUARE, a novel two-stage training-free framework that leverages Multimodal\nLarge Language Models (MLLMs) to enhance ZS-CIR. In the Semantic\nQuery-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from\na vision-language model (VLM) such as CLIP with MLLM-generated captions of the\ntarget image. These captions provide high-level semantic guidance, enabling the\nquery to better capture the user's intent and improve global retrieval quality.\nIn the Efficient Batch Reranking (EBR) stage, top-ranked candidates are\npresented as an image grid with visual marks to the MLLM, which performs joint\nvisual-semantic reasoning across all candidates. Our reranking strategy\noperates in a single pass and yields more accurate rankings. Experiments show\nthat SQUARE, with its simplicity and effectiveness, delivers strong performance\non four standard CIR benchmarks. Notably, it maintains high performance even\nwith lightweight pre-trained, demonstrating its potential applicability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/664ddfff6beee47c792f5836/dvSqb_9b1DAqROBFgsUf5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26330.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664ddfff6beee47c792f5836",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ddfff6beee47c792f5836/Wd0LIMCiHc9fq5iTslYgb.png",
      "fullname": "Ren-Di Wu",
      "name": "whats2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]