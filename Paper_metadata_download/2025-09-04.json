[
  {
    "paper": {
      "id": "2509.01106",
      "authors": [
        {
          "_id": "68b8f795d43cadaf7a688b04",
          "name": "Huang Fang",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b05",
          "name": "Mengxi Zhang",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b06",
          "name": "Heng Dong",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b07",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b08",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b09",
          "name": "Qifeng Zhang",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b0a",
          "name": "Xueyun Tian",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b0b",
          "name": "Yucheng Hu",
          "hidden": false
        },
        {
          "_id": "68b8f795d43cadaf7a688b0c",
          "name": "Hang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T03:53:47.000Z",
      "submittedOnDailyAt": "2025-09-04T00:57:07.946Z",
      "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
      "submittedOnDailyBy": {
        "_id": "63044e025c70c21d0eaf08bc",
        "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
        "isPro": false,
        "fullname": "Wei Li",
        "user": "Wiley085",
        "type": "user"
      },
      "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
      "upvotes": 24,
      "discussionId": "68b8f796d43cadaf7a688b0d",
      "projectPage": "https://robix-seed.github.io/robix/",
      "ai_summary": "Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "three-stage training strategy",
        "continued pretraining",
        "supervised finetuning",
        "reinforcement learning",
        "embodied reasoning",
        "3D spatial understanding",
        "visual grounding",
        "task-centric reasoning",
        "human-robot interaction",
        "task planning",
        "reasoning-action sequence",
        "reasoning-action consistency",
        "long-horizon task coherence"
      ]
    },
    "publishedAt": "2025-08-31T23:53:47.000Z",
    "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
    "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044e025c70c21d0eaf08bc",
      "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
      "fullname": "Wei Li",
      "name": "Wiley085",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00375",
      "authors": [
        {
          "_id": "68b902b5d43cadaf7a688b4c",
          "name": "Ziyi Xia",
          "hidden": false
        },
        {
          "_id": "68b902b5d43cadaf7a688b4d",
          "name": "Kun Luo",
          "hidden": false
        },
        {
          "_id": "68b902b5d43cadaf7a688b4e",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "68b902b5d43cadaf7a688b4f",
          "name": "Zheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-30T06:02:56.000Z",
      "submittedOnDailyAt": "2025-09-04T01:42:16.942Z",
      "title": "Open Data Synthesis For Deep Research",
      "submittedOnDailyBy": {
        "_id": "6540617c7cadb2d1b42007c5",
        "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
        "isPro": false,
        "fullname": "Ziyi Xia",
        "user": "ZiyiXia",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
      "upvotes": 18,
      "discussionId": "68b902b5d43cadaf7a688b50",
      "ai_summary": "InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.",
      "ai_keywords": [
        "Hierarchical Constraint Satisfaction Problems",
        "HCSPs",
        "dual-agent system",
        "Research Tree",
        "natural language questions",
        "reasoning trajectories",
        "reject sampling",
        "compound reward design",
        "trajectory-level exploration"
      ]
    },
    "publishedAt": "2025-08-30T02:02:56.000Z",
    "title": "Open Data Synthesis For Deep Research",
    "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540617c7cadb2d1b42007c5",
      "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
      "fullname": "Ziyi Xia",
      "name": "ZiyiXia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01977",
      "authors": [
        {
          "_id": "68b8f158d43cadaf7a688afb",
          "name": "Dong She",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688afc",
          "name": "Siming Fu",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688afd",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688afe",
          "name": "Qiaoqiao Jin",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688aff",
          "name": "Hualiang Wang",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688b00",
          "name": "Mu Liu",
          "hidden": false
        },
        {
          "_id": "68b8f158d43cadaf7a688b01",
          "name": "Jidong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T05:40:07.000Z",
      "submittedOnDailyAt": "2025-09-04T05:05:42.538Z",
      "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
      "submittedOnDailyBy": {
        "_id": "6485dd6d07a2c1915060f603",
        "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
        "isPro": false,
        "fullname": "fu",
        "user": "simingfu",
        "type": "user"
      },
      "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
      "upvotes": 3,
      "discussionId": "68b8f158d43cadaf7a688b02",
      "projectPage": "https://bytedance-fanqie-ai.github.io/MOSAIC/",
      "githubRepo": "https://github.com/bytedance-fanqie-ai/MOSAIC",
      "ai_summary": "MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.",
      "ai_keywords": [
        "representation-centric framework",
        "semantic correspondence",
        "orthogonal feature disentanglement",
        "SemAlign-MS",
        "semantic correspondence attention loss",
        "multi-reference disentanglement loss"
      ],
      "githubStars": 272
    },
    "publishedAt": "2025-09-02T01:40:07.000Z",
    "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
    "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485dd6d07a2c1915060f603",
      "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
      "fullname": "fu",
      "name": "simingfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00428",
      "authors": [
        {
          "_id": "68b93229d43cadaf7a688bc6",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bc7",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bc8",
          "name": "Xing Fu",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bc9",
          "name": "Yue Li",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bca",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bcb",
          "name": "Yushe Cao",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bcc",
          "name": "Congyan Lang",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bcd",
          "name": "Pin Tao",
          "hidden": false
        },
        {
          "_id": "68b93229d43cadaf7a688bce",
          "name": "Junliang Xing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png"
      ],
      "publishedAt": "2025-08-30T09:21:07.000Z",
      "submittedOnDailyAt": "2025-09-04T05:17:48.471Z",
      "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
      "submittedOnDailyBy": {
        "_id": "6617af2beab5eef6b1e8bb9e",
        "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
        "isPro": false,
        "fullname": "XavierJiezou",
        "user": "XavierJiezou",
        "type": "user"
      },
      "summary": "Controllable face generation poses critical challenges in generative modeling\ndue to the intricate balance required between semantic controllability and\nphotorealism. While existing approaches struggle with disentangling semantic\ncontrols from generation pipelines, we revisit the architectural potential of\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\npaper introduces Face-MoGLE, a novel framework featuring: (1)\nSemantic-decoupled latent modeling through mask-conditioned space\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\nand local experts that captures holistic structure and region-level semantics\nfor fine-grained controllability; (3) A dynamic gating network producing\ntime-dependent coefficients that evolve with diffusion steps and spatial\nlocations. Face-MoGLE provides a powerful and flexible solution for\nhigh-quality, controllable face generation, with strong potential in generative\nmodeling and security applications. Extensive experiments demonstrate its\neffectiveness in multimodal and monomodal face generation settings and its\nrobust zero-shot generalization capability. Project page is available at\nhttps://github.com/XavierJiezou/Face-MoGLE.",
      "upvotes": 1,
      "discussionId": "68b93229d43cadaf7a688bcf",
      "projectPage": "https://xavierjiezou.github.io/Face-MoGLE/",
      "githubRepo": "https://github.com/XavierJiezou/Face-MoGLE",
      "ai_summary": "Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.",
      "ai_keywords": [
        "Diffusion Transformers",
        "Semantic-decoupled latent modeling",
        "mask-conditioned space factorization",
        "global experts",
        "local experts",
        "dynamic gating network",
        "multimodal face generation",
        "monomodal face generation",
        "zero-shot generalization"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-08-30T05:21:07.000Z",
    "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
    "summary": "Controllable face generation poses critical challenges in generative modeling\ndue to the intricate balance required between semantic controllability and\nphotorealism. While existing approaches struggle with disentangling semantic\ncontrols from generation pipelines, we revisit the architectural potential of\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\npaper introduces Face-MoGLE, a novel framework featuring: (1)\nSemantic-decoupled latent modeling through mask-conditioned space\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\nand local experts that captures holistic structure and region-level semantics\nfor fine-grained controllability; (3) A dynamic gating network producing\ntime-dependent coefficients that evolve with diffusion steps and spatial\nlocations. Face-MoGLE provides a powerful and flexible solution for\nhigh-quality, controllable face generation, with strong potential in generative\nmodeling and security applications. Extensive experiments demonstrate its\neffectiveness in multimodal and monomodal face generation settings and its\nrobust zero-shot generalization capability. Project page is available at\nhttps://github.com/XavierJiezou/Face-MoGLE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6617af2beab5eef6b1e8bb9e",
      "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
      "fullname": "XavierJiezou",
      "name": "XavierJiezou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]