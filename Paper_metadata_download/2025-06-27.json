[
  {
    "paper": {
      "id": "2506.20670",
      "authors": [
        {
          "_id": "685c9ef4696820ba1f28f263",
          "user": {
            "_id": "652fbe8cb2acab0b82f855a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
            "isPro": false,
            "fullname": "Jinming Wu",
            "user": "kimingng",
            "type": "user"
          },
          "name": "Jinming Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:24:03.068Z",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f264",
          "name": "Zihao Deng",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f265",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f266",
          "name": "Yiding Liu",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f267",
          "name": "Bo You",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f268",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f269",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "685c9ef4696820ba1f28f26a",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T17:59:42.000Z",
      "submittedOnDailyAt": "2025-06-27T00:45:57.876Z",
      "title": "MMSearch-R1: Incentivizing LMMs to Search",
      "submittedOnDailyBy": {
        "_id": "652fbe8cb2acab0b82f855a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
        "isPro": false,
        "fullname": "Jinming Wu",
        "user": "kimingng",
        "type": "user"
      },
      "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
      "upvotes": 28,
      "discussionId": "685c9ef5696820ba1f28f26b",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/multimodal-search-r1",
      "ai_summary": "MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.",
      "ai_keywords": [
        "multimodal models",
        "retrieval-augmented generation",
        "prompt engineered search agents",
        "reinforcement learning",
        "image search",
        "text search",
        "outcome-based reward",
        "search penalty",
        "multimodal search VQA dataset",
        "knowledge-intensive VQA tasks",
        "info-seeking VQA tasks"
      ],
      "githubStars": 140
    },
    "publishedAt": "2025-06-25T13:59:42.000Z",
    "title": "MMSearch-R1: Incentivizing LMMs to Search",
    "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20670.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652fbe8cb2acab0b82f855a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
      "fullname": "Jinming Wu",
      "name": "kimingng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21539",
      "authors": [
        {
          "_id": "685e06f771131fa43be08abe",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08abf",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac0",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac1",
          "name": "Yuming Jiang",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac2",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac3",
          "name": "Jiayan Guo",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac4",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac5",
          "name": "Yibing Song",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac6",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac7",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac8",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "685e06f771131fa43be08ac9",
          "name": "Hao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:55:40.000Z",
      "submittedOnDailyAt": "2025-06-27T01:21:09.686Z",
      "title": "WorldVLA: Towards Autoregressive Action World Model",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.",
      "upvotes": 11,
      "discussionId": "685e06f871131fa43be08aca",
      "projectPage": "https://github.com/alibaba-damo-academy/WorldVLA",
      "githubRepo": "https://github.com/alibaba-damo-academy/WorldVLA",
      "ai_summary": "WorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.",
      "ai_keywords": [
        "autoregressive action world model",
        "Vision-Language-Action (VLA) model",
        "world model",
        "action generation",
        "action prediction",
        "attention mask strategy"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-06-26T13:55:40.000Z",
    "title": "WorldVLA: Towards Autoregressive Action World Model",
    "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21551",
      "authors": [
        {
          "_id": "685e12a171131fa43be08af1",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "685e12a171131fa43be08af2",
          "name": "Chenrui Fan",
          "hidden": false
        },
        {
          "_id": "685e12a171131fa43be08af3",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/B_SAKrX_CUkbOy_G2Utsz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/GMugplIqsF3q7PFg2Ywia.png"
      ],
      "publishedAt": "2025-06-26T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-27T02:17:48.590Z",
      "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
      "upvotes": 9,
      "discussionId": "685e12a271131fa43be08af4",
      "ai_summary": "Grokking, or continued test performance improvement after training loss convergence, is observed during pretraining of a large language model, showcasing a memorization-to-generalization process.",
      "ai_keywords": [
        "grokking",
        "training loss",
        "generalization",
        "pretraining",
        "large language model",
        "OLMoE",
        "math reasoning",
        "code generation",
        "knowledge retrieval",
        "expert choices",
        "pathway distance",
        "pathway complexity",
        "generalization bound"
      ]
    },
    "publishedAt": "2025-06-26T13:59:58.000Z",
    "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
    "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/B_SAKrX_CUkbOy_G2Utsz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/GMugplIqsF3q7PFg2Ywia.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21551.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21506",
      "authors": [
        {
          "_id": "685df93d71131fa43be08a96",
          "name": "Boyu Gou",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a97",
          "name": "Zanming Huang",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a98",
          "name": "Yuting Ning",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a99",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9a",
          "name": "Michael Lin",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9b",
          "name": "Weijian Qi",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9c",
          "name": "Andrei Kopanev",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9d",
          "name": "Botao Yu",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9e",
          "name": "Bernal Jiménez Gutiérrez",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08a9f",
          "name": "Yiheng Shu",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa0",
          "name": "Chan Hee Song",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa1",
          "name": "Jiaman Wu",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa2",
          "name": "Shijie Chen",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa3",
          "name": "Hanane Nour Moussa",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa4",
          "name": "Tianshu Zhang",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa5",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa6",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa7",
          "name": "Tianci Xue",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa8",
          "name": "Zeyi Liao",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aa9",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aaa",
          "name": "Boyuan Zheng",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aab",
          "name": "Zhaowei Cai",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aac",
          "name": "Viktor Rozgic",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aad",
          "name": "Morteza Ziyadi",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aae",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "685df93d71131fa43be08aaf",
          "name": "Yu Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:32:50.000Z",
      "submittedOnDailyAt": "2025-06-27T00:23:59.896Z",
      "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
      "submittedOnDailyBy": {
        "_id": "6500870f1e14749e84f8f887",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
        "isPro": false,
        "fullname": "Boyu Gou",
        "user": "BoyuNLP",
        "type": "user"
      },
      "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.",
      "upvotes": 5,
      "discussionId": "685df93d71131fa43be08ab0",
      "projectPage": "https://osu-nlp-group.github.io/Mind2Web-2",
      "githubRepo": "https://github.com/OSU-NLP-Group/Mind2Web-2/",
      "ai_summary": "Mind2Web 2 benchmark evaluates agentic search systems with a suite of realistic, long-horizon tasks, introducing an Agent-as-a-Judge framework to assess accuracy and source attribution.",
      "ai_keywords": [
        "Deep Research systems",
        "large language models",
        "autonomous browsing",
        "information synthesis",
        "citation-backed answers",
        "evaluation benchmarks",
        "search horizons",
        "static answers",
        "Mind2Web 2",
        "high-quality tasks",
        "real-time web browsing",
        "extensive information synthesis",
        "task-specific judge agents",
        "tree-structured rubric design",
        "answer correctness",
        "source attribution",
        "agentic search systems",
        "human performance",
        "error analysis",
        "OpenAI Deep Research"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-26T13:32:50.000Z",
    "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
    "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21506.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6500870f1e14749e84f8f887",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
      "fullname": "Boyu Gou",
      "name": "BoyuNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21547",
      "authors": [
        {
          "_id": "685e004071131fa43be08ab2",
          "name": "Jianyun Xu",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab3",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab4",
          "name": "Ziqian Ni",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab5",
          "name": "Chunyong Hu",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab6",
          "name": "Sheng Yang",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab7",
          "name": "Jianke Zhu",
          "hidden": false
        },
        {
          "_id": "685e004071131fa43be08ab8",
          "name": "Qiang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-27T00:53:00.453Z",
      "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
      "submittedOnDailyBy": {
        "_id": "66863d26e2b71e3d09189ae9",
        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
        "isPro": false,
        "fullname": "Song Wang",
        "user": "songw-zju",
        "type": "user"
      },
      "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.",
      "upvotes": 4,
      "discussionId": "685e004071131fa43be08ab9",
      "projectPage": "https://SAM4D-Project.github.io",
      "githubRepo": "https://github.com/CN-ADLab/SAM4D",
      "ai_summary": "SAM4D is a multi-modal and temporal foundation model for segmentation in autonomous driving using Unified Multi-modal Positional Encoding and Motion-aware Cross-modal Memory Attention, with a multi-modal automated data engine generating pseudo-labels.",
      "ai_keywords": [
        "multi-modal",
        "temporal foundation model",
        "promptable segmentation",
        "camera",
        "LiDAR",
        "Unified Multi-modal Positional Encoding",
        "shared 3D space",
        "cross-modal prompting",
        "Motion-aware Cross-modal Memory Attention",
        "ego-motion compensation",
        "temporal consistency",
        "spatiotemporal 4D reconstruction",
        "cross-modal masklet fusion",
        "pseudo-labels",
        "Waymo-4DSeg"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-06-26T13:59:14.000Z",
    "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
    "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863d26e2b71e3d09189ae9",
      "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
      "fullname": "Song Wang",
      "name": "songw-zju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21552",
      "authors": [
        {
          "_id": "685e161b71131fa43be08b04",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "685e161b71131fa43be08b05",
          "name": "Danny Tran",
          "hidden": false
        },
        {
          "_id": "685e161b71131fa43be08b06",
          "name": "Amir Bar",
          "hidden": false
        },
        {
          "_id": "685e161b71131fa43be08b07",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "685e161b71131fa43be08b08",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "685e161b71131fa43be08b09",
          "name": "Jitendra Malik",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:59:59.000Z",
      "submittedOnDailyAt": "2025-06-27T03:04:29.837Z",
      "title": "Whole-Body Conditioned Egocentric Video Prediction",
      "submittedOnDailyBy": {
        "_id": "6332253749a95639154cc894",
        "avatarUrl": "/avatars/f8e53fff5a324591026114c431cd407e.svg",
        "isPro": false,
        "fullname": "Yutong Bai",
        "user": "Emma02",
        "type": "user"
      },
      "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.",
      "upvotes": 2,
      "discussionId": "685e161b71131fa43be08b0a",
      "ai_summary": "A model trained on real-world egocentric video and body pose predicts video from human actions using an auto-regressive conditional diffusion transformer, evaluated with a hierarchical protocol of tasks.",
      "ai_keywords": [
        "auto-regressive conditional diffusion transformer"
      ]
    },
    "publishedAt": "2025-06-26T13:59:59.000Z",
    "title": "Whole-Body Conditioned Egocentric Video Prediction",
    "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332253749a95639154cc894",
      "avatarUrl": "/avatars/f8e53fff5a324591026114c431cd407e.svg",
      "fullname": "Yutong Bai",
      "name": "Emma02",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16655",
      "authors": [
        {
          "_id": "6858de6bc0c8e29df8ea3d03",
          "name": "Co Tran",
          "hidden": false
        },
        {
          "_id": "6858de6bc0c8e29df8ea3d04",
          "user": {
            "_id": "66b681906c8d3b36786b764c",
            "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
            "isPro": true,
            "fullname": "Salman",
            "user": "parachas",
            "type": "user"
          },
          "name": "Salman Paracha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:14:15.659Z",
          "hidden": false
        },
        {
          "_id": "6858de6bc0c8e29df8ea3d05",
          "name": "Adil Hafeez",
          "hidden": false
        },
        {
          "_id": "6858de6bc0c8e29df8ea3d06",
          "user": {
            "_id": "622e9e56165ba2c1bcbc76da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648869498279-622e9e56165ba2c1bcbc76da.jpeg",
            "isPro": false,
            "fullname": "Shuguang Chen",
            "user": "nehcgs",
            "type": "user"
          },
          "name": "Shuguang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:09:48.654Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b681906c8d3b36786b764c/yAiHzS_Ymy5geNcfh2s1K.png"
      ],
      "publishedAt": "2025-06-19T23:57:41.000Z",
      "submittedOnDailyAt": "2025-06-27T03:00:21.170Z",
      "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
      "submittedOnDailyBy": {
        "_id": "66b681906c8d3b36786b764c",
        "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
        "isPro": true,
        "fullname": "Salman",
        "user": "parachas",
        "type": "user"
      },
      "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce Arch-Router, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: https://huggingface.co/katanemo/Arch-Router-1.5B.",
      "upvotes": 2,
      "discussionId": "6858de6bc0c8e29df8ea3d07",
      "githubRepo": "https://github.com/katanemo/archgw/",
      "ai_summary": "A preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.",
      "ai_keywords": [
        "large language models",
        "LLM routing",
        "Arch-Router",
        "domain-action preferences"
      ],
      "githubStars": 2765
    },
    "publishedAt": "2025-06-19T19:57:41.000Z",
    "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
    "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce Arch-Router, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: https://huggingface.co/katanemo/Arch-Router-1.5B.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b681906c8d3b36786b764c/yAiHzS_Ymy5geNcfh2s1K.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16655.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b681906c8d3b36786b764c",
      "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
      "fullname": "Salman",
      "name": "parachas",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20911",
      "authors": [
        {
          "_id": "685e151c71131fa43be08afe",
          "name": "Advait Gupta",
          "hidden": false
        },
        {
          "_id": "685e151c71131fa43be08aff",
          "name": "Rishie Raj",
          "hidden": false
        },
        {
          "_id": "685e151c71131fa43be08b00",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "685e151c71131fa43be08b01",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/NtOZn2DhSh9N1uAbfrj7k.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ZIpYLZ5fG1gO1XJd-jFkj.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wxV2SepqrDMNMwErA1pdC.png"
      ],
      "publishedAt": "2025-06-26T00:33:43.000Z",
      "submittedOnDailyAt": "2025-06-27T03:18:51.228Z",
      "title": "FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A^*\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A^* on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A^*\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A^*\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA^* is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.",
      "upvotes": 1,
      "discussionId": "685e151d71131fa43be08b02",
      "githubRepo": "https://github.com/tianyi-lab/FaSTAR",
      "ai_summary": "A neurosymbolic agent combines language models for fast subtask planning with A$^*$ search for detailed toolpaths, creating a cost-efficient multi-turn image editing solution.",
      "ai_keywords": [
        "neurosymbolic agent",
        "LLM",
        "A$^*$ search",
        "subtask planning",
        "toolpath",
        "inductive reasoning",
        "symbolic subroutines",
        "adaptive fast-slow planning"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-06-25T20:33:43.000Z",
    "title": "FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
    "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A^*\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A^* on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A^*\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A^*\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA^* is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/NtOZn2DhSh9N1uAbfrj7k.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ZIpYLZ5fG1gO1XJd-jFkj.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wxV2SepqrDMNMwErA1pdC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20911.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20430",
      "authors": [
        {
          "_id": "685e119b71131fa43be08adf",
          "name": "Weike Zhao",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae0",
          "name": "Chaoyi Wu",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae1",
          "name": "Yanjie Fan",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae2",
          "name": "Xiaoman Zhang",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae3",
          "name": "Pengcheng Qiu",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae4",
          "name": "Yuze Sun",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae5",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae6",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae7",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae8",
          "name": "Yongguo Yu",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08ae9",
          "name": "Kun Sun",
          "hidden": false
        },
        {
          "_id": "685e119b71131fa43be08aea",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64365addfae287005149dd24/fw_obfpXb7KTCzzHWtZIc.jpeg"
      ],
      "publishedAt": "2025-06-25T13:42:26.000Z",
      "submittedOnDailyAt": "2025-06-27T02:10:11.490Z",
      "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
      "submittedOnDailyBy": {
        "_id": "64365addfae287005149dd24",
        "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
        "isPro": false,
        "fullname": "Weike Zhao",
        "user": "Angelakeke",
        "type": "user"
      },
      "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
      "upvotes": 0,
      "discussionId": "685e119b71131fa43be08aeb",
      "ai_summary": "DeepRare, a large language model-based system, provides accurate rare disease diagnoses using heterogeneous clinical inputs and outperforms other diagnostic methods across various datasets.",
      "ai_keywords": [
        "large language model",
        "LLm",
        "diagnostic hypotheses",
        "chain of reasoning",
        "long-term memory module",
        "domain-specific analytical tasks",
        "medical knowledge sources",
        "HPO-based evaluations",
        "Recall@1 score",
        "multi-modal input scenarios",
        "web application"
      ]
    },
    "publishedAt": "2025-06-25T09:42:26.000Z",
    "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
    "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64365addfae287005149dd24/fw_obfpXb7KTCzzHWtZIc.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64365addfae287005149dd24",
      "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
      "fullname": "Weike Zhao",
      "name": "Angelakeke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]