[
  {
    "paper": {
      "id": "2504.17761",
      "authors": [
        {
          "_id": "680af2df3b93130c9b2b90a7",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a8",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a9",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90aa",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ab",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ac",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ad",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ae",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90af",
          "name": "Honghao Fu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b0",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b1",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b2",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b3",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b4",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b5",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b6",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b7",
          "name": "Ranchen Ming",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b8",
          "name": "Lei Xia",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b9",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ba",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bb",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bc",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bd",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90be",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
      ],
      "publishedAt": "2025-04-24T17:25:12.000Z",
      "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
      "title": "Step1X-Edit: A Practical Framework for General Image Editing",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
      "upvotes": 35,
      "discussionId": "680af2e13b93130c9b2b9132",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
      "ai_keywords": [
        "Multimodal LLM",
        "latent embedding",
        "diffusion image decoder",
        "data generation pipeline",
        "GEdit-Bench",
        "real-world user instructions"
      ]
    },
    "publishedAt": "2025-04-24T13:25:12.000Z",
    "title": "Step1X-Edit: A Practical Framework for General Image Editing",
    "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17192",
      "authors": [
        {
          "_id": "680aee7bcf67477f2c00ca53",
          "name": "Minju Seo",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca54",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca55",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca56",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T01:57:01.000Z",
      "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
      "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
      "upvotes": 26,
      "discussionId": "680aee7dcf67477f2c00ca96"
    },
    "publishedAt": "2025-04-23T21:57:01.000Z",
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
    "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17432",
      "authors": [
        {
          "_id": "680adfbe464a44cea0b843c1",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c2",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c4",
          "name": "Xingjun Wang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c5",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c6",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c7",
          "name": "Yingda Chen",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c8",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c9",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:51:52.000Z",
      "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
      "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
      "upvotes": 19,
      "discussionId": "680adfbf464a44cea0b8440f",
      "projectPage": "https://garygutc.github.io/UniME/",
      "githubRepo": "https://github.com/deepglint/UniME",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "Multimodal Large Language Models (MLLMs)",
        "Generalized vision-language understanding",
        "UniME (Universal Multimodal Embedding)",
        "Discriminative representations",
        "Textual discriminative knowledge distillation",
        "LLM-based teacher model",
        "Hard negative enhanced instruction tuning",
        "False negative contamination",
        "Challenging samples",
        "Discriminative power",
        "Instruction-following ability",
        "MMEB benchmark",
        "Short caption retrieval",
        "Long caption retrieval",
        "Compositional retrieval"
      ]
    },
    "publishedAt": "2025-04-24T06:51:52.000Z",
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
    "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17207",
      "authors": [
        {
          "_id": "680af2bf2fa10fbf21684bde",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684bdf",
          "name": "Jihyeon Je",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be0",
          "name": "Chanho Park",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be1",
          "name": "Mikaela Angelina Uy",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be2",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be3",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T02:41:34.000Z",
      "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
      "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
      "upvotes": 13,
      "discussionId": "680af2c02fa10fbf21684c1f",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "mental imagery simulation",
        "perspective-taking",
        "visual understanding",
        "environmental interaction",
        "autonomous agents",
        "spatial reasoning",
        "perspective-aware reasoning capabilities",
        "egocentric interpretations",
        "mental imagery",
        "scene abstractions",
        "perspective transformations",
        "object detection",
        "segmentation",
        "orientation estimation",
        "synthetic benchmarks",
        "real-image benchmarks",
        "fine-tuned spatial reasoning models",
        "novel-view-synthesis-based approaches"
      ]
    },
    "publishedAt": "2025-04-23T22:41:34.000Z",
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
    "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16511",
      "authors": [
        {
          "_id": "680b2a95c94724c1465c20dd",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20de",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20df",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e0",
          "name": "Zhimiao Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e1",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e2",
          "name": "Haobin Lin",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e3",
          "name": "Yifeng Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e5",
          "name": "Taifeng Wang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e6",
          "name": "Yong Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T08:36:50.000Z",
      "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
      "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
      "upvotes": 11,
      "discussionId": "680b2a97c94724c1465c21a3",
      "ai_keywords": [
        "large language models (LLMs)",
        "QuaDMix",
        "data selection framework",
        "parameterized data sampling function",
        "domain classification",
        "LightGBM",
        "RegMix"
      ]
    },
    "publishedAt": "2025-04-23T04:36:50.000Z",
    "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
    "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17069",
      "authors": [
        {
          "_id": "680b1b33388bb2cfd497ebdb",
          "name": "Rishav Pramanik",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdc",
          "name": "Antoine Poupon",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdd",
          "name": "Juan A. Rodriguez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebde",
          "name": "Masih Aminbeidokhti",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdf",
          "name": "David Vazquez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe0",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe1",
          "name": "Zhaozheng Yin",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe2",
          "name": "Marco Pedersoli",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
      ],
      "publishedAt": "2025-04-23T19:33:58.000Z",
      "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
      "title": "Distilling semantically aware orders for autoregressive image generation",
      "submittedOnDailyBy": {
        "_id": "63a614d264f470027818b066",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
        "isPro": false,
        "fullname": "Juan A. Rodriguez",
        "user": "joanrodai",
        "type": "user"
      },
      "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
      "upvotes": 2,
      "discussionId": "680b1b35388bb2cfd497ec76",
      "ai_keywords": [
        "autoregressive patch-based image generation",
        "Vision-Language models",
        "raster-scan order",
        "causality",
        "any-given-order",
        "patch content",
        "patch location",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-04-23T15:33:58.000Z",
    "title": "Distilling semantically aware orders for autoregressive image generation",
    "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a614d264f470027818b066",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
      "fullname": "Juan A. Rodriguez",
      "name": "joanrodai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17601",
      "authors": [
        {
          "_id": "680b2b8c6bd146aa35a48222",
          "user": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "isPro": false,
            "fullname": "Erik Bergh",
            "user": "erikbergh",
            "type": "user"
          },
          "name": "Erik Bergh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T14:26:42.000Z",
      "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
      "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
      "submittedOnDailyBy": {
        "_id": "64d496b04ab89be0de7fb1a9",
        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
        "isPro": false,
        "fullname": "Erik Bergh",
        "user": "erikbergh",
        "type": "user"
      },
      "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
      "upvotes": 1,
      "discussionId": "680b2b8d6bd146aa35a48252",
      "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "ai_keywords": [
        "t-SNE",
        "PCA",
        "non-linear mapping",
        "Gaussian functions",
        "linear transformations",
        "interpretability",
        "dimensionality reduction",
        "suppressed dimensions",
        "geometric relationships"
      ]
    },
    "publishedAt": "2025-04-24T10:26:42.000Z",
    "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
    "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d496b04ab89be0de7fb1a9",
      "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
      "fullname": "Erik Bergh",
      "name": "erikbergh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17789",
      "authors": [
        {
          "_id": "680b318bbbebf87944bc9595",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9596",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9597",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9598",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9599",
          "name": "Chih-Yao Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959a",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959b",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959c",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959d",
          "name": "Yujun Shi",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959e",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959f",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a0",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a1",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a2",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a3",
          "name": "Junjiao Tian",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a4",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a5",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a6",
          "name": "Yen-Cheng Liu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a7",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a8",
          "name": "Zijian He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a9",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95aa",
          "name": "Peizhao Zhang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ab",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ac",
          "name": "Sam Tsai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ad",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
      "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
      "upvotes": 0,
      "discussionId": "680b3191bbebf87944bc9739",
      "ai_keywords": [
        "autoregressive (AR) models",
        "image synthesis",
        "diffusion-based models",
        "image tokens",
        "training and inference efficiency",
        "Transformer",
        "dimensional redundancy",
        "visual vocabularies",
        "Multimodal Large Language Models (MLLMs)",
        "visual encoder",
        "high-dimensional language vocabularies",
        "token-shuffle",
        "spatially local tokens",
        "channel dimension",
        "token-unshuffle",
        "spatial arrangement",
        "unified next-token prediction",
        "text-to-image generation",
        "resolution",
        "generation performance",
        "GenAI-benchmark",
        "textual prompts",
        "pretrained text-encoder",
        "text-alignment",
        "visual flaw",
        "visual appearance"
      ]
    },
    "publishedAt": "2025-04-24T13:59:56.000Z",
    "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6712
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17040",
      "authors": [
        {
          "_id": "680af0c4175842e433ae348e",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae348f",
          "name": "Senthil Purushwalkam",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3490",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3491",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3492",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3493",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T18:38:18.000Z",
      "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
      "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
      "upvotes": 0,
      "discussionId": "680af0c7175842e433ae3544",
      "ai_keywords": [
        "Dynamic Token Merging (DToMe)",
        "Virtual Token Unmerging (VTU)",
        "vision transformers",
        "token compression",
        "attention dynamics",
        "visual encoders",
        "image complexity",
        "computational costs"
      ]
    },
    "publishedAt": "2025-04-23T14:38:18.000Z",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6712
    },
    "isAuthorParticipating": false
  }
]