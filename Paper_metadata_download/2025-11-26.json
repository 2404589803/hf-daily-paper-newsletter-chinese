[
  {
    "paper": {
      "id": "2511.19046",
      "authors": [
        {
          "_id": "69267465243b2216fb75ca9d",
          "name": "Anglin Liu",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75ca9e",
          "name": "Rundong Xue",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75ca9f",
          "name": "Xu R. Cao",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75caa0",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75caa1",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75caa2",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75caa3",
          "name": "Qianqian Chen",
          "hidden": false
        },
        {
          "_id": "69267465243b2216fb75caa4",
          "name": "Jintai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T12:34:38.000Z",
      "submittedOnDailyAt": "2025-11-26T01:01:06.095Z",
      "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
      "upvotes": 28,
      "discussionId": "69267466243b2216fb75caa5",
      "githubRepo": "https://github.com/Joey-S-Liu/MedSAM3",
      "ai_summary": "MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.",
      "ai_keywords": [
        "Segment Anything Model (SAM)",
        "text promptable",
        "medical segmentation",
        "Promptable Concept Segmentation (PCS)",
        "Multimodal Large Language Models (MLLMs)",
        "X-ray",
        "MRI",
        "Ultrasound",
        "CT",
        "video"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-11-24T07:34:38.000Z",
    "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
    "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19900",
      "authors": [
        {
          "_id": "6926877a243b2216fb75caca",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cacb",
          "name": "Kaiwen Xiong",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cacc",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cacd",
          "name": "Yiyang Zhou",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cace",
          "name": "Haonian Ji",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cacf",
          "name": "Lu Feng",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cad0",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cad1",
          "name": "Mingyu Ding",
          "hidden": false
        },
        {
          "_id": "6926877a243b2216fb75cad2",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T04:15:14.000Z",
      "submittedOnDailyAt": "2025-11-26T02:25:56.245Z",
      "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
      "submittedOnDailyBy": {
        "_id": "684ff37fa383bc5d6b0ff77f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png",
        "isPro": false,
        "fullname": "JiaqiLiu",
        "user": "JiaaqiLiu",
        "type": "user"
      },
      "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.",
      "upvotes": 24,
      "discussionId": "6926877a243b2216fb75cad3",
      "projectPage": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL",
      "githubRepo": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL",
      "ai_summary": "Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.",
      "ai_keywords": [
        "self-rewarding approaches",
        "tool-integrated reasoning",
        "self-evaluation",
        "self-repair",
        "introspect",
        "evidence-grounded analysis",
        "Solver",
        "Verifier",
        "Self-Evolving Reasoning Cycle",
        "reinforcement learning",
        "geometric problem solving",
        "visual scientific analysis"
      ],
      "githubStars": 273,
      "organization": {
        "_id": "669f9d1fec8789263c0e355a",
        "name": "UNC-ChapelHill",
        "fullname": "University of North Carolina at Chapel Hill",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
      }
    },
    "publishedAt": "2025-11-24T23:15:14.000Z",
    "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
    "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "684ff37fa383bc5d6b0ff77f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png",
      "fullname": "JiaqiLiu",
      "name": "JiaaqiLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "669f9d1fec8789263c0e355a",
      "name": "UNC-ChapelHill",
      "fullname": "University of North Carolina at Chapel Hill",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20561",
      "authors": [
        {
          "_id": "692671a3243b2216fb75ca91",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca92",
          "name": "Weiyang Jin",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca93",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca94",
          "name": "Chaoran Feng",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca95",
          "name": "Peng Jin",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca96",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca97",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca98",
          "name": "Bin Zhu",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca99",
          "name": "Weihao Yu",
          "hidden": false
        },
        {
          "_id": "692671a3243b2216fb75ca9a",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"
      ],
      "publishedAt": "2025-11-25T17:58:48.000Z",
      "submittedOnDailyAt": "2025-11-26T01:19:47.715Z",
      "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
      "submittedOnDailyBy": {
        "_id": "66608add236f958513d21d2e",
        "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
        "isPro": false,
        "fullname": "Weiyang Jin",
        "user": "Wayne-King",
        "type": "user"
      },
      "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
      "upvotes": 18,
      "discussionId": "692671a3243b2216fb75ca9b",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniSandBox",
      "ai_summary": "UniSandbox evaluates Unified Multimodal Models, revealing a gap between understanding and generation, and identifies Chain-of-Thought and self-training as means to bridge this gap.",
      "ai_keywords": [
        "Unified Multimodal Models",
        "UniSandbox",
        "decoupled evaluation framework",
        "synthetic datasets",
        "understanding-generation gap",
        "reasoning generation",
        "knowledge transfer",
        "Chain-of-Thought",
        "self-training",
        "query-based architectures"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-11-25T12:58:48.000Z",
    "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
    "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66608add236f958513d21d2e",
      "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
      "fullname": "Weiyang Jin",
      "name": "Wayne-King",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19861",
      "authors": [
        {
          "_id": "69266d2b243b2216fb75ca14",
          "name": "GigaWorld Team",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca15",
          "name": "Angen Ye",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca16",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca17",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca18",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca19",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1a",
          "name": "Haoyun Li",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1b",
          "name": "Jiagang Zhu",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1c",
          "name": "Kerui Li",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1d",
          "name": "Mengyuan Xu",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1e",
          "name": "Qiuping Deng",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca1f",
          "name": "Siting Wang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca20",
          "name": "Wenkang Qin",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca21",
          "name": "Xinze Chen",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca22",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca23",
          "name": "Yankai Wang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca24",
          "name": "Yu Cao",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca25",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca26",
          "name": "Yuan Xu",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca27",
          "name": "Yun Ye",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca28",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca29",
          "name": "Yukun Zhou",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca2a",
          "name": "Zhengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca2b",
          "name": "Zhehao Dong",
          "hidden": false
        },
        {
          "_id": "69266d2b243b2216fb75ca2c",
          "name": "Zheng Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T03:00:42.000Z",
      "submittedOnDailyAt": "2025-11-26T00:30:02.812Z",
      "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
      "upvotes": 18,
      "discussionId": "69266d2b243b2216fb75ca2d",
      "projectPage": "https://gigaworld0.github.io/",
      "githubRepo": "https://github.com/open-gigaai/giga-world-0",
      "ai_summary": "GigaWorld-0 is a unified world model framework that integrates video generation and 3D modeling to produce high-quality, diverse, and physically plausible VLA data, enabling strong real-world performance in embodied AI without real-world training.",
      "ai_keywords": [
        "GigaWorld-0",
        "GigaWorld-0-Video",
        "GigaWorld-0-3D",
        "3D generative modeling",
        "3D Gaussian Splatting",
        "physically differentiable system identification",
        "executable motion planning",
        "GigaTrain",
        "FP8-precision",
        "sparse attention",
        "VLA model",
        "GigaBrain-0",
        "embodied interaction data",
        "real-world performance",
        "task success"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-11-24T22:00:42.000Z",
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "summary": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19861.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 170
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20635",
      "authors": [
        {
          "_id": "69266dec243b2216fb75ca56",
          "name": "Zhoujie Fu",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca57",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca58",
          "name": "Jinghong Lan",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca59",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5a",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5b",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5c",
          "name": "Jiacheng Wei",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5d",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5e",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca5f",
          "name": "Yunuo Chen",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca60",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "69266dec243b2216fb75ca61",
          "name": "Guosheng Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T18:54:16.000Z",
      "submittedOnDailyAt": "2025-11-26T00:33:21.436Z",
      "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
      "upvotes": 15,
      "discussionId": "69266dec243b2216fb75ca62",
      "projectPage": "https://kr1sjfu.github.io/iMontage-web/",
      "ai_summary": "iMontage repurposes pre-trained video models to generate high-quality, diverse image sets with natural transitions and enhanced dynamics through a unified framework and tailored adaptation strategy.",
      "ai_keywords": [
        "pre-trained video models",
        "temporal coherence",
        "continuous nature",
        "image data",
        "content diversity",
        "iMontage",
        "unified framework",
        "image generator",
        "variable-length image sets",
        "image generation",
        "image editing",
        "adaptation strategy",
        "data curation",
        "training paradigm",
        "image manipulation",
        "motion priors",
        "cross-image contextual consistency"
      ],
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2025-11-25T13:54:16.000Z",
    "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
    "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 170
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19320",
      "authors": [
        {
          "_id": "692525d916eb3a9f13103974",
          "user": {
            "_id": "65645169ec7e239899136895",
            "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg",
            "isPro": false,
            "fullname": "Jiaming Zhang",
            "user": "jiamingZ",
            "type": "user"
          },
          "name": "Jiaming Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-25T12:10:05.653Z",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f13103975",
          "name": "Shengming Cao",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f13103976",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f13103977",
          "name": "Xiaotong Zhao",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f13103978",
          "name": "Yutao Cui",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f13103979",
          "name": "Xinglin Hou",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f1310397a",
          "name": "Gangshan Wu",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f1310397b",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f1310397c",
          "name": "Yu Xu",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f1310397d",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "692525d916eb3a9f1310397e",
          "name": "Kai Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T17:15:55.000Z",
      "submittedOnDailyAt": "2025-11-26T00:24:22.899Z",
      "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
      "submittedOnDailyBy": {
        "_id": "65645169ec7e239899136895",
        "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg",
        "isPro": false,
        "fullname": "Jiaming Zhang",
        "user": "jiamingZ",
        "type": "user"
      },
      "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
      "upvotes": 12,
      "discussionId": "692525d916eb3a9f1310397f",
      "projectPage": "https://mcg-nju.github.io/steadydancer-web",
      "githubRepo": "https://github.com/MCG-NJU/SteadyDancer",
      "ai_summary": "SteadyDancer, an Image-to-Video framework, ensures first-frame identity preservation and precise motion control through harmonized conditions, adaptive pose representation, and hierarchical training objectives.",
      "ai_keywords": [
        "Image-to-Motion Binding",
        "Reference-to-Video",
        "Image-to-Video",
        "Condition-Reconciliation Mechanism",
        "Synergistic Pose Modulation Modules",
        "Staged Decoupled-Objective Training Pipeline",
        "motion fidelity",
        "visual quality",
        "temporal coherence"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "62c77fde1e080b83746468bd",
        "name": "MCG-NJU",
        "fullname": "Multimedia Computing Group-Nanjing University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"
      }
    },
    "publishedAt": "2025-11-24T12:15:55.000Z",
    "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
    "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65645169ec7e239899136895",
      "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg",
      "fullname": "Jiaming Zhang",
      "name": "jiamingZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "62c77fde1e080b83746468bd",
      "name": "MCG-NJU",
      "fullname": "Multimedia Computing Group-Nanjing University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.20123",
      "authors": [
        {
          "_id": "6926928b243b2216fb75cb17",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb18",
          "name": "Hongzhou Zhu",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb19",
          "name": "Yingze Wang",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1a",
          "name": "Bokai Yan",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1b",
          "name": "Jintao Zhang",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1c",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1d",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1e",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "6926928b243b2216fb75cb1f",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T09:44:10.000Z",
      "submittedOnDailyAt": "2025-11-26T03:16:49.152Z",
      "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64c269a52d73768f07ac266c",
        "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
        "isPro": false,
        "fullname": "Zhu Hongzhou",
        "user": "zhuhz22",
        "type": "user"
      },
      "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.",
      "upvotes": 11,
      "discussionId": "6926928b243b2216fb75cb20",
      "projectPage": "https://thu-ml.github.io/UltraViCo.github.io/",
      "githubRepo": "https://github.com/thu-ml/DiT-Extrapolation",
      "ai_summary": "UltraViCo addresses video length extrapolation by suppressing attention dispersion, improving quality and reducing repetition beyond training length.",
      "ai_keywords": [
        "video diffusion transformers",
        "video length extrapolation",
        "positional encodings",
        "attention maps",
        "attention dispersion",
        "UltraViCo",
        "Dynamic Degree",
        "Imaging Quality",
        "controllable video synthesis",
        "editing"
      ],
      "githubStars": 738,
      "organization": {
        "_id": "640d3084536d9fe0f005cac3",
        "name": "thu-ml",
        "fullname": "Tsinghua Machine Learning Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
      }
    },
    "publishedAt": "2025-11-25T04:44:10.000Z",
    "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
    "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c269a52d73768f07ac266c",
      "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
      "fullname": "Zhu Hongzhou",
      "name": "zhuhz22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "640d3084536d9fe0f005cac3",
      "name": "thu-ml",
      "fullname": "Tsinghua Machine Learning Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19827",
      "authors": [
        {
          "_id": "69268cde243b2216fb75caf9",
          "name": "Byeongjun Park",
          "hidden": false
        },
        {
          "_id": "69268cde243b2216fb75cafa",
          "name": "Byung-Hoon Kim",
          "hidden": false
        },
        {
          "_id": "69268cde243b2216fb75cafb",
          "name": "Hyungjin Chung",
          "hidden": false
        },
        {
          "_id": "69268cde243b2216fb75cafc",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T01:38:56.000Z",
      "submittedOnDailyAt": "2025-11-26T02:48:52.934Z",
      "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding",
      "submittedOnDailyBy": {
        "_id": "653929a66da48e0d21e65e17",
        "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg",
        "isPro": false,
        "fullname": "Byeongjun Park",
        "user": "byeongjun-park",
        "type": "user"
      },
      "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.",
      "upvotes": 8,
      "discussionId": "69268cdf243b2216fb75cafd",
      "projectPage": "https://byeongjun-park.github.io/ReDirector/",
      "githubRepo": "https://github.com/byeongjun-park/ReDirector",
      "ai_summary": "ReDirector uses a novel camera-controlled video retake method with Rotary Camera Encoding (RoCE) to improve dynamic object localization and static background preservation in variable-length videos.",
      "ai_keywords": [
        "RoPE",
        "Rotary Camera Encoding",
        "RoCE",
        "camera-controlled video retake",
        "spatiotemporal positions",
        "multi-view relationships",
        "dynamic object localization",
        "static background preservation",
        "camera controllability",
        "geometric consistency",
        "video quality"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "64ab689073790912c7a8717a",
        "name": "everex",
        "fullname": "EverEx",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"
      }
    },
    "publishedAt": "2025-11-24T20:38:56.000Z",
    "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding",
    "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653929a66da48e0d21e65e17",
      "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg",
      "fullname": "Byeongjun Park",
      "name": "byeongjun-park",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "64ab689073790912c7a8717a",
      "name": "everex",
      "fullname": "EverEx",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20573",
      "authors": [
        {
          "_id": "69266e53243b2216fb75ca64",
          "name": "Chenhui Gou",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca65",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca66",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca67",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca68",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca69",
          "name": "Zicheng Duan",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6a",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6b",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6c",
          "name": "Hongyi Yuan",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6d",
          "name": "Haoqi Fan",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6e",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca6f",
          "name": "Jianfei Cai",
          "hidden": false
        },
        {
          "_id": "69266e53243b2216fb75ca70",
          "name": "Hamid Rezatofighi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T18:06:22.000Z",
      "submittedOnDailyAt": "2025-11-26T03:38:13.028Z",
      "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
      "submittedOnDailyBy": {
        "_id": "652e9c5774d1b0d7ff73d091",
        "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg",
        "isPro": true,
        "fullname": "Chenhui Gou",
        "user": "gouc",
        "type": "user"
      },
      "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
      "upvotes": 7,
      "discussionId": "69266e53243b2216fb75ca71",
      "ai_summary": "A data-centric framework and benchmark for Visual Question-Visual Answering (VQ-VA) improve open-source model performance, narrowing the gap with proprietary systems.",
      "ai_keywords": [
        "Visual Question-Visual Answering",
        "VQ-VA",
        "agentic pipeline",
        "web-scale deployment",
        "image-text samples",
        "IntelligentBench",
        "world knowledge",
        "design knowledge",
        "reasoning",
        "LightFusion",
        "UniWorld-V1",
        "NanoBanana",
        "GPT-Image"
      ]
    },
    "publishedAt": "2025-11-25T13:06:22.000Z",
    "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
    "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652e9c5774d1b0d7ff73d091",
      "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg",
      "fullname": "Chenhui Gou",
      "name": "gouc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20347",
      "authors": [
        {
          "_id": "69268ffb243b2216fb75cb0b",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb0c",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb0d",
          "name": "Xiong-Hui Chen",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb0e",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb0f",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb10",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb11",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb12",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb13",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69268ffb243b2216fb75cb14",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T14:25:19.000Z",
      "submittedOnDailyAt": "2025-11-26T04:31:56.255Z",
      "title": "Soft Adaptive Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "63451cf0a05b51f7ded25505",
        "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
        "isPro": false,
        "fullname": "shuai bai",
        "user": "ShuaiBai623",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
      "upvotes": 5,
      "discussionId": "69268ffb243b2216fb75cb15",
      "ai_summary": "Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "policy optimization",
        "token-level importance ratios",
        "Mixture-of-Experts models",
        "GSPO",
        "GRPO",
        "Soft Adaptive Policy Optimization",
        "sequence-coherent",
        "token-adaptive",
        "sequence-level coherence",
        "off-policy updates",
        "sample efficiency",
        "Pass@1 performance",
        "Qwen3-VL model series"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2025-11-25T09:25:19.000Z",
    "title": "Soft Adaptive Policy Optimization",
    "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63451cf0a05b51f7ded25505",
      "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
      "fullname": "shuai bai",
      "name": "ShuaiBai623",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19773",
      "authors": [
        {
          "_id": "69268437243b2216fb75cab7",
          "name": "Meng Lu",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cab8",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cab9",
          "name": "Yi Fang",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75caba",
          "name": "Wenxuan Zhang",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cabb",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cabc",
          "name": "Gaurav Srivastava",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cabd",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cabe",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cabf",
          "name": "Charles Fleming",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac0",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac1",
          "name": "Zhengzhong Tu",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac2",
          "name": "Yang Xie",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac3",
          "name": "Guanghua Xiao",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac4",
          "name": "Hanrui Wang",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac5",
          "name": "Di Jin",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac6",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "69268437243b2216fb75cac7",
          "name": "Xuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T22:58:26.000Z",
      "submittedOnDailyAt": "2025-11-26T02:09:15.051Z",
      "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
      "submittedOnDailyBy": {
        "_id": "65cae89119683f9817c049ea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
        "isPro": false,
        "fullname": "Wenqi Shi",
        "user": "wshi83",
        "type": "user"
      },
      "summary": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
      "upvotes": 4,
      "discussionId": "69268438243b2216fb75cac8",
      "ai_summary": "VISTA-Gym enhances vision-language models' tool-integrated visual reasoning through a scalable training environment with diverse multimodal tasks and reinforcement learning.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "VISTA-Gym",
        "multimodal reasoning",
        "visual tools",
        "executable interaction loops",
        "verifiable feedback signals",
        "trajectory logging",
        "visual agentic reinforcement learning",
        "tool selection",
        "tool invocation",
        "tool coordination",
        "multi-turn trajectory sampling",
        "end-to-end reinforcement learning",
        "VQA benchmarks",
        "VISTA-R1-8B"
      ],
      "organization": {
        "_id": "688ec8e2ea39631b70dc34db",
        "name": "eigen-ai-labs",
        "fullname": "Eigen AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"
      }
    },
    "publishedAt": "2025-11-24T17:58:26.000Z",
    "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
    "summary": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19773.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cae89119683f9817c049ea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
      "fullname": "Wenqi Shi",
      "name": "wshi83",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "688ec8e2ea39631b70dc34db",
      "name": "eigen-ai-labs",
      "fullname": "Eigen AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19575",
      "authors": [
        {
          "_id": "69266d8a243b2216fb75ca2f",
          "name": "Hunyuan Vision Team",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca30",
          "name": "Pengyuan Lyu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca31",
          "name": "Xingyu Wan",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca32",
          "name": "Gengluo Li",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca33",
          "name": "Shangpin Peng",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca34",
          "name": "Weinong Wang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca35",
          "name": "Liang Wu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca36",
          "name": "Huawen Shen",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca37",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca38",
          "name": "Canhui Tang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca39",
          "name": "Qi Yang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3a",
          "name": "Qiming Peng",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3b",
          "name": "Bin Luo",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3c",
          "name": "Hower Yang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3d",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3e",
          "name": "Hongming Yang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca3f",
          "name": "Senhao Xie",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca40",
          "name": "Binghong Wu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca41",
          "name": "Mana Yang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca42",
          "name": "Sergey Wang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca43",
          "name": "Raccoon Liu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca44",
          "name": "Dick Zhu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca45",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca46",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca47",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "69266d8a243b2216fb75ca48",
          "name": "Chengquan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T17:59:59.000Z",
      "submittedOnDailyAt": "2025-11-26T00:31:43.086Z",
      "title": "HunyuanOCR Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
      "upvotes": 4,
      "discussionId": "69266d8a243b2216fb75ca49",
      "githubRepo": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "ai_summary": "HunyuanOCR, a lightweight Vision-Language Model, achieves state-of-the-art performance in OCR tasks through a unified end-to-end architecture combining Vision Transformer and lightweight LLM, supported by data-driven and RL strategies.",
      "ai_keywords": [
        "Vision-Language Model",
        "Vision Transformer",
        "lightweight LLM",
        "MLP adapter",
        "Text Spotting",
        "Parsing",
        "Information Extraction",
        "VQA",
        "Text Image Translation",
        "end-to-end architecture",
        "Reinforcement Learning"
      ],
      "githubStars": 360,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2025-11-24T12:59:59.000Z",
    "title": "HunyuanOCR Technical Report",
    "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19575.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 170
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19663",
      "authors": [
        {
          "_id": "69266fb9243b2216fb75ca7c",
          "name": "Ahmed Awadallah",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca7d",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca7e",
          "name": "Raghav Magazine",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca7f",
          "name": "Hussein Mozannar",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca80",
          "name": "Akshay Nambi",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca81",
          "name": "Yash Pandya",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca82",
          "name": "Aravind Rajeswaran",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca83",
          "name": "Corby Rosset",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca84",
          "name": "Alexey Taymanov",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca85",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca86",
          "name": "Spencer Whitehead",
          "hidden": false
        },
        {
          "_id": "69266fb9243b2216fb75ca87",
          "name": "Andrew Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T19:56:28.000Z",
      "submittedOnDailyAt": "2025-11-26T00:41:03.809Z",
      "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.",
      "upvotes": 3,
      "discussionId": "69266fb9243b2216fb75ca88",
      "ai_summary": "FaraGen creates synthetic datasets for computer use agents, enabling the training of efficient and high-performing models like Fara-7B on diverse web tasks, outperforming larger models on benchmarks.",
      "ai_keywords": [
        "FaraGen",
        "synthetic data generation",
        "multi-step web tasks",
        "CUA trajectories",
        "verifiers",
        "Fara-7B",
        "on-device model",
        "native CUA model",
        "screenshots",
        "predicted coordinates",
        "WebVoyager",
        "Online-Mind2Web",
        "WebTailBench"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-11-24T14:56:28.000Z",
    "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
    "summary": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 170
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20562",
      "authors": [
        {
          "_id": "69266e71243b2216fb75ca73",
          "name": "Haoze Zhang",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca74",
          "name": "Tianyu Huang",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca75",
          "name": "Zichen Wan",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca76",
          "name": "Xiaowei Jin",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca77",
          "name": "Hongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca78",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "69266e71243b2216fb75ca79",
          "name": "Wangmeng Zuo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/muW87bIo2158vk7hvH4P7.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/15nSdHYXMyYAhBaFAuxFt.qt"
      ],
      "publishedAt": "2025-11-25T17:59:04.000Z",
      "submittedOnDailyAt": "2025-11-26T00:37:01.908Z",
      "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
      "upvotes": 1,
      "discussionId": "69266e71243b2216fb75ca7a",
      "ai_summary": "PhysChoreo generates physically realistic and controllable videos from a single image using part-aware physical property reconstruction and temporally instructed simulation.",
      "ai_keywords": [
        "video generation",
        "physical controllability",
        "plausibility",
        "physics-based rendering",
        "physical property reconstruction",
        "temporally instructed simulation",
        "physical realism"
      ]
    },
    "publishedAt": "2025-11-25T12:59:04.000Z",
    "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
    "summary": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/muW87bIo2158vk7hvH4P7.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/15nSdHYXMyYAhBaFAuxFt.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 170
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.15906",
      "authors": [
        {
          "_id": "692069e58c38b39d6a482eb4",
          "user": {
            "_id": "69111dd0eed4fe1e4fcf63dd",
            "avatarUrl": "/avatars/1be6490daa72ea9e2e366a572ab7e438.svg",
            "isPro": false,
            "fullname": "Matthieu Kirchmeyer",
            "user": "mkirchmeyer",
            "type": "user"
          },
          "name": "Matthieu Kirchmeyer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-25T15:53:22.361Z",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eb5",
          "name": "Pedro O. Pinheiro",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eb6",
          "name": "Emma Willett",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eb7",
          "name": "Karolis Martinkus",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eb8",
          "name": "Joseph Kleinhenz",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eb9",
          "name": "Emily K. Makowski",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482eba",
          "name": "Andrew M. Watkins",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482ebb",
          "name": "Vladimir Gligorijevic",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482ebc",
          "name": "Richard Bonneau",
          "hidden": false
        },
        {
          "_id": "692069e58c38b39d6a482ebd",
          "name": "Saeed Saremi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/zrf-c-Jf9abcl8jqv7ymr.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/0miig01yongdkfn3ByH8M.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/bM0VQA4QSXXWXz-FEWHGM.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/KEssstpCbDKJDpmwqDdrD.gif"
      ],
      "publishedAt": "2025-11-19T22:18:13.000Z",
      "submittedOnDailyAt": "2025-11-26T00:22:36.233Z",
      "title": "Unified all-atom molecule generation with neural fields",
      "submittedOnDailyBy": {
        "_id": "69111dd0eed4fe1e4fcf63dd",
        "avatarUrl": "/avatars/1be6490daa72ea9e2e366a572ab7e438.svg",
        "isPro": false,
        "fullname": "Matthieu Kirchmeyer",
        "user": "mkirchmeyer",
        "type": "user"
      },
      "summary": "Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.",
      "upvotes": 1,
      "discussionId": "692069e58c38b39d6a482ebe",
      "githubRepo": "https://github.com/prescient-design/funcbind/",
      "ai_summary": "FuncBind, a framework using neural fields and score-based generative models from computer vision, generates diverse atomic structures across modalities, achieving competitive performance in structure-conditioned molecular design.",
      "ai_keywords": [
        "neural fields",
        "score-based generative models",
        "structure-conditioned",
        "target-conditioned",
        "all-atom molecules",
        "atomic systems",
        "macrocyclic peptides",
        "antibody complementarity-determining region loops",
        "de novo redesign"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "63053d3c9d2531fabd152815",
        "name": "Genentech",
        "fullname": "Genentech",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661287901949-63053cd653b3899eeb61d17d.png"
      }
    },
    "publishedAt": "2025-11-19T17:18:13.000Z",
    "title": "Unified all-atom molecule generation with neural fields",
    "summary": "Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/zrf-c-Jf9abcl8jqv7ymr.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/0miig01yongdkfn3ByH8M.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/bM0VQA4QSXXWXz-FEWHGM.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/KEssstpCbDKJDpmwqDdrD.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15906.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "69111dd0eed4fe1e4fcf63dd",
      "avatarUrl": "/avatars/1be6490daa72ea9e2e366a572ab7e438.svg",
      "fullname": "Matthieu Kirchmeyer",
      "name": "mkirchmeyer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "63053d3c9d2531fabd152815",
      "name": "Genentech",
      "fullname": "Genentech",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661287901949-63053cd653b3899eeb61d17d.png"
    },
    "isAuthorParticipating": true
  }
]