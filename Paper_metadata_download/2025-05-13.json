[
  {
    "paper": {
      "id": "2505.07062",
      "authors": [
        {
          "_id": "6822ab54a79983c00c8164eb",
          "name": "Dong Guo",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164ec",
          "name": "Faming Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164ed",
          "name": "Feida Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164ee",
          "name": "Fuxing Leng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164ef",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f0",
          "name": "Haobin Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f1",
          "name": "Haoqi Fan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f2",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f3",
          "name": "Jianyu Jiang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f4",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f5",
          "name": "Jingji Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f6",
          "name": "Jingjia Huang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f7",
          "name": "Kang Lei",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f8",
          "name": "Liping Yuan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164f9",
          "name": "Lishu Luo",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164fa",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164fb",
          "name": "Qinghao Ye",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164fc",
          "name": "Rui Qian",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164fd",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164fe",
          "name": "Shixiong Zhao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8164ff",
          "name": "Shuai Peng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816500",
          "name": "Shuangye Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816501",
          "name": "Sihang Yuan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816502",
          "name": "Sijin Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816503",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816504",
          "name": "Weiwei Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816505",
          "name": "Wenqian Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816506",
          "name": "Xianhan Zeng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816507",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816508",
          "name": "Xiaobo Qin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816509",
          "name": "Xiaohan Ding",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650a",
          "name": "Xiaojun Xiao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650b",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650c",
          "name": "Xuanwei Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650d",
          "name": "Xuehan Xiong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650e",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81650f",
          "name": "Yangrui Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816510",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816511",
          "name": "Yanxu Hu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816512",
          "name": "Yi Lin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816513",
          "name": "Yiyuan Hu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816514",
          "name": "Yiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816515",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816516",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816517",
          "name": "Yudong Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816518",
          "name": "Yue Ling",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816519",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651a",
          "name": "Zanbo Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651b",
          "name": "Zhiwu He",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651c",
          "name": "Aoxue Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651d",
          "name": "Bairen Yi",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651e",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81651f",
          "name": "Can Huang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816520",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816521",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816522",
          "name": "Chaoyi Deng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816523",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816524",
          "name": "Cheng Yuan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816525",
          "name": "Chenggang Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816526",
          "name": "Chenhui Gou",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816527",
          "name": "Chenwei Lou",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816528",
          "name": "Chengzhi Wei",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816529",
          "name": "Chundian Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652a",
          "name": "Chunyuan Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652b",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652c",
          "name": "Donghong Zhong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652d",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652e",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81652f",
          "name": "Gang Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816530",
          "name": "Guodong Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816531",
          "name": "Guohong Xiao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816532",
          "name": "Haibin Lin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816533",
          "name": "Haihua Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816534",
          "name": "Haoming Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816535",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816536",
          "name": "Hongxiang Hao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816537",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816538",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816539",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653a",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653b",
          "name": "Jianhua Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653c",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653d",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653e",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81653f",
          "name": "Jianhui Duan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816540",
          "name": "Jihao Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816541",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816542",
          "name": "Jingqun Tang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816543",
          "name": "Jingyu Sun",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816544",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816545",
          "name": "Jun Long",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816546",
          "name": "Junda Feng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816547",
          "name": "Junfeng Zhan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816548",
          "name": "Junjie Fang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816549",
          "name": "Junting Lu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654a",
          "name": "Kai Hua",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654b",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654c",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654d",
          "name": "Kaiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654e",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81654f",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816550",
          "name": "Keyu Pan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816551",
          "name": "Kun Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816552",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816553",
          "name": "Lanxin Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816554",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816555",
          "name": "Lei Shi",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816556",
          "name": "Li Han",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816557",
          "name": "Liang Xiang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816558",
          "name": "Liangqiang Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816559",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655a",
          "name": "Lin Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655c",
          "name": "Liying Chi",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655d",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655e",
          "name": "Mengfei Du",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81655f",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816560",
          "name": "Ningxin Pan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816561",
          "name": "Peibin Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816562",
          "name": "Pengfei Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816563",
          "name": "Pengfei Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816564",
          "name": "Qingqing Yuan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816565",
          "name": "Qingyao Shuai",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816566",
          "name": "Qiuyan Tao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816567",
          "name": "Renjie Zheng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816568",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816569",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656b",
          "name": "Rui Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656c",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656d",
          "name": "Shaoqiang Xu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656e",
          "name": "Shihao Liang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81656f",
          "name": "Shipeng Yan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816570",
          "name": "Shu Zhong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816571",
          "name": "Shuaishuai Cao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816572",
          "name": "Shuangzhi Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816573",
          "name": "Shufan Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816574",
          "name": "Shuhan Chang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816575",
          "name": "Songhua Cai",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816576",
          "name": "Tenglong Ao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816577",
          "name": "Tianhao Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816578",
          "name": "Tingting Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816579",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657a",
          "name": "Wei Jia",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657b",
          "name": "Wei Weng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657c",
          "name": "Weihao Yu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657d",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657e",
          "name": "Wenjia Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81657f",
          "name": "Wenli Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816580",
          "name": "Wenzhi Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816581",
          "name": "Xiang Long",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816582",
          "name": "XiangRui Yin",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816583",
          "name": "Xiao Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816584",
          "name": "Xiaolei Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816585",
          "name": "Xiaoying Jia",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816586",
          "name": "Xijin Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816587",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816588",
          "name": "Xinchen Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816589",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658a",
          "name": "Xiongcai Luo",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658b",
          "name": "Xiuli Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658c",
          "name": "Xuantong Zhong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658d",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658e",
          "name": "Xujing Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81658f",
          "name": "Yan Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816590",
          "name": "Yawei Wen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816591",
          "name": "Yifan Du",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816592",
          "name": "Yihao Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816593",
          "name": "Yining Ye",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816594",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816595",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816596",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816597",
          "name": "Yufeng Zhou",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816598",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c816599",
          "name": "Yuhang Xu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659a",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659b",
          "name": "Yun Zhang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659c",
          "name": "Yunhao Fang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659d",
          "name": "Yuntao Li",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659e",
          "name": "Yurui Ren",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c81659f",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a0",
          "name": "Zehua Hong",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a1",
          "name": "Zehua Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a2",
          "name": "Zewei Sun",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a3",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a4",
          "name": "Zhao Cai",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a5",
          "name": "Zhaoyue Zha",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a6",
          "name": "Zhecheng An",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a7",
          "name": "Zhehui Zhao",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a8",
          "name": "Zhengzhuo Xu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165a9",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165aa",
          "name": "Zhiyong Wu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165ab",
          "name": "Zhuofan Zheng",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165ac",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165ad",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165ae",
          "name": "Ziyu Zhu",
          "hidden": false
        },
        {
          "_id": "6822ab54a79983c00c8165af",
          "name": "Zuquan Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-11T17:28:30.000Z",
      "submittedOnDailyAt": "2025-05-13T00:47:43.019Z",
      "title": "Seed1.5-VL Technical Report",
      "submittedOnDailyBy": {
        "_id": "646b3db131968a60a01e4cf5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
        "isPro": false,
        "fullname": "Tianheng Cheng",
        "user": "wondervictor",
        "type": "user"
      },
      "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
      "upvotes": 53,
      "discussionId": "6822ab59a79983c00c8166ff",
      "githubRepo": "https://github.com/ByteDance-Seed/Seed1.5-VL",
      "ai_keywords": [
        "vision-language foundation model",
        "multimodal understanding",
        "reasoning",
        "vision encoder",
        "Mixture-of-Experts (MoE)",
        "LLM",
        "active parameters",
        "public VLM benchmarks",
        "internal evaluation suites",
        "state-of-the-art performance",
        "GUI control",
        "gameplay",
        "multimodal systems",
        "visual puzzles",
        "multimodal reasoning"
      ]
    },
    "publishedAt": "2025-05-11T13:28:30.000Z",
    "title": "Seed1.5-VL Technical Report",
    "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07062.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646b3db131968a60a01e4cf5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
      "fullname": "Tianheng Cheng",
      "name": "wondervictor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07608",
      "authors": [
        {
          "_id": "6822b5441f775dd12f753469",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75346b",
          "name": "Bingquan Xia",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75346c",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75346d",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75346e",
          "name": "Dawei Zhu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75346f",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753470",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753471",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753472",
          "name": "Huaqiu Liu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753473",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753474",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753475",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753476",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753477",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753478",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753479",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347b",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347c",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347d",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347e",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75347f",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753480",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753481",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753482",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753483",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753484",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753485",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753486",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753487",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753488",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753489",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348a",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348b",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348c",
          "name": "QingKai Fang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348e",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75348f",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753490",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753491",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753492",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753493",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753494",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753495",
          "name": "Shuhao Gu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753496",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753497",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753498",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f753499",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349a",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349b",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349c",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349d",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349e",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f75349f",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a0",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a1",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a2",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a3",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a4",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a5",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a6",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a7",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a8",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6822b5441f775dd12f7534a9",
          "name": "Zihao Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T14:30:11.000Z",
      "submittedOnDailyAt": "2025-05-13T01:36:30.279Z",
      "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
      "submittedOnDailyBy": {
        "_id": "61d595f0bfc2372aaf42b766",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675437310060-61d595f0bfc2372aaf42b766.jpeg",
        "isPro": false,
        "fullname": "Yifan Song",
        "user": "Solaris99",
        "type": "user"
      },
      "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.",
      "upvotes": 34,
      "discussionId": "6822b5451f775dd12f7534e6",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo",
      "ai_keywords": [
        "large language model",
        "reasoning tasks",
        "data preprocessing pipeline",
        "three-stage data mixing strategy",
        "pre-training",
        "post-training",
        "Multi-Token Prediction objective",
        "verifiable mathematics and programming problems",
        "reinforcement learning",
        "test-difficulty-driven code-reward scheme",
        "strategic data resampling",
        "RL-tuned model"
      ]
    },
    "publishedAt": "2025-05-12T10:30:11.000Z",
    "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
    "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07608.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61d595f0bfc2372aaf42b766",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675437310060-61d595f0bfc2372aaf42b766.jpeg",
      "fullname": "Yifan Song",
      "name": "Solaris99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07747",
      "authors": [
        {
          "_id": "6822b668a307d0f1bf9bb4e0",
          "name": "Weiyu Li",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e1",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e2",
          "name": "Zheng Sun",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e3",
          "name": "Di Qi",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e4",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e5",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e6",
          "name": "Weiwei Cai",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e7",
          "name": "Shihao Wu",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e8",
          "name": "Jiarui Liu",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4e9",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4ea",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4eb",
          "name": "Feipeng Tian",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4ec",
          "name": "Jianxiong Pan",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4ed",
          "name": "Zeming Li",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4ee",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4ef",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4f0",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "6822b668a307d0f1bf9bb4f1",
          "name": "Ping Tan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6675854966c4fa6d0cee4d50/hd94r_Cvaejm26QVh2iaM.png"
      ],
      "publishedAt": "2025-05-12T16:56:30.000Z",
      "submittedOnDailyAt": "2025-05-13T02:07:41.773Z",
      "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
      "submittedOnDailyBy": {
        "_id": "6675854966c4fa6d0cee4d50",
        "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
        "isPro": false,
        "fullname": "xuanyang zhang",
        "user": "xuanyangz",
        "type": "user"
      },
      "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
      "upvotes": 28,
      "discussionId": "6822b66ea307d0f1bf9bb688",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-3D",
      "ai_keywords": [
        "VAE",
        "DiT",
        "diffusion-based",
        "TSDF",
        "perceiver-based",
        "latent encoding",
        "edge sampling",
        "cross-view consistency",
        "geometric conditioning",
        "latent-space synchronization",
        "LoRA"
      ]
    },
    "publishedAt": "2025-05-12T12:56:30.000Z",
    "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
    "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6675854966c4fa6d0cee4d50/hd94r_Cvaejm26QVh2iaM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6675854966c4fa6d0cee4d50",
      "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
      "fullname": "xuanyang zhang",
      "name": "xuanyangz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07787",
      "authors": [
        {
          "_id": "6822a9b0e6328d29a9c721d6",
          "name": "Tongxu Luo",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721d7",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721d8",
          "name": "Jiaxi Bi",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721d9",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721da",
          "name": "Zhengyang Tang",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721db",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721dc",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "6822a9b0e6328d29a9c721dd",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:39:56.000Z",
      "submittedOnDailyAt": "2025-05-13T00:46:25.483Z",
      "title": "Learning from Peers in Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "6421b07e918f0fd889f0a682",
        "avatarUrl": "/avatars/314b55c2428426c846d9449f98db4355.svg",
        "isPro": false,
        "fullname": "Tongxu Luo",
        "user": "Zeno-Luo",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .",
      "upvotes": 24,
      "discussionId": "6822a9b1e6328d29a9c7222c",
      "projectPage": "https://learning-from-peers.github.io/",
      "githubRepo": "https://github.com/tongxuluo/LeaP",
      "ai_keywords": [
        "Learning from Peers (LeaP)",
        "LeaP-T",
        "Prefix Dominance Trap",
        "routing mechanism",
        "summarization",
        "reflection",
        "DeepSeek-R1-671B",
        "DeepSeek-R1-Distill-Qwen-14B",
        "error correction",
        "error tolerance"
      ]
    },
    "publishedAt": "2025-05-12T13:39:56.000Z",
    "title": "Learning from Peers in Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07787.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6421b07e918f0fd889f0a682",
      "avatarUrl": "/avatars/314b55c2428426c846d9449f98db4355.svg",
      "fullname": "Tongxu Luo",
      "name": "Zeno-Luo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.06548",
      "authors": [
        {
          "_id": "6822a0f7f90521b5d8cb4573",
          "name": "Aniruddha Roy",
          "hidden": false
        },
        {
          "_id": "6822a0f7f90521b5d8cb4574",
          "name": "Pretam Ray",
          "hidden": false
        },
        {
          "_id": "6822a0f7f90521b5d8cb4575",
          "name": "Abhilash Nandy",
          "hidden": false
        },
        {
          "_id": "6822a0f7f90521b5d8cb4576",
          "name": "Somak Aditya",
          "hidden": false
        },
        {
          "_id": "6822a0f7f90521b5d8cb4577",
          "name": "Pawan Goyal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-10T07:23:19.000Z",
      "submittedOnDailyAt": "2025-05-13T00:03:01.117Z",
      "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.",
      "upvotes": 17,
      "discussionId": "6822a0f8f90521b5d8cb45c1",
      "ai_keywords": [
        "small LLMs",
        "LLaMA 2-7B",
        "LLaMA 2-13B",
        "Mistral 7B",
        "semi-automated framework",
        "Reinforcement Learning (RL) based training algorithm"
      ]
    },
    "publishedAt": "2025-05-10T03:23:19.000Z",
    "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
    "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07447",
      "authors": [
        {
          "_id": "6822c7d78fc623ca8c6b2578",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "6822c7d78fc623ca8c6b2579",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "6822c7d78fc623ca8c6b257a",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T11:15:39.000Z",
      "submittedOnDailyAt": "2025-05-13T03:12:37.227Z",
      "title": "Unified Continuous Generative Models",
      "submittedOnDailyBy": {
        "_id": "65028e8389707f182386588c",
        "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
        "isPro": false,
        "fullname": "Zhenglin Cheng",
        "user": "kenshinn",
        "type": "user"
      },
      "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.",
      "upvotes": 14,
      "discussionId": "6822c7d98fc623ca8c6b260c",
      "githubRepo": "https://github.com/LINs-lab/UCGM",
      "ai_keywords": [
        "diffusion",
        "flow-matching",
        "consistency models",
        "generative performance",
        "unified framework",
        "training",
        "sampling",
        "analysis",
        "Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S})",
        "state-of-the-art (SOTA)",
        "ImageNet 256x256",
        "diffusion transformer",
        "FID (Frchet Inception Distance)"
      ]
    },
    "publishedAt": "2025-05-12T07:15:39.000Z",
    "title": "Unified Continuous Generative Models",
    "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07447.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65028e8389707f182386588c",
      "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
      "fullname": "Zhenglin Cheng",
      "name": "kenshinn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07818",
      "authors": [
        {
          "_id": "6822b53e731ca42bb4ba7d2f",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d30",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d31",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d32",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d33",
          "name": "Lingting Zhu",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d34",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d35",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d36",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d37",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d38",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "6822b53e731ca42bb4ba7d39",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:59:34.000Z",
      "submittedOnDailyAt": "2025-05-13T01:28:55.906Z",
      "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
      "upvotes": 12,
      "discussionId": "6822b541731ca42bb4ba7e19",
      "projectPage": "https://dancegrpo.github.io/",
      "githubRepo": "https://github.com/XueZeyue/DanceGRPO",
      "ai_keywords": [
        "diffusion models",
        "rectified flows",
        "reinforcement learning (RL)",
        "Ordinary Differential Equations (ODEs)",
        "Group Relative Policy Optimization (GRPO)",
        "text-to-image",
        "text-to-video",
        "image-to-video",
        "Stable Diffusion",
        "HunyuanVideo",
        "FLUX",
        "SkyReel-I2V",
        "image/video aesthetics",
        "text-image alignment",
        "video motion quality",
        "binary reward",
        "HPS-v2.1",
        "CLIP Score",
        "VideoAlign",
        "GenEval",
        "Best-of-N",
        "policy optimization",
        "denoising trajectories",
        "sparse binary feedback",
        "Reinforcement Learning from Human Feedback (RLHF)"
      ]
    },
    "publishedAt": "2025-05-12T13:59:34.000Z",
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07818.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07293",
      "authors": [
        {
          "_id": "6822d1f9c9cb5b62cd606495",
          "name": "Kai Hua",
          "hidden": false
        },
        {
          "_id": "6822d1f9c9cb5b62cd606496",
          "name": "Steven Wu",
          "hidden": false
        },
        {
          "_id": "6822d1f9c9cb5b62cd606497",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6822d1f9c9cb5b62cd606498",
          "user": {
            "_id": "645604eebabbbbd3486dc615",
            "avatarUrl": "/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg",
            "isPro": false,
            "fullname": "shenke",
            "user": "shenke18",
            "type": "user"
          },
          "name": "Ke Shen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-13T05:00:43.145Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T07:25:51.000Z",
      "submittedOnDailyAt": "2025-05-13T03:31:43.177Z",
      "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection.",
      "upvotes": 11,
      "discussionId": "6822d1fbc9cb5b62cd6064f4",
      "ai_keywords": [
        "AttentionInfluence",
        "attention heads",
        "in-context reasoning",
        "attention head masking",
        "retrieval heads",
        "SmolLM corpus",
        "WSD learning rate scheduling",
        "MMLU",
        "MMLU-Pro",
        "AGIEval-en",
        "GSM8K",
        "HumanEval",
        "weak-to-strong scaling"
      ]
    },
    "publishedAt": "2025-05-12T03:25:51.000Z",
    "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection",
    "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03733",
      "authors": [
        {
          "_id": "681cb3dc815cbdd72910e310",
          "name": "Zimu Lu",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e311",
          "name": "Yunqiao Yang",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e312",
          "name": "Houxing Ren",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e313",
          "name": "Haotian Hou",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e314",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e315",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e316",
          "name": "Weikang Shi",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e317",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e318",
          "name": "Mingjie Zhan",
          "hidden": false
        },
        {
          "_id": "681cb3dc815cbdd72910e319",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:15.000Z",
      "submittedOnDailyAt": "2025-05-13T00:00:09.592Z",
      "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
      "submittedOnDailyBy": {
        "_id": "64b0bfef2f2f9c345b87e673",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
        "isPro": false,
        "fullname": "Zimu Lu",
        "user": "luzimu",
        "type": "user"
      },
      "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
      "upvotes": 10,
      "discussionId": "681cb3de815cbdd72910e38b",
      "githubRepo": "https://github.com/mnluzimu/WebGen-Bench",
      "ai_keywords": [
        "LLM-based agents",
        "WebGen-Bench",
        "multi-file website codebases",
        "GPT-4o",
        "web applications",
        "test cases",
        "web-navigation agent",
        "Bolt.diy",
        "OpenHands",
        "Aider",
        "DeepSeek-R1",
        "WebGen-Instruct",
        "Qwen2.5-Coder-32B-Instruct"
      ]
    },
    "publishedAt": "2025-05-06T13:59:15.000Z",
    "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
    "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b0bfef2f2f9c345b87e673",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
      "fullname": "Zimu Lu",
      "name": "luzimu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07796",
      "authors": [
        {
          "_id": "6822b079c10ac9c466c546e3",
          "name": "Xingjin Wang",
          "hidden": false
        },
        {
          "_id": "6822b079c10ac9c466c546e4",
          "name": "Howe Tissue",
          "hidden": false
        },
        {
          "_id": "6822b079c10ac9c466c546e5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6822b079c10ac9c466c546e6",
          "name": "Linjing Li",
          "hidden": false
        },
        {
          "_id": "6822b079c10ac9c466c546e7",
          "name": "Daniel Dajun Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:47:32.000Z",
      "submittedOnDailyAt": "2025-05-13T01:10:50.906Z",
      "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters.",
      "upvotes": 9,
      "discussionId": "6822b07ac10ac9c466c5470f",
      "ai_keywords": [
        "Continual Pre-Training (CPT)",
        "large language models",
        "distribution shift",
        "learning rate annealing",
        "CPT scaling law",
        "loss potential",
        "peak learning rate",
        "training steps",
        "replay ratio"
      ]
    },
    "publishedAt": "2025-05-12T13:47:32.000Z",
    "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
    "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07596",
      "authors": [
        {
          "_id": "6822ba9e4867f1e9dc14c294",
          "name": "Ziyang Huang",
          "hidden": false
        },
        {
          "_id": "6822ba9e4867f1e9dc14c295",
          "name": "Xiaowei Yuan",
          "hidden": false
        },
        {
          "_id": "6822ba9e4867f1e9dc14c296",
          "name": "Yiming Ju",
          "hidden": false
        },
        {
          "_id": "6822ba9e4867f1e9dc14c297",
          "name": "Jun Zhao",
          "hidden": false
        },
        {
          "_id": "6822ba9e4867f1e9dc14c298",
          "name": "Kang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T14:21:57.000Z",
      "submittedOnDailyAt": "2025-05-13T01:51:27.880Z",
      "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
      "submittedOnDailyBy": {
        "_id": "616648c84c0937d31946f21b",
        "avatarUrl": "/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg",
        "isPro": false,
        "fullname": "Ziyang",
        "user": "hzy",
        "type": "user"
      },
      "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.",
      "upvotes": 7,
      "discussionId": "6822ba9f4867f1e9dc14c2b9",
      "githubRepo": "https://github.com/hzy312/knowledge-r1",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "Large Language Models (LLMs)",
        "reinforcement learning (RL)",
        "search agent",
        "retrieval capabilities",
        "internal knowledge",
        "external knowledge",
        "redundant retrievals",
        "knowledge conflicts",
        "inference latency",
        "Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA)",
        "knowledge boundary",
        "knowledge-boundary aware reward function",
        "knowledge-boundary aware training dataset",
        "internal-external knowledge synergy",
        "knowledge reasoning tasks",
        "generalization capabilities"
      ]
    },
    "publishedAt": "2025-05-12T10:21:57.000Z",
    "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
    "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616648c84c0937d31946f21b",
      "avatarUrl": "/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg",
      "fullname": "Ziyang",
      "name": "hzy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.06324",
      "authors": [
        {
          "_id": "6822bfeaa371047fe1545a13",
          "name": "Vipula Rawte",
          "hidden": false
        },
        {
          "_id": "6822bfeaa371047fe1545a14",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6822bfeaa371047fe1545a15",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "6822bfeaa371047fe1545a16",
          "name": "Nedim Lipka",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T04:40:11.000Z",
      "submittedOnDailyAt": "2025-05-13T02:13:48.132Z",
      "title": "Document Attribution: Examining Citation Relationships using Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.",
      "upvotes": 1,
      "discussionId": "6822bfeba371047fe1545a39",
      "ai_keywords": [
        "Large Language Models",
        "document summarization",
        "question answering",
        "information extraction",
        "attribution",
        "textual entailment",
        "flan-ul2",
        "AttributionBench",
        "F1 scores",
        "attention mechanism"
      ]
    },
    "publishedAt": "2025-05-09T00:40:11.000Z",
    "title": "Document Attribution: Examining Citation Relationships using Large\n  Language Models",
    "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06324.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07086",
      "authors": [
        {
          "_id": "6822c81dbda1314b4c967f53",
          "name": "Tong Chen",
          "hidden": false
        },
        {
          "_id": "6822c81dbda1314b4c967f54",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6822c81dbda1314b4c967f55",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6822c81dbda1314b4c967f56",
          "user": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "isPro": false,
            "fullname": "Pranam Chatterjee",
            "user": "pranamanam",
            "type": "user"
          },
          "name": "Pranam Chatterjee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-13T04:18:39.835Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-11T18:17:44.000Z",
      "submittedOnDailyAt": "2025-05-13T03:01:46.396Z",
      "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign.",
      "upvotes": 0,
      "discussionId": "6822c81fbda1314b4c967fda",
      "ai_keywords": [
        "discrete flow matching models",
        "flow matching generators",
        "Pareto-efficient trade-offs",
        "hybrid rank-directional score",
        "adaptive hypercone filter",
        "unconditional discrete flow matching models",
        "PepDFM",
        "EnhancerDFM",
        "peptide binders",
        "hemolysis",
        "non-fouling",
        "solubility",
        "half-life",
        "binding affinity",
        "DNA sequences",
        "specific enhancer classes",
        "DNA shapes"
      ]
    },
    "publishedAt": "2025-05-11T14:17:44.000Z",
    "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design",
    "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  }
]