[
  {
    "paper": {
      "id": "2504.02605",
      "authors": [
        {
          "_id": "67ef4d92c1e251f239495a13",
          "name": "Daoguang Zan",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a14",
          "name": "Zhirong Huang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a15",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a16",
          "name": "Hanwu Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a17",
          "name": "Linhao Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a18",
          "name": "Shulin Xin",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a19",
          "name": "Lu Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1a",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1b",
          "name": "Xiaojian Zhong",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1c",
          "name": "Aoyan Li",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1d",
          "name": "Siyao Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1e",
          "name": "Yongsheng Xiao",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1f",
          "name": "Liangqiang Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a20",
          "name": "Yuyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a21",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a22",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a23",
          "name": "Rui Long",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a24",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a25",
          "name": "Liang Xiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
        "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
      ],
      "publishedAt": "2025-04-03T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-07T02:30:50.286Z",
      "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
      "submittedOnDailyBy": {
        "_id": "61527edf8b55dbdae72874fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
        "isPro": false,
        "fullname": "Daoguang Zan",
        "user": "Daoguang",
        "type": "user"
      },
      "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
      "upvotes": 23,
      "discussionId": "67ef4d93c1e251f239495a9b",
      "projectPage": "https://multi-swe-bench.github.io",
      "githubRepo": "https://github.com/multi-swe-bench/multi-swe-bench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-SWE-bench",
        "Agentless",
        "SWE-agent",
        "OpenHands",
        "Multi-SWE-RL",
        "reinforcement learning (RL)",
        "AGI"
      ]
    },
    "publishedAt": "2025-04-03T10:06:17.000Z",
    "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
      "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61527edf8b55dbdae72874fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
      "fullname": "Daoguang Zan",
      "name": "Daoguang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03553",
      "authors": [
        {
          "_id": "67f345c983edbd64f15deeb3",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb4",
          "name": "Zhisong Qiu",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb5",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb6",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb7",
          "name": "Xiangyuan Ru",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb8",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb9",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeba",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebb",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebc",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebd",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T16:03:38.000Z",
      "submittedOnDailyAt": "2025-04-07T02:45:21.106Z",
      "title": "Agentic Knowledgeable Self-awareness",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
      "upvotes": 10,
      "discussionId": "67f345cd83edbd64f15def73",
      "githubRepo": "https://github.com/zjunlp/KnowSelf",
      "ai_keywords": [
        "agentic planning",
        "flood irrigation methodology",
        "gold trajectories",
        "external feedback",
        "domain knowledge",
        "self-awareness",
        "decision-making",
        "agentic knowledgeable self-awareness",
        "KnowSelf",
        "data-centric approach",
        "situation judgement criterion",
        "special tokens",
        "two-stage training process",
        "trajectory-based training"
      ]
    },
    "publishedAt": "2025-04-04T12:03:38.000Z",
    "title": "Agentic Knowledgeable Self-awareness",
    "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03561",
      "authors": [
        {
          "_id": "67f351f068751b2bb84cc751",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc752",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc753",
          "name": "Yuan Liang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc754",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc755",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc756",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc757",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc758",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc759",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc75a",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc75b",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T16:10:57.000Z",
      "submittedOnDailyAt": "2025-04-07T02:48:19.567Z",
      "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
      "upvotes": 8,
      "discussionId": "67f351f168751b2bb84cc789",
      "ai_keywords": [
        "LLM-based agents",
        "multi-step action invocation",
        "Monte Carlo Tree Search (MCTS)"
      ]
    },
    "publishedAt": "2025-04-04T12:10:57.000Z",
    "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
    "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03641",
      "authors": [
        {
          "_id": "67f34f5dfb6d8a613926ac2b",
          "name": "Wulin Xie",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2c",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2d",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2e",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2f",
          "name": "Bingyan Nie",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac30",
          "name": "Hongkai Chen",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac31",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac32",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac33",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T17:59:55.000Z",
      "submittedOnDailyAt": "2025-04-07T02:38:07.467Z",
      "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.",
      "upvotes": 5,
      "discussionId": "67f34f64fb6d8a613926ada9",
      "projectPage": "https://mme-unify.github.io/",
      "githubRepo": "https://github.com/MME-Benchmarks/MME-Unify"
    },
    "publishedAt": "2025-04-04T13:59:55.000Z",
    "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
    "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03641.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02949",
      "authors": [
        {
          "_id": "67f350a5e11bd4b05575a831",
          "name": "Xianwei Zhuang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a832",
          "name": "Yuxin Xie",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a833",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a834",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a835",
          "name": "Liming Liang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a836",
          "name": "Jinghan Ru",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a837",
          "name": "Yuguo Yin",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a838",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T18:06:28.000Z",
      "submittedOnDailyAt": "2025-04-07T02:42:39.671Z",
      "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
      "upvotes": 5,
      "discussionId": "67f350a9e11bd4b05575a921",
      "githubRepo": "https://github.com/VARGPT-family/VARGPT-v1.1",
      "ai_keywords": [
        "unified visual autoregressive model",
        "next-token prediction",
        "next-scale generation",
        "iterative visual instruction tuning",
        "reinforcement learning",
        "Direct Preference Optimization (DPO)",
        "visual-generative instruction pairs",
        "Qwen2",
        "image generation resolution",
        "emergent image editing capabilities",
        "multimodal understanding",
        "text-to-image instruction-following tasks",
        "comprehension and generation metrics",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-03T14:06:28.000Z",
    "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
    "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03601",
      "authors": [
        {
          "_id": "67f36505e11bd4b05579afbf",
          "name": "Akshara Prabhakar",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc0",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc1",
          "name": "Weiran Yao",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc2",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc3",
          "name": "Ming Zhu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc4",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc5",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc6",
          "name": "Tulika Awalgaonkar",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc7",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc8",
          "name": "Thai Hoang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc9",
          "name": "Juan Carlos Niebles",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afca",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcb",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcc",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcd",
          "name": "Caiming Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T17:13:57.000Z",
      "submittedOnDailyAt": "2025-04-07T04:10:25.529Z",
      "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
      "upvotes": 2,
      "discussionId": "67f36507e11bd4b05579b020",
      "ai_keywords": [
        "agentic pipeline",
        "task blueprints",
        "ground-truth actions",
        "LLM reviewers",
        "iterative feedback loops",
        "simulated human-agent interplay",
        "xLAM-2-fc-r series",
        "$\\tau$-bench",
        "BFCL benchmarks",
        "multi-turn settings",
        "verified blueprint-to-details approach"
      ]
    },
    "publishedAt": "2025-04-04T13:13:57.000Z",
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
    "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6591
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03011",
      "authors": [
        {
          "_id": "67f35d8bdb1a843e1ceff38f",
          "name": "Junying Wang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff390",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff391",
          "name": "Xin Sun",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff392",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff393",
          "name": "Zhixin Shu",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff394",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff395",
          "name": "Jimei Yang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff396",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff397",
          "name": "Tuanfeng Y. Wang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff398",
          "name": "Simon S. Chen",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff399",
          "name": "Ulrich Neumann",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff39a",
          "name": "Jae Shin Yoon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T20:10:50.000Z",
      "submittedOnDailyAt": "2025-04-07T03:39:50.539Z",
      "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.",
      "upvotes": 2,
      "discussionId": "67f35d91db1a843e1ceff47c",
      "ai_keywords": [
        "diffusion models",
        "image prior",
        "coarse-to-fine framework",
        "temporal lighting model",
        "lighting cycle consistency",
        "spatio-temporal feature blending",
        "guided refinement"
      ]
    },
    "publishedAt": "2025-04-03T16:10:50.000Z",
    "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
    "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6591
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03536",
      "authors": [
        {
          "_id": "67f364118188d683931bec4a",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4b",
          "name": "Runqi Ouyang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4c",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4d",
          "user": {
            "_id": "656e9b562cd7a3e348011d26",
            "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
            "isPro": false,
            "fullname": "Zheng Zhu",
            "user": "ZhengZhu",
            "type": "user"
          },
          "name": "Zheng Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-07T05:35:15.515Z",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4e",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4f",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec50",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec51",
          "name": "Lihong Liu",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec52",
          "name": "Xingang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T15:35:14.000Z",
      "submittedOnDailyAt": "2025-04-07T04:06:19.815Z",
      "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce HumanDreamer-X, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, HumanFixer is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.",
      "upvotes": 1,
      "discussionId": "67f364138188d683931beca2",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "HumanDreamer-X",
        "HumanFixer",
        "attention modulation",
        "PSNR"
      ]
    },
    "publishedAt": "2025-04-04T11:35:14.000Z",
    "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
    "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce HumanDreamer-X, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, HumanFixer is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6591
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02402",
      "authors": [
        {
          "_id": "67f0a09a2c873f5ba90cd14a",
          "user": {
            "_id": "67ee782979018bf61e2522a4",
            "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
            "isPro": false,
            "fullname": "HaoYin",
            "user": "yyzqy",
            "type": "user"
          },
          "name": "Hao Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:07.967Z",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14b",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14c",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14d",
          "name": "Xudong XU",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14e",
          "name": "Lu Zhang",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14f",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd150",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd151",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd152",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T08:51:17.000Z",
      "submittedOnDailyAt": "2025-04-07T05:44:25.380Z",
      "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
      "submittedOnDailyBy": {
        "_id": "67ee782979018bf61e2522a4",
        "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
        "isPro": false,
        "fullname": "HaoYin",
        "user": "yyzqy",
        "type": "user"
      },
      "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
      "upvotes": 1,
      "discussionId": "67f0a09e2c873f5ba90cd26c",
      "projectPage": "https://yyzq1.github.io/EvMic/",
      "githubRepo": "https://github.com/yyzq1/EvMic",
      "ai_keywords": [
        "event camera",
        "high-frequency signals",
        "event stream",
        "spatial-temporal information",
        "novel simulation pipeline",
        "Mamba",
        "spatial aggregation block",
        "laser matrix"
      ]
    },
    "publishedAt": "2025-04-03T04:51:17.000Z",
    "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
    "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02402.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ee782979018bf61e2522a4",
      "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
      "fullname": "HaoYin",
      "name": "yyzqy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24310",
      "authors": [
        {
          "_id": "67ecb513c8ae971f9ad15bd1",
          "user": {
            "_id": "6478fc1512ae749b62ebbbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
            "isPro": false,
            "fullname": "Alok Abhishek",
            "user": "alokabhishek",
            "type": "user"
          },
          "name": "Alok Abhishek",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-02T03:59:10.671Z",
          "hidden": false
        },
        {
          "_id": "67ecb513c8ae971f9ad15bd2",
          "name": "Lisa Erickson",
          "hidden": false
        },
        {
          "_id": "67ecb513c8ae971f9ad15bd3",
          "user": {
            "_id": "657372396da136b50f5489a0",
            "avatarUrl": "/avatars/57a693058e9a05a2c32f02bab1d8e819.svg",
            "isPro": false,
            "fullname": "Tushar Bandopadhyay",
            "user": "tbandopa",
            "type": "user"
          },
          "name": "Tushar Bandopadhyay",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-02T08:13:38.947Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:56:52.000Z",
      "submittedOnDailyAt": "2025-04-07T03:47:34.752Z",
      "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "6478fc1512ae749b62ebbbd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
        "isPro": false,
        "fullname": "Alok Abhishek",
        "user": "alokabhishek",
        "type": "user"
      },
      "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
      "upvotes": 1,
      "discussionId": "67ecb513c8ae971f9ad15c02",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Bias benchmark",
        "Bias",
        "Ethics",
        "Fairness",
        "Factuality",
        "Demographic biases",
        "Cognitive biases",
        "Social biases",
        "Ethical reasoning",
        "Group fairness",
        "Factuality related misinformation risk",
        "Equitable behavior",
        "Responsible AI evaluation",
        "Critical decision making systems",
        "Scalable methodology",
        "Statistically rigorous methodology",
        "Diagnose factors driving biases",
        "Mitigation strategies",
        "Socially responsible AI models",
        "Ethically aligned AI models"
      ]
    },
    "publishedAt": "2025-03-31T12:56:52.000Z",
    "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
    "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24310.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478fc1512ae749b62ebbbd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
      "fullname": "Alok Abhishek",
      "name": "alokabhishek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  }
]