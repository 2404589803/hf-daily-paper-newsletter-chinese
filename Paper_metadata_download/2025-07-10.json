[
  {
    "paper": {
      "id": "2507.07095",
      "authors": [
        {
          "_id": "686f2579d938c25d68441b43",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b44",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b45",
          "user": {
            "_id": "6853b71ec1be83a29eb5ba36",
            "avatarUrl": "/avatars/ae205c2ec2c421a0d7851755b4f123a2.svg",
            "isPro": false,
            "fullname": "Minyue Dai",
            "user": "Jixi111",
            "type": "user"
          },
          "name": "Minyue Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:19.845Z",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b46",
          "name": "Runyi Yu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b47",
          "name": "Lixing Xiao",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b48",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b49",
          "name": "Junting Dong",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4a",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4b",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T17:52:04.000Z",
      "submittedOnDailyAt": "2025-07-10T01:12:54.854Z",
      "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
      "upvotes": 24,
      "discussionId": "686f2579d938c25d68441b4c",
      "ai_summary": "A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.",
      "ai_keywords": [
        "MotionMillion",
        "MotionMillion-Eval",
        "zero-shot motion generation",
        "scalable architecture"
      ]
    },
    "publishedAt": "2025-07-09T13:52:04.000Z",
    "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
    "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06448",
      "authors": [
        {
          "_id": "686f326dd938c25d68441b6a",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6b",
          "name": "Xuehang Guo",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6c",
          "name": "Sofia Stoica",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6d",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:17.834Z",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6e",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6f",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b70",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b71",
          "name": "Yangyi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b72",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b73",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b74",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:22:34.000Z",
      "submittedOnDailyAt": "2025-07-10T01:56:27.555Z",
      "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "628d7265db4cd1d1717c884f",
        "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
        "isPro": false,
        "fullname": "Zhenhailong Wang",
        "user": "mikewang",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
      "upvotes": 21,
      "discussionId": "686f326dd938c25d68441b75",
      "projectPage": "https://mikewangwzhl.github.io/PAPO",
      "githubRepo": "https://github.com/MikeWangWZHL/PAPO",
      "ai_summary": "Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large Language Models (LLMs)",
        "multimodal reasoning tasks",
        "Perception-Aware Policy Optimization (PAPO)",
        "GRPO",
        "Implicit Perception Loss",
        "KL divergence",
        "Double Entropy Loss",
        "visually grounded reasoning"
      ]
    },
    "publishedAt": "2025-07-08T19:22:34.000Z",
    "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628d7265db4cd1d1717c884f",
      "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
      "fullname": "Zhenhailong Wang",
      "name": "mikewang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06920",
      "authors": [
        {
          "_id": "686f1da1d938c25d68441b1b",
          "user": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "isPro": false,
            "fullname": "Zihan Ma",
            "user": "MichaelErchi",
            "type": "user"
          },
          "name": "Zihan Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:33.413Z",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1c",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1d",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1e",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1f",
          "name": "Minnan Luo",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b20",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b21",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T14:58:47.000Z",
      "submittedOnDailyAt": "2025-07-10T00:31:10.862Z",
      "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
      "submittedOnDailyBy": {
        "_id": "677e869467f3bb8d8215eec6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
        "isPro": false,
        "fullname": "Zihan Ma",
        "user": "MichaelErchi",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
      "upvotes": 15,
      "discussionId": "686f1da1d938c25d68441b22",
      "ai_summary": "A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.",
      "ai_keywords": [
        "large language models",
        "code-generation",
        "HumanEval",
        "LiveCodeBench",
        "test-case generation",
        "multi-dimensional metrics",
        "human-LLM collaboration",
        "SAGA",
        "TCGBench",
        "reinforcement learning frameworks",
        "verifiable rewards",
        "RLVR",
        "verifier accuracy",
        "adversarial test synthesis",
        "adaptive benchmark integration"
      ]
    },
    "publishedAt": "2025-07-09T10:58:47.000Z",
    "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
    "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06920.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677e869467f3bb8d8215eec6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
      "fullname": "Zihan Ma",
      "name": "MichaelErchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06457",
      "authors": [
        {
          "_id": "686f2371d938c25d68441b36",
          "name": "Dustin Wang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b37",
          "user": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "isPro": false,
            "fullname": "Rui-Jie Zhu",
            "user": "ridger",
            "type": "user"
          },
          "name": "Rui-Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:30.737Z",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b38",
          "name": "Steven Abreu",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b39",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3a",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3b",
          "name": "Yuqi Pan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3c",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3d",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3f",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b40",
          "name": "Jason Eshraghian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:54:11.000Z",
      "submittedOnDailyAt": "2025-07-10T02:04:35.136Z",
      "title": "A Systematic Analysis of Hybrid Linear Attention",
      "submittedOnDailyBy": {
        "_id": "63ff09f24852102d4871c19c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
        "isPro": false,
        "fullname": "Rui-Jie Zhu",
        "user": "ridger",
        "type": "user"
      },
      "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
      "upvotes": 12,
      "discussionId": "686f2371d938c25d68441b41",
      "projectPage": "https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e",
      "ai_summary": "Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.",
      "ai_keywords": [
        "quadratic complexity",
        "linear attention mechanisms",
        "full attention layers",
        "hybrid architectures",
        "vector recurrences",
        "gating mechanisms",
        "recall performance",
        "language modeling",
        "recall tasks",
        "selective gating",
        "hierarchical recurrence",
        "controlled forgetting",
        "HGRN-2",
        "GatedDeltaNet"
      ]
    },
    "publishedAt": "2025-07-08T19:54:11.000Z",
    "title": "A Systematic Analysis of Hybrid Linear Attention",
    "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ff09f24852102d4871c19c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
      "fullname": "Rui-Jie Zhu",
      "name": "ridger",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06804",
      "authors": [
        {
          "_id": "686f1f4dd938c25d68441b24",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b25",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b26",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b27",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b28",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b29",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b2a",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T22:38:49.000Z",
      "submittedOnDailyAt": "2025-07-10T00:33:32.493Z",
      "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
      "submittedOnDailyBy": {
        "_id": "62ffa3f8311cad266f9af236",
        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
        "isPro": false,
        "fullname": "Zhenwen Liang",
        "user": "invokerliang",
        "type": "user"
      },
      "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
      "upvotes": 8,
      "discussionId": "686f1f4ed938c25d68441b2b",
      "ai_summary": "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.",
      "ai_keywords": [
        "Automated Theorem Proving",
        "Large Language Models",
        "formal proving",
        "informal reasoning",
        "PutnamBench",
        "subgoal lemmas",
        "Reasoner",
        "Prover",
        "modular design",
        "end-to-end training",
        "IMO problems"
      ]
    },
    "publishedAt": "2025-07-07T18:38:49.000Z",
    "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
    "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ffa3f8311cad266f9af236",
      "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
      "fullname": "Zhenwen Liang",
      "name": "invokerliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05687",
      "authors": [
        {
          "_id": "686e2c00a5f0f70d9de40c8c",
          "name": "Shangzhan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8d",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8e",
          "name": "Ye He",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8f",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c90",
          "user": {
            "_id": "62ccd26d376917c022420a46",
            "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
            "isPro": false,
            "fullname": "Qi Shi",
            "user": "qshi",
            "type": "user"
          },
          "name": "Qi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:03.237Z",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c91",
          "name": "Jianling Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c92",
          "name": "Yonggang Hu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c93",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c94",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c95",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c96",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T05:38:24.000Z",
      "submittedOnDailyAt": "2025-07-10T00:44:20.590Z",
      "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "62ccd26d376917c022420a46",
        "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
        "isPro": false,
        "fullname": "Qi Shi",
        "user": "qshi",
        "type": "user"
      },
      "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
      "upvotes": 6,
      "discussionId": "686e2c00a5f0f70d9de40c97"
    },
    "publishedAt": "2025-07-08T01:38:24.000Z",
    "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
    "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05687.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ccd26d376917c022420a46",
      "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
      "fullname": "Qi Shi",
      "name": "qshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.24044",
      "authors": [
        {
          "_id": "68681d82213f123a1f88b973",
          "user": {
            "_id": "683daabc402acb18654e4674",
            "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
            "isPro": false,
            "fullname": "Sicong Jiang",
            "user": "Max2045",
            "type": "user"
          },
          "name": "Sicong Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:40.893Z",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b974",
          "name": "Zilin Huang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b975",
          "name": "Kangan Qian",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b976",
          "name": "Ziang Luo",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b977",
          "name": "Tianze Zhu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b978",
          "name": "Yang Zhong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b979",
          "name": "Yihong Tang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97a",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97b",
          "name": "Yunlong Wang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97c",
          "name": "Siwen Jiao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97d",
          "name": "Hao Ye",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97e",
          "name": "Zihao Sheng",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97f",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b980",
          "name": "Tuopu Wen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b981",
          "name": "Zheng Fu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b982",
          "name": "Sikai Chen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b983",
          "name": "Kun Jiang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b984",
          "name": "Diange Yang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b985",
          "name": "Seongjin Choi",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b986",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T16:50:02.000Z",
      "submittedOnDailyAt": "2025-07-10T01:10:17.895Z",
      "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "683daabc402acb18654e4674",
        "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
        "isPro": false,
        "fullname": "Sicong Jiang",
        "user": "Max2045",
        "type": "user"
      },
      "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
      "upvotes": 6,
      "discussionId": "68681d82213f123a1f88b987",
      "githubRepo": "https://github.com/JohnsonJiang1996/Awesome-VLA4AD",
      "ai_summary": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.",
      "ai_keywords": [
        "multimodal large language models",
        "Vision-Language-Action",
        "VLA",
        "VLA for Autonomous Driving",
        "VLA4AD",
        "explainer",
        "reasoning-centric models",
        "autonomous driving",
        "driving safety",
        "accuracy",
        "explanation quality",
        "robustness",
        "real-time efficiency",
        "formal verification",
        "interpretable socially aligned autonomous vehicles"
      ],
      "githubStars": 172
    },
    "publishedAt": "2025-06-30T12:50:02.000Z",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683daabc402acb18654e4674",
      "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
      "fullname": "Sicong Jiang",
      "name": "Max2045",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07105",
      "authors": [
        {
          "_id": "686f5cbad938c25d68441bb2",
          "name": "Yushen Zuo",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb3",
          "name": "Qi Zheng",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb4",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb5",
          "name": "Xinrui Jiang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb6",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb7",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb8",
          "name": "Yide Zhang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb9",
          "name": "Gengchen Mai",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bba",
          "name": "Lihong V. Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbb",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbc",
          "name": "Xiaoyu Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbd",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbe",
          "name": "Zhengzhong Tu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
      ],
      "publishedAt": "2025-07-09T17:59:19.000Z",
      "submittedOnDailyAt": "2025-07-10T04:56:36.746Z",
      "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
      "upvotes": 4,
      "discussionId": "686f5cbbd938c25d68441bbf",
      "projectPage": "https://4kagent.github.io/",
      "githubRepo": "https://github.com/taco-group/4KAgent",
      "ai_summary": "4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.",
      "ai_keywords": [
        "agentic super-resolution",
        "Profiling",
        "Perception Agent",
        "vision-language models",
        "image quality assessment",
        "Restoration Agent",
        "recursive execution-reflection",
        "quality-driven mixture-of-experts",
        "face restoration pipeline",
        "NIQE",
        "MUSIQ",
        "PSNR",
        "low-level vision tasks",
        "autonomous agents"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-09T13:59:19.000Z",
    "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
    "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07105.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07017",
      "authors": [
        {
          "_id": "686f39e8d938c25d68441b89",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:04.799Z",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8a",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8b",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8c",
          "name": "Taoran Liang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8d",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8f",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b90",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b91",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b92",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b93",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b94",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b95",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T16:45:48.000Z",
      "submittedOnDailyAt": "2025-07-10T02:29:04.666Z",
      "title": "First Return, Entropy-Eliciting Explore",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
      "upvotes": 4,
      "discussionId": "686f39e8d938c25d68441b96",
      "ai_summary": "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.",
      "ai_keywords": [
        "Reinforcement Learning from Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "FR3E",
        "First Return",
        "Entropy-Eliciting Explore",
        "structured exploration",
        "high-uncertainty decision points",
        "reasoning trajectories",
        "targeted rollouts",
        "semantically grounded intermediate feedback",
        "mathematical reasoning benchmarks",
        "AIME24",
        "stable training",
        "coherent responses",
        "fully correct trajectories"
      ]
    },
    "publishedAt": "2025-07-09T12:45:48.000Z",
    "title": "First Return, Entropy-Eliciting Explore",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06853",
      "authors": [
        {
          "_id": "686f4142d938c25d68441b98",
          "user": {
            "_id": "64e84ec6d41a68b065bf78a7",
            "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
            "isPro": false,
            "fullname": "Liang Wang",
            "user": "AzureLeon1",
            "type": "user"
          },
          "name": "Liang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:02.989Z",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b99",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9a",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9b",
          "name": "Zhenyi Zhong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9c",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9d",
          "name": "Pengju Wang",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9e",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9f",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba0",
          "name": "Shu Wu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba1",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
      ],
      "publishedAt": "2025-07-09T13:57:20.000Z",
      "submittedOnDailyAt": "2025-07-10T03:05:48.871Z",
      "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "64e84ec6d41a68b065bf78a7",
        "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
        "isPro": false,
        "fullname": "Liang Wang",
        "user": "AzureLeon1",
        "type": "user"
      },
      "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
      "upvotes": 2,
      "discussionId": "686f4142d938c25d68441ba2",
      "ai_summary": "DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.",
      "ai_keywords": [
        "diffusion models",
        "SE(3)-equivariant architecture",
        "Diffusion Molecule Transformer",
        "SpecFormer",
        "spectral encoder",
        "multi-modal spectral reasoning",
        "joint 2D/3D generative modeling",
        "de novo molecular structure elucidation"
      ]
    },
    "publishedAt": "2025-07-09T09:57:20.000Z",
    "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
    "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e84ec6d41a68b065bf78a7",
      "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
      "fullname": "Liang Wang",
      "name": "AzureLeon1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05455",
      "authors": [
        {
          "_id": "686f5ec6d938c25d68441bc1",
          "name": "Ashima Suvarna",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc2",
          "name": "Christina Chance",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc3",
          "name": "Karolina Naranjo",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc4",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc5",
          "name": "Sophie Hao",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc6",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc7",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T20:15:18.000Z",
      "submittedOnDailyAt": "2025-07-10T05:04:25.096Z",
      "title": "ModelCitizens: Representing Community Voices in Online Safety",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
      "upvotes": 2,
      "discussionId": "686f5ec7d938c25d68441bc8",
      "ai_summary": "A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.",
      "ai_keywords": [
        "MODELCITIZENS",
        "LLM-generated conversational scenarios",
        "OpenAI Moderation API",
        "GPT-o4-mini",
        "LLAMACITIZEN-8B",
        "GEMMACITIZEN-12B",
        "LLaMA-based models",
        "Gemma-based models",
        "in-distribution evaluations",
        "community-informed annotation",
        "inclusive content moderation"
      ]
    },
    "publishedAt": "2025-07-07T16:15:18.000Z",
    "title": "ModelCitizens: Representing Community Voices in Online Safety",
    "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06260",
      "authors": [
        {
          "_id": "686f14e3d938c25d68441af8",
          "name": "Satyapriya Krishna",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441af9",
          "name": "Ninareh Mehrabi",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afa",
          "name": "Abhinav Mohanty",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afb",
          "name": "Matteo Memelli",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afc",
          "name": "Vincent Ponzo",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afd",
          "name": "Payal Motwani",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afe",
          "name": "Rahul Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:33:35.000Z",
      "submittedOnDailyAt": "2025-07-10T00:10:50.485Z",
      "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
      "submittedOnDailyBy": {
        "_id": "6186fef1b1085ab638324e7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
        "isPro": false,
        "fullname": "Satya",
        "user": "skrishna",
        "type": "user"
      },
      "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
      "upvotes": 0,
      "discussionId": "686f14e4d938c25d68441aff"
    },
    "publishedAt": "2025-07-07T09:33:35.000Z",
    "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
    "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06260.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6186fef1b1085ab638324e7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
      "fullname": "Satya",
      "name": "skrishna",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]