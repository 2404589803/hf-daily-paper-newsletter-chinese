[
  {
    "paper": {
      "id": "2509.25541",
      "authors": [
        {
          "_id": "68dc90f34159d1f2418f9abf",
          "name": "Qinsi Wang",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac0",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac1",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac2",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac3",
          "name": "Yueqian Lin",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac4",
          "name": "Yiran Chen",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac5",
          "name": "Hai Helen Li",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac6",
          "name": "Kun Wan",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac7",
          "name": "Wentian Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T21:55:55.000Z",
      "submittedOnDailyAt": "2025-10-01T00:55:06.135Z",
      "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
      "upvotes": 55,
      "discussionId": "68dc90f34159d1f2418f9ac8",
      "githubRepo": "https://github.com/wangqinsi1/Vision-Zero",
      "ai_summary": "Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.",
      "ai_keywords": [
        "reinforcement learning",
        "vision-language models",
        "strategic self-play framework",
        "self-play",
        "reinforcement learning with verifiable rewards",
        "Iterative Self-Play Policy Optimization"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-09-29T17:55:55.000Z",
    "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
    "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25760",
      "authors": [
        {
          "_id": "68dc90b84159d1f2418f9aa7",
          "name": "Zhepei Wei",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aa8",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aa9",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aaa",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aab",
          "name": "Rulin Shao",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aac",
          "name": "Sean Chen",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aad",
          "name": "Mohammad Kachuee",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aae",
          "name": "Teja Gollapudi",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aaf",
          "name": "Tony Liao",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab0",
          "name": "Nicolas Scheffer",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab1",
          "name": "Rakesh Wanga",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab2",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab3",
          "name": "Yu Meng",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab4",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab5",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T04:25:17.000Z",
      "submittedOnDailyAt": "2025-10-01T00:56:26.188Z",
      "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6526307af06ac0cf9a922e86",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
        "isPro": false,
        "fullname": "Zhepei Wei",
        "user": "weizhepei",
        "type": "user"
      },
      "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
      "upvotes": 30,
      "discussionId": "68dc90b84159d1f2418f9ab6",
      "ai_summary": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "hallucination",
        "untruthful responses",
        "parametric knowledge",
        "truthfulness",
        "reinforcement learning",
        "RL",
        "GRPO",
        "ternary reward",
        "abstention",
        "accuracy-driven methods",
        "supervised fine-tuning",
        "binary reward",
        "knowledge-intensive benchmarks",
        "Qwen",
        "Llama",
        "retrieval",
        "non-retrieval setups",
        "ablation study"
      ]
    },
    "publishedAt": "2025-09-30T00:25:17.000Z",
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6526307af06ac0cf9a922e86",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
      "fullname": "Zhepei Wei",
      "name": "weizhepei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26536",
      "authors": [
        {
          "_id": "68dc947d4159d1f2418f9af3",
          "name": "Yida Xue",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af4",
          "name": "Mingjun Mao",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af5",
          "name": "Xiangyuan Ru",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af6",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af7",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af8",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9af9",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9afa",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9afb",
          "name": "Xinyu An",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9afc",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9afd",
          "name": "Ying Chen",
          "hidden": false
        },
        {
          "_id": "68dc947d4159d1f2418f9afe",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png"
      ],
      "publishedAt": "2025-09-30T17:09:32.000Z",
      "submittedOnDailyAt": "2025-10-01T01:11:27.179Z",
      "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
      "upvotes": 23,
      "discussionId": "68dc947d4159d1f2418f9aff",
      "projectPage": "https://oceangpt.github.io/OceanGym/",
      "githubRepo": "https://github.com/OceanGPT/OceanGym",
      "ai_summary": "OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.",
      "ai_keywords": [
        "Multi-modal Large Language Models",
        "MLLMs",
        "optical data",
        "sonar data",
        "sequential decision-making",
        "embodied AI",
        "autonomous ocean underwater vehicles"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-09-30T13:09:32.000Z",
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "submitterOrganization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25154",
      "authors": [
        {
          "_id": "68dc97c24159d1f2418f9b54",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b55",
          "name": "Zhen Tan",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b56",
          "name": "Chengshuai Zhao",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b57",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b58",
          "name": "Baixiang Huang",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b59",
          "name": "Pingchuan Ma",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b5a",
          "name": "Abdullah Alnaibari",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b5b",
          "name": "Kai Shu",
          "hidden": false
        },
        {
          "_id": "68dc97c24159d1f2418f9b5c",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:54:57.000Z",
      "submittedOnDailyAt": "2025-10-01T01:24:59.807Z",
      "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
      "submittedOnDailyBy": {
        "_id": "6474e1afb68461d5cf7c41cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
        "isPro": false,
        "fullname": "Dawei Li",
        "user": "wjldw",
        "type": "user"
      },
      "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\nJ-Detector, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of J-Detector\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.",
      "upvotes": 21,
      "discussionId": "68dc97c24159d1f2418f9b5d",
      "ai_summary": "J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.",
      "ai_keywords": [
        "Large Language Model",
        "judgment detection",
        "neural detector",
        "linguistic features",
        "LLM-enhanced features",
        "judgment scores",
        "candidate content",
        "biases",
        "detectability"
      ]
    },
    "publishedAt": "2025-09-29T13:54:57.000Z",
    "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
    "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\nJ-Detector, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of J-Detector\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25154.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "submitterOrganization": {
      "_id": "6830bdb6802db9cd255151d8",
      "name": "DMML",
      "fullname": "Data Mining and Machine Learning lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6474e1afb68461d5cf7c41cc/jxoUTsOe3Yhnz3Zng3LFh.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25182",
      "authors": [
        {
          "_id": "68dc88d34159d1f2418f9a36",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a37",
          "name": "Wenkun He",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a38",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a39",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3a",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3b",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3c",
          "name": "Dongyun Zou",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3d",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3e",
          "name": "Zhekai Zhang",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3f",
          "name": "Muyang Li",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a40",
          "name": "Haocheng Xi",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a41",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a42",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a43",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a44",
          "name": "Han Cai",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
      ],
      "publishedAt": "2025-09-29T17:59:31.000Z",
      "submittedOnDailyAt": "2025-10-01T00:21:53.066Z",
      "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
      "submittedOnDailyBy": {
        "_id": "650e2b14c945dfc9386a7e28",
        "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
        "isPro": false,
        "fullname": "Han Cai",
        "user": "han-cai",
        "type": "user"
      },
      "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
      "upvotes": 19,
      "discussionId": "68dc88d34159d1f2418f9a45",
      "ai_summary": "DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.",
      "ai_keywords": [
        "DC-VideoGen",
        "video diffusion model",
        "deep compression latent space",
        "lightweight fine-tuning",
        "Deep Compression Video Autoencoder",
        "chunk-causal temporal design",
        "AE-Adapt-V",
        "Wan-2.1-14B model",
        "inference latency",
        "high-resolution video generation"
      ]
    },
    "publishedAt": "2025-09-29T13:59:31.000Z",
    "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
    "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e2b14c945dfc9386a7e28",
      "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
      "fullname": "Han Cai",
      "name": "han-cai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "submitterOrganization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25758",
      "authors": [
        {
          "_id": "68dc8a1f4159d1f2418f9a64",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "68dc8a1f4159d1f2418f9a65",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "68dc8a1f4159d1f2418f9a66",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T04:23:43.000Z",
      "submittedOnDailyAt": "2025-10-01T00:26:22.625Z",
      "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
      "submittedOnDailyBy": {
        "_id": "64587be872b60ae7a3817858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
        "isPro": false,
        "fullname": "Minbyul Jeong",
        "user": "Minbyul",
        "type": "user"
      },
      "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
      "upvotes": 16,
      "discussionId": "68dc8a1f4159d1f2418f9a67",
      "ai_summary": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.",
      "ai_keywords": [
        "supervised fine-tuning",
        "reinforcement learning",
        "circuit analysis",
        "attention heads",
        "structured reasoning",
        "Qwen families",
        "DeepSeek-distilled model",
        "group relative policy optimization",
        "think on/off models",
        "ablation analysis",
        "qualitative analysis",
        "over-thinking failure modes"
      ]
    },
    "publishedAt": "2025-09-30T00:23:43.000Z",
    "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
    "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "6621bc39e774284ec1742ab8",
      "name": "KoreaUniversity",
      "fullname": "Korea University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26625",
      "authors": [
        {
          "_id": "68dc87ff4159d1f2418f9a1a",
          "name": "Junlin Han",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1c",
          "name": "David Fan",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1d",
          "name": "Yufan Ren",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1e",
          "name": "Koustuv Sinha",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1f",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a20",
          "name": "Filippos Kokkinos",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:57:44.000Z",
      "submittedOnDailyAt": "2025-10-01T00:18:03.860Z",
      "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
      "submittedOnDailyBy": {
        "_id": "636e6ee287545ca5a136b4c3",
        "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
        "isPro": false,
        "fullname": "Junlin Han",
        "user": "Junlinh",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
      "upvotes": 15,
      "discussionId": "68dc87ff4159d1f2418f9a21",
      "ai_summary": "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "visual priors",
        "latent visual capabilities",
        "multimodal data",
        "visual tasks",
        "implicit knowledge",
        "visual world",
        "perception priors",
        "reasoning priors",
        "pre-training",
        "reasoning-centric data",
        "transferable",
        "visual reasoning",
        "perception ability",
        "vision encoder",
        "visual instruction tuning",
        "text describing the visual world",
        "vision-aware LLMs",
        "data-centric recipe",
        "pre-training",
        "visual alignment",
        "supervised multimodal fine-tuning",
        "Multi-Level Existence Bench",
        "MLE-Bench"
      ]
    },
    "publishedAt": "2025-09-30T13:57:44.000Z",
    "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
    "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636e6ee287545ca5a136b4c3",
      "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
      "fullname": "Junlin Han",
      "name": "Junlinh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24002",
      "authors": [
        {
          "_id": "68dc9eff4159d1f2418f9b97",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b98",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b99",
          "name": "Xinyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9a",
          "name": "Lingjun Chen",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9b",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9c",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9d",
          "name": "Yiran Zhao",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9e",
          "name": "Fanshi Zhang",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9b9f",
          "name": "Yaoqi Ye",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba0",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba1",
          "name": "Zirui Wang",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba2",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba3",
          "name": "Yufan Yang",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba4",
          "name": "Arvin Xu",
          "hidden": false
        },
        {
          "_id": "68dc9eff4159d1f2418f9ba5",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T17:53:27.000Z",
      "submittedOnDailyAt": "2025-10-01T01:58:54.337Z",
      "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
      "submittedOnDailyBy": {
        "_id": "626d268d5f7327906f05cad1",
        "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
        "isPro": true,
        "fullname": "Zijian Wu",
        "user": "Jakumetsu",
        "type": "user"
      },
      "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of 127\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution\nturns and 17.4 tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
      "upvotes": 14,
      "discussionId": "68dc9eff4159d1f2418f9ba6",
      "projectPage": "https://mcpmark.ai/",
      "githubRepo": "https://github.com/eval-sys/mcpmark",
      "ai_summary": "MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.",
      "ai_keywords": [
        "MCP",
        "LLMs",
        "general agents",
        "MCP benchmarks",
        "MCPMark",
        "high-quality tasks",
        "domain experts",
        "AI agents",
        "initial state",
        "programmatic script",
        "automatic verification",
        "CRUD operations",
        "minimal agent framework",
        "tool-calling loop",
        "gpt-5-medium",
        "claude-sonnet-4",
        "o3",
        "pass@1",
        "pass^4",
        "execution turns",
        "tool calls"
      ],
      "githubStars": 170
    },
    "publishedAt": "2025-09-28T13:53:27.000Z",
    "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
    "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of 127\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution\nturns and 17.4 tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626d268d5f7327906f05cad1",
      "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
      "fullname": "Zijian Wu",
      "name": "Jakumetsu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26490",
      "authors": [
        {
          "_id": "68dc8ca34159d1f2418f9a7e",
          "name": "Wei He",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a7f",
          "name": "Yueqing Sun",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a80",
          "name": "Hongyan Hao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a81",
          "name": "Xueyuan Hao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a82",
          "name": "Zhikang Xia",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a83",
          "name": "Qi Gu",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a84",
          "name": "Chengcheng Han",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a85",
          "name": "Dengchang Zhao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a86",
          "name": "Hui Su",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a87",
          "name": "Kefeng Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a88",
          "name": "Man Gao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a89",
          "name": "Xi Su",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8a",
          "name": "Xiaodong Cai",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8b",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8c",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8d",
          "name": "Yunke Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T16:33:49.000Z",
      "submittedOnDailyAt": "2025-10-01T00:38:43.265Z",
      "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
      "submittedOnDailyBy": {
        "_id": "66ecee857264238429a1211f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
        "isPro": false,
        "fullname": "Wei He",
        "user": "hewei2001",
        "type": "user"
      },
      "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
      "upvotes": 13,
      "discussionId": "68dc8ca44159d1f2418f9a8e",
      "projectPage": "https://vitabench.github.io/",
      "githubRepo": "https://github.com/meituan/vitabench",
      "ai_summary": "VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.",
      "ai_keywords": [
        "LLM-based agents",
        "VitaBench",
        "interactive tasks",
        "real-world settings",
        "food delivery",
        "in-store consumption",
        "online travel services",
        "life-serving simulation environment",
        "domain-specific policies",
        "flexible composition",
        "cross-scenario tasks",
        "single-scenario tasks",
        "real user requests",
        "temporal dimensions",
        "spatial dimensions",
        "complex tool sets",
        "ambiguous instructions",
        "shifting user intent",
        "multi-turn conversations",
        "rubric-based sliding window evaluator",
        "stochastic interactions"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-30T12:33:49.000Z",
    "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
    "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26490.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ecee857264238429a1211f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
      "fullname": "Wei He",
      "name": "hewei2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26488",
      "authors": [
        {
          "_id": "68dc88d74159d1f2418f9a47",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a48",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a49",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a4a",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a4b",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
      ],
      "publishedAt": "2025-09-30T16:32:52.000Z",
      "submittedOnDailyAt": "2025-10-01T00:34:29.943Z",
      "title": "dParallel: Learnable Parallel Decoding for dLLMs",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": false,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
      "upvotes": 13,
      "discussionId": "68dc88d74159d1f2418f9a4c",
      "githubRepo": "https://github.com/czg1225/dParallel",
      "ai_summary": "dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.",
      "ai_keywords": [
        "diffusion large language models",
        "dLLMs",
        "autoregressive generation",
        "parallel token prediction",
        "parallel decoding",
        "masked tokens",
        "certainty-forcing distillation",
        "LLaDA-8B-Instruct",
        "GSM8K",
        "MBPP benchmark"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-30T12:32:52.000Z",
    "title": "dParallel: Learnable Parallel Decoding for dLLMs",
    "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26488.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "submitterOrganization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26231",
      "authors": [
        {
          "_id": "68dc9cd34159d1f2418f9b8e",
          "name": "Jiayi Guo",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b8f",
          "name": "Chuanhao Yan",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b90",
          "name": "Xingqian Xu",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b91",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b92",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b93",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "68dc9cd34159d1f2418f9b94",
          "name": "Humphrey Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T13:27:03.000Z",
      "submittedOnDailyAt": "2025-10-01T01:48:17.637Z",
      "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
      "submittedOnDailyBy": {
        "_id": "6417c111dce1e4c0229ec182",
        "avatarUrl": "/avatars/de65da1f02b136b1af57862d9d0c48f2.svg",
        "isPro": false,
        "fullname": "Jiayi Guo",
        "user": "JiayiGuo821",
        "type": "user"
      },
      "summary": "Ensuring precise multimodal alignment between diffusion-generated images and\ninput prompts has been a long-standing challenge. Earlier works finetune\ndiffusion weight using high-quality preference data, which tends to be limited\nand difficult to scale up. Recent editing-based methods further refine local\nregions of generated images but may compromise overall image quality. In this\nwork, we propose Implicit Multimodal Guidance (IMG), a novel\nre-generation-based multimodal alignment framework that requires no extra data\nor editing operations. Specifically, given a generated image and its prompt,\nIMG a) utilizes a multimodal large language model (MLLM) to identify\nmisalignments; b) introduces an Implicit Aligner that manipulates diffusion\nconditioning features to reduce misalignments and enable re-generation; and c)\nformulates the re-alignment goal into a trainable objective, namely Iteratively\nUpdated Preference Objective. Extensive qualitative and quantitative\nevaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing\nalignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter,\nseamlessly enhancing prior finetuning-based alignment methods. Our code will be\navailable at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
      "upvotes": 12,
      "discussionId": "68dc9cd44159d1f2418f9b95",
      "githubRepo": "https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment",
      "ai_summary": "Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods.",
      "ai_keywords": [
        "diffusion-generated images",
        "multimodal alignment",
        "diffusion weight",
        "preference data",
        "editing-based methods",
        "Implicit Multimodal Guidance",
        "multimodal large language model",
        "Implicit Aligner",
        "diffusion conditioning features",
        "Iteratively Updated Preference Objective",
        "SDXL",
        "SDXL-DPO",
        "FLUX"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-09-30T09:27:03.000Z",
    "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
    "summary": "Ensuring precise multimodal alignment between diffusion-generated images and\ninput prompts has been a long-standing challenge. Earlier works finetune\ndiffusion weight using high-quality preference data, which tends to be limited\nand difficult to scale up. Recent editing-based methods further refine local\nregions of generated images but may compromise overall image quality. In this\nwork, we propose Implicit Multimodal Guidance (IMG), a novel\nre-generation-based multimodal alignment framework that requires no extra data\nor editing operations. Specifically, given a generated image and its prompt,\nIMG a) utilizes a multimodal large language model (MLLM) to identify\nmisalignments; b) introduces an Implicit Aligner that manipulates diffusion\nconditioning features to reduce misalignments and enable re-generation; and c)\nformulates the re-alignment goal into a trainable objective, namely Iteratively\nUpdated Preference Objective. Extensive qualitative and quantitative\nevaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing\nalignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter,\nseamlessly enhancing prior finetuning-based alignment methods. Our code will be\navailable at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26231.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6417c111dce1e4c0229ec182",
      "avatarUrl": "/avatars/de65da1f02b136b1af57862d9d0c48f2.svg",
      "fullname": "Jiayi Guo",
      "name": "JiayiGuo821",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "submitterOrganization": {
      "_id": "62675f8cdacab364889b6ca7",
      "name": "shi-labs",
      "fullname": "SHI Labs",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1650941798672-61e1188afc27c0f5e3641eb3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26391",
      "authors": [
        {
          "_id": "68dc90ca4159d1f2418f9ab8",
          "name": "Chenhui Zhu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9ab9",
          "name": "Yilu Wu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9aba",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9abb",
          "name": "Gangshan Wu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9abc",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T15:26:04.000Z",
      "submittedOnDailyAt": "2025-10-01T01:00:58.800Z",
      "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
      "submittedOnDailyBy": {
        "_id": "65e55ca4a0681de63022843e",
        "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
        "isPro": false,
        "fullname": "zhu chenhui",
        "user": "flateon",
        "type": "user"
      },
      "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
      "upvotes": 11,
      "discussionId": "68dc90ca4159d1f2418f9abd",
      "githubRepo": "https://github.com/MCG-NJU/MotionRAG",
      "ai_summary": "MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.",
      "ai_keywords": [
        "diffusion models",
        "MotionRAG",
        "Context-Aware Motion Adaptation (CAMA)",
        "video encoder",
        "specialized resamplers",
        "causal transformer architecture",
        "attention-based motion injection adapter",
        "zero-shot generalization"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-30T11:26:04.000Z",
    "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
    "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e55ca4a0681de63022843e",
      "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
      "fullname": "zhu chenhui",
      "name": "flateon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23610",
      "authors": [
        {
          "_id": "68dba99cd2bf1f4b15ec7950",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "68dba99cd2bf1f4b15ec7951",
          "name": "Kejun Gao",
          "hidden": false
        },
        {
          "_id": "68dba99cd2bf1f4b15ec7952",
          "name": "Xiaolin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T03:25:34.000Z",
      "submittedOnDailyAt": "2025-10-01T00:42:52.519Z",
      "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": true,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
      "upvotes": 11,
      "discussionId": "68dba99cd2bf1f4b15ec7953",
      "projectPage": "https://cslikai.cn/Dolphin",
      "githubRepo": "https://github.com/JusperLee/Dolphin",
      "ai_summary": "Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.",
      "ai_keywords": [
        "dual-path lightweight video encoder",
        "DP-LipCoder",
        "discrete audio-aligned semantic tokens",
        "lightweight encoder-decoder separator",
        "global-local attention (GLA) block",
        "multi-scale dependencies",
        "state-of-the-art (SOTA)",
        "MACs",
        "GPU inference speed"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-27T23:25:34.000Z",
    "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
    "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "submitterOrganization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25911",
      "authors": [
        {
          "_id": "68dcc1a04159d1f2418f9c28",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c29",
          "name": "Ryuichi Takanobu",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c2a",
          "name": "Zhiqi Liang",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c2b",
          "name": "Yuzhen Mao",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c2c",
          "name": "Yuanzhe Hu",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c2d",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "68dcc1a04159d1f2418f9c2e",
          "name": "Xiaojian Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T08:02:34.000Z",
      "submittedOnDailyAt": "2025-10-01T04:23:00.484Z",
      "title": "Mem-: Learning Memory Construction via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "686647f3e2cca4bc45818704",
        "avatarUrl": "/avatars/1efeff78892223b6e5592d4ef994f7c3.svg",
        "isPro": false,
        "fullname": "YUANZHE HU",
        "user": "ai-hyz",
        "type": "user"
      },
      "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.",
      "upvotes": 9,
      "discussionId": "68dcc1a04159d1f2418f9c2f",
      "ai_summary": "Mem-alpha, a reinforcement learning framework, enhances memory management in large language models through interaction and feedback, improving performance and generalization in long-term information understanding.",
      "ai_keywords": [
        "reinforcement learning",
        "memory-augmented agents",
        "memory management",
        "interaction patterns",
        "episodic memory",
        "semantic memory",
        "memory operations",
        "question-answering accuracy",
        "generalization"
      ]
    },
    "publishedAt": "2025-09-30T04:02:34.000Z",
    "title": "Mem-: Learning Memory Construction via Reinforcement Learning",
    "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25911.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "686647f3e2cca4bc45818704",
      "avatarUrl": "/avatars/1efeff78892223b6e5592d4ef994f7c3.svg",
      "fullname": "YUANZHE HU",
      "name": "ai-hyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22646",
      "authors": [
        {
          "_id": "68dcb64b4159d1f2418f9bf5",
          "name": "Xingyu Fu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bf6",
          "name": "Siyi Liu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bf7",
          "name": "Yinuo Xu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bf8",
          "name": "Pan Lu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bf9",
          "name": "Guangqiuse Hu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bfa",
          "name": "Tianbo Yang",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bfb",
          "name": "Taran Anantasagar",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bfc",
          "name": "Christopher Shen",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bfd",
          "name": "Yikai Mao",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bfe",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9bff",
          "name": "Keyush Shah",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9c00",
          "name": "Chung Un Lee",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9c01",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9c02",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9c03",
          "name": "Dan Roth",
          "hidden": false
        },
        {
          "_id": "68dcb64b4159d1f2418f9c04",
          "name": "Chris Callison-Burch",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T17:59:54.000Z",
      "submittedOnDailyAt": "2025-10-01T03:36:03.020Z",
      "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "6336091b2db86a181ccd6054",
        "avatarUrl": "/avatars/829f69436225d05d2c2136bc90f640d7.svg",
        "isPro": false,
        "fullname": "Xingyu Fu",
        "user": "Fiaa",
        "type": "user"
      },
      "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
      "upvotes": 9,
      "discussionId": "68dcb64c4159d1f2418f9c05",
      "projectPage": "https://deeptracereward.github.io/",
      "ai_summary": "DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos.",
      "ai_keywords": [
        "deepfake traces",
        "spatiotemporal grounded visual artifacts",
        "fine-grained",
        "spatially- and temporally- aware benchmark",
        "multimodal language models",
        "reward models",
        "binary fake v.s. real classification",
        "natural language explanations",
        "spatial grounding",
        "temporal labeling"
      ]
    },
    "publishedAt": "2025-09-26T13:59:54.000Z",
    "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
    "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6336091b2db86a181ccd6054",
      "avatarUrl": "/avatars/829f69436225d05d2c2136bc90f640d7.svg",
      "fullname": "Xingyu Fu",
      "name": "Fiaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "submitterOrganization": {
      "_id": "6735d51c08a190b1caea1f29",
      "name": "PrincetonUniversity",
      "fullname": "Princeton University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26628",
      "authors": [
        {
          "_id": "68dc8bf34159d1f2418f9a6e",
          "name": "Runze Liu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a6f",
          "name": "Jiakang Wang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a70",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a71",
          "name": "Zhihui Xie",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a72",
          "name": "Chenxin An",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a73",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a74",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a75",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a76",
          "name": "Lei Lin",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a77",
          "name": "Wenping Hu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a78",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a79",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a7a",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a7b",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:58:34.000Z",
      "submittedOnDailyAt": "2025-10-01T00:35:07.177Z",
      "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
      "upvotes": 7,
      "discussionId": "68dc8bf34159d1f2418f9a7c",
      "ai_summary": "A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large Language Models",
        "Process-Supervised RL",
        "PSRL",
        "AttnRL",
        "attention scores",
        "reasoning behaviors",
        "adaptive sampling strategy",
        "one-step off-policy training",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-09-30T13:58:34.000Z",
    "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
    "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26495",
      "authors": [
        {
          "_id": "68dc986b4159d1f2418f9b7a",
          "name": "Jingdi Lei",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b7b",
          "name": "Varun Gumma",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b7c",
          "name": "Rishabh Bhardwaj",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b7d",
          "name": "Seok Min Lim",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b7e",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b7f",
          "name": "Amir Zadeh",
          "hidden": false
        },
        {
          "_id": "68dc986b4159d1f2418f9b80",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T16:39:17.000Z",
      "submittedOnDailyAt": "2025-10-01T01:58:41.393Z",
      "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
      "upvotes": 7,
      "discussionId": "68dc986c4159d1f2418f9b81",
      "ai_summary": "Operational safety, measured by OffTopicEval, is a critical issue for LLMs, with most models falling short, but prompt-based steering methods show promise in improving out-of-distribution refusal.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "operational safety",
        "OffTopicEval",
        "query grounding (Q-ground)",
        "system-prompt grounding (P-ground)",
        "out-of-distribution (OOD) refusal"
      ]
    },
    "publishedAt": "2025-09-30T12:39:17.000Z",
    "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
    "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "submitterOrganization": {
      "_id": "626ab9dac804c432c1b27a48",
      "name": "declare-lab",
      "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26618",
      "authors": [
        {
          "_id": "68dcbd594159d1f2418f9c0e",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c0f",
          "name": "Wangguangdong Zheng",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c10",
          "name": "Jing He",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c11",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c12",
          "name": "Xin Lin",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c13",
          "name": "Xin Yang",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c14",
          "name": "Ying-Cong Chen",
          "hidden": false
        },
        {
          "_id": "68dcbd594159d1f2418f9c15",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/OZ55UfUOPi7nlgeuUyRyV.mp4"
      ],
      "publishedAt": "2025-09-30T17:55:37.000Z",
      "submittedOnDailyAt": "2025-10-01T04:05:15.902Z",
      "title": "DA^2: Depth Anything in Any Direction",
      "submittedOnDailyBy": {
        "_id": "641d211e353524fe41f16387",
        "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
        "isPro": false,
        "fullname": "Haodong Li",
        "user": "haodongli",
        "type": "user"
      },
      "summary": "Panorama has a full FoV (360^circtimes180^circ), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\nDA^{2}: Depth Anything in\nAny Direction, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create sim543K panoramic\nRGB-depth pairs, bringing the total to sim607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
      "upvotes": 6,
      "discussionId": "68dcbd5a4159d1f2418f9c16",
      "projectPage": "https://depth-any-in-any-dir.github.io/",
      "githubRepo": "https://github.com/EnVision-Research/DA-2",
      "ai_summary": "DA, a zero-shot generalizable and fully end-to-end panoramic depth estimator, addresses challenges in panoramic depth estimation by using a data curation engine and SphereViT to handle spherical distortions, achieving state-of-the-art performance.",
      "ai_keywords": [
        "panoramic depth estimation",
        "3D vision",
        "zero-shot generalization",
        "perspective splitting",
        "cubemaps",
        "data curation engine",
        "SphereViT",
        "spherical coordinates",
        "spherical geometric consistency",
        "AbsRel",
        "in-domain methods",
        "fusion-based approaches"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-09-30T13:55:37.000Z",
    "title": "DA^2: Depth Anything in Any Direction",
    "summary": "Panorama has a full FoV (360^circtimes180^circ), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\nDA^{2}: Depth Anything in\nAny Direction, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create sim543K panoramic\nRGB-depth pairs, bringing the total to sim607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/OZ55UfUOPi7nlgeuUyRyV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641d211e353524fe41f16387",
      "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
      "fullname": "Haodong Li",
      "name": "haodongli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "submitterOrganization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24207",
      "authors": [
        {
          "_id": "68dcaada4159d1f2418f9bb8",
          "name": "Sijia Liu",
          "hidden": false
        },
        {
          "_id": "68dcaada4159d1f2418f9bb9",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "68dcaada4159d1f2418f9bba",
          "name": "Kawin Ethayarajh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T02:41:16.000Z",
      "submittedOnDailyAt": "2025-10-01T02:48:25.499Z",
      "title": "Humanline: Online Alignment as Perceptual Loss",
      "submittedOnDailyBy": {
        "_id": "651d2b485d3519c0b7595af7",
        "avatarUrl": "/avatars/00ce2ecbc35e22a90f72b9015299aa29.svg",
        "isPro": false,
        "fullname": "Sijia L",
        "user": "sijial430",
        "type": "user"
      },
      "summary": "Online alignment (e.g., GRPO) is generally more performant than offline\nalignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral\neconomics, we propose a human-centric explanation. We prove that online\non-policy sampling better approximates the human-perceived distribution of what\nthe model can produce, and PPO/GRPO-style clipping -- originally introduced to\njust stabilize training -- recovers a perceptual bias in how humans perceive\nprobability. In this sense, PPO/GRPO act as perceptual losses already. Our\ntheory further suggests that the online/offline dichotomy is itself incidental\nto maximizing human utility, since we can achieve the same effect by\nselectively training on any data in a manner that mimics human perception,\nrather than restricting ourselves to online on-policy data. Doing so would\nallow us to post-train more quickly, cheaply, and flexibly without sacrificing\nperformance. To this end, we propose a design pattern that explicitly\nincorporates perceptual distortions of probability into objectives like\nDPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that\nthese humanline variants, even when trained with offline off-policy data, can\nmatch the performance of their online counterparts on both verifiable and\nunverifiable tasks.",
      "upvotes": 5,
      "discussionId": "68dcaadb4159d1f2418f9bbb",
      "ai_summary": "Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions, and introducing perceptual biases into offline training can achieve similar performance.",
      "ai_keywords": [
        "GRPO",
        "DPO",
        "prospect theory",
        "on-policy sampling",
        "PPO",
        "perceptual bias",
        "perceptual losses",
        "humanline variants",
        "KTO"
      ]
    },
    "publishedAt": "2025-09-28T22:41:16.000Z",
    "title": "Humanline: Online Alignment as Perceptual Loss",
    "summary": "Online alignment (e.g., GRPO) is generally more performant than offline\nalignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral\neconomics, we propose a human-centric explanation. We prove that online\non-policy sampling better approximates the human-perceived distribution of what\nthe model can produce, and PPO/GRPO-style clipping -- originally introduced to\njust stabilize training -- recovers a perceptual bias in how humans perceive\nprobability. In this sense, PPO/GRPO act as perceptual losses already. Our\ntheory further suggests that the online/offline dichotomy is itself incidental\nto maximizing human utility, since we can achieve the same effect by\nselectively training on any data in a manner that mimics human perception,\nrather than restricting ourselves to online on-policy data. Doing so would\nallow us to post-train more quickly, cheaply, and flexibly without sacrificing\nperformance. To this end, we propose a design pattern that explicitly\nincorporates perceptual distortions of probability into objectives like\nDPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that\nthese humanline variants, even when trained with offline off-policy data, can\nmatch the performance of their online counterparts on both verifiable and\nunverifiable tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24207.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651d2b485d3519c0b7595af7",
      "avatarUrl": "/avatars/00ce2ecbc35e22a90f72b9015299aa29.svg",
      "fullname": "Sijia L",
      "name": "sijial430",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "submitterOrganization": {
      "_id": "6735d51c08a190b1caea1f29",
      "name": "PrincetonUniversity",
      "fullname": "Princeton University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26539",
      "authors": [
        {
          "_id": "68dc8f6f4159d1f2418f9a95",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a96",
          "name": "Zi-Yi Dou",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a97",
          "name": "Di Feng",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a98",
          "name": "Forrest Huang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a99",
          "name": "Anh Nguyen",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9a",
          "name": "Keen You",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9b",
          "name": "Omar Attia",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9c",
          "name": "Yuhao Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9d",
          "name": "Michael Feng",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9e",
          "name": "Haotian Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9f",
          "name": "Ram Ramrakhya",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa0",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa1",
          "name": "Jeffrey Nichols",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa2",
          "name": "Alexander Toshev",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa3",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa4",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:13:56.000Z",
      "submittedOnDailyAt": "2025-10-01T00:48:59.196Z",
      "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
      "upvotes": 4,
      "discussionId": "68dc8f704159d1f2418f9aa5",
      "ai_summary": "Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "visual tool-use",
        "reinforcement learning",
        "GUI agent",
        "GUI grounding",
        "GUI navigation",
        "ScreenSpot-V2",
        "ScreenSpot-Pro",
        "OSWorld-G",
        "AndroidWorld",
        "OSWorld"
      ]
    },
    "publishedAt": "2025-09-30T13:13:56.000Z",
    "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
    "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "submitterOrganization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26476",
      "authors": [
        {
          "_id": "68dc92b64159d1f2418f9ae2",
          "name": "Yash Akhauri",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae3",
          "name": "Xingyou Song",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae4",
          "name": "Arissa Wongpanich",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae5",
          "name": "Bryan Lewandowski",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae6",
          "name": "Mohamed S. Abdelfattah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T16:25:23.000Z",
      "submittedOnDailyAt": "2025-10-01T01:02:49.611Z",
      "title": "Regression Language Models for Code",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
      "upvotes": 4,
      "discussionId": "68dc92b64159d1f2418f9ae7",
      "ai_summary": "A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.",
      "ai_keywords": [
        "Regression Language Model (RLM)",
        "T5Gemma",
        "Spearman-rank",
        "Kendall-Tau",
        "NAS design spaces",
        "graph neural networks"
      ]
    },
    "publishedAt": "2025-09-30T12:25:23.000Z",
    "title": "Regression Language Models for Code",
    "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26476.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "submitterOrganization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25848",
      "authors": [
        {
          "_id": "68dc984b4159d1f2418f9b70",
          "name": "Xinyu Tian",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b71",
          "name": "Shu Zou",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b72",
          "name": "Zhaoyuan Yang",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b73",
          "name": "Mengqi He",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b74",
          "name": "Fabian Waschkowski",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b75",
          "name": "Lukas Wesemann",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b76",
          "name": "Peter Tu",
          "hidden": false
        },
        {
          "_id": "68dc984b4159d1f2418f9b77",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T06:37:47.000Z",
      "submittedOnDailyAt": "2025-10-01T02:32:09.976Z",
      "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "630c7d9ecb1d6a2384273f1a",
        "avatarUrl": "/avatars/8ba76de6092e5d9fcc4f23d548befe9a.svg",
        "isPro": false,
        "fullname": "Xinyu Tian",
        "user": "xytian1008",
        "type": "user"
      },
      "summary": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/",
      "upvotes": 4,
      "discussionId": "68dc984b4159d1f2418f9b78",
      "projectPage": "https://xytian1008.github.io/VAPO/",
      "githubRepo": "https://github.com/xytian1008/VAPO",
      "ai_summary": "VAPO-Thinker-7B enhances multimodal reasoning by anchoring the process to visual information, improving performance on visual tasks while maintaining logical inference.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Group Relative Policy Optimization",
        "Vision-Language Models",
        "multimodal reasoning",
        "logical inference",
        "perceptual grounding",
        "visual forgetting",
        "Vision-Anchored Policy Optimization"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-30T02:37:47.000Z",
    "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
    "summary": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c7d9ecb1d6a2384273f1a",
      "avatarUrl": "/avatars/8ba76de6092e5d9fcc4f23d548befe9a.svg",
      "fullname": "Xinyu Tian",
      "name": "xytian1008",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25189",
      "authors": [
        {
          "_id": "68dcae184159d1f2418f9bd9",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bda",
          "name": "Jialiang Zhu",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bdb",
          "name": "Ruiqi Yang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bdc",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bdd",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bde",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bdf",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be0",
          "name": "Bei Liu",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be1",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be3",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be4",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be5",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be6",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be7",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be8",
          "name": "Zhaoyi Liu",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9be9",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "68dcae184159d1f2418f9bea",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-01T03:00:32.005Z",
      "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
      "submittedOnDailyBy": {
        "_id": "62a30bf72dac39c2173c0a8c",
        "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
        "isPro": false,
        "fullname": "Miaosen Zhang",
        "user": "Miaosen",
        "type": "user"
      },
      "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
      "upvotes": 4,
      "discussionId": "68dcae184159d1f2418f9beb",
      "ai_summary": "InfoAgent, a deep research agent using a custom data synthesis pipeline and search infrastructure, outperforms existing agents by improving tool use and reasoning.",
      "ai_keywords": [
        "deep research agent",
        "data synthesis pipeline",
        "entity trees",
        "sub-tree sampling",
        "entity fuzzification",
        "self-hosted search infrastructure",
        "cold-start supervised finetuning",
        "reinforcement learning",
        "BrowseComp",
        "BrowseComp-ZH",
        "Xbench-DS",
        "WebSailor-72B",
        "DeepDive-32B"
      ]
    },
    "publishedAt": "2025-09-29T13:59:57.000Z",
    "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
    "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a30bf72dac39c2173c0a8c",
      "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
      "fullname": "Miaosen Zhang",
      "name": "Miaosen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "submitterOrganization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22613",
      "authors": [
        {
          "_id": "68dc86ef4159d1f2418f9a09",
          "name": "Siwei Wang",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0a",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0b",
          "name": "Haoran Sun",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0c",
          "name": "Shi Feng",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0d",
          "name": "Shang-Hua Teng",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0e",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0f",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a10",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T17:39:48.000Z",
      "submittedOnDailyAt": "2025-10-01T00:20:26.292Z",
      "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
      "submittedOnDailyBy": {
        "_id": "649aa367c6cf3cc95bc1b7f6",
        "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
        "isPro": false,
        "fullname": "Yifei Shen",
        "user": "yshenaw",
        "type": "user"
      },
      "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
      "upvotes": 4,
      "discussionId": "68dc86f04159d1f2418f9a11",
      "ai_summary": "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.",
      "ai_keywords": [
        "reinforcement learning",
        "Large Language Models",
        "policy gradient",
        "Q-learning",
        "supervised fine-tuning",
        "co-occurrence-based spurious solutions",
        "diversity collapse",
        "off-policy learning",
        "reward hacking",
        "Blocksworld"
      ]
    },
    "publishedAt": "2025-09-26T13:39:48.000Z",
    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649aa367c6cf3cc95bc1b7f6",
      "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
      "fullname": "Yifei Shen",
      "name": "yshenaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26603",
      "authors": [
        {
          "_id": "68dcd65b4159d1f2418f9c68",
          "name": "Yixuan Weng",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c69",
          "name": "Minjun Zhu",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c6a",
          "name": "Qiujie Xie",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c6b",
          "name": "Qiyao Sun",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c6c",
          "name": "Zhen Lin",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c6d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "68dcd65b4159d1f2418f9c6e",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/611568222999876a45605af5/ngc1-pgZhq8lIVqYyEbqa.mp4"
      ],
      "publishedAt": "2025-09-30T17:49:32.000Z",
      "submittedOnDailyAt": "2025-10-01T06:04:39.689Z",
      "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
      "submittedOnDailyBy": {
        "_id": "611568222999876a45605af5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg",
        "isPro": false,
        "fullname": "WENGSYX",
        "user": "WENGSYX",
        "type": "user"
      },
      "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.",
      "upvotes": 3,
      "discussionId": "68dcd65b4159d1f2418f9c6f",
      "projectPage": "https://ai-researcher.net",
      "githubRepo": "https://github.com/ResearAI/DeepScientist",
      "ai_summary": "DeepScientist autonomously conducts scientific discovery through Bayesian Optimization, surpassing human state-of-the-art methods on multiple AI tasks.",
      "ai_keywords": [
        "Bayesian Optimization",
        "hierarchical evaluation",
        "Findings Memory",
        "exploration",
        "exploitation",
        "scientific discovery",
        "state-of-the-art",
        "AI tasks"
      ]
    },
    "publishedAt": "2025-09-30T13:49:32.000Z",
    "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
    "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/611568222999876a45605af5/ngc1-pgZhq8lIVqYyEbqa.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "611568222999876a45605af5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg",
      "fullname": "WENGSYX",
      "name": "WENGSYX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "66bb231e40d36c70d6ad0c4b",
      "name": "WestlakeNLP",
      "fullname": "Text Intelligence Lab of Westlake University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25397",
      "authors": [
        {
          "_id": "68dcd5a44159d1f2418f9c5d",
          "name": "Johan Linker",
          "hidden": false
        },
        {
          "_id": "68dcd5a44159d1f2418f9c5e",
          "name": "Cailean Osborne",
          "hidden": false
        },
        {
          "_id": "68dcd5a44159d1f2418f9c5f",
          "name": "Jennifer Ding",
          "hidden": false
        },
        {
          "_id": "68dcd5a44159d1f2418f9c60",
          "name": "Ben Burtenshaw",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T18:55:18.000Z",
      "submittedOnDailyAt": "2025-10-01T05:49:23.206Z",
      "title": "A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects",
      "submittedOnDailyBy": {
        "_id": "62d648291fa3e4e7ae3fa6e8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png",
        "isPro": true,
        "fullname": "ben burtenshaw",
        "user": "burtenshaw",
        "type": "user"
      },
      "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant\necosystem of research and innovation in artificial intelligence (AI). However,\nthe methods of collaboration used to develop open LLMs both before and after\ntheir public release have not yet been comprehensively studied, limiting our\nunderstanding of how open LLM projects are initiated, organized, and governed\nas well as what opportunities there are to foster this ecosystem even further.\nWe address this gap through an exploratory analysis of open collaboration\nthroughout the development and reuse lifecycle of open LLMs, drawing on\nsemi-structured interviews with the developers of 14 open LLMs from grassroots\nprojects, research institutes, startups, and Big Tech companies in North\nAmerica, Europe, Africa, and Asia. We make three key contributions to research\nand practice. First, collaboration in open LLM projects extends far beyond the\nLLMs themselves, encompassing datasets, benchmarks, open source frameworks,\nleaderboards, knowledge sharing and discussion forums, and compute\npartnerships, among others. Second, open LLM developers have a variety of\nsocial, economic, and technological motivations, from democratizing AI access\nand promoting open science to building regional ecosystems and expanding\nlanguage representation. Third, the sampled open LLM projects exhibit five\ndistinct organizational models, ranging from single company projects to\nnon-profit-sponsored grassroots projects, which vary in their centralization of\ncontrol and community engagement strategies used throughout the open LLM\nlifecycle. We conclude with practical recommendations for stakeholders seeking\nto support the global community building a more open future for AI.",
      "upvotes": 3,
      "discussionId": "68dcd5a44159d1f2418f9c61",
      "ai_summary": "Research explores collaboration in open large language models, identifying diverse motivations and organizational models among developers from various sectors.",
      "ai_keywords": [
        "large language models",
        "open collaboration",
        "datasets",
        "benchmarks",
        "open source frameworks",
        "leaderboards",
        "knowledge sharing",
        "discussion forums",
        "compute partnerships",
        "democratizing AI",
        "open science",
        "regional ecosystems",
        "language representation",
        "organizational models",
        "community engagement"
      ]
    },
    "publishedAt": "2025-09-29T14:55:18.000Z",
    "title": "A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects",
    "summary": "The proliferation of open large language models (LLMs) is fostering a vibrant\necosystem of research and innovation in artificial intelligence (AI). However,\nthe methods of collaboration used to develop open LLMs both before and after\ntheir public release have not yet been comprehensively studied, limiting our\nunderstanding of how open LLM projects are initiated, organized, and governed\nas well as what opportunities there are to foster this ecosystem even further.\nWe address this gap through an exploratory analysis of open collaboration\nthroughout the development and reuse lifecycle of open LLMs, drawing on\nsemi-structured interviews with the developers of 14 open LLMs from grassroots\nprojects, research institutes, startups, and Big Tech companies in North\nAmerica, Europe, Africa, and Asia. We make three key contributions to research\nand practice. First, collaboration in open LLM projects extends far beyond the\nLLMs themselves, encompassing datasets, benchmarks, open source frameworks,\nleaderboards, knowledge sharing and discussion forums, and compute\npartnerships, among others. Second, open LLM developers have a variety of\nsocial, economic, and technological motivations, from democratizing AI access\nand promoting open science to building regional ecosystems and expanding\nlanguage representation. Third, the sampled open LLM projects exhibit five\ndistinct organizational models, ranging from single company projects to\nnon-profit-sponsored grassroots projects, which vary in their centralization of\ncontrol and community engagement strategies used throughout the open LLM\nlifecycle. We conclude with practical recommendations for stakeholders seeking\nto support the global community building a more open future for AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d648291fa3e4e7ae3fa6e8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png",
      "fullname": "ben burtenshaw",
      "name": "burtenshaw",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3784
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23166",
      "authors": [
        {
          "_id": "68dca79d4159d1f2418f9ba8",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "68dca79d4159d1f2418f9ba9",
          "name": "Hong Wang",
          "hidden": false
        },
        {
          "_id": "68dca79d4159d1f2418f9baa",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "68dca79d4159d1f2418f9bab",
          "name": "Fei Yu",
          "hidden": false
        },
        {
          "_id": "68dca79d4159d1f2418f9bac",
          "name": "Yao Shu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/LVF04XieiNnP-D38XBARY.png"
      ],
      "publishedAt": "2025-09-27T07:46:15.000Z",
      "submittedOnDailyAt": "2025-10-01T02:46:07.690Z",
      "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "65ed3051492a7f35db21fea2",
        "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
        "isPro": false,
        "fullname": "Chenxing Wei",
        "user": "kittttttt",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental\nparadigm for completing complex tasks. However, their performance often\ndegrades in extended interactions, as they are typically trained on static,\nsingle-turn data, which hinders their ability to adapt to real-time user\nfeedback. To address this limitation, we first propose a new paradigm:\nTest-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes\nuser feedback from the ongoing interaction as a reward signal to estimate a\nlatent optimal policy aligned with user preferences, then updates a small\nsubset of parameters to steer the model toward this policy, ultimately enabling\nefficient in-conversation self-correction. We then introduce Optimum-Referenced\nOne-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.\nROSA guides the model parameters toward a theoretical optimal policy in a\nsingle, efficient update step, avoiding costly iterative gradient-based\noptimization and minimizing computational overhead. We provide a rigorous\ntheoretical analysis guaranteeing that the policy of ROSA converges to the\npreference of user as the number of interactions increases. Extensive\nexperiments on challenging benchmark demonstrate that ROSA achieves significant\nimprovements in both task effectiveness and efficiency.",
      "upvotes": 3,
      "discussionId": "68dca79e4159d1f2418f9bad",
      "githubRepo": "https://github.com/kithib/ROSA",
      "ai_summary": "ROSA, a lightweight algorithm, enhances multi-turn interactions in LLMs by adapting to user feedback in real-time, improving both task effectiveness and efficiency.",
      "ai_keywords": [
        "multi-turn interaction",
        "Test-Time Policy Adaptation",
        "T2PAM",
        "latent optimal policy",
        "parameter adaptation",
        "Optimum-Referenced One-Step Adaptation",
        "ROSA",
        "theoretical optimal policy",
        "gradient-based optimization",
        "computational overhead"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-27T03:46:15.000Z",
    "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
    "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental\nparadigm for completing complex tasks. However, their performance often\ndegrades in extended interactions, as they are typically trained on static,\nsingle-turn data, which hinders their ability to adapt to real-time user\nfeedback. To address this limitation, we first propose a new paradigm:\nTest-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes\nuser feedback from the ongoing interaction as a reward signal to estimate a\nlatent optimal policy aligned with user preferences, then updates a small\nsubset of parameters to steer the model toward this policy, ultimately enabling\nefficient in-conversation self-correction. We then introduce Optimum-Referenced\nOne-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.\nROSA guides the model parameters toward a theoretical optimal policy in a\nsingle, efficient update step, avoiding costly iterative gradient-based\noptimization and minimizing computational overhead. We provide a rigorous\ntheoretical analysis guaranteeing that the policy of ROSA converges to the\npreference of user as the number of interactions increases. Extensive\nexperiments on challenging benchmark demonstrate that ROSA achieves significant\nimprovements in both task effectiveness and efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/LVF04XieiNnP-D38XBARY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23166.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26329",
      "authors": [
        {
          "_id": "68dc98004159d1f2418f9b5f",
          "name": "Yi-Cheng Lin",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b60",
          "name": "Yu-Hua Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b61",
          "name": "Jia-Kai Dong",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b62",
          "name": "Yueh-Hsuan Huang",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b63",
          "name": "Szu-Chi Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b64",
          "name": "Yu-Chen Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b65",
          "name": "Chih-Yao Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b66",
          "name": "Yu-Jung Lin",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b67",
          "name": "Yu-Ling Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b68",
          "name": "Zih-Yu Chen",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b69",
          "name": "I-Ning Tsai",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b6a",
          "name": "Hsiu-Hsuan Wang",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b6b",
          "name": "Ho-Lam Chung",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b6c",
          "name": "Ke-Han Lu",
          "hidden": false
        },
        {
          "_id": "68dc98004159d1f2418f9b6d",
          "name": "Hung-yi Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T14:40:45.000Z",
      "submittedOnDailyAt": "2025-10-01T01:25:55.123Z",
      "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
      "submittedOnDailyBy": {
        "_id": "650b0d66664f7b7d088ca281",
        "avatarUrl": "/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg",
        "isPro": false,
        "fullname": "Yi-Cheng Lin",
        "user": "dlion168",
        "type": "user"
      },
      "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream.",
      "upvotes": 2,
      "discussionId": "68dc98004159d1f2418f9b6e",
      "ai_summary": "TAU, a benchmark of culturally specific Taiwanese soundmarks, reveals that state-of-the-art large audio-language models underperform compared to local humans, highlighting the need for localized evaluations.",
      "ai_keywords": [
        "audio-language models",
        "soundmarks",
        "benchmark",
        "LLM-assisted question generation",
        "multimodal evaluation"
      ]
    },
    "publishedAt": "2025-09-30T10:40:45.000Z",
    "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
    "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650b0d66664f7b7d088ca281",
      "avatarUrl": "/avatars/fce475c301f53e166fc3c8f5c5112c4a.svg",
      "fullname": "Yi-Cheng Lin",
      "name": "dlion168",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25339",
      "authors": [
        {
          "_id": "68dc916b4159d1f2418f9aca",
          "name": "Paul Gavrikov",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acb",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acc",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acd",
          "name": "Soumya Jahagirdar",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ace",
          "name": "Muhammad Huzaifa",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acf",
          "name": "Sivan Doveh",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad0",
          "name": "Serena Yeung-Levy",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad1",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad2",
          "name": "Hilde Kuehne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T18:00:25.000Z",
      "submittedOnDailyAt": "2025-10-01T00:57:09.721Z",
      "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
      "upvotes": 2,
      "discussionId": "68dc916b4159d1f2418f9ad3",
      "projectPage": "https://paulgavrikov.github.io/visualoverload/",
      "githubRepo": "https://github.com/paulgavrikov/visualoverload",
      "ai_summary": "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.",
      "ai_keywords": [
        "visual question answering",
        "VQA",
        "densely populated scenes",
        "high-resolution scans",
        "public-domain paintings",
        "manual annotation",
        "task categories",
        "error analysis",
        "counting skills",
        "OCR",
        "logical inconsistencies",
        "vision models"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-09-29T14:00:25.000Z",
    "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
    "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25339.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26645",
      "authors": [
        {
          "_id": "68dcd6a04159d1f2418f9c71",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "68dcd6a04159d1f2418f9c72",
          "name": "Yue Chen",
          "hidden": false
        },
        {
          "_id": "68dcd6a04159d1f2418f9c73",
          "name": "Yuliang Xiu",
          "hidden": false
        },
        {
          "_id": "68dcd6a04159d1f2418f9c74",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "68dcd6a04159d1f2418f9c75",
          "name": "Anpei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/o_7JHVjCwILwL1xpGK-t6.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/M67C9yUMFsTHvYNDyCTwy.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/KJXn0Q-Xnlp7YBH08_jz9.mp4"
      ],
      "publishedAt": "2025-09-30T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-01T05:57:29.331Z",
      "title": "TTT3R: 3D Reconstruction as Test-Time Training",
      "submittedOnDailyBy": {
        "_id": "66606a13fc6c0816442bd161",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
        "isPro": false,
        "fullname": "Xingyu Chen",
        "user": "rover-xingyu",
        "type": "user"
      },
      "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a 2times improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
      "upvotes": 1,
      "discussionId": "68dcd6a04159d1f2418f9c76",
      "ai_summary": "TTT3R, a test-time training intervention, enhances length generalization in 3D reconstruction by dynamically adjusting memory updates based on alignment confidence, improving global pose estimation and processing efficiency.",
      "ai_keywords": [
        "Recurrent Neural Networks",
        "3D reconstruction",
        "linear-time complexity",
        "length generalization",
        "Test-Time Training",
        "memory state",
        "incoming observations",
        "closed-form learning rate",
        "global pose estimation",
        "FPS",
        "GPU memory"
      ]
    },
    "publishedAt": "2025-09-30T13:59:51.000Z",
    "title": "TTT3R: 3D Reconstruction as Test-Time Training",
    "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a 2times improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/o_7JHVjCwILwL1xpGK-t6.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/M67C9yUMFsTHvYNDyCTwy.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66606a13fc6c0816442bd161/KJXn0Q-Xnlp7YBH08_jz9.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66606a13fc6c0816442bd161",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
      "fullname": "Xingyu Chen",
      "name": "rover-xingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26574",
      "authors": [
        {
          "_id": "68dc94be4159d1f2418f9b01",
          "name": "Minhui Zhu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b02",
          "name": "Minyang Tian",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b03",
          "name": "Xiaocheng Yang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b04",
          "name": "Tianci Zhou",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b05",
          "name": "Penghao Zhu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b06",
          "name": "Eli Chertkov",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b07",
          "name": "Shengyan Liu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b08",
          "name": "Yufeng Du",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b09",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0a",
          "name": "Ziming Ji",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0b",
          "name": "Indranil Das",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0c",
          "name": "Junyi Cao",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0d",
          "name": "Yufeng Du",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0e",
          "name": "Jinchen He",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b0f",
          "name": "Yifan Su",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b10",
          "name": "Jiabin Yu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b11",
          "name": "Yikun Jiang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b12",
          "name": "Yujie Zhang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b13",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b14",
          "name": "Ze-Min Huang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b15",
          "name": "Weizhen Jia",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b16",
          "name": "Xinan Chen",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b17",
          "name": "Peixue Wu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b18",
          "name": "Yunkai Wang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b19",
          "name": "Juntai Zhou",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1a",
          "name": "Yong Zhao",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1b",
          "name": "Farshid Jafarpour",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1c",
          "name": "Jessie Shelton",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1d",
          "name": "Aaron Young",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1e",
          "name": "John Bartolotta",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b1f",
          "name": "Wenchao Xu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b20",
          "name": "Yue Sun",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b21",
          "name": "Anjun Chu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b22",
          "name": "Victor Colussi",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b23",
          "name": "Chris Akers",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b24",
          "name": "Nathan Brooks",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b25",
          "name": "Wenbo Fu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b26",
          "name": "Christopher Wilson",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b27",
          "name": "Jinchao Zhao",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b28",
          "name": "Marvin Qi",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b29",
          "name": "Anqi Mu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2a",
          "name": "Yubo Yang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2b",
          "name": "Allen Zang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2c",
          "name": "Yang Lyu",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2d",
          "name": "Peizhi Mai",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2e",
          "name": "Xuefei Guo",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b2f",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b30",
          "name": "Ze Yang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b31",
          "name": "Chi Xue",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b32",
          "name": "Dmytro Bandak",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b33",
          "name": "Yar Hein",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b34",
          "name": "Yonatan Kahn",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b35",
          "name": "Kevin Zhou",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b36",
          "name": "John Drew Wilson Jarrod T. Reilly",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b37",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b38",
          "name": "Daniel Inafuku",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b39",
          "name": "Hao Tong",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3a",
          "name": "Liang Yang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3b",
          "name": "Ruixing Zhang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3c",
          "name": "Xueying Wang",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3d",
          "name": "Ofir Press",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3e",
          "name": "Nicolas Chia",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b3f",
          "name": "Eliu Huerta",
          "hidden": false
        },
        {
          "_id": "68dc94be4159d1f2418f9b40",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:34:03.000Z",
      "submittedOnDailyAt": "2025-10-01T01:11:13.379Z",
      "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
      "upvotes": 1,
      "discussionId": "68dc94be4159d1f2418f9b41",
      "ai_summary": "CritPt, a benchmark for evaluating LLMs on research-level physics tasks, reveals significant gaps between current model capabilities and the demands of physics research.",
      "ai_keywords": [
        "large language models",
        "reasoning capabilities",
        "high-school math competitions",
        "coding",
        "CritPt",
        "benchmark",
        "research-level reasoning tasks",
        "condensed matter",
        "quantum physics",
        "atomic",
        "molecular & optical physics",
        "astrophysics",
        "high energy physics",
        "mathematical physics",
        "statistical physics",
        "nuclear physics",
        "nonlinear dynamics",
        "fluid dynamics",
        "biophysics",
        "composite research challenges",
        "checkpoint tasks",
        "guess-resistant",
        "machine-verifiable",
        "automated grading pipeline",
        "advanced physics-specific output formats",
        "GPT-5"
      ]
    },
    "publishedAt": "2025-09-30T13:34:03.000Z",
    "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
    "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26542",
      "authors": [
        {
          "_id": "68dc885c4159d1f2418f9a23",
          "name": "Yueqian Lin",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a24",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a25",
          "name": "Qinsi Wang",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a26",
          "name": "Yudong Liu",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a27",
          "name": "Hengfan Zhang",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a28",
          "name": "Jayakumar Subramanian",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a29",
          "name": "Nikos Vlassis",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a2a",
          "name": "Hai Helen Li",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a2b",
          "name": "Yiran Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:17:09.000Z",
      "submittedOnDailyAt": "2025-10-01T00:34:18.271Z",
      "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
      "submittedOnDailyBy": {
        "_id": "64b5198c25882acb62fb77ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
        "isPro": false,
        "fullname": "Yueqian Lin",
        "user": "linyueqian",
        "type": "user"
      },
      "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
      "upvotes": 1,
      "discussionId": "68dc885c4159d1f2418f9a2c",
      "ai_summary": "VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.",
      "ai_keywords": [
        "voice-interactive systems",
        "reasoning ability",
        "real-time conversational constraints",
        "voice-native episodes",
        "text benchmarks",
        "speech interaction",
        "text-voice comparison",
        "architectural choices",
        "reliability",
        "latency-accuracy analyses",
        "low-latency plateau",
        "thinking time",
        "decoupled cascade",
        "error signatures",
        "native streaming",
        "end-to-end",
        "cascade designs",
        "real-time voice assistants",
        "fluent reasoning"
      ]
    },
    "publishedAt": "2025-09-30T13:17:09.000Z",
    "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
    "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b5198c25882acb62fb77ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
      "fullname": "Yueqian Lin",
      "name": "linyueqian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23773",
      "authors": [
        {
          "_id": "68dc9a964159d1f2418f9b83",
          "name": "Utkarsh Sahu",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b84",
          "name": "Zhisheng Qi",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b85",
          "name": "Mahantesh Halappanavar",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b86",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b87",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b88",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b89",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b8a",
          "name": "Yao Ma",
          "hidden": false
        },
        {
          "_id": "68dc9a964159d1f2418f9b8b",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T09:40:27.000Z",
      "submittedOnDailyAt": "2025-10-01T01:36:08.796Z",
      "title": "Knowledge Homophily in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.",
      "upvotes": 1,
      "discussionId": "68dc9a964159d1f2418f9b8c",
      "ai_summary": "Graph Neural Network regression models estimate entity-level knowledgeability in Large Language Models to improve active labeling and multi-hop reasoning.",
      "ai_keywords": [
        "Large Language Models",
        "knowledge homophily",
        "graph representation",
        "knowledge checking",
        "triplet",
        "entity",
        "Graph Neural Network",
        "knowledgeability scores",
        "active labeling",
        "multi-hop path retrieval"
      ]
    },
    "publishedAt": "2025-09-28T05:40:27.000Z",
    "title": "Knowledge Homophily in Large Language Models",
    "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23773.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26555",
      "authors": [
        {
          "_id": "68dcad4f4159d1f2418f9bc7",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bc8",
          "name": "Rahim Entezari",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bc9",
          "name": "Maksym Zhuravinskyi",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bca",
          "name": "Maksim Lapin",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bcb",
          "name": "Reshinth Adithyan",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bcc",
          "name": "Amit Raj",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bcd",
          "name": "Chitta Baral",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bce",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "68dcad4f4159d1f2418f9bcf",
          "name": "Varun Jampani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:22:18.000Z",
      "submittedOnDailyAt": "2025-10-01T02:57:48.807Z",
      "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation",
      "submittedOnDailyBy": {
        "_id": "6320c537a023aad6a7680c8b",
        "avatarUrl": "/avatars/057dc492b8f756b83f12ced0b74fae65.svg",
        "isPro": false,
        "fullname": "Agneet Chatterjee",
        "user": "agneet",
        "type": "user"
      },
      "summary": "Recent advances in video generation have enabled high-fidelity video\nsynthesis from user provided prompts. However, existing models and benchmarks\nfail to capture the complexity and requirements of professional video\ngeneration. Towards that goal, we introduce Stable Cinemetrics, a structured\nevaluation framework that formalizes filmmaking controls into four\ndisentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.\nTogether, these taxonomies define 76 fine-grained control nodes grounded in\nindustry practices. Using these taxonomies, we construct a benchmark of prompts\naligned with professional use cases and develop an automated pipeline for\nprompt categorization and question generation, enabling independent evaluation\nof each control dimension. We conduct a large-scale human study spanning 10+\nmodels and 20K videos, annotated by a pool of 80+ film professionals. Our\nanalysis, both coarse and fine-grained reveal that even the strongest current\nmodels exhibit significant gaps, particularly in Events and Camera-related\ncontrols. To enable scalable evaluation, we train an automatic evaluator, a\nvision-language model aligned with expert annotations that outperforms existing\nzero-shot baselines. SCINE is the first approach to situate professional video\ngeneration within the landscape of video generative models, introducing\ntaxonomies centered around cinematic controls and supporting them with\nstructured evaluation pipelines and detailed analyses to guide future research.",
      "upvotes": 0,
      "discussionId": "68dcad4f4159d1f2418f9bd0",
      "projectPage": "https://stable-cinemetrics.github.io/",
      "ai_summary": "Stable Cinemetrics introduces a structured evaluation framework for professional video generation, using taxonomies to assess models across specific filmmaking controls.",
      "ai_keywords": [
        "video generation",
        "Stable Cinemetrics",
        "taxonomies",
        "Setup",
        "Event",
        "Lighting",
        "Camera",
        "fine-grained control nodes",
        "benchmark",
        "prompt categorization",
        "question generation",
        "human study",
        "automatic evaluator",
        "vision-language model",
        "zero-shot baselines",
        "SCINE"
      ]
    },
    "publishedAt": "2025-09-30T13:22:18.000Z",
    "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation",
    "summary": "Recent advances in video generation have enabled high-fidelity video\nsynthesis from user provided prompts. However, existing models and benchmarks\nfail to capture the complexity and requirements of professional video\ngeneration. Towards that goal, we introduce Stable Cinemetrics, a structured\nevaluation framework that formalizes filmmaking controls into four\ndisentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.\nTogether, these taxonomies define 76 fine-grained control nodes grounded in\nindustry practices. Using these taxonomies, we construct a benchmark of prompts\naligned with professional use cases and develop an automated pipeline for\nprompt categorization and question generation, enabling independent evaluation\nof each control dimension. We conduct a large-scale human study spanning 10+\nmodels and 20K videos, annotated by a pool of 80+ film professionals. Our\nanalysis, both coarse and fine-grained reveal that even the strongest current\nmodels exhibit significant gaps, particularly in Events and Camera-related\ncontrols. To enable scalable evaluation, we train an automatic evaluator, a\nvision-language model aligned with expert annotations that outperforms existing\nzero-shot baselines. SCINE is the first approach to situate professional video\ngeneration within the landscape of video generative models, introducing\ntaxonomies centered around cinematic controls and supporting them with\nstructured evaluation pipelines and detailed analyses to guide future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26555.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6320c537a023aad6a7680c8b",
      "avatarUrl": "/avatars/057dc492b8f756b83f12ced0b74fae65.svg",
      "fullname": "Agneet Chatterjee",
      "name": "agneet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "62e1573a6fb6e362b4a90690",
      "name": "stabilityai",
      "fullname": "Stability AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25134",
      "authors": [
        {
          "_id": "68db6506d2bf1f4b15ec75fc",
          "name": "Tomoyuki Suzuki",
          "hidden": false
        },
        {
          "_id": "68db6506d2bf1f4b15ec75fd",
          "name": "Kang-Jun Liu",
          "hidden": false
        },
        {
          "_id": "68db6506d2bf1f4b15ec75fe",
          "name": "Naoto Inoue",
          "hidden": false
        },
        {
          "_id": "68db6506d2bf1f4b15ec75ff",
          "name": "Kota Yamaguchi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/628b45cf488ff0ea1c7007dc/xhZBntJXvNREfocYxWcDi.png"
      ],
      "publishedAt": "2025-09-29T17:50:12.000Z",
      "submittedOnDailyAt": "2025-10-01T03:35:02.847Z",
      "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
      "submittedOnDailyBy": {
        "_id": "628b45cf488ff0ea1c7007dc",
        "avatarUrl": "/avatars/8000a1f6ea159cad9f0ffa384cd50f2a.svg",
        "isPro": false,
        "fullname": "Tomoyuki Suzuki",
        "user": "tomoyukun",
        "type": "user"
      },
      "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
      "upvotes": 0,
      "discussionId": "68db6506d2bf1f4b15ec7600",
      "projectPage": "https://cyberagentailab.github.io/LayerD/",
      "githubRepo": "https://github.com/CyberAgentAILab/LayerD",
      "ai_summary": "LayerD decomposes raster images into editable layers using iterative extraction and refinement, outperforming existing methods and enabling use with advanced image generators.",
      "ai_keywords": [
        "layer representation",
        "raster image",
        "decomposition",
        "foreground layers",
        "uniform appearance",
        "quality metric",
        "state-of-the-art image generators",
        "layer-based editing"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-09-29T13:50:12.000Z",
    "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
    "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/628b45cf488ff0ea1c7007dc/xhZBntJXvNREfocYxWcDi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25134.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628b45cf488ff0ea1c7007dc",
      "avatarUrl": "/avatars/8000a1f6ea159cad9f0ffa384cd50f2a.svg",
      "fullname": "Tomoyuki Suzuki",
      "name": "tomoyukun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25082",
      "authors": [
        {
          "_id": "68dcc85e4159d1f2418f9c43",
          "name": "Xiaoyi Huang",
          "hidden": false
        },
        {
          "_id": "68dcc85e4159d1f2418f9c44",
          "name": "Junwei Wu",
          "hidden": false
        },
        {
          "_id": "68dcc85e4159d1f2418f9c45",
          "name": "Kejia Zhang",
          "hidden": false
        },
        {
          "_id": "68dcc85e4159d1f2418f9c46",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "68dcc85e4159d1f2418f9c47",
          "name": "Zhiming Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:22:40.000Z",
      "submittedOnDailyAt": "2025-10-01T04:52:50.918Z",
      "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
      "submittedOnDailyBy": {
        "_id": "655469586bc4180700cf7a34",
        "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
        "isPro": false,
        "fullname": "Kejia Zhang",
        "user": "KejiaRobust",
        "type": "user"
      },
      "summary": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.",
      "upvotes": 0,
      "discussionId": "68dcc85e4159d1f2418f9c48",
      "ai_summary": "MANI-Pure, a magnitude-adaptive purification framework using diffusion models, effectively suppresses high-frequency adversarial perturbations while preserving low-frequency content, enhancing robust accuracy.",
      "ai_keywords": [
        "adversarial purification",
        "diffusion models",
        "uniform noise",
        "high-frequency regions",
        "magnitude spectrum",
        "frequency-targeted noise",
        "CIFAR-10",
        "ImageNet-1K",
        "RobustBench"
      ]
    },
    "publishedAt": "2025-09-29T13:22:40.000Z",
    "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
    "summary": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655469586bc4180700cf7a34",
      "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
      "fullname": "Kejia Zhang",
      "name": "KejiaRobust",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24732",
      "authors": [
        {
          "_id": "68dbc577d2bf1f4b15ec7a6b",
          "name": "Juergen Schmidhuber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T12:57:35.000Z",
      "submittedOnDailyAt": "2025-10-01T05:38:13.437Z",
      "title": "Who invented deep residual learning?",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.",
      "upvotes": 0,
      "discussionId": "68dbc578d2bf1f4b15ec7a6c",
      "ai_summary": "A timeline of the evolution of deep residual learning, a key advancement in neural network architecture.",
      "ai_keywords": [
        "deep artificial neural networks",
        "NNs",
        "deep residual learning",
        "residual connections"
      ]
    },
    "publishedAt": "2025-09-29T08:57:35.000Z",
    "title": "Who invented deep residual learning?",
    "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]