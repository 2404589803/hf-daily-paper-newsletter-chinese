[
  {
    "paper": {
      "id": "2508.02324",
      "authors": [
        {
          "_id": "68919a57f01a094725f83598",
          "name": "Chenfei Wu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f83599",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359a",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359b",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359c",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359d",
          "name": "Kun Yan",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359e",
          "name": "Sheng-ming Yin",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f8359f",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a0",
          "name": "Xiao Xu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a1",
          "name": "Yilei Chen",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a2",
          "name": "Yuxiang Chen",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a3",
          "name": "Zecheng Tang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a4",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a5",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a6",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a7",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a8",
          "name": "Chen Cheng",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835a9",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835aa",
          "name": "Deqing Li",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835ab",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835ac",
          "name": "Hao Meng",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835ad",
          "name": "Hu Wei",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835ae",
          "name": "Jingyuan Ni",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835af",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b0",
          "name": "Kuan Cao",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b1",
          "name": "Liang Peng",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b2",
          "name": "Lin Qu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b3",
          "name": "Minggang Wu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b4",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b5",
          "name": "Shuting Yu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b6",
          "name": "Tingkun Wen",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b7",
          "name": "Wensen Feng",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b8",
          "name": "Xiaoxiao Xu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835b9",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835ba",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835bb",
          "name": "Yongqiang Zhu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835bc",
          "name": "Yujia Wu",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835bd",
          "name": "Yuxuan Cai",
          "hidden": false
        },
        {
          "_id": "68919a57f01a094725f835be",
          "name": "Zenan Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/IGGtaa2qHZfwfhjAJfV-Q.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/wBudoV6xlkMnSdkPjkC4C.png"
      ],
      "publishedAt": "2025-08-04T11:49:20.000Z",
      "submittedOnDailyAt": "2025-08-05T04:39:37.929Z",
      "title": "Qwen-Image Technical Report",
      "submittedOnDailyBy": {
        "_id": "616fb788e2ad27af26561b1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
        "isPro": false,
        "fullname": "Xiao Xu",
        "user": "LooperXX",
        "type": "user"
      },
      "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
      "upvotes": 20,
      "discussionId": "68919a57f01a094725f835bf",
      "githubRepo": "https://github.com/QwenLM/Qwen-Image",
      "ai_summary": "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.",
      "ai_keywords": [
        "data pipeline",
        "progressive training",
        "curriculum learning",
        "text-to-image",
        "text-image-to-image",
        "image-to-image",
        "latent representations",
        "dual-encoding mechanism",
        "semantic consistency",
        "visual fidelity"
      ],
      "githubStars": 925
    },
    "publishedAt": "2025-08-04T07:49:20.000Z",
    "title": "Qwen-Image Technical Report",
    "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/IGGtaa2qHZfwfhjAJfV-Q.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/wBudoV6xlkMnSdkPjkC4C.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02324.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616fb788e2ad27af26561b1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
      "fullname": "Xiao Xu",
      "name": "LooperXX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02276",
      "authors": [
        {
          "_id": "68917696f01a094725f834b1",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b2",
          "name": "Zhuoyun Yu",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b3",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b4",
          "name": "Yan Cui",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b5",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b6",
          "name": "Weixu Wang",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b7",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b8",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834b9",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834ba",
          "name": "Zhi Huang",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834bb",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834bc",
          "name": "Xihong Lin",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834bd",
          "name": "Fabian Theis",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834be",
          "name": "Smita Krishnaswamy",
          "hidden": false
        },
        {
          "_id": "68917696f01a094725f834bf",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T10:43:31.000Z",
      "submittedOnDailyAt": "2025-08-05T01:42:34.345Z",
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "submittedOnDailyBy": {
        "_id": "63357c608adfa81faf2ac180",
        "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
        "isPro": false,
        "fullname": "Xiangru Tang",
        "user": "RTT1",
        "type": "user"
      },
      "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
      "upvotes": 19,
      "discussionId": "68917697f01a094725f834c2",
      "ai_summary": "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.",
      "ai_keywords": [
        "multi-agent framework",
        "single-cell multi-omics data",
        "task analysis",
        "method design",
        "experiment execution",
        "LLM agents",
        "gene knockouts",
        "drug treatments",
        "cytokine stimulations"
      ]
    },
    "publishedAt": "2025-08-04T06:43:31.000Z",
    "title": "CellForge: Agentic Design of Virtual Cell Models",
    "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02276.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "63357c608adfa81faf2ac180",
      "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
      "fullname": "Xiangru Tang",
      "name": "RTT1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01959",
      "authors": [
        {
          "_id": "68919172f01a094725f8355d",
          "name": "Junjie Wu",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f8355e",
          "name": "Jiangnan Li",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f8355f",
          "name": "Yuqing Li",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83560",
          "name": "Lemao Liu",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83561",
          "name": "Liyan Xu",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83562",
          "name": "Jiwei Li",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83563",
          "name": "Dit-Yan Yeung",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83564",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68919172f01a094725f83565",
          "name": "Mo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-03T23:59:31.000Z",
      "submittedOnDailyAt": "2025-08-05T03:38:04.443Z",
      "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
      "submittedOnDailyBy": {
        "_id": "64d36ca3036ae0b3756588e3",
        "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
        "isPro": false,
        "fullname": "Junjie Wu",
        "user": "junjiewu",
        "type": "user"
      },
      "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
      "upvotes": 13,
      "discussionId": "68919173f01a094725f83566",
      "projectPage": "https://huggingface.co/SituatedEmbedding",
      "ai_summary": "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "context window",
        "embedding models",
        "situated context",
        "situated embedding models (SitEmb)",
        "BGE-M3",
        "downstream applications"
      ]
    },
    "publishedAt": "2025-08-03T19:59:31.000Z",
    "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
    "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01959.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d36ca3036ae0b3756588e3",
      "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
      "fullname": "Junjie Wu",
      "name": "junjiewu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01059",
      "authors": [
        {
          "_id": "68916c3af01a094725f83460",
          "name": "Sajana Weerawardhena",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83461",
          "name": "Paul Kassianik",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83462",
          "name": "Blaine Nelson",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83463",
          "name": "Baturay Saglam",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83464",
          "name": "Anu Vellore",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83465",
          "name": "Aman Priyanshu",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83466",
          "name": "Supriti Vijay",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83467",
          "name": "Massimo Aufiero",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83468",
          "name": "Arthur Goldblatt",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83469",
          "name": "Fraser Burch",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346a",
          "name": "Ed Li",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346b",
          "name": "Jianliang He",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346c",
          "name": "Dhruv Kedia",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346d",
          "name": "Kojin Oshiba",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346e",
          "name": "Zhouran Yang",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f8346f",
          "name": "Yaron Singer",
          "hidden": false
        },
        {
          "_id": "68916c3af01a094725f83470",
          "name": "Amin Karbasi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T20:25:57.000Z",
      "submittedOnDailyAt": "2025-08-05T01:01:25.435Z",
      "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
      "submittedOnDailyBy": {
        "_id": "620042b28c2eb991da50d34e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
        "isPro": true,
        "fullname": "Aman Priyanshu",
        "user": "AmanPriyanshu",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
      "upvotes": 12,
      "discussionId": "68916c3af01a094725f83471",
      "ai_summary": "Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "cybersecurity",
        "Foundation-Sec-8B",
        "fine-tuning",
        "instruction-following",
        "conversational capabilities",
        "human preferences",
        "cybersecurity dialogue",
        "Llama 3.1-8B-Instruct",
        "GPT-4o-mini",
        "cyber threat intelligence"
      ]
    },
    "publishedAt": "2025-08-01T16:25:57.000Z",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
    "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01059.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620042b28c2eb991da50d34e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
      "fullname": "Aman Priyanshu",
      "name": "AmanPriyanshu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02150",
      "authors": [
        {
          "_id": "6891804cf01a094725f8350e",
          "name": "Qingyu Ren",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f8350f",
          "name": "Qianyu He",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83510",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83511",
          "name": "Jie Zeng",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83512",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83513",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83514",
          "name": "Weikang Zhou",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83515",
          "name": "Zeye Sun",
          "hidden": false
        },
        {
          "_id": "6891804cf01a094725f83516",
          "name": "Fei Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T07:48:59.000Z",
      "submittedOnDailyAt": "2025-08-05T04:44:57.940Z",
      "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
      "submittedOnDailyBy": {
        "_id": "636b36351340f879a2ec2bb1",
        "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
        "isPro": false,
        "fullname": "QianyuHe",
        "user": "Abbey4799",
        "type": "user"
      },
      "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
      "upvotes": 9,
      "discussionId": "6891804df01a094725f83517",
      "githubRepo": "https://github.com/Rainier-rq/verl-if",
      "ai_summary": "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.",
      "ai_keywords": [
        "self-supervised RL",
        "reasoning models",
        "instruction following",
        "internal signals",
        "scalability",
        "cost-effectiveness"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-08-04T03:48:59.000Z",
    "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
    "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02150.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b36351340f879a2ec2bb1",
      "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
      "fullname": "QianyuHe",
      "name": "Abbey4799",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.17520",
      "authors": [
        {
          "_id": "68917181f01a094725f8348d",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f8348e",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f8348f",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83490",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83491",
          "name": "Yang Tian",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83492",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83493",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83494",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83495",
          "name": "Yiyi Liao",
          "hidden": false
        },
        {
          "_id": "68917181f01a094725f83496",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-23T13:57:06.000Z",
      "submittedOnDailyAt": "2025-08-05T01:24:22.392Z",
      "title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation",
      "submittedOnDailyBy": {
        "_id": "64548f6c363bb3aaf9cba136",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
        "isPro": false,
        "fullname": "Shuai Yang",
        "user": "ShuaiYang03",
        "type": "user"
      },
      "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.",
      "upvotes": 5,
      "discussionId": "68917182f01a094725f83497",
      "ai_summary": "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.",
      "ai_keywords": [
        "vision-language-action (VLA) models",
        "multimodal reasoning",
        "precise action generation",
        "catastrophic forgetting",
        "large vision-language models (VLMs)",
        "Vision-Language-Action Instruction Tuning (VLA-IT)",
        "mixture-of-experts adaptation",
        "SimplerEnv tasks",
        "SimplerEnv-Instruct",
        "OpenVLA",
        "GPT-4o",
        "multimodal tasks",
        "inference-time scaling",
        "policy learning"
      ]
    },
    "publishedAt": "2025-07-23T09:57:06.000Z",
    "title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation",
    "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17520.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64548f6c363bb3aaf9cba136",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
      "fullname": "Shuai Yang",
      "name": "ShuaiYang03",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01548",
      "authors": [
        {
          "_id": "689186aef01a094725f83537",
          "name": "Quan-Sheng Zeng",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f83538",
          "name": "Yunheng Li",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f83539",
          "name": "Qilong Wang",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f8353a",
          "name": "Peng-Tao Jiang",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f8353b",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f8353c",
          "name": "Ming-Ming Cheng",
          "hidden": false
        },
        {
          "_id": "689186aef01a094725f8353d",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-03T02:15:43.000Z",
      "submittedOnDailyAt": "2025-08-05T05:07:36.709Z",
      "title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "Visual token compression is critical for Large Vision-Language Models (LVLMs)\nto efficiently process high-resolution inputs. Existing methods that typically\nadopt fixed compression ratios cannot adapt to scenes of varying complexity,\noften causing imprecise pruning that discards informative visual tokens and\nresults in degraded model performance. To address this issue, we introduce a\ndynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes\na data-driven ''glimpse'' and prunes irrelevant visual tokens in a single\nforward pass before answer generation. This approach prunes 92.6% of visual\ntokens while on average fully retaining the baseline performance on free-form\nVQA tasks. The reduced computational cost also enables more effective\nfine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline\nperformance while maintaining a similarly high pruning rate. Our work paves a\nnew way for building more powerful and efficient LVLMs.",
      "upvotes": 3,
      "discussionId": "689186aef01a094725f8353e",
      "githubRepo": "https://github.com/HVision-NKU/GlimpsePrune",
      "ai_summary": "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.",
      "ai_keywords": [
        "Visual token compression",
        "Large Vision-Language Models",
        "dynamic pruning",
        "GlimpsePrune",
        "free-form VQA tasks",
        "parameter-efficient fine-tuning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-08-02T22:15:43.000Z",
    "title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models",
    "summary": "Visual token compression is critical for Large Vision-Language Models (LVLMs)\nto efficiently process high-resolution inputs. Existing methods that typically\nadopt fixed compression ratios cannot adapt to scenes of varying complexity,\noften causing imprecise pruning that discards informative visual tokens and\nresults in degraded model performance. To address this issue, we introduce a\ndynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes\na data-driven ''glimpse'' and prunes irrelevant visual tokens in a single\nforward pass before answer generation. This approach prunes 92.6% of visual\ntokens while on average fully retaining the baseline performance on free-form\nVQA tasks. The reduced computational cost also enables more effective\nfine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline\nperformance while maintaining a similarly high pruning rate. Our work paves a\nnew way for building more powerful and efficient LVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01151",
      "authors": [
        {
          "_id": "68917c7af01a094725f834db",
          "name": "Yu Lei",
          "hidden": false
        },
        {
          "_id": "68917c7af01a094725f834dc",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "68917c7af01a094725f834dd",
          "name": "Qingyu Shi",
          "hidden": false
        },
        {
          "_id": "68917c7af01a094725f834de",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "68917c7af01a094725f834df",
          "name": "Kaidong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T02:23:20.000Z",
      "submittedOnDailyAt": "2025-08-05T02:08:49.581Z",
      "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.",
      "upvotes": 3,
      "discussionId": "68917c7af01a094725f834e0",
      "ai_summary": "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Personalized Safety Alignment (PSA)",
        "user-specific profiles",
        "cross-attention mechanism",
        "harmful content suppression",
        "Win Rate",
        "Pass Rate"
      ]
    },
    "publishedAt": "2025-08-01T22:23:20.000Z",
    "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02317",
      "authors": [
        {
          "_id": "68917ae7f01a094725f834cd",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834ce",
          "name": "Yaowei Zheng",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834cf",
          "name": "Zhelun Shi",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d0",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d1",
          "name": "Bin Jia",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d2",
          "name": "Ziyue Huang",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d3",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d4",
          "name": "Youjie Li",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d5",
          "name": "Jiacheng Yang",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d6",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d7",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68917ae7f01a094725f834d8",
          "name": "Xin Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T11:33:04.000Z",
      "submittedOnDailyAt": "2025-08-05T02:01:45.888Z",
      "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
      "submittedOnDailyBy": {
        "_id": "642fef28a043f0ac7defa8a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
        "isPro": false,
        "fullname": "Yaowei Zheng",
        "user": "hiyouga",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
      "upvotes": 2,
      "discussionId": "68917ae8f01a094725f834d9",
      "githubRepo": "https://github.com/ByteDance-Seed/VeOmni",
      "ai_summary": "A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.",
      "ai_keywords": [
        "large language models",
        "omni-modal understanding",
        "omni-modal generation",
        "heterogeneous model architectures",
        "parallel logic",
        "model-centric distributed recipes",
        "communication",
        "computation",
        "3D parallelism",
        "flexible configuration interface",
        "mixture-of-experts",
        "tokens/sec/GPU throughput",
        "context lengths"
      ],
      "githubStars": 400
    },
    "publishedAt": "2025-08-04T07:33:04.000Z",
    "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
    "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642fef28a043f0ac7defa8a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
      "fullname": "Yaowei Zheng",
      "name": "hiyouga",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2447
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00890",
      "authors": [
        {
          "_id": "68919443f01a094725f83574",
          "name": "Fali Wang",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f83575",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f83576",
          "name": "Zhenwei Dai",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f83577",
          "name": "Jingying Zeng",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f83578",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f83579",
          "name": "Zongyu Wu",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f8357a",
          "name": "Chen Luo",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f8357b",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f8357c",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f8357d",
          "name": "Qi He",
          "hidden": false
        },
        {
          "_id": "68919443f01a094725f8357e",
          "name": "Suhang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T19:21:18.000Z",
      "submittedOnDailyAt": "2025-08-05T03:49:48.335Z",
      "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks",
      "submittedOnDailyBy": {
        "_id": "644a8ca97c5c68c7762906a0",
        "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
        "isPro": false,
        "fullname": "Fali Wang",
        "user": "FairyFali",
        "type": "user"
      },
      "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.",
      "upvotes": 2,
      "discussionId": "68919443f01a094725f8357f",
      "ai_summary": "AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.",
      "ai_keywords": [
        "test-time scaling",
        "large language models",
        "multi-stage complex tasks",
        "combinatorial search space",
        "inference cost",
        "compute-optimal scaling",
        "AgentTTS",
        "iterative feedback-driven interactions"
      ]
    },
    "publishedAt": "2025-07-26T15:21:18.000Z",
    "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks",
    "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a8ca97c5c68c7762906a0",
      "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
      "fullname": "Fali Wang",
      "name": "FairyFali",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01415",
      "authors": [
        {
          "_id": "689183e9f01a094725f83527",
          "name": "Mingcong Lei",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83528",
          "name": "Honghao Cai",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83529",
          "name": "Zezhou Cui",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352a",
          "name": "Liangchen Tan",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352b",
          "name": "Junkun Hong",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352c",
          "name": "Gehan Hu",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352d",
          "name": "Shuangyu Zhu",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352e",
          "name": "Yimou Wu",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f8352f",
          "name": "Shaohan Jiang",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83530",
          "name": "Ge Wang",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83531",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83532",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83533",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "689183e9f01a094725f83534",
          "name": "Yatong Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T15:39:42.000Z",
      "submittedOnDailyAt": "2025-08-05T02:39:59.148Z",
      "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
      "submittedOnDailyBy": {
        "_id": "6628c6107751d297d7025a71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
        "isPro": false,
        "fullname": "Lei Mingcong",
        "user": "SP4595",
        "type": "user"
      },
      "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
      "upvotes": 1,
      "discussionId": "689183eaf01a094725f83535",
      "ai_summary": "RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.",
      "ai_keywords": [
        "Information Preprocessor",
        "Lifelong Embodied Memory System",
        "Closed-Loop Planning Module",
        "Low-Level Executer",
        "Spatial submodule",
        "Temporal submodule",
        "Episodic submodule",
        "Semantic submodule",
        "Knowledge Graph",
        "dynamic Knowledge Graph",
        "consistent architectural design",
        "memory consistency",
        "memory scalability",
        "lifelong learning",
        "ablation studies",
        "real-world deployment",
        "EmbodiedBench",
        "Qwen2.5-VL-72B-Ins",
        "Claude3.5-Sonnet",
        "state-of-the-art",
        "SOTA",
        "critic",
        "spatial memory",
        "long-term memory"
      ]
    },
    "publishedAt": "2025-08-02T11:39:42.000Z",
    "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
    "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628c6107751d297d7025a71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
      "fullname": "Lei Mingcong",
      "name": "SP4595",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01287",
      "authors": [
        {
          "_id": "6891995ff01a094725f83594",
          "name": "Micah Rentschler",
          "hidden": false
        },
        {
          "_id": "6891995ff01a094725f83595",
          "name": "Jesse Roberts",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T09:42:59.000Z",
      "submittedOnDailyAt": "2025-08-05T04:13:13.413Z",
      "title": "Exploitation Is All You Need... for Exploration",
      "submittedOnDailyBy": {
        "_id": "66fffe1b3ec4cc293d40f2d5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png",
        "isPro": false,
        "fullname": "Micah Rentschler",
        "user": "micahr234",
        "type": "user"
      },
      "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.",
      "upvotes": 1,
      "discussionId": "68919960f01a094725f83596",
      "ai_summary": "Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.",
      "ai_keywords": [
        "meta-reinforcement learning",
        "meta-RL",
        "exploration-exploitation dilemma",
        "intrinsic rewards",
        "greedy objective",
        "emergent exploratory behavior",
        "Recurring Environmental Structure",
        "Agent Memory",
        "Long-Horizon Credit Assignment",
        "stochastic multi-armed bandits",
        "temporally extended gridworlds",
        "pseudo-Thompson Sampling"
      ]
    },
    "publishedAt": "2025-08-02T05:42:59.000Z",
    "title": "Exploitation Is All You Need... for Exploration",
    "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fffe1b3ec4cc293d40f2d5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png",
      "fullname": "Micah Rentschler",
      "name": "micahr234",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00910",
      "authors": [
        {
          "_id": "68917e31f01a094725f834f9",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "68917e31f01a094725f834fa",
          "name": "Dingmin Wang",
          "hidden": false
        },
        {
          "_id": "68917e31f01a094725f834fb",
          "name": "Hantian Ding",
          "hidden": false
        },
        {
          "_id": "68917e31f01a094725f834fc",
          "name": "Varun Kumar",
          "hidden": false
        },
        {
          "_id": "68917e31f01a094725f834fd",
          "name": "Zijian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T18:10:37.000Z",
      "submittedOnDailyAt": "2025-08-05T02:18:02.073Z",
      "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime",
      "submittedOnDailyBy": {
        "_id": "62b7fb545233925f253531c8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
        "isPro": false,
        "fullname": "Terry Yue Zhuo",
        "user": "terryyz",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.",
      "upvotes": 1,
      "discussionId": "68917e31f01a094725f834fe",
      "projectPage": "https://github.com/amazon-science/Cyber-Zero",
      "githubRepo": "https://github.com/amazon-science/Cyber-Zero",
      "ai_summary": "Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "Cyber-Zero",
        "CTF writeups",
        "persona-driven LLM simulation",
        "agent trajectories",
        "InterCode-CTF",
        "NYU CTF Bench",
        "Cybench",
        "Cyber-Zero-32B",
        "DeepSeek-V3-0324",
        "Claude-3.5-Sonnet"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-29T14:10:37.000Z",
    "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b7fb545233925f253531c8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
      "fullname": "Terry Yue Zhuo",
      "name": "terryyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  }
]