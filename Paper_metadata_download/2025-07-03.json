[
  {
    "paper": {
      "id": "2507.01949",
      "authors": [
        {
          "_id": "6865e6218c83dab5f72d1e47",
          "name": "Kwai Keye Team",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e48",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e49",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4a",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4b",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4c",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4d",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4e",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4f",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e50",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e51",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e52",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e53",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e54",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e55",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e56",
          "user": {
            "_id": "65802e81b701ff85a37caba8",
            "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
            "isPro": false,
            "fullname": "jiangxia cao",
            "user": "caojiangxia",
            "type": "user"
          },
          "name": "Jiangxia Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e57",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e58",
          "user": {
            "_id": "61540338e5b9ae6774201e58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632895895007-61540338e5b9ae6774201e58.png",
            "isPro": false,
            "fullname": "jingyun",
            "user": "hjy",
            "type": "user"
          },
          "name": "Jingyun Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e59",
          "name": "Jin Ouyang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5e",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5f",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e60",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e61",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e62",
          "user": {
            "_id": "68652063e29f1407b58da60f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
            "isPro": false,
            "fullname": "tingting gao",
            "user": "TinaGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e63",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e64",
          "user": {
            "_id": "65423daba385933e812516d5",
            "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
            "isPro": false,
            "fullname": "wei yuan",
            "user": "yw95",
            "type": "user"
          },
          "name": "Wei Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e65",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e66",
          "user": {
            "_id": "64a4dba8fe950993d2d89113",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
            "isPro": false,
            "fullname": "Xiao Hu",
            "user": "huxiao09",
            "type": "user"
          },
          "name": "Xiao Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e67",
          "user": {
            "_id": "673597cfc2424474d12ca58c",
            "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
            "isPro": false,
            "fullname": "xingyulu",
            "user": "Xingyulu47",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e68",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e69",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6a",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6b",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6c",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6d",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6e",
          "user": {
            "_id": "641948b7d13ffa40812eb239",
            "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
            "isPro": false,
            "fullname": "Zhixin Ling",
            "user": "NamingIsTroublesome",
            "type": "user"
          },
          "name": "Zhixin Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6f",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e70",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e71",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e72",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e73",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e74",
          "name": "Jiawei Guo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e75",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e76",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e77",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e78",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e79",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7a",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7c",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7d",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7e",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7f",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e80",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e81",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e82",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:57:28.000Z",
      "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
      "title": "Kwai Keye-VL Technical Report",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
      "upvotes": 74,
      "discussionId": "6865e6218c83dab5f72d1e83",
      "projectPage": "https://kwai-keye.github.io/",
      "githubRepo": "https://github.com/Kwai-Keye/Keye",
      "githubStars": 365
    },
    "publishedAt": "2025-07-02T13:57:28.000Z",
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01945",
      "authors": [
        {
          "_id": "6865e4b88c83dab5f72d1e41",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e42",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e43",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e44",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
      ],
      "publishedAt": "2025-07-02T17:55:50.000Z",
      "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
      "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
      "upvotes": 46,
      "discussionId": "6865e4b88c83dab5f72d1e45",
      "projectPage": "https://cn-makers.github.io/long_animation_web/",
      "githubRepo": "https://github.com/CN-makers/LongAnimation",
      "githubStars": 65
    },
    "publishedAt": "2025-07-02T13:55:50.000Z",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
    "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01634",
      "authors": [
        {
          "_id": "6865e04b8c83dab5f72d1e2d",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2e",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2f",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e30",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T12:05:57.000Z",
      "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
      "title": "Depth Anything at Any Condition",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
      "upvotes": 21,
      "discussionId": "6865e04b8c83dab5f72d1e31",
      "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
      "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
      "githubStars": 67
    },
    "publishedAt": "2025-07-02T08:05:57.000Z",
    "title": "Depth Anything at Any Condition",
    "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01925",
      "authors": [
        {
          "_id": "686600cf8c83dab5f72d1ed0",
          "name": "Yifan Zhong",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed1",
          "name": "Fengshuo Bai",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed2",
          "user": {
            "_id": "6578459d62d3ac1817ed79fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
            "isPro": false,
            "fullname": "Shaofei Cai",
            "user": "phython96",
            "type": "user"
          },
          "name": "Shaofei Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed3",
          "user": {
            "_id": "66ee5cf1801ea45d7a44a542",
            "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
            "isPro": false,
            "fullname": "XUCHUAN HUANG",
            "user": "Feernnn",
            "type": "user"
          },
          "name": "Xuchuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed4",
          "name": "Zhang Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed5",
          "name": "Xiaowei Zhang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed6",
          "name": "Yuanfei Wang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed7",
          "name": "Shaoyang Guo",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed8",
          "name": "Tianrui Guan",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed9",
          "name": "Ka Nam Lui",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1eda",
          "name": "Zhiquan Qi",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edb",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edc",
          "name": "Yuanpei Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edd",
          "name": "Yaodong Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:34:52.000Z",
      "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
      "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
      "submittedOnDailyBy": {
        "_id": "655d9f43b5da99edaf3f2f81",
        "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
        "isPro": false,
        "fullname": "Yifan Zhong",
        "user": "Yifan-Zhong",
        "type": "user"
      },
      "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
      "upvotes": 12,
      "discussionId": "686600cf8c83dab5f72d1ede",
      "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
      "githubStars": 14
    },
    "publishedAt": "2025-07-02T13:34:52.000Z",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
    "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655d9f43b5da99edaf3f2f81",
      "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
      "fullname": "Yifan Zhong",
      "name": "Yifan-Zhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01953",
      "authors": [
        {
          "_id": "686601648c83dab5f72d1ee0",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee1",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee2",
          "name": "Jinghao Wang",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee3",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:58:20.000Z",
      "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
      "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
      "upvotes": 7,
      "discussionId": "686601658c83dab5f72d1ee4",
      "projectPage": "https://yukangcao.github.io/FreeMorph/",
      "githubRepo": "https://github.com/yukangcao/FreeMorph",
      "githubStars": 3
    },
    "publishedAt": "2025-07-02T13:58:20.000Z",
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23552",
      "authors": [
        {
          "_id": "6865e0148c83dab5f72d1e26",
          "name": "Mingi Kwon",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e27",
          "user": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "isPro": false,
            "fullname": "Joonghyuk Shin",
            "user": "alex4727",
            "type": "user"
          },
          "name": "Joonghyuk Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:56.605Z",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e28",
          "name": "Jaeseok Jung",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e29",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e2a",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:51:40.000Z",
      "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
      "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
      "upvotes": 2,
      "discussionId": "6865e0148c83dab5f72d1e2b"
    },
    "publishedAt": "2025-06-30T02:51:40.000Z",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01957",
      "authors": [
        {
          "_id": "686633d28c83dab5f72d1f39",
          "name": "Zhuoyang Zhang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3a",
          "name": "Luke J. Huang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3b",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3c",
          "name": "Shang Yang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3d",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3e",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3f",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:23.000Z",
      "submittedOnDailyAt": "2025-07-03T06:13:07.255Z",
      "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
      "submittedOnDailyBy": {
        "_id": "650e6ab08f3228d807707735",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
        "isPro": false,
        "fullname": "Zhuoyang Zhang",
        "user": "zhuoyang20",
        "type": "user"
      },
      "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
      "upvotes": 1,
      "discussionId": "686633d28c83dab5f72d1f40"
    },
    "publishedAt": "2025-07-02T13:59:23.000Z",
    "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
    "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e6ab08f3228d807707735",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
      "fullname": "Zhuoyang Zhang",
      "name": "zhuoyang20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22868",
      "authors": [
        {
          "_id": "68637f0d588cea0da970c95e",
          "user": {
            "_id": "6719de3235b6494469ab69f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
            "isPro": false,
            "fullname": "Junsung Lee",
            "user": "jslee525",
            "type": "user"
          },
          "name": "Junsung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:32:55.680Z",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c95f",
          "name": "Junoh Kang",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c960",
          "name": "Bohyung Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T12:36:19.000Z",
      "submittedOnDailyAt": "2025-07-03T06:08:50.749Z",
      "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
      "submittedOnDailyBy": {
        "_id": "6719de3235b6494469ab69f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
        "isPro": false,
        "fullname": "Junsung Lee",
        "user": "jslee525",
        "type": "user"
      },
      "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
      "upvotes": 1,
      "discussionId": "68637f0e588cea0da970c961",
      "projectPage": "https://jslee525.github.io/str-match",
      "githubRepo": "https://github.com/jslee525/STR-Match_official",
      "ai_summary": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.",
      "ai_keywords": [
        "T2V diffusion models",
        "latent optimization",
        "spatiotemporal pixel relevance",
        "latent mask",
        "2D spatial attention",
        "1D temporal modules"
      ]
    },
    "publishedAt": "2025-06-28T08:36:19.000Z",
    "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
    "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6719de3235b6494469ab69f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
      "fullname": "Junsung Lee",
      "name": "jslee525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]