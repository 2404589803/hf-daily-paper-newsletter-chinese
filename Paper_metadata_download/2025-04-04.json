[
  {
    "paper": {
      "id": "2504.02782",
      "authors": [
        {
          "_id": "67ef502ce803d818f00e1b94",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b95",
          "user": {
            "_id": "66978ee0b8656f6506b4acb2",
            "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
            "isPro": false,
            "fullname": "Junyan Ye",
            "user": "Yejy53",
            "type": "user"
          },
          "name": "Junyan Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b96",
          "user": {
            "_id": "66d5b56c77a026c3d2086a79",
            "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
            "isPro": false,
            "fullname": "Weijia Li",
            "user": "liweijia",
            "type": "user"
          },
          "name": "Weijia Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b97",
          "user": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "isPro": false,
            "fullname": "Zilong Huang",
            "user": "SereinH",
            "type": "user"
          },
          "name": "Zilong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b98",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b99",
          "user": {
            "_id": "67ef53656c7ba428e7c2e605",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
            "isPro": false,
            "fullname": "He",
            "user": "shawnxyh",
            "type": "user"
          },
          "name": "Xiangyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9a",
          "user": {
            "_id": "6459a47e4fe72fae522b4fc9",
            "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
            "isPro": false,
            "fullname": "Kaiqing.Lin",
            "user": "lin6123",
            "type": "user"
          },
          "name": "Kaiqing Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9b",
          "user": {
            "_id": "670ddb69d6ac6394419d88c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
            "isPro": false,
            "fullname": "Jun He",
            "user": "JunHe0915",
            "type": "user"
          },
          "name": "Jun He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9d",
          "user": {
            "_id": "66135a5e50350afe76beebce",
            "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
            "isPro": false,
            "fullname": "yl2488",
            "user": "yl2488",
            "type": "user"
          },
          "name": "Li Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:23:16.000Z",
      "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
      "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
      "upvotes": 25,
      "discussionId": "67ef502fe803d818f00e1c70",
      "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
      "ai_keywords": [
        "auto-regressive (AR)",
        "diffusion-based head",
        "VAR-like architectures",
        "multi-round image editing",
        "image forensic models"
      ]
    },
    "publishedAt": "2025-04-03T13:23:16.000Z",
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02826",
      "authors": [
        {
          "_id": "67ef4be0985aa66b67021ddc",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddd",
          "user": {
            "_id": "6710be3e6d1b33cf24417e38",
            "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
            "isPro": false,
            "fullname": "zpy",
            "user": "zpy777",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021dde",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddf",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de0",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de1",
          "user": {
            "_id": "65535b125413c1a54e6fb243",
            "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
            "isPro": false,
            "fullname": "Guangtao Zhai",
            "user": "GTZhai",
            "type": "user"
          },
          "name": "Guangtao Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de2",
          "user": {
            "_id": "667289f903c802764985d8c6",
            "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
            "isPro": false,
            "fullname": "Junchi Yan",
            "user": "Rethinker",
            "type": "user"
          },
          "name": "Junchi Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de3",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de4",
          "user": {
            "_id": "648e77184cae4f6921dbb382",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
            "isPro": false,
            "fullname": "Xue Yang",
            "user": "yangxue",
            "type": "user"
          },
          "name": "Xue Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
      ],
      "publishedAt": "2025-04-03T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
      "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
      "upvotes": 14,
      "discussionId": "67ef4be4985aa66b67021ef2",
      "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
      "ai_keywords": [
        "Large Multi-modality Models (LMMs)",
        "General Visual Editing",
        "Temporal Reasoning",
        "Causal Reasoning",
        "Spatial Reasoning",
        "Logical Reasoning",
        "RISEBench",
        "Instruction Reasoning",
        "Appearance Consistency",
        "Visual Plausibility",
        "GPT-4o-Native",
        "multimodal systems"
      ]
    },
    "publishedAt": "2025-04-03T13:59:56.000Z",
    "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
    "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02587",
      "authors": [
        {
          "_id": "67ef3f9804be7fba0c882738",
          "user": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "isPro": false,
            "fullname": "Yan Ma",
            "user": "ManTle",
            "type": "user"
          },
          "name": "Yan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c882739",
          "user": {
            "_id": "64b370fe6d953e7c75ede314",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
            "isPro": false,
            "fullname": "Steffi Chern",
            "user": "steffichern",
            "type": "user"
          },
          "name": "Steffi Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273a",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273b",
          "user": {
            "_id": "64c525e4d68946edad6c7067",
            "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
            "isPro": false,
            "fullname": "Yiran Zhong",
            "user": "IanZhong",
            "type": "user"
          },
          "name": "Yiran Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273c",
          "user": {
            "_id": "6144a0c4ff1146bbd84d9865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
            "isPro": false,
            "fullname": "Pengfei Liu",
            "user": "Pengfei",
            "type": "user"
          },
          "name": "Pengfei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T13:53:28.000Z",
      "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
      "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
      "upvotes": 13,
      "discussionId": "67ef3f9904be7fba0c882772",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning capabilities",
        "large language models",
        "vision-language models",
        "reproducibility",
        "accessibility",
        "standardized evaluation protocols",
        "transparent framework",
        "four-step pipeline",
        "training dynamics",
        "reflective behaviors",
        "visual reasoning tasks",
        "response length",
        "reflection",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-04-03T09:53:28.000Z",
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02436",
      "authors": [
        {
          "_id": "67ef3dfae8b932ae7a832950",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832951",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832952",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832953",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832954",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832955",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832956",
          "user": {
            "_id": "666a674967c686801acf25bb",
            "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
            "isPro": false,
            "fullname": "jingtao xu",
            "user": "raul678",
            "type": "user"
          },
          "name": "Jingtao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832957",
          "user": {
            "_id": "634672bfb7b4e71c7f45360f",
            "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
            "isPro": false,
            "fullname": "Fan Mingyuan",
            "user": "MichaelFan",
            "type": "user"
          },
          "name": "Mingyuan Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832958",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832959",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a83295a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T09:50:50.000Z",
      "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
      "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
      "upvotes": 10,
      "discussionId": "67ef3dfee8b932ae7a832a97",
      "ai_keywords": [
        "elements-to-video (E2V)",
        "image-text joint embedding model",
        "prompt-reference-video triplets",
        "generative process",
        "multi-element representations",
        "strict consistency",
        "coherent composition",
        "natural outputs",
        "output stability",
        "A2 Bench (benchmark)",
        "high-quality videos",
        "precise element control",
        "open-source commercial grade model"
      ]
    },
    "publishedAt": "2025-04-03T05:50:50.000Z",
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6572
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01990",
      "authors": [
        {
          "_id": "67ef8723d325fe100f36107e",
          "user": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "isPro": false,
            "fullname": "Bang Liu",
            "user": "Bang-UdeM-Mila",
            "type": "user"
          },
          "name": "Bang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T07:15:51.456Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36107f",
          "user": {
            "_id": "65af5f8f3db2280ece7fac79",
            "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
            "isPro": false,
            "fullname": "Xin-Feng Li",
            "user": "xinfeng1i",
            "type": "user"
          },
          "name": "Xinfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361080",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361081",
          "user": {
            "_id": "64324bb3034ecbefddd99863",
            "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
            "isPro": false,
            "fullname": "Jinlin Wang",
            "user": "JinlinW",
            "type": "user"
          },
          "name": "Jinlin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361082",
          "name": "Tanjin He",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361083",
          "name": "Sirui Hong",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361084",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361085",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361086",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361087",
          "user": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "isPro": false,
            "fullname": "KunlunZhu",
            "user": "KunlunZhu",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361088",
          "user": {
            "_id": "6664783b0ab8b63cbb4a3156",
            "avatarUrl": "/avatars/71859e6f76c157191bd2e968061f08b0.svg",
            "isPro": false,
            "fullname": "cyh",
            "user": "chengyuheng",
            "type": "user"
          },
          "name": "Yuheng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:14.543Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361089",
          "user": {
            "_id": "62bb1e0f3ff437e49a3088e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
            "isPro": true,
            "fullname": "Suyuchen Wang",
            "user": "sheryc",
            "type": "user"
          },
          "name": "Suyuchen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108a",
          "user": {
            "_id": "655c092183186f133f959108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
            "isPro": false,
            "fullname": "Xiaoqiang Wang",
            "user": "qindomitable",
            "type": "user"
          },
          "name": "Xiaoqiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108c",
          "user": {
            "_id": "648ee5fe9ae7cc4fcffa9aef",
            "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
            "isPro": false,
            "fullname": "Haibo Jin",
            "user": "Nick233",
            "type": "user"
          },
          "name": "Haibo Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108d",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Peiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108e",
          "user": {
            "_id": "66197a8afeb55cbe39e50ae8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
            "isPro": false,
            "fullname": "Ollie Liu",
            "user": "oliu-io",
            "type": "user"
          },
          "name": "Ollie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108f",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361090",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361091",
          "user": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
            "isPro": false,
            "fullname": "Zhaoyang Yu",
            "user": "MoshiQAQ",
            "type": "user"
          },
          "name": "Zhaoyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361092",
          "name": "Haochen Shi",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361093",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361094",
          "name": "Dekun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361095",
          "user": {
            "_id": "6402e8fb06c715b93407442d",
            "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
            "isPro": false,
            "fullname": "Fengwei Teng",
            "user": "leavendough",
            "type": "user"
          },
          "name": "Fengwei Teng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361096",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361097",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361098",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361099",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109a",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109b",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109d",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109e",
          "user": {
            "_id": "66a8fa5fd909c30167f1f5cd",
            "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
            "isPro": false,
            "fullname": "Glen Berseth",
            "user": "gberseth",
            "type": "user"
          },
          "name": "Glen Berseth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109f",
          "name": "Jianyun Nie",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a0",
          "name": "Ian Foster",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a1",
          "name": "Logan Ward",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a3",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a4",
          "user": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "isPro": false,
            "fullname": "mingchen zhuge",
            "user": "tjpxiaoming",
            "type": "user"
          },
          "name": "Mingchen Zhuge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a5",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a6",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a7",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a8",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a9",
          "user": {
            "_id": "670918b02806bda07e44780c",
            "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
            "isPro": false,
            "fullname": "Jian Pei",
            "user": "StrawHat2333",
            "type": "user"
          },
          "name": "Jian Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610aa",
          "name": "Qiang Yang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ab",
          "user": {
            "_id": "6342829eb9454d65a2e7a4c4",
            "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
            "isPro": false,
            "fullname": "Xiaoliang Qi",
            "user": "phynics",
            "type": "user"
          },
          "name": "Xiaoliang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ac",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T18:00:29.000Z",
      "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
      "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
      "submittedOnDailyBy": {
        "_id": "64403daae44f30a72323e4ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
        "isPro": false,
        "fullname": "mingchen zhuge",
        "user": "tjpxiaoming",
        "type": "user"
      },
      "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
      "upvotes": 10,
      "discussionId": "67ef8727d325fe100f3611aa",
      "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
    },
    "publishedAt": "2025-03-31T14:00:29.000Z",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64403daae44f30a72323e4ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
      "fullname": "mingchen zhuge",
      "name": "tjpxiaoming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00502",
      "authors": [
        {
          "_id": "67ef72898667ee5c99026d16",
          "user": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "isPro": false,
            "fullname": "Qianhao Yuan",
            "user": "yuanqianhao",
            "type": "user"
          },
          "name": "Qianhao Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d17",
          "name": "Qingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d18",
          "name": "Yanjiang Liu",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d19",
          "user": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "isPro": false,
            "fullname": "Jiawei Chen",
            "user": "chenjiawei-icip",
            "type": "user"
          },
          "name": "Jiawei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1a",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1b",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1c",
          "name": "Jia Zheng",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1d",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1e",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:47:55.000Z",
      "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
      "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
      "submittedOnDailyBy": {
        "_id": "67014d33126f9ab39fc52481",
        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
        "isPro": false,
        "fullname": "Qianhao Yuan",
        "user": "yuanqianhao",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
      "upvotes": 8,
      "discussionId": "67ef728a8667ee5c99026d69",
      "githubRepo": "https://github.com/icip-cas/ShortV",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Layer Contribution (LC)",
        "visual tokens",
        "transformations",
        "layer-wise redundancy",
        "model output",
        "divergence",
        "ineffective layers",
        "training-free method",
        "visual token updates",
        "computational costs",
        "FLOPs",
        "LLaVA-NeXT-13B"
      ]
    },
    "publishedAt": "2025-04-01T03:47:55.000Z",
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67014d33126f9ab39fc52481",
      "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
      "fullname": "Qianhao Yuan",
      "name": "yuanqianhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02542",
      "authors": [
        {
          "_id": "67ef3773ac0c701df7fd98aa",
          "user": {
            "_id": "6264a7dfc39850dc093eb68a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
            "isPro": false,
            "fullname": "Fa-Ting Hong",
            "user": "HarlanHong",
            "type": "user"
          },
          "name": "Fa-Ting Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ab",
          "user": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "isPro": false,
            "fullname": "Zunnan Xu",
            "user": "xuzn",
            "type": "user"
          },
          "name": "Zunnan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ac",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ad",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ae",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98af",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b0",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b1",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
      ],
      "publishedAt": "2025-04-03T12:44:41.000Z",
      "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
      "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
      "submittedOnDailyBy": {
        "_id": "66feab48651e00e22f33222e",
        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
        "isPro": false,
        "fullname": "Dan Xu",
        "user": "danxuhk",
        "type": "user"
      },
      "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
      "upvotes": 7,
      "discussionId": "67ef3775ac0c701df7fd994c",
      "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
      "githubRepo": "https://github.com/harlanhong/ACTalker",
      "ai_keywords": [
        "ACTalker",
        "video diffusion framework",
        "multi-signals control",
        "parallel mamba structure",
        "driving signals",
        "gate mechanism",
        "temporal coordination",
        "spatial coordination",
        "feature tokens",
        "mask-drop strategy",
        "facial videos",
        "multiple driving modalities"
      ]
    },
    "publishedAt": "2025-04-03T08:44:41.000Z",
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
    "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feab48651e00e22f33222e",
      "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
      "fullname": "Dan Xu",
      "name": "danxuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02507",
      "authors": [
        {
          "_id": "67ef5a3d4417508df8d99dad",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99dae",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99daf",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99db0",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T11:41:55.000Z",
      "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
      "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
      "upvotes": 7,
      "discussionId": "67ef5a3e4417508df8d99dfc",
      "githubRepo": "https://github.com/bluorion-com/ZClip/",
      "ai_keywords": [
        "large language models (LLMs)",
        "gradient instability",
        "loss spikes",
        "catastrophic divergence",
        "checkpoint restoration",
        "data batch skipping",
        "traditional gradient clipping techniques",
        "norm-based methods",
        "adaptive gradient clipping",
        "clipping threshold",
        "statistical properties of gradient norms",
        "z-score-based anomaly detection",
        "malignant loss spikes",
        "convergence"
      ]
    },
    "publishedAt": "2025-04-03T07:41:55.000Z",
    "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
    "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02119",
      "authors": [
        {
          "_id": "67ef41e7efcb0a2fbfbb6a32",
          "user": {
            "_id": "670826649e319cca029ff240",
            "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
            "isPro": false,
            "fullname": "rtfvbhkuj",
            "user": "wwdd7718",
            "type": "user"
          },
          "name": "Wang Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a33",
          "user": {
            "_id": "66e4e50a52356419c4a1ad14",
            "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
            "isPro": false,
            "fullname": "Tiankai Yang",
            "user": "tiankaiy",
            "type": "user"
          },
          "name": "Tiankai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a34",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a35",
          "user": {
            "_id": "62a3ab83e4dd6252344d27cd",
            "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
            "isPro": false,
            "fullname": "Ryan A. Rossi",
            "user": "ryanrossi",
            "type": "user"
          },
          "name": "Ryan A. Rossi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a36",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a37",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a38",
          "name": "Hoda Eldardiry",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T20:33:27.000Z",
      "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
      "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
      "upvotes": 4,
      "discussionId": "67ef41e8efcb0a2fbfbb6a93",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "model selection",
        "time series forecasting",
        "meta-learning approaches",
        "pre-constructed performance matrices",
        "reasoning capabilities",
        "experiments",
        "LLaMA",
        "GPT",
        "Gemini",
        "heuristic baselines",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-04-02T16:33:27.000Z",
    "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02012",
      "authors": [
        {
          "_id": "67ef5af0724d484dd41afe5c",
          "name": "Soro Bedionita",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5d",
          "name": "Bruno Andreis",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5e",
          "name": "Song Chong",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:50:19.000Z",
      "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
      "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
      "submittedOnDailyBy": {
        "_id": "66189b980da4c017c401fb5d",
        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
        "isPro": false,
        "fullname": "soro bedio",
        "user": "bedio",
        "type": "user"
      },
      "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
      "upvotes": 3,
      "discussionId": "67ef5af1724d484dd41afef3",
      "ai_keywords": [
        "diffusion models",
        "IGPG (Instruction Guided Parameter Generation)",
        "VQ-VAE",
        "autoregressive framework",
        "token level",
        "parameter synthesis",
        "inter-layer coherence",
        "vision datasets",
        "pretrained models",
        "pretrained weight retrieval",
        "model selection",
        "task-specific fine-tuning"
      ]
    },
    "publishedAt": "2025-04-02T01:50:19.000Z",
    "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66189b980da4c017c401fb5d",
      "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
      "fullname": "soro bedio",
      "name": "bedio",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00891",
      "authors": [
        {
          "_id": "67ef62342a18e60aeee0ea02",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea03",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea04",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea05",
          "name": "Zhimu Zhou",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea06",
          "user": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "isPro": false,
            "fullname": "Junqi Gao",
            "user": "ChetKao",
            "type": "user"
          },
          "name": "Junqi Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea07",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea08",
          "user": {
            "_id": "6562db314e8918182da42706",
            "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
            "isPro": false,
            "fullname": "Jiafei Lyu",
            "user": "dmux",
            "type": "user"
          },
          "name": "Jiafei Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea09",
          "user": {
            "_id": "65b34c5785b6c2144807db37",
            "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
            "isPro": false,
            "fullname": "Qianzhouyi",
            "user": "Saputello",
            "type": "user"
          },
          "name": "Zhouyi Qian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0a",
          "user": {
            "_id": "645d9c3058f9ee315148116d",
            "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
            "isPro": false,
            "fullname": "Biqing Qi",
            "user": "jackqi7",
            "type": "user"
          },
          "name": "Biqing Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0b",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0c",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:21:05.000Z",
      "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
      "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
      "upvotes": 3,
      "discussionId": "67ef62352a18e60aeee0ea4b",
      "projectPage": "https://ryanliu112.github.io/GenPRM",
      "githubRepo": "https://github.com/RyanLiu112/GenPRM",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Process Reward Models (PRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Relative Progress Estimation (RPE)",
        "ProcessBench",
        "MATH dataset",
        "GPT-4",
        "Qwen2.5-Math-PRM-72B",
        "critic model",
        "policy model refinement"
      ]
    },
    "publishedAt": "2025-04-01T11:21:05.000Z",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02398",
      "authors": [
        {
          "_id": "67ef63b5e8b932ae7a8d3043",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3044",
          "user": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "isPro": false,
            "fullname": "Michael Hassid",
            "user": "hassid",
            "type": "user"
          },
          "name": "Michael Hassid",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3045",
          "user": {
            "_id": "64b7b7b38ba7d6c922d753d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
            "isPro": false,
            "fullname": "Amit Roth",
            "user": "MajoRoth",
            "type": "user"
          },
          "name": "Amit Roth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3046",
          "user": {
            "_id": "6481e135578646b5c2386728",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
            "isPro": false,
            "fullname": "Yossi Adi",
            "user": "adiyoss",
            "type": "user"
          },
          "name": "Yossi Adi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
      ],
      "publishedAt": "2025-04-03T08:46:56.000Z",
      "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
      "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
      "upvotes": 2,
      "discussionId": "67ef63b6e8b932ae7a8d306d",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
      "githubRepo": "https://github.com/slp-rl/slamkit",
      "ai_keywords": [
        "Speech Language Model (SLM)",
        "TextLMs",
        "speech-text interleaving",
        "scaling analysis",
        "compute",
        "knowledge transfer",
        "textless-SLMs",
        "scaling trends",
        "scaling-dynamics",
        "training tokens",
        "synthetic data",
        "model families",
        "speech semantic metrics"
      ]
    },
    "publishedAt": "2025-04-03T04:46:56.000Z",
    "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
    "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  }
]