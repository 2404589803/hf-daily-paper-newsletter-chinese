[
  {
    "paper": {
      "id": "2601.03017",
      "authors": [
        {
          "_id": "696488cc138cc47cbd765365",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765366",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765367",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765368",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765369",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536b",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536c",
          "name": "Hengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536d",
          "name": "Taiqiang Wu",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536e",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd76536f",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765370",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765371",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "696488cc138cc47cbd765372",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T13:42:51.000Z",
      "submittedOnDailyAt": "2026-01-12T03:10:40.203Z",
      "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
      "submittedOnDailyBy": {
        "_id": "60851545a5da133ac6c38686",
        "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
        "isPro": false,
        "fullname": "Jing Xiong",
        "user": "menik1126",
        "type": "user"
      },
      "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
      "upvotes": 59,
      "discussionId": "696488cc138cc47cbd765373",
      "projectPage": "https://mmformalizer.github.io/",
      "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.",
      "ai_keywords": [
        "autoformalization",
        "multimodal",
        "perceptually grounded primitives",
        "recursive grounding",
        "axiom composition",
        "adaptive recursive termination",
        "dimensional grounding",
        "axiomatic grounding",
        "PhyX-AF",
        "MathVerse",
        "PhyX",
        "Synthetic Geometry",
        "Analytic Geometry",
        "GPT-5",
        "Gemini-3-Pro",
        "classical mechanics",
        "relativity",
        "quantum mechanics",
        "thermodynamics"
      ]
    },
    "publishedAt": "2026-01-06T08:42:51.000Z",
    "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
    "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60851545a5da133ac6c38686",
      "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
      "fullname": "Jing Xiong",
      "name": "menik1126",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05432",
      "authors": [
        {
          "_id": "69646268138cc47cbd76527e",
          "name": "Yuxiang Ji",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd76527f",
          "name": "Yong Wang",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765280",
          "name": "Ziyu Ma",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765281",
          "name": "Yiming Hu",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765282",
          "name": "Hailang Huang",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765283",
          "name": "Xuecai Hu",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765284",
          "name": "Guanhua Chen",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765285",
          "name": "Liaoni Wu",
          "hidden": false
        },
        {
          "_id": "69646268138cc47cbd765286",
          "name": "Xiangxiang Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T23:47:30.000Z",
      "submittedOnDailyAt": "2026-01-12T01:15:15.959Z",
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
      "upvotes": 57,
      "discussionId": "69646268138cc47cbd765287",
      "projectPage": "https://amap-ml.github.io/Thinking-with-Map/",
      "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map",
      "githubRepoAddedBy": "user",
      "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.",
      "ai_keywords": [
        "vision-language model",
        "geolocalization",
        "chain-of-thought reasoning",
        "agentic capabilities",
        "agentic reinforcement learning",
        "parallel test-time scaling",
        "agent-in-the-map loop",
        "MAPBench",
        "Acc@500m"
      ],
      "githubStars": 35,
      "organization": {
        "_id": "64488b334988ee01f2a8d856",
        "name": "alibaba-inc",
        "fullname": "alibaba-inc",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
      }
    },
    "publishedAt": "2026-01-08T18:47:30.000Z",
    "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
    "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64488b334988ee01f2a8d856",
      "name": "alibaba-inc",
      "fullname": "alibaba-inc",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.06002",
      "authors": [
        {
          "_id": "6964644c138cc47cbd76529b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd76529c",
          "name": "Yantao Du",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd76529d",
          "name": "Ziniu Li",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd76529e",
          "name": "Jinhao Liu",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd76529f",
          "name": "Songyao Duan",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a0",
          "name": "Jiarui Guo",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a1",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a2",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a3",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a4",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a5",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a6",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "6964644c138cc47cbd7652a7",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
      ],
      "publishedAt": "2026-01-09T18:39:01.000Z",
      "submittedOnDailyAt": "2026-01-12T01:02:27.368Z",
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "submittedOnDailyBy": {
        "_id": "636f526a6cd69d9a36ff2b53",
        "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
        "isPro": false,
        "fullname": "Qiguang Chen",
        "user": "LightChen2333",
        "type": "user"
      },
      "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "upvotes": 28,
      "discussionId": "6964644c138cc47cbd7652a8",
      "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.",
      "ai_keywords": [
        "chain-of-thought",
        "large language models",
        "Long CoT",
        "fine-tuning",
        "entropy convergence",
        "semantic isomers",
        "distribution-transfer-graph",
        "molecular-like structures",
        "deep reasoning",
        "self-reflection",
        "self-exploration"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-01-09T13:39:01.000Z",
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636f526a6cd69d9a36ff2b53",
      "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
      "fullname": "Qiguang Chen",
      "name": "LightChen2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.06021",
      "authors": [
        {
          "_id": "69645f30138cc47cbd765248",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "69645f30138cc47cbd765249",
          "name": "Xin Lv",
          "hidden": false
        },
        {
          "_id": "69645f30138cc47cbd76524a",
          "name": "Ling Feng",
          "hidden": false
        },
        {
          "_id": "69645f30138cc47cbd76524b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "69645f30138cc47cbd76524c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T18:57:53.000Z",
      "submittedOnDailyAt": "2026-01-12T00:13:19.034Z",
      "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
      "submittedOnDailyBy": {
        "_id": "66cdd285c51a915bd5f2d017",
        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
        "isPro": false,
        "fullname": "Jiajie Zhang",
        "user": "NeoZ123",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
      "upvotes": 26,
      "discussionId": "69645f30138cc47cbd76524d",
      "githubRepo": "https://github.com/THUDM/CaRR",
      "githubRepoAddedBy": "user",
      "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.",
      "ai_keywords": [
        "reinforcement learning",
        "deep search agents",
        "fine-grained reward framework",
        "reasoning comprehensiveness",
        "factual grounding",
        "evidence connectivity",
        "verifiable single-hop rubrics",
        "citation-aware group relative policy optimization",
        "outcome rewards",
        "shortcut exploitation",
        "hallucinations"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "62ad27f19096e7f9ecb1853a",
        "name": "zai-org",
        "fullname": "Z.ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
      }
    },
    "publishedAt": "2026-01-09T13:57:53.000Z",
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cdd285c51a915bd5f2d017",
      "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
      "fullname": "Jiajie Zhang",
      "name": "NeoZ123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62ad27f19096e7f9ecb1853a",
      "name": "zai-org",
      "fullname": "Z.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05808",
      "authors": [
        {
          "_id": "696462fd138cc47cbd765289",
          "name": "Xiaoshuai Song",
          "hidden": false
        },
        {
          "_id": "696462fd138cc47cbd76528a",
          "name": "Haofei Chang",
          "hidden": false
        },
        {
          "_id": "696462fd138cc47cbd76528b",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "696462fd138cc47cbd76528c",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "696462fd138cc47cbd76528d",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "696462fd138cc47cbd76528e",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T14:32:06.000Z",
      "submittedOnDailyAt": "2026-01-12T00:32:15.159Z",
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "submittedOnDailyBy": {
        "_id": "6621ec2524eb2673fe0790fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
        "isPro": false,
        "fullname": "Ania Forge",
        "user": "zhangboguodong",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
      "upvotes": 22,
      "discussionId": "696462fe138cc47cbd76528f",
      "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.",
      "ai_keywords": [
        "tool-interaction environments",
        "programmatic synthesis",
        "environment skeletons",
        "task scenarios",
        "rule-based trajectory validation",
        "supervised fine-tuning",
        "reinforcement learning",
        "multi-turn interactions",
        "multi-tool interactions"
      ]
    },
    "publishedAt": "2026-01-09T09:32:06.000Z",
    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
    "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6621ec2524eb2673fe0790fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
      "fullname": "Ania Forge",
      "name": "zhangboguodong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05930",
      "authors": [
        {
          "_id": "69646f2d138cc47cbd7652db",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652dc",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652dd",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652de",
          "name": "Yuren Mao",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652df",
          "name": "Yunjun Gao",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652e0",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652e1",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "69646f2d138cc47cbd7652e2",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T16:44:17.000Z",
      "submittedOnDailyAt": "2026-01-12T01:19:32.404Z",
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
      "upvotes": 16,
      "discussionId": "69646f2d138cc47cbd7652e3",
      "githubRepo": "https://github.com/zjunlp/predict-before-execute",
      "githubRepoAddedBy": "user",
      "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.",
      "ai_keywords": [
        "autonomous machine learning agents",
        "Generate-Execute-Feedback paradigm",
        "Execution Bottleneck",
        "World Models",
        "Data-centric Solution Preference",
        "LLMs",
        "Predict-then-Verify loop",
        "convergence acceleration"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2026-01-09T11:44:17.000Z",
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03319",
      "authors": [
        {
          "_id": "6960e7365b7998385e6396e3",
          "user": {
            "_id": "6761f183e5b85d453550147a",
            "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
            "isPro": false,
            "fullname": "Eld Mat",
            "user": "eldad929",
            "type": "user"
          },
          "name": "Eldad Matmon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-09T15:45:52.792Z",
          "hidden": false
        },
        {
          "_id": "6960e7365b7998385e6396e4",
          "name": "Amit Bracha",
          "hidden": false
        },
        {
          "_id": "6960e7365b7998385e6396e5",
          "name": "Noam Rotstein",
          "hidden": false
        },
        {
          "_id": "6960e7365b7998385e6396e6",
          "name": "Ron Kimmel",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T13:56:28.000Z",
      "submittedOnDailyAt": "2026-01-12T03:43:15.850Z",
      "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
      "submittedOnDailyBy": {
        "_id": "6761f183e5b85d453550147a",
        "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
        "isPro": false,
        "fullname": "Eld Mat",
        "user": "eldad929",
        "type": "user"
      },
      "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
      "upvotes": 12,
      "discussionId": "6960e7365b7998385e6396e7",
      "projectPage": "https://c4ricaturegs.github.io/",
      "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "FLAME mesh",
        "curvature-weighted Poisson equation",
        "pseudo-ground-truth caricature images",
        "local affine transformations",
        "real-time deformations",
        "closed-form solutions"
      ],
      "organization": {
        "_id": "6393322be2364bc1eea56e45",
        "name": "Technion",
        "fullname": "Technion Israel institute of technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
      }
    },
    "publishedAt": "2026-01-06T08:56:28.000Z",
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6761f183e5b85d453550147a",
      "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg",
      "fullname": "Eld Mat",
      "name": "eldad929",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6393322be2364bc1eea56e45",
      "name": "Technion",
      "fullname": "Technion Israel institute of technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.05966",
      "authors": [
        {
          "_id": "69645e19138cc47cbd76523f",
          "name": "Longbin Ji",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765240",
          "name": "Xiaoxiong Liu",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765241",
          "name": "Junyuan Shang",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765242",
          "name": "Shuohuan Wang",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765243",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765244",
          "name": "Hua Wu",
          "hidden": false
        },
        {
          "_id": "69645e19138cc47cbd765245",
          "name": "Haifeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T17:34:59.000Z",
      "submittedOnDailyAt": "2026-01-12T00:06:52.890Z",
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
      "upvotes": 11,
      "discussionId": "69645e19138cc47cbd765246",
      "ai_summary": "VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.",
      "ai_keywords": [
        "Visual Autoregressive",
        "diffusion models",
        "flow-matching models",
        "video generation",
        "multi-scale next-frame prediction",
        "autoregressive modeling",
        "intra-frame VAR modeling",
        "causal next-frame prediction",
        "3D multi-scale tokenizer",
        "temporal dependencies",
        "spatial dependencies",
        "Multi-scale Temporal RoPE",
        "Cross-Frame Error Correction",
        "Random Frame Mask",
        "multi-stage pretraining pipeline",
        "FVD",
        "VBench"
      ]
    },
    "publishedAt": "2026-01-09T12:34:59.000Z",
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 206,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05905",
      "authors": [
        {
          "_id": "69646dd9138cc47cbd7652c7",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652c8",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652c9",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652ca",
          "name": "Weihong Xu",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652cb",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652cc",
          "name": "Xinle Deng",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652cd",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652ce",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652cf",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "69646dd9138cc47cbd7652d0",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T16:23:21.000Z",
      "submittedOnDailyAt": "2026-01-12T01:16:58.337Z",
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
      "upvotes": 10,
      "discussionId": "69646dda138cc47cbd7652d1",
      "githubRepo": "https://github.com/zjunlp/belief",
      "githubRepoAddedBy": "user",
      "ai_summary": "Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.",
      "ai_keywords": [
        "Large Language Models",
        "Self-Consistency",
        "belief robustness",
        "conceptual neighborhood",
        "cognitive stress-testing",
        "context-invariant belief structure",
        "long-tail knowledge brittleness",
        "Structure-Aware Training"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6345aadf5efccdc07f1365a5",
        "name": "ZhejiangUniversity",
        "fullname": "Zhejiang University"
      }
    },
    "publishedAt": "2026-01-09T11:23:21.000Z",
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04720",
      "authors": [
        {
          "_id": "69647212138cc47cbd765311",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765312",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765313",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765314",
          "name": "Keqin Chen",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765315",
          "name": "Sibo Song",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765316",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765317",
          "name": "Zhibo Yang",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765318",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd765319",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd76531a",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd76531b",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69647212138cc47cbd76531c",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T08:36:06.000Z",
      "submittedOnDailyAt": "2026-01-12T02:49:49.573Z",
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "submittedOnDailyBy": {
        "_id": "616adb8578833ce5997e441a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
        "isPro": false,
        "fullname": "Dingkun Long",
        "user": "thenlper",
        "type": "user"
      },
      "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
      "upvotes": 8,
      "discussionId": "69647212138cc47cbd76531d",
      "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding",
      "githubRepoAddedBy": "auto",
      "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.",
      "ai_keywords": [
        "Qwen3-VL-Embedding",
        "Qwen3-VL-Reranker",
        "multimodal search",
        "multi-stage training",
        "contrastive pre-training",
        "model distillation",
        "Matryoshka Representation Learning",
        "cross-encoder architecture",
        "cross-attention mechanisms",
        "multimodal embedding",
        "visual question answering",
        "video-text matching"
      ],
      "githubStars": 551,
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-01-08T03:36:06.000Z",
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616adb8578833ce5997e441a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg",
      "fullname": "Dingkun Long",
      "name": "thenlper",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 121,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04786",
      "authors": [
        {
          "_id": "6964788b138cc47cbd76533c",
          "name": "Lang Feng",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd76533d",
          "name": "Fuchao Yang",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd76533e",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd76533f",
          "name": "Xin Cheng",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd765340",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd765341",
          "name": "Zhenglin Wan",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd765342",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6964788b138cc47cbd765343",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T10:10:20.000Z",
      "submittedOnDailyAt": "2026-01-12T02:18:55.645Z",
      "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
      "submittedOnDailyBy": {
        "_id": "66ba29dd59e8e7a957154c5f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
        "isPro": false,
        "fullname": "Lang Feng",
        "user": "langfeng01",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
      "upvotes": 7,
      "discussionId": "6964788b138cc47cbd765344",
      "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.",
      "ai_keywords": [
        "large language models",
        "reinforcement learning",
        "multi-turn interaction trajectories",
        "visual tokens",
        "segment optical caching",
        "agentic self-compression",
        "token efficiency",
        "rendering speedup"
      ],
      "organization": {
        "_id": "6508b28cf36bb51c50faad98",
        "name": "NanyangTechnologicalUniversity",
        "fullname": "Nanyang Technological University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
      }
    },
    "publishedAt": "2026-01-08T05:10:20.000Z",
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ba29dd59e8e7a957154c5f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
      "fullname": "Lang Feng",
      "name": "langfeng01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508b28cf36bb51c50faad98",
      "name": "NanyangTechnologicalUniversity",
      "fullname": "Nanyang Technological University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05573",
      "authors": [
        {
          "_id": "69646388138cc47cbd765291",
          "name": "Zehan Wang",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765292",
          "name": "Ziang Zhang",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765293",
          "name": "Jiayang Xu",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765294",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765295",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765296",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765297",
          "name": "HengShuang Zhao",
          "hidden": false
        },
        {
          "_id": "69646388138cc47cbd765298",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T06:43:59.000Z",
      "submittedOnDailyAt": "2026-01-12T00:32:29.042Z",
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "submittedOnDailyBy": {
        "_id": "65b36a383a41095a56d0736d",
        "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
        "isPro": false,
        "fullname": "ZiangZhang",
        "user": "Viglong",
        "type": "user"
      },
      "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
      "upvotes": 6,
      "discussionId": "69646388138cc47cbd765299",
      "projectPage": "https://orient-anythingv2.github.io/",
      "githubRepo": "https://github.com/SpatialVision/Orient-Anything-V2",
      "githubRepoAddedBy": "user",
      "ai_summary": "Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.",
      "ai_keywords": [
        "generative models",
        "3D assets",
        "model-in-the-loop annotation",
        "symmetry-aware",
        "periodic distribution fitting",
        "multi-frame architecture",
        "relative rotation prediction",
        "zero-shot performance",
        "6DoF pose estimation",
        "object symmetry recognition"
      ],
      "githubStars": 25
    },
    "publishedAt": "2026-01-09T01:43:59.000Z",
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "summary": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b36a383a41095a56d0736d",
      "avatarUrl": "/avatars/378f011d9ca08839fe80ffa259620c67.svg",
      "fullname": "ZiangZhang",
      "name": "Viglong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04888",
      "authors": [
        {
          "_id": "696079e75b7998385e639515",
          "user": {
            "_id": "6883352ff256918bd89942b5",
            "avatarUrl": "/avatars/ba27f7184d96c1288858c848cb656eec.svg",
            "isPro": false,
            "fullname": "TongyuWen",
            "user": "vvv111222",
            "type": "user"
          },
          "name": "Tongyu Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-09T08:35:05.308Z",
          "hidden": false
        },
        {
          "_id": "696079e75b7998385e639516",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "696079e75b7998385e639517",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T12:39:05.000Z",
      "submittedOnDailyAt": "2026-01-12T00:35:47.731Z",
      "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
      "submittedOnDailyBy": {
        "_id": "64b8e82aa62c52b252c827fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
        "isPro": true,
        "fullname": "Rajkumar rawal",
        "user": "rajkumarrawal",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
      "upvotes": 5,
      "discussionId": "696079e75b7998385e639518",
      "githubRepo": "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file",
      "githubRepoAddedBy": "user",
      "ai_summary": "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.",
      "ai_keywords": [
        "search agents",
        "large language models",
        "process rewards",
        "dual-level credit assessment",
        "query refinement",
        "curriculum learning",
        "intermediate search queries",
        "retrieval results"
      ],
      "githubStars": 8
    },
    "publishedAt": "2026-01-08T07:39:05.000Z",
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04888.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8e82aa62c52b252c827fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
      "fullname": "Rajkumar rawal",
      "name": "rajkumarrawal",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.05848",
      "authors": [
        {
          "_id": "69645dac138cc47cbd76522e",
          "name": "Nate Gillman",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd76522f",
          "name": "Yinghua Zhou",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765230",
          "name": "Zitian Tang",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765231",
          "name": "Evan Luo",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765232",
          "name": "Arjan Chakravarthy",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765233",
          "name": "Daksh Aggarwal",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765234",
          "name": "Michael Freeman",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765235",
          "name": "Charles Herrmann",
          "hidden": false
        },
        {
          "_id": "69645dac138cc47cbd765236",
          "name": "Chen Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
      ],
      "publishedAt": "2026-01-09T15:23:36.000Z",
      "submittedOnDailyAt": "2026-01-12T00:05:34.589Z",
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
      "upvotes": 4,
      "discussionId": "69645dad138cc47cbd765237",
      "projectPage": "https://goal-force.github.io/",
      "ai_summary": "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.",
      "ai_keywords": [
        "video generation",
        "world models",
        "causal primitives",
        "force vectors",
        "physics simulation",
        "zero-shot generalization",
        "neural physics simulators"
      ]
    },
    "publishedAt": "2026-01-09T10:23:36.000Z",
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "summary": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1uUuhcgQYc4UEJP4tfPbV.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 206,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04823",
      "authors": [
        {
          "_id": "6964607c138cc47cbd765259",
          "name": "Guanzhi Deng",
          "hidden": false
        },
        {
          "_id": "6964607c138cc47cbd76525a",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6964607c138cc47cbd76525b",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "6964607c138cc47cbd76525c",
          "name": "Huacan Wang",
          "hidden": false
        },
        {
          "_id": "6964607c138cc47cbd76525d",
          "name": "Linqi Song",
          "hidden": false
        },
        {
          "_id": "6964607c138cc47cbd76525e",
          "name": "Lijie Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T10:58:51.000Z",
      "submittedOnDailyAt": "2026-01-12T00:21:36.615Z",
      "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
      "submittedOnDailyBy": {
        "_id": "6582c482f3006507ea10302a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
        "isPro": false,
        "fullname": "Bo Li",
        "user": "liboaccn",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
      "upvotes": 2,
      "discussionId": "6964607c138cc47cbd76525f",
      "ai_summary": "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Large Language Models",
        "parameter-efficient fine-tuning",
        "LoRA",
        "dynamic rank allocation",
        "expert routing frequency",
        "expert saliency scoring",
        "heterogeneous rank distribution"
      ]
    },
    "publishedAt": "2026-01-08T05:58:51.000Z",
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6582c482f3006507ea10302a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
      "fullname": "Bo Li",
      "name": "liboaccn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05637",
      "authors": [
        {
          "_id": "69645d4e138cc47cbd765225",
          "name": "Emily Cheng",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd765226",
          "name": "Carmen Amo Alonso",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd765227",
          "name": "Federico Danieli",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd765228",
          "name": "Arno Blaas",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd765229",
          "name": "Luca Zappella",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd76522a",
          "name": "Pau Rodriguez",
          "hidden": false
        },
        {
          "_id": "69645d4e138cc47cbd76522b",
          "name": "Xavier Suau",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T08:50:02.000Z",
      "submittedOnDailyAt": "2026-01-12T00:02:49.226Z",
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
      "upvotes": 1,
      "discussionId": "69645d4e138cc47cbd76522c",
      "ai_summary": "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.",
      "ai_keywords": [
        "generative models",
        "controllable sets",
        "control process",
        "dialogue setting",
        "probably-approximately correct bounds",
        "black-box nonlinear control system",
        "language models",
        "text-to-image generation"
      ],
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2026-01-09T03:50:02.000Z",
    "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "summary": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05637.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 206,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05503",
      "authors": [
        {
          "_id": "69646131138cc47cbd765268",
          "name": "Roy Xie",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd765269",
          "name": "Deepak Gopinath",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd76526a",
          "name": "David Qiu",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd76526b",
          "name": "Dong Lin",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd76526c",
          "name": "Haitian Sun",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd76526d",
          "name": "Saloni Potdar",
          "hidden": false
        },
        {
          "_id": "69646131138cc47cbd76526e",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T03:24:46.000Z",
      "submittedOnDailyAt": "2026-01-12T00:19:38.713Z",
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
      "upvotes": 1,
      "discussionId": "69646131138cc47cbd76526f",
      "githubRepo": "https://github.com/ruoyuxie/OversearchQA",
      "githubRepoAddedBy": "user",
      "ai_summary": "Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.",
      "ai_keywords": [
        "search-augmented large language models",
        "over-searching",
        "retrieval",
        "hallucinations",
        "Tokens Per Correctness",
        "multi-turn conversations",
        "answerable queries",
        "unanswerable queries",
        "complex reasoning models",
        "deep research systems",
        "noisy retrieval",
        "negative evidence"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2026-01-08T22:24:46.000Z",
    "title": "Over-Searching in Search-Augmented Large Language Models",
    "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05503.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 206,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04726",
      "authors": [
        {
          "_id": "69649a72138cc47cbd7653d1",
          "name": "Yuyang Hu",
          "hidden": false
        },
        {
          "_id": "69649a72138cc47cbd7653d2",
          "name": "Jiongnan Liu",
          "hidden": false
        },
        {
          "_id": "69649a72138cc47cbd7653d3",
          "name": "Jiejun Tan",
          "hidden": false
        },
        {
          "_id": "69649a72138cc47cbd7653d4",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "69649a72138cc47cbd7653d5",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T08:44:07.000Z",
      "submittedOnDailyAt": "2026-01-12T04:25:56.987Z",
      "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
      "submittedOnDailyBy": {
        "_id": "6544b9b646dbdeca34ee5f52",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
        "isPro": false,
        "fullname": "Yuyang Hu",
        "user": "namespace-ERI",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
      "upvotes": 1,
      "discussionId": "69649a73138cc47cbd7653d6",
      "ai_summary": "CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.",
      "ai_keywords": [
        "event-centric memory",
        "Event Segmentation Theory",
        "Event Graph",
        "structured memory",
        "logical relationships",
        "memory organization",
        "long-horizon reasoning",
        "memory retrieval",
        "goal-directed navigation",
        "semantic retrieval"
      ]
    },
    "publishedAt": "2026-01-08T03:44:07.000Z",
    "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "summary": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04726.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6544b9b646dbdeca34ee5f52",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
      "fullname": "Yuyang Hu",
      "name": "namespace-ERI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02760",
      "authors": [
        {
          "_id": "696486dd138cc47cbd765353",
          "name": "Zeyu Ren",
          "hidden": false
        },
        {
          "_id": "696486dd138cc47cbd765354",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "696486dd138cc47cbd765355",
          "name": "Wukai Li",
          "hidden": false
        },
        {
          "_id": "696486dd138cc47cbd765356",
          "name": "Qingxiang Liu",
          "hidden": false
        },
        {
          "_id": "696486dd138cc47cbd765357",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T06:51:35.000Z",
      "submittedOnDailyAt": "2026-01-12T03:01:02.577Z",
      "title": "AnyDepth: Depth Estimation Made Easy",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
      "upvotes": 1,
      "discussionId": "696486dd138cc47cbd765358",
      "projectPage": "https://aigeeksgroup.github.io/AnyDepth",
      "githubRepo": "https://github.com/AIGeeksGroup/AnyDepth",
      "githubRepoAddedBy": "user",
      "ai_summary": "A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.",
      "ai_keywords": [
        "DINOv3",
        "visual encoder",
        "dense features",
        "Simple Depth Transformer",
        "SDT",
        "transformer-based decoder",
        "cross-scale feature fusion",
        "parameter efficiency",
        "quality-based filtering strategy",
        "zero-shot monocular depth estimation"
      ],
      "githubStars": 25,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-01-06T01:51:35.000Z",
    "title": "AnyDepth: Depth Estimation Made Easy",
    "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05870",
      "authors": [
        {
          "_id": "69646166138cc47cbd765271",
          "name": "Huilin Deng",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765272",
          "name": "Hongchen Luo",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765273",
          "name": "Yue Zhu",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765274",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765275",
          "name": "Zhuoyue Chen",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765276",
          "name": "Xinghao Zhao",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765277",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765278",
          "name": "Jihai Zhang",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd765279",
          "name": "Mengchang Wang",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd76527a",
          "name": "Yang Cao",
          "hidden": false
        },
        {
          "_id": "69646166138cc47cbd76527b",
          "name": "Yu Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T15:46:40.000Z",
      "submittedOnDailyAt": "2026-01-12T00:20:25.251Z",
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
      "upvotes": 0,
      "discussionId": "69646166138cc47cbd76527c",
      "ai_summary": "Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "large language models",
        "exploration collapse",
        "policy entropy",
        "reward hacking",
        "latent policy optimization",
        "iterative information bottleneck",
        "topological branching",
        "reasoning trajectories",
        "information bottleneck principle",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2026-01-09T10:46:40.000Z",
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05870.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 206,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05851",
      "authors": [
        {
          "_id": "69646ab0138cc47cbd7652ad",
          "name": "Sandeep Mishra",
          "hidden": false
        },
        {
          "_id": "69646ab0138cc47cbd7652ae",
          "name": "Devichand Budagam",
          "hidden": false
        },
        {
          "_id": "69646ab0138cc47cbd7652af",
          "name": "Anubhab Mandal",
          "hidden": false
        },
        {
          "_id": "69646ab0138cc47cbd7652b0",
          "name": "Bishal Santra",
          "hidden": false
        },
        {
          "_id": "69646ab0138cc47cbd7652b1",
          "name": "Pawan Goyal",
          "hidden": false
        },
        {
          "_id": "69646ab0138cc47cbd7652b2",
          "name": "Manish Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-09T15:29:50.000Z",
      "submittedOnDailyAt": "2026-01-12T01:00:51.036Z",
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "submittedOnDailyBy": {
        "_id": "653f2d18eefdcdc9fdb446b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
        "isPro": false,
        "fullname": "Devichand Budagam",
        "user": "devichand",
        "type": "user"
      },
      "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "upvotes": 0,
      "discussionId": "69646ab0138cc47cbd7652b3",
      "ai_summary": "Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.",
      "ai_keywords": [
        "multimodal auto-completion",
        "vision-language models",
        "VLMs",
        "router framework",
        "multimodal context",
        "real-time prediction",
        "user satisfaction",
        "user typing effort",
        "multi-turn conversations"
      ],
      "organization": {
        "_id": "61bbf43e544c3241f255e0fe",
        "name": "IITKGP",
        "fullname": "Indian Institute of Technology, Kharagpur"
      }
    },
    "publishedAt": "2026-01-09T10:29:50.000Z",
    "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
    "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653f2d18eefdcdc9fdb446b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f2d18eefdcdc9fdb446b3/RaQ5RmqGIK2xgNOelW9m-.png",
      "fullname": "Devichand Budagam",
      "name": "devichand",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61bbf43e544c3241f255e0fe",
      "name": "IITKGP",
      "fullname": "Indian Institute of Technology, Kharagpur"
    },
    "isAuthorParticipating": false
  }
]