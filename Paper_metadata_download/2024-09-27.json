[
    {
        "paper": {
            "id": "2409.18125",
            "authors": [
                {
                    "_id": "66f62da4793e1463e406fda2",
                    "user": {
                        "_id": "6433aba4546e16f17a0f19f6",
                        "avatarUrl": "/avatars/e72e4438fe2f275cc6a530c26ea08b20.svg",
                        "isPro": false,
                        "fullname": "Zhu Chenming",
                        "user": "ChaimZhu",
                        "type": "user"
                    },
                    "name": "Chenming Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T08:09:16.387Z",
                    "hidden": false
                },
                {
                    "_id": "66f62da4793e1463e406fda3",
                    "user": {
                        "_id": "64e6d9d229a548f66aff6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                        "isPro": false,
                        "fullname": "Tai Wang",
                        "user": "taiwang",
                        "type": "user"
                    },
                    "name": "Tai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T08:09:23.888Z",
                    "hidden": false
                },
                {
                    "_id": "66f62da4793e1463e406fda4",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f62da4793e1463e406fda5",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T08:10:00.337Z",
                    "hidden": false
                },
                {
                    "_id": "66f62da4793e1463e406fda6",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T08:10:10.152Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T17:59:11.000Z",
            "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with\n  3D-awareness",
            "summary": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\ntheir proficiency in 2D visual understanding tasks, enabling them to\neffectively process and understand images and videos. However, the development\nof LMMs with 3D-awareness for 3D scene understanding has been hindered by the\nlack of large-scale 3D vision-language datasets and powerful 3D encoders. In\nthis paper, we introduce a simple yet effective framework called LLaVA-3D.\nLeveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\nunderstanding capabilities. To achieve this, we employ a simple yet effective\nrepresentation, 3D Patch, which connects 2D CLIP patch features with their\ncorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs\nand employing joint 2D and 3D vision-language instruction tuning, we establish\na unified architecture for both 2D image understanding and 3D scene\nunderstanding. Experimental results show that LLaVA-3D converges 3.5x faster\nthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\nbut also maintains comparable 2D image understanding and vision-language\nconversation capabilities with LLaVA.",
            "upvotes": 20,
            "discussionId": "66f62dad793e1463e406fff4"
        },
        "publishedAt": "2024-09-27T02:29:44.972Z",
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.18125.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.17481",
            "authors": [
                {
                    "_id": "66f61ce4b35008620e525842",
                    "user": {
                        "_id": "646a1939c37ca1e12308fe81",
                        "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
                        "isPro": false,
                        "fullname": "Gongfan Fang",
                        "user": "Vinnnf",
                        "type": "user"
                    },
                    "name": "Gongfan Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-27T06:58:11.629Z",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525843",
                    "user": {
                        "_id": "65a8b7f69aec1645994e7a15",
                        "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
                        "isPro": false,
                        "fullname": "Hongxu Yin",
                        "user": "yinhongxu",
                        "type": "user"
                    },
                    "name": "Hongxu Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T07:25:47.927Z",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525844",
                    "user": {
                        "_id": "64caca9b4ca74090b97a8795",
                        "avatarUrl": "/avatars/310f7703ff3c07d283c02aca56fe86df.svg",
                        "isPro": false,
                        "fullname": "Saurav Muralidharan",
                        "user": "srvm",
                        "type": "user"
                    },
                    "name": "Saurav Muralidharan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T07:25:53.759Z",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525845",
                    "user": {
                        "_id": "64c23e06203c836866ed0bdd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c23e06203c836866ed0bdd/rPUrP8PqL1dhWNO6L3XwI.png",
                        "isPro": false,
                        "fullname": "Greg Heinrich",
                        "user": "gheinrich",
                        "type": "user"
                    },
                    "name": "Greg Heinrich",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T07:25:59.492Z",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525846",
                    "name": "Jeff Pool",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525847",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525848",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "66f61ce4b35008620e525849",
                    "user": {
                        "_id": "63fc03a50aab060792ffef39",
                        "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
                        "isPro": false,
                        "fullname": "Wangxinchao",
                        "user": "wxcTest",
                        "type": "user"
                    },
                    "name": "Xinchao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T07:26:31.331Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T02:37:41.000Z",
            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
            "summary": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\nhttps://github.com/NVlabs/MaskLLM.",
            "upvotes": 18,
            "discussionId": "66f61ce5b35008620e525882"
        },
        "publishedAt": "2024-09-27T01:18:19.219Z",
        "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17481.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.18042",
            "authors": [
                {
                    "_id": "66f61f0b5e1ef72590bbc638",
                    "user": {
                        "_id": "65bf3b304b5f8c270d50d6c8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bf3b304b5f8c270d50d6c8/iSC_Em-JtfcsKFoGw4KFc.jpeg",
                        "isPro": false,
                        "fullname": "Kai Chen",
                        "user": "KaiChen1998",
                        "type": "user"
                    },
                    "name": "Kai Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-27T06:58:09.820Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc639",
                    "user": {
                        "_id": "6181435169071e2bddc65578",
                        "avatarUrl": "/avatars/c898f76c13d380f47b131bcb6289bb9a.svg",
                        "isPro": false,
                        "fullname": "Yunhao Gou",
                        "user": "gyhdog",
                        "type": "user"
                    },
                    "name": "Yunhao Gou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T09:32:01.795Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63a",
                    "name": "Runhui Huang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63b",
                    "user": {
                        "_id": "64f395cef2eeda919ab980a5",
                        "avatarUrl": "/avatars/f92116eaf94defd9873d9cdb4f98c598.svg",
                        "isPro": false,
                        "fullname": "ZHILI LIU",
                        "user": "zhili-liu",
                        "type": "user"
                    },
                    "name": "Zhili Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T09:33:02.396Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63c",
                    "name": "Daxin Tan",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63d",
                    "name": "Jing Xu",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63e",
                    "user": {
                        "_id": "644131e5666947ad561349c1",
                        "avatarUrl": "/avatars/8a2d9be81cef30cb3bc71a695c802d2f.svg",
                        "isPro": false,
                        "fullname": "Wangchunwei",
                        "user": "17day",
                        "type": "user"
                    },
                    "name": "Chunwei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T09:33:32.389Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc63f",
                    "name": "Yi Zhu",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc640",
                    "user": {
                        "_id": "65beb318358734fd090f742e",
                        "avatarUrl": "/avatars/962420a21b2aa8689dd0e1d4531fbf35.svg",
                        "isPro": false,
                        "fullname": "Yihan Zeng",
                        "user": "vikyzeng2",
                        "type": "user"
                    },
                    "name": "Yihan Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T09:33:41.204Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc641",
                    "user": {
                        "_id": "6366c38a95204b4649cc582a",
                        "avatarUrl": "/avatars/ab97094994b6e82f047b43e436b9e852.svg",
                        "isPro": false,
                        "fullname": "Kuo Yang",
                        "user": "kuoyang1999",
                        "type": "user"
                    },
                    "name": "Kuo Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-27T09:33:47.438Z",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc642",
                    "name": "Dingdong Wang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc643",
                    "name": "Kun Xiang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc644",
                    "name": "Haoyuan Li",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc645",
                    "name": "Haoli Bai",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc646",
                    "name": "Jianhua Han",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc647",
                    "name": "Xiaohui Li",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc648",
                    "name": "Weike Jin",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc649",
                    "name": "Nian Xie",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64a",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64b",
                    "name": "James T. Kwok",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64c",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64d",
                    "name": "Xiaodan Liang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64e",
                    "name": "Dit-Yan Yeung",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc64f",
                    "name": "Xiao Chen",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc650",
                    "name": "Zhenguo Li",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc651",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc652",
                    "name": "Qun Liu",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc653",
                    "name": "Lanqing Hong",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc654",
                    "name": "Lu Hou",
                    "hidden": false
                },
                {
                    "_id": "66f61f0b5e1ef72590bbc655",
                    "name": "Hang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T16:44:02.000Z",
            "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions",
            "summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nin the open-source community. Existing vision-language models rely on external\ntools for the speech processing, while speech-language models still suffer from\nlimited or even without vision-understanding abilities. To address this gap, we\npropose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large\nLanguage Models with end-to-end speech capabilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we notice surprisingly that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the corresponding\nbi-modal aligned counterparts. Moreover, a lightweight style module is proposed\nfor flexible speech style controls (e.g., emotions and pitches). For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.",
            "upvotes": 17,
            "discussionId": "66f61f0d5e1ef72590bbc6d1"
        },
        "publishedAt": "2024-09-27T01:28:22.466Z",
        "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.18042.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.18124",
            "authors": [
                {
                    "_id": "66f62b506b0e782fa32a02db",
                    "name": "Jing He",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02dc",
                    "user": {
                        "_id": "641d211e353524fe41f16387",
                        "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
                        "isPro": false,
                        "fullname": "Haodong Li",
                        "user": "haodongli",
                        "type": "user"
                    },
                    "name": "Haodong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-27T06:58:00.560Z",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02dd",
                    "name": "Wei Yin",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02de",
                    "name": "Yixun Liang",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02df",
                    "name": "Leheng Li",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02e0",
                    "name": "Kaiqiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02e1",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02e2",
                    "name": "Bingbing Liu",
                    "hidden": false
                },
                {
                    "_id": "66f62b506b0e782fa32a02e3",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T17:58:55.000Z",
            "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction",
            "summary": "Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also significantly enhances efficiency,\nbeing hundreds of times faster than most existing diffusion-based methods.",
            "upvotes": 11,
            "discussionId": "66f62b586b0e782fa32a04b7"
        },
        "publishedAt": "2024-09-27T02:19:47.994Z",
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.18124.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.17422",
            "authors": [
                {
                    "_id": "66f61c3e21cc430244ffbd46",
                    "name": "Zhenmei Shi",
                    "hidden": false
                },
                {
                    "_id": "66f61c3e21cc430244ffbd47",
                    "user": {
                        "_id": "641787ad1f1f3b0fa80f5340",
                        "avatarUrl": "/avatars/489b7c999bce2160e27c9b487e1026ef.svg",
                        "isPro": false,
                        "fullname": "yifei ming",
                        "user": "alvinming",
                        "type": "user"
                    },
                    "name": "Yifei Ming",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-09-27T02:45:19.948Z",
                    "hidden": false
                },
                {
                    "_id": "66f61c3e21cc430244ffbd48",
                    "name": "Xuan-Phi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "66f61c3e21cc430244ffbd49",
                    "name": "Yingyu Liang",
                    "hidden": false
                },
                {
                    "_id": "66f61c3e21cc430244ffbd4a",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-25T23:14:47.000Z",
            "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs\n  with 1000x Input Token Reduction",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long context inputs, but this comes at the cost of increased\ncomputational resources and latency. Our research introduces a novel approach\nfor the long context bottleneck to accelerate LLM inference and reduce GPU\nmemory consumption. Our research demonstrates that LLMs can identify relevant\ntokens in the early layers before generating answers to a query. Leveraging\nthis insight, we propose an algorithm that uses early layers of an LLM as\nfilters to select and compress input tokens, significantly reducing the context\nlength for subsequent processing. Our method, GemFilter, demonstrates\nsubstantial improvements in both speed and memory efficiency compared to\nexisting techniques, such as standard attention and SnapKV/H2O. Notably, it\nachieves a 2.4times speedup and 30\\% reduction in GPU memory usage compared\nto SOTA methods. Evaluation on the Needle in a Haystack task shows that\nGemFilter significantly outperforms standard attention, SnapKV and demonstrates\ncomparable performance on the LongBench challenge. GemFilter is simple,\ntraining-free, and broadly applicable across different LLMs. Crucially, it\nprovides interpretability by allowing humans to inspect the selected input\nsequence. These findings not only offer practical benefits for LLM deployment,\nbut also enhance our understanding of LLM internal mechanisms, paving the way\nfor further optimizations in LLM design and inference. Our code is available at\nhttps://github.com/SalesforceAIResearch/GemFilter.",
            "upvotes": 11,
            "discussionId": "66f61c3f21cc430244ffbd85"
        },
        "publishedAt": "2024-09-27T01:15:22.583Z",
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17422.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.17565",
            "authors": [
                {
                    "_id": "66f63450c6cef3d86eec706d",
                    "name": "Christina Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec706e",
                    "name": "Simran Motwani",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec706f",
                    "name": "Matthew Yu",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7070",
                    "name": "Ji Hou",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7071",
                    "name": "Felix Juefei-Xu",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7072",
                    "name": "Sam Tsai",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7073",
                    "name": "Peter Vajda",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7074",
                    "name": "Zijian He",
                    "hidden": false
                },
                {
                    "_id": "66f63450c6cef3d86eec7075",
                    "name": "Jialiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T06:27:26.000Z",
            "title": "Pixel-Space Post-Training of Latent Diffusion Models",
            "summary": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically 8 times 8\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality.",
            "upvotes": 9,
            "discussionId": "66f6345ac6cef3d86eec72f2"
        },
        "publishedAt": "2024-09-27T02:58:25.978Z",
        "title": "Pixel-Space Post-Training of Latent Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17565.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.14683",
            "authors": [
                {
                    "_id": "66f64f2b8f96964e4b024e82",
                    "name": "Benjamin Clavié",
                    "hidden": false
                },
                {
                    "_id": "66f64f2b8f96964e4b024e83",
                    "user": {
                        "_id": "609bbe2f4932693ca2009d6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg",
                        "isPro": false,
                        "fullname": "Antoine Chaffin",
                        "user": "NohTow",
                        "type": "user"
                    },
                    "name": "Antoine Chaffin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-27T13:48:00.209Z",
                    "hidden": false
                },
                {
                    "_id": "66f64f2b8f96964e4b024e84",
                    "name": "Griffin Adams",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-23T03:12:43.000Z",
            "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal\n  Performance Impact via Token Pooling",
            "summary": "Over the last few years, multi-vector retrieval methods, spearheaded by\nColBERT, have become an increasingly popular approach to Neural IR. By storing\nrepresentations at the token level rather than at the document level, these\nmethods have demonstrated very strong retrieval performance, especially in\nout-of-domain settings. However, the storage and memory requirements necessary\nto store the large number of associated vectors remain an important drawback,\nhindering practical adoption. In this paper, we introduce a simple\nclustering-based token pooling approach to aggressively reduce the number of\nvectors that need to be stored. This method can reduce the space & memory\nfootprint of ColBERT indexes by 50% with virtually no retrieval performance\ndegradation. This method also allows for further reductions, reducing the\nvector count by 66%-to-75% , with degradation remaining below 5% on a vast\nmajority of datasets. Importantly, this approach requires no architectural\nchange nor query-time processing, and can be used as a simple drop-in during\nindexation with any ColBERT-like model.",
            "upvotes": 5,
            "discussionId": "66f64f2c8f96964e4b024ec9"
        },
        "publishedAt": "2024-09-27T04:54:39.762Z",
        "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60107b385ac3e86b3ea4fc34/t40IuWnMyRpmzotWZ82Zi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14683.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.17280",
            "authors": [
                {
                    "_id": "66f61aef793e1463e402813e",
                    "name": "Hui En Pang",
                    "hidden": false
                },
                {
                    "_id": "66f61aef793e1463e402813f",
                    "name": "Shuai Liu",
                    "hidden": false
                },
                {
                    "_id": "66f61aef793e1463e4028140",
                    "name": "Zhongang Cai",
                    "hidden": false
                },
                {
                    "_id": "66f61aef793e1463e4028141",
                    "name": "Lei Yang",
                    "hidden": false
                },
                {
                    "_id": "66f61aef793e1463e4028142",
                    "name": "Tianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f61aef793e1463e4028143",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-25T18:46:06.000Z",
            "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single\n  Image",
            "summary": "We present Disco4D, a novel Gaussian Splatting framework for 4D\nhuman generation and animation from a single image. Different from existing\nmethods, Disco4D distinctively disentangles clothings (with Gaussian models)\nfrom the human body (with SMPL-X model), significantly enhancing the generation\ndetails and flexibility. It has the following technical innovations.\n1) Disco4D learns to efficiently fit the clothing Gaussians over the\nSMPL-X Gaussians. 2) It adopts diffusion models to enhance the 3D\ngeneration process, e.g., modeling occluded parts not visible in the\ninput image. 3) It learns an identity encoding for each clothing\nGaussian to facilitate the separation and extraction of clothing assets.\nFurthermore, Disco4D naturally supports 4D human animation with vivid dynamics.\nExtensive experiments demonstrate the superiority of Disco4D on 4D human\ngeneration and animation tasks. Our visualizations can be found in\nhttps://disco-4d.github.io/.",
            "upvotes": 4,
            "discussionId": "66f61af2793e1463e4028216"
        },
        "publishedAt": "2024-09-27T01:10:00.742Z",
        "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17280.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.14254",
            "authors": [
                {
                    "_id": "66f69c7519fa4bbc452507bb",
                    "name": "John Hewitt",
                    "hidden": false
                },
                {
                    "_id": "66f69c7519fa4bbc452507bc",
                    "name": "Nelson F. Liu",
                    "hidden": false
                },
                {
                    "_id": "66f69c7519fa4bbc452507bd",
                    "name": "Percy Liang",
                    "hidden": false
                },
                {
                    "_id": "66f69c7519fa4bbc452507be",
                    "name": "Christopher D. Manning",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-21T22:36:22.000Z",
            "title": "Instruction Following without Instruction Tuning",
            "summary": "Instruction tuning commonly means finetuning a language model on\ninstruction-response pairs. We discover two forms of adaptation (tuning) that\nare deficient compared to instruction tuning, yet still yield instruction\nfollowing; we call this implicit instruction tuning. We first find that\ninstruction-response pairs are not necessary: training solely on responses,\nwithout any corresponding instructions, yields instruction following. This\nsuggests pretrained models have an instruction-response mapping which is\nrevealed by teaching the model the desired distribution of responses. However,\nwe then find it's not necessary to teach the desired distribution of responses:\ninstruction-response training on narrow-domain data like poetry still leads to\nbroad instruction-following behavior like recipe generation. In particular,\nwhen instructions are very different from those in the narrow finetuning\ndomain, models' responses do not adhere to the style of the finetuning domain.\nTo begin to explain implicit instruction tuning, we hypothesize that very\nsimple changes to a language model's distribution yield instruction following.\nWe support this by hand-writing a rule-based language model which yields\ninstruction following in a product-of-experts with a pretrained model. The\nrules are to slowly increase the probability of ending the sequence, penalize\nrepetition, and uniformly change 15 words' probabilities. In summary,\nadaptations made without being designed to yield instruction following can do\nso implicitly.",
            "upvotes": 3,
            "discussionId": "66f69c7719fa4bbc45250837"
        },
        "publishedAt": "2024-09-27T10:24:59.502Z",
        "title": "Instruction Following without Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14254.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669dbd709a4bf63e08f1ddc2/aV10ZJPPzH5LbnHFZNqc7.png",
            "fullname": "Yi Cui",
            "name": "onekq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.14195",
            "authors": [
                {
                    "_id": "66f6a945cd978208b9e30aee",
                    "name": "Xinghua Zhang",
                    "hidden": false
                },
                {
                    "_id": "66f6a945cd978208b9e30aef",
                    "name": "Haiyang Yu",
                    "hidden": false
                },
                {
                    "_id": "66f6a945cd978208b9e30af0",
                    "name": "Yongbin Li",
                    "hidden": false
                },
                {
                    "_id": "66f6a945cd978208b9e30af1",
                    "name": "Minzheng Wang",
                    "hidden": false
                },
                {
                    "_id": "66f6a945cd978208b9e30af2",
                    "name": "Longze Chen",
                    "hidden": false
                },
                {
                    "_id": "66f6a945cd978208b9e30af3",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-21T16:52:43.000Z",
            "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of\n  Tasks, Techniques, and Trends",
            "summary": "In the era of large language models (LLMs), a vast amount of conversation\nlogs will be accumulated thanks to the rapid development trend of language UI.\nConversation Analysis (CA) strives to uncover and analyze critical information\nfrom conversation data, streamlining manual processes and supporting business\ninsights and decision-making. The need for CA to extract actionable insights\nand drive empowerment is becoming increasingly prominent and attracting\nwidespread attention. However, the lack of a clear scope for CA leads to a\ndispersion of various techniques, making it difficult to form a systematic\ntechnical synergy to empower business applications. In this paper, we perform a\nthorough review and systematize CA task to summarize the existing related work.\nSpecifically, we formally define CA task to confront the fragmented and chaotic\nlandscape in this field, and derive four key steps of CA from conversation\nscene reconstruction, to in-depth attribution analysis, and then to performing\ntargeted training, finally generating conversations based on the targeted\ntraining for achieving the specific goals. In addition, we showcase the\nrelevant benchmarks, discuss potential challenges and point out future\ndirections in both industry and academia. In view of current advancements, it\nis evident that the majority of efforts are still concentrated on the analysis\nof shallow conversation elements, which presents a considerable gap between the\nresearch and business, and with the assist of LLMs, recent work has shown a\ntrend towards research on causality and strategic tasks which are sophisticated\nand high-level. The analyzed experiences and insights will inevitably have\nbroader application value in business operations that target conversation logs.",
            "upvotes": 2,
            "discussionId": "66f6a946cd978208b9e30b18"
        },
        "publishedAt": "2024-09-27T11:20:10.309Z",
        "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14195.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
            "fullname": "AIRobotZ",
            "name": "AIRobotZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.18121",
            "authors": [
                {
                    "_id": "66f6277ce5d2d88a93bc9331",
                    "name": "Justin Kerr",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9332",
                    "name": "Chung Min Kim",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9333",
                    "name": "Mingxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9334",
                    "name": "Brent Yi",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9335",
                    "name": "Qianqian Wang",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9336",
                    "name": "Ken Goldberg",
                    "hidden": false
                },
                {
                    "_id": "66f6277ce5d2d88a93bc9337",
                    "name": "Angjoo Kanazawa",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T17:57:16.000Z",
            "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with\n  Monocular 4D Reconstruction",
            "summary": "Humans can learn to manipulate new objects by simply watching others;\nproviding robots with the ability to learn from such demonstrations would\nenable a natural interface specifying new behaviors. This work develops Robot\nSee Robot Do (RSRD), a method for imitating articulated object manipulation\nfrom a single monocular RGB human demonstration given a single static\nmulti-view object scan. We first propose 4D Differentiable Part Models\n(4D-DPM), a method for recovering 3D part motion from a monocular video with\ndifferentiable rendering. This analysis-by-synthesis approach uses part-centric\nfeature fields in an iterative optimization which enables the use of geometric\nregularizers to recover 3D motions from only a single video. Given this 4D\nreconstruction, the robot replicates object trajectories by planning bimanual\narm motions that induce the demonstrated object part motion. By representing\ndemonstrations as part-centric trajectories, RSRD focuses on replicating the\ndemonstration's intended behavior while considering the robot's own\nmorphological limits, rather than attempting to reproduce the hand's motion. We\nevaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part\ntrajectories and RSRD's physical execution performance on 9 objects across 10\ntrials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of\n87% success rate, for a total end-to-end success rate of 60% across 90 trials.\nNotably, this is accomplished using only feature fields distilled from large\npretrained vision models -- without any task-specific training, fine-tuning,\ndataset collection, or annotation. Project page:\nhttps://robot-see-robot-do.github.io",
            "upvotes": 2,
            "discussionId": "66f62781e5d2d88a93bc9489"
        },
        "publishedAt": "2024-09-27T02:03:23.364Z",
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.18121.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.17580",
            "authors": [
                {
                    "_id": "66f67233e8dde585ba259a57",
                    "name": "Zahra Sepasdar",
                    "hidden": false
                },
                {
                    "_id": "66f67233e8dde585ba259a58",
                    "name": "Sushant Gautam",
                    "hidden": false
                },
                {
                    "_id": "66f67233e8dde585ba259a59",
                    "name": "Cise Midoglu",
                    "hidden": false
                },
                {
                    "_id": "66f67233e8dde585ba259a5a",
                    "name": "Michael A. Riegler",
                    "hidden": false
                },
                {
                    "_id": "66f67233e8dde585ba259a5b",
                    "name": "Pål Halvorsen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-26T06:53:29.000Z",
            "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case\n  Study",
            "summary": "Extracting meaningful insights from large and complex datasets poses\nsignificant challenges, particularly in ensuring the accuracy and relevance of\nretrieved information. Traditional data retrieval methods such as sequential\nsearch and index-based retrieval often fail when handling intricate and\ninterconnected data structures, resulting in incomplete or misleading outputs.\nTo overcome these limitations, we introduce Structured-GraphRAG, a versatile\nframework designed to enhance information retrieval across structured datasets\nin natural language queries. Structured-GraphRAG utilizes multiple knowledge\ngraphs, which represent data in a structured format and capture complex\nrelationships between entities, enabling a more nuanced and comprehensive\nretrieval of information. This graph-based approach reduces the risk of errors\nin language model outputs by grounding responses in a structured format,\nthereby enhancing the reliability of results. We demonstrate the effectiveness\nof Structured-GraphRAG by comparing its performance with that of a recently\npublished method using traditional retrieval-augmented generation. Our findings\nshow that Structured-GraphRAG significantly improves query processing\nefficiency and reduces response times. While our case study focuses on soccer\ndata, the framework's design is broadly applicable, offering a powerful tool\nfor data analysis and enhancing language model applications across various\nstructured domains.",
            "upvotes": 1,
            "discussionId": "66f67234e8dde585ba259a9c"
        },
        "publishedAt": "2024-09-27T07:22:46.198Z",
        "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62c4779e0a06c039315fb57d/fbhciH5HX9NtCRq2roJeh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17580.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c4779e0a06c039315fb57d/LWdhrmJBGoyCqTFGS5Ap3.jpeg",
            "fullname": "Sushant Gautam",
            "name": "SushantGautam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]