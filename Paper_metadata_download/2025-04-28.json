[
  {
    "paper": {
      "id": "2504.15376",
      "authors": [
        {
          "_id": "680bda9c34c8d0bd08e01a25",
          "user": {
            "_id": "64c170190bfb901b04399295",
            "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
            "isPro": false,
            "fullname": "Zhiqiu Lin",
            "user": "zhiqiulin",
            "type": "user"
          },
          "name": "Zhiqiu Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a26",
          "user": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "isPro": false,
            "fullname": "Alan",
            "user": "syCen",
            "type": "user"
          },
          "name": "Siyuan Cen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a27",
          "name": "Daniel Jiang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a28",
          "name": "Jay Karhade",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a29",
          "user": {
            "_id": "67b2db158904ba09ca8feb79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
            "isPro": false,
            "fullname": "Hewei Wang",
            "user": "Stephen624",
            "type": "user"
          },
          "name": "Hewei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2a",
          "name": "Chancharik Mitra",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2b",
          "name": "Tiffany Ling",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2c",
          "name": "Yuhan Huang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2e",
          "name": "Mingyu Chen",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2f",
          "name": "Rushikesh Zawar",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a30",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a31",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a32",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a33",
          "name": "Deva Ramanan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T18:34:57.000Z",
      "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
      "title": "Towards Understanding Camera Motions in Any Video",
      "submittedOnDailyBy": {
        "_id": "65f82fb0de5e636ca20184fa",
        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
        "isPro": false,
        "fullname": "Alan",
        "user": "syCen",
        "type": "user"
      },
      "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
      "upvotes": 77,
      "discussionId": "680bda9e34c8d0bd08e01ae9",
      "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
      "githubRepo": "https://github.com/sy77777en/CameraBench",
      "ai_keywords": [
        "Structure-from-Motion (SfM)",
        "Video-Language Models (VLMs)",
        "semantic primitives",
        "geometric primitives",
        "generative VLM",
        "motion-augmented captioning",
        "video question answering",
        "video-text retrieval"
      ]
    },
    "publishedAt": "2025-04-21T14:34:57.000Z",
    "title": "Towards Understanding Camera Motions in Any Video",
    "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f82fb0de5e636ca20184fa",
      "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
      "fullname": "Alan",
      "name": "syCen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16656",
      "authors": [
        {
          "_id": "6809a4ac81a95c83f0c81c83",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c84",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c85",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c86",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c87",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c88",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c89",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8a",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8b",
          "name": "Jianhao Zhang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8c",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8d",
          "user": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "isPro": false,
            "fullname": "Xuchen Song",
            "user": "xuchensong",
            "type": "user"
          },
          "name": "Xuchen Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8f",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T12:24:10.000Z",
      "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
      "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
      "upvotes": 18,
      "discussionId": "6809a4ae81a95c83f0c81cda",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
      "ai_keywords": [
        "reinforcement learning",
        "reward-model guidance",
        "rule-based strategies",
        "Selective Sample Buffer (SSB)",
        "Vanishing Advantages",
        "Group Relative Policy Optimization (GRPO)",
        "visual hallucinations",
        "calibrated reward thresholds",
        "benchmark-leading performances",
        "OlympiadBench",
        "AIME2024",
        "LiveCodeBench",
        "MMMU",
        "Skywork R1V2-38B"
      ]
    },
    "publishedAt": "2025-04-23T08:24:10.000Z",
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18415",
      "authors": [
        {
          "_id": "680ef1549cc294f617fb14b4",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b5",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b6",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T15:17:52.000Z",
      "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
      "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
      "upvotes": 9,
      "discussionId": "680ef1559cc294f617fb1536",
      "ai_keywords": [
        "BitNet v2",
        "1-bit Large Language Models (LLMs)",
        "activation outliers",
        "quantization",
        "4-bit activation quantization",
        "H-BitLinear",
        "Hadamard transformation",
        "activation distributions",
        "Gaussian-like forms",
        "low-bit representation",
        "8-bit activations",
        "BitNet b1.58",
        "batched inference"
      ]
    },
    "publishedAt": "2025-04-25T11:17:52.000Z",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16427",
      "authors": [
        {
          "_id": "680c48805ec65044c2861a6a",
          "user": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "isPro": false,
            "fullname": "Hanlei Zhang",
            "user": "HanleiZhang",
            "type": "user"
          },
          "name": "Hanlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6b",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6c",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6d",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6e",
          "name": "Peiwu Wang",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6f",
          "name": "Haige Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a70",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a71",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T05:25:13.000Z",
      "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
      "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
      "submittedOnDailyBy": {
        "_id": "669090c01e3f5b16ce22b535",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
        "isPro": false,
        "fullname": "Hanlei Zhang",
        "user": "HanleiZhang",
        "type": "user"
      },
      "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
      "upvotes": 6,
      "discussionId": "680c48825ec65044c2861ac4",
      "githubRepo": "https://github.com/thuiar/MMLA",
      "ai_keywords": [
        "multimodal language models (MLLMs)",
        "MMLA (Multimodal Language Analysis)",
        "multimodal utterances",
        "intent",
        "emotion",
        "dialogue act",
        "sentiment",
        "speaking style",
        "communication behavior",
        "zero-shot inference",
        "supervised fine-tuning",
        "instruction tuning",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-23T01:25:13.000Z",
    "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
    "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669090c01e3f5b16ce22b535",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
      "fullname": "Hanlei Zhang",
      "name": "HanleiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17768",
      "authors": [
        {
          "_id": "680f2668db85fd31cd5080ff",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508100",
          "name": "Robert Li",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508101",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508102",
          "name": "Sebastian Ruder",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508103",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508104",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:39:25.000Z",
      "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
      "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
      "upvotes": 2,
      "discussionId": "680f2669db85fd31cd50815e",
      "ai_keywords": [
        "Sparse attention",
        "Transformer LLMs",
        "Training-free",
        "IsoFLOPS analysis",
        "Sequence lengths",
        "Sparsity levels",
        "Long-sequence tasks",
        "Natural language",
        "Accuracy preservation",
        "Decoding",
        "Prefilling",
        "Budget adaptivity",
        "Performance degradation",
        "Scaling laws"
      ]
    },
    "publishedAt": "2025-04-24T13:39:25.000Z",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15716",
      "authors": [
        {
          "_id": "680dcc5d3478de07603a8036",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8037",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8038",
          "name": "Huaixia Dou",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8039",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803a",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803b",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803c",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T09:01:04.000Z",
      "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
      "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
      "upvotes": 2,
      "discussionId": "680dcc5e3478de07603a807e",
      "ai_keywords": [
        "reasoning-enhanced framework",
        "reasoning-augmented supervision",
        "reinforcement learning",
        "DianJin-R1-Data",
        "CFLUE",
        "FinQA",
        "Chinese Compliance Check (CCC)",
        "high-quality dataset",
        "DianJin-R1-7B",
        "DianJin-R1-32B",
        "Qwen2.5-7B-Instruct",
        "Qwen2.5-32B-Instruct",
        "structured format",
        "reasoning steps",
        "Group Relative Policy Optimization (GRPO)",
        "dual reward signals",
        "structured outputs",
        "answer correctness",
        "MATH-500",
        "GPQA-Diamond",
        "financial datasets",
        "single-call reasoning models",
        "multi-agent systems",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-22T05:01:04.000Z",
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12080",
      "authors": [
        {
          "_id": "680afc5f2c4b584e1d786eee",
          "name": "Mengshi Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786eef",
          "user": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "isPro": false,
            "fullname": "Pengfei Zhu",
            "user": "zaplm",
            "type": "user"
          },
          "name": "Pengfei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef0",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef1",
          "name": "Xiaoyang Bi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef2",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef3",
          "name": "Huadong Ma",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T13:41:59.000Z",
      "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
      "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
      "submittedOnDailyBy": {
        "_id": "66a8c8e4f5cda7b8690205ef",
        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
        "isPro": false,
        "fullname": "Pengfei Zhu",
        "user": "zaplm",
        "type": "user"
      },
      "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
      "upvotes": 2,
      "discussionId": "680afc622c4b584e1d786f9e",
      "ai_keywords": [
        "prompt-tuning",
        "prompt encoder",
        "mask prior",
        "cycle-consistent cross-attention",
        "dual-branch design",
        "discriminative positive prompts",
        "negative prompts",
        "mask-tube",
        "In-Context Video Object Segmentation (IC-VOS)",
        "mIoU"
      ]
    },
    "publishedAt": "2025-04-16T09:41:59.000Z",
    "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
    "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a8c8e4f5cda7b8690205ef",
      "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
      "fullname": "Pengfei Zhu",
      "name": "zaplm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]