[
    {
        "paper": {
            "id": "2411.10640",
            "authors": [
                {
                    "_id": "673c0e397c1c0ded76fb21d1",
                    "user": {
                        "_id": "642e686bbe01b88c9446db8b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e686bbe01b88c9446db8b/tb1DKe5xt50ykOeXiUuTE.jpeg",
                        "isPro": false,
                        "fullname": "Lu Xudong",
                        "user": "lucky-lance",
                        "type": "user"
                    },
                    "name": "Xudong Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T08:38:19.862Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d2",
                    "name": "Yinghao Chen",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d3",
                    "name": "Cheng Chen",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d4",
                    "name": "Hui Tan",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d5",
                    "name": "Boheng Chen",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d6",
                    "user": {
                        "_id": "66faa34b7f5d72452e4ddaf6",
                        "avatarUrl": "/avatars/e5687132d1ebc9b1e5225996899b117a.svg",
                        "isPro": false,
                        "fullname": "yina x",
                        "user": "yina0",
                        "type": "user"
                    },
                    "name": "Yina Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T08:54:54.265Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d7",
                    "name": "Rui Hu",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d8",
                    "name": "Guanxin Tan",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21d9",
                    "name": "Renshou Wu",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21da",
                    "name": "Yan Hu",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21db",
                    "name": "Yi Zeng",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21dc",
                    "name": "Lei Wu",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21dd",
                    "user": {
                        "_id": "66d459ca1dbd780574b42e2d",
                        "avatarUrl": "/avatars/d573a92487628bf6c727b77eedf80b9d.svg",
                        "isPro": false,
                        "fullname": "bian liuyang",
                        "user": "liuyangbian",
                        "type": "user"
                    },
                    "name": "Liuyang Bian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T08:53:25.632Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21de",
                    "name": "Zhaoxiong Wang",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21df",
                    "name": "Long Liu",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e0",
                    "name": "Yanzhou Yang",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e1",
                    "name": "Han Xiao",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e2",
                    "user": {
                        "_id": "637de1520d5bb06fbe5207a9",
                        "avatarUrl": "/avatars/1090851217270c5a858b13e013356d4f.svg",
                        "isPro": false,
                        "fullname": "AJ.Zhou",
                        "user": "AJZhou",
                        "type": "user"
                    },
                    "name": "Aojun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T08:38:17.782Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e3",
                    "user": {
                        "_id": "67134b1ce58b557752c54a1c",
                        "avatarUrl": "/avatars/5cc07748dac424a6f5c7ca09f1c45068.svg",
                        "isPro": false,
                        "fullname": "yafei wen",
                        "user": "wolf1110",
                        "type": "user"
                    },
                    "name": "Yafei Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T08:53:02.294Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e4",
                    "user": {
                        "_id": "65389a669c474315d7425f96",
                        "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
                        "isPro": false,
                        "fullname": "chenxiaoxin",
                        "user": "steelozazala",
                        "type": "user"
                    },
                    "name": "Xiaoxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T08:52:55.372Z",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e5",
                    "name": "Shuai Ren",
                    "hidden": false
                },
                {
                    "_id": "673c0e397c1c0ded76fb21e6",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T08:52:25.727Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-16T00:14:51.000Z",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large\n  Language Models on Mobile Devices",
            "summary": "The emergence and growing popularity of multimodal large language models\n(MLLMs) have significant potential to enhance various aspects of daily life,\nfrom improving communication to facilitating learning and problem-solving.\nMobile phones, as essential daily companions, represent the most effective and\naccessible deployment platform for MLLMs, enabling seamless integration into\neveryday tasks. However, deploying MLLMs on mobile phones presents challenges\ndue to limitations in memory size and computational capability, making it\ndifficult to achieve smooth and real-time processing without extensive\noptimization. In this paper, we present BlueLM-V-3B, an algorithm and system\nco-design approach specifically tailored for the efficient deployment of MLLMs\non mobile platforms. To be specific, we redesign the dynamic resolution scheme\nadopted by mainstream MLLMs and implement system optimization for\nhardware-aware deployment to optimize model inference on mobile phones.\nBlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B\nfeatures a language model with 2.7B parameters and a vision encoder with 400M\nparameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4\ntoken/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight\nquantization. (3) Strong Performance: BlueLM-V-3B has attained the highest\naverage score of 66.1 on the OpenCompass benchmark among models with leq 4B\nparameters and surpassed a series of models with much larger parameter sizes\n(e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "upvotes": 27,
            "discussionId": "673c0e3a7c1c0ded76fb222a"
        },
        "publishedAt": "2024-11-19T02:34:50.579Z",
        "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10640.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/1090851217270c5a858b13e013356d4f.svg",
            "fullname": "AJ.Zhou",
            "name": "AJZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.11844",
            "authors": [
                {
                    "_id": "673bfe4c2bf4163ef13f497f",
                    "user": {
                        "_id": "656a9b9f9496f21be8271f1b",
                        "avatarUrl": "/avatars/cfba9f835bf5eef80c6c5f52be69abd4.svg",
                        "isPro": false,
                        "fullname": "Terry",
                        "user": "TaiMingLu",
                        "type": "user"
                    },
                    "name": "Taiming Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T08:39:37.063Z",
                    "hidden": false
                },
                {
                    "_id": "673bfe4c2bf4163ef13f4980",
                    "user": {
                        "_id": "618af79af281834c95311402",
                        "avatarUrl": "/avatars/58ba086eb2309657d823b5768616eafe.svg",
                        "isPro": false,
                        "fullname": "Tianmin Shu",
                        "user": "tshu",
                        "type": "user"
                    },
                    "name": "Tianmin Shu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:07:10.536Z",
                    "hidden": false
                },
                {
                    "_id": "673bfe4c2bf4163ef13f4981",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "673bfe4c2bf4163ef13f4982",
                    "user": {
                        "_id": "5f6540c65e78cc6b0ed3199d",
                        "avatarUrl": "/avatars/0280d4df417855965a0964d22766c012.svg",
                        "isPro": false,
                        "fullname": "Daniel Khashabi",
                        "user": "danyaljj",
                        "type": "user"
                    },
                    "name": "Daniel Khashabi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:07:23.603Z",
                    "hidden": false
                },
                {
                    "_id": "673bfe4c2bf4163ef13f4983",
                    "user": {
                        "_id": "660c9ac4b202fcf3892f62fa",
                        "avatarUrl": "/avatars/7314fd5f3f642096d0e37d3194f1aa7e.svg",
                        "isPro": false,
                        "fullname": "Jieneng Chen",
                        "user": "jienengchen",
                        "type": "user"
                    },
                    "name": "Jieneng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T08:39:33.842Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-18T18:59:31.000Z",
            "title": "Generative World Explorer",
            "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can imagine unseen parts of the world\nthrough a mental exploration and revise their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the Generative\nWorld Explorer (Genex), an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain Genex, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) Genex can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
            "upvotes": 24,
            "discussionId": "673bfe502bf4163ef13f4a96"
        },
        "publishedAt": "2024-11-19T05:24:16.683Z",
        "title": "Generative World Explorer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11844.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/07709b503d8b71ac92b3d834c4873eed.svg",
            "fullname": "Jieneng Chen",
            "name": "bbexx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.11504",
            "authors": [
                {
                    "_id": "673c296853734795380a3dd4",
                    "user": {
                        "_id": "643407dd4b34368fdb0149e8",
                        "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
                        "isPro": false,
                        "fullname": "Xinyan Guan",
                        "user": "xinyan233333",
                        "type": "user"
                    },
                    "name": "Xinyan Guan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T13:09:12.237Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dd5",
                    "name": "Yanjiang Liu",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dd6",
                    "user": {
                        "_id": "63115e4daf1fce227a404b16",
                        "avatarUrl": "/avatars/f2e9ee3f1f70bcfabb2b660b241b7aee.svg",
                        "isPro": false,
                        "fullname": "Xinyu Lu",
                        "user": "luxinyu",
                        "type": "user"
                    },
                    "name": "Xinyu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:10:22.537Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dd7",
                    "user": {
                        "_id": "5fc052241160c47d1d438556",
                        "avatarUrl": "/avatars/f508707fc92f1b42f7897b12b727754c.svg",
                        "isPro": false,
                        "fullname": "Boxi Cao",
                        "user": "Bowieee",
                        "type": "user"
                    },
                    "name": "Boxi Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:10:29.884Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dd8",
                    "name": "Ben He",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dd9",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:07:58.820Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dda",
                    "name": "Le Sun",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3ddb",
                    "name": "Jie Lou",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3ddc",
                    "user": {
                        "_id": "6438b43ab2ea24b52ebac2b9",
                        "avatarUrl": "/avatars/84133cd719a4b1e2f5c1a74178425f86.svg",
                        "isPro": false,
                        "fullname": "Bowen Yu",
                        "user": "bwy",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:08:19.304Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3ddd",
                    "user": {
                        "_id": "6216496a9b34d2fb49144599",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
                        "isPro": false,
                        "fullname": "Yaojie Lu",
                        "user": "luyaojie",
                        "type": "user"
                    },
                    "name": "Yaojie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:08:11.873Z",
                    "hidden": false
                },
                {
                    "_id": "673c296853734795380a3dde",
                    "user": {
                        "_id": "6711c702f858a456b4b9f3a4",
                        "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
                        "isPro": false,
                        "fullname": "Hongyu  Lin",
                        "user": "sanmusunrise",
                        "type": "user"
                    },
                    "name": "Hongyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T13:08:05.720Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-18T12:04:52.000Z",
            "title": "Search, Verify and Feedback: Towards Next Generation Post-training\n  Paradigm of Foundation Models via Verifier Engineering",
            "summary": "The evolution of machine learning has increasingly prioritized the\ndevelopment of powerful models and more scalable supervision signals. However,\nthe emergence of foundation models presents significant challenges in providing\neffective supervision signals necessary for further enhancing their\ncapabilities. Consequently, there is an urgent need to explore novel\nsupervision signals and technical approaches. In this paper, we propose\nverifier engineering, a novel post-training paradigm specifically designed for\nthe era of foundation models. The core of verifier engineering involves\nleveraging a suite of automated verifiers to perform verification tasks and\ndeliver meaningful feedback to foundation models. We systematically categorize\nthe verifier engineering process into three essential stages: search, verify,\nand feedback, and provide a comprehensive review of state-of-the-art research\ndevelopments within each stage. We believe that verifier engineering\nconstitutes a fundamental pathway toward achieving Artificial General\nIntelligence.",
            "upvotes": 11,
            "discussionId": "673c296853734795380a3e17"
        },
        "publishedAt": "2024-11-19T04:31:35.752Z",
        "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11504.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "fullname": "Bowen Yu",
            "name": "Tigerph",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.07641",
            "authors": [
                {
                    "_id": "67340d47a445f1d61926ce8e",
                    "user": {
                        "_id": "65d2fa0b6236ca85a4091763",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2fa0b6236ca85a4091763/Cc6eS-Bj-dIb0mUv2QSSh.jpeg",
                        "isPro": false,
                        "fullname": "Tang Chenxia",
                        "user": "tomorrowdawn",
                        "type": "user"
                    },
                    "name": "Chenxia Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-15T09:25:26.830Z",
                    "hidden": false
                },
                {
                    "_id": "67340d47a445f1d61926ce8f",
                    "name": "Jianchun Liu",
                    "hidden": false
                },
                {
                    "_id": "67340d47a445f1d61926ce90",
                    "name": "Hongli Xu",
                    "hidden": false
                },
                {
                    "_id": "67340d47a445f1d61926ce91",
                    "name": "Liusheng Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T08:46:43.000Z",
            "title": "Top-nσ: Not All Logits Are You Need",
            "summary": "Large language models (LLMs) typically employ greedy decoding or\nlow-temperature sampling for reasoning tasks, reflecting a perceived trade-off\nbetween diversity and accuracy. We challenge this convention by introducing\ntop-nsigma, a novel sampling method that operates directly on pre-softmax\nlogits by leveraging a statistical threshold. Our key insight is that logits\nnaturally separate into a Gaussian-distributed noisy region and a distinct\ninformative region, enabling efficient token filtering without complex\nprobability manipulations. Unlike existing methods (e.g., top-p, min-p)\nthat inadvertently include more noise tokens at higher temperatures,\ntop-nsigma maintains a stable sampling space regardless of temperature\nscaling. We also provide a theoretical analysis of top-nsigma to better\nunderstand its behavior. The extensive experimental results across four\nreasoning-focused datasets demonstrate that our method not only outperforms\nexisting sampling approaches but also surpasses greedy decoding, while\nmaintaining consistent performance even at high temperatures.",
            "upvotes": 9,
            "discussionId": "67340d48a445f1d61926ced9"
        },
        "publishedAt": "2024-11-19T01:40:29.738Z",
        "title": "Top-$nσ$: Not All Logits Are You Need",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07641.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2fa0b6236ca85a4091763/Cc6eS-Bj-dIb0mUv2QSSh.jpeg",
            "fullname": "Tang Chenxia",
            "name": "tomorrowdawn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.10836",
            "authors": [
                {
                    "_id": "673c2cebabddf849496a677e",
                    "name": "Guojun Lei",
                    "hidden": false
                },
                {
                    "_id": "673c2cebabddf849496a677f",
                    "name": "Chi Wang",
                    "hidden": false
                },
                {
                    "_id": "673c2cebabddf849496a6780",
                    "name": "Hong Li",
                    "hidden": false
                },
                {
                    "_id": "673c2cebabddf849496a6781",
                    "name": "Rong Zhang",
                    "hidden": false
                },
                {
                    "_id": "673c2cebabddf849496a6782",
                    "user": {
                        "_id": "64206b35e40f66bcd1e3c985",
                        "avatarUrl": "/avatars/a8f830d1c47702f642045399e59c9fe8.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiw",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-19T06:25:38.876Z",
                    "hidden": true
                },
                {
                    "_id": "673c2cebabddf849496a6783",
                    "name": "Weiwei Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-16T16:36:49.000Z",
            "title": "AnimateAnything: Consistent and Controllable Animation for Video\n  Generation",
            "summary": "We present a unified controllable video generation approach AnimateAnything\nthat facilitates precise and consistent video manipulation across various\nconditions, including camera trajectories, text prompts, and user motion\nannotations. Specifically, we carefully design a multi-scale control feature\nfusion network to construct a common motion representation for different\nconditions. It explicitly converts all control information into frame-by-frame\noptical flows. Then we incorporate the optical flows as motion priors to guide\nfinal video generation. In addition, to reduce the flickering issues caused by\nlarge-scale motion, we propose a frequency-based stabilization module. It can\nenhance temporal coherence by ensuring the video's frequency domain\nconsistency. Experiments demonstrate that our method outperforms the\nstate-of-the-art approaches. For more details and videos, please refer to the\nwebpage: https://yu-shaonian.github.io/Animate_Anything/.",
            "upvotes": 8,
            "discussionId": "673c2cefabddf849496a68b7"
        },
        "publishedAt": "2024-11-19T04:50:58.737Z",
        "title": "AnimateAnything: Consistent and Controllable Animation for Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10836.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5172
        }
    },
    {
        "paper": {
            "id": "2411.11767",
            "authors": [
                {
                    "_id": "673c0575955070e4cfbc9570",
                    "user": {
                        "_id": "649949e057368427491c3b7d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649949e057368427491c3b7d/jxX_8l7XB0rYlkNB_YBGM.jpeg",
                        "isPro": false,
                        "fullname": "Jacob",
                        "user": "MathewJacob",
                        "type": "user"
                    },
                    "name": "Mathew Jacob",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:24:22.335Z",
                    "hidden": false
                },
                {
                    "_id": "673c0575955070e4cfbc9571",
                    "name": "Erik Lindgren",
                    "hidden": false
                },
                {
                    "_id": "673c0575955070e4cfbc9572",
                    "user": {
                        "_id": "642509c6d476e4ad5568dd31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642509c6d476e4ad5568dd31/dPiEV70hMeuY-cveLNPQg.jpeg",
                        "isPro": false,
                        "fullname": "Matei Zaharia",
                        "user": "mateiz",
                        "type": "user"
                    },
                    "name": "Matei Zaharia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:24:53.605Z",
                    "hidden": false
                },
                {
                    "_id": "673c0575955070e4cfbc9573",
                    "user": {
                        "_id": "639b4452c522f98dd1fed3d3",
                        "avatarUrl": "/avatars/f11a8b69b34f8219eb63f914023e8bd1.svg",
                        "isPro": false,
                        "fullname": "Michael Carbin",
                        "user": "mcarbin-mosaicml",
                        "type": "user"
                    },
                    "name": "Michael Carbin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:24:45.070Z",
                    "hidden": false
                },
                {
                    "_id": "673c0575955070e4cfbc9574",
                    "user": {
                        "_id": "5f764762f1e7ef6e919a1e0f",
                        "avatarUrl": "/avatars/9445f6efa3f78a870e338988d9ce5573.svg",
                        "isPro": false,
                        "fullname": "Omar Khattab",
                        "user": "okhattab",
                        "type": "user"
                    },
                    "name": "Omar Khattab",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:25:41.417Z",
                    "hidden": false
                },
                {
                    "_id": "673c0575955070e4cfbc9575",
                    "user": {
                        "_id": "613fbc22be1df5145babcad0",
                        "avatarUrl": "/avatars/2048e58c43997a60e2dabcfc13112eec.svg",
                        "isPro": false,
                        "fullname": "Andrew Drozdov",
                        "user": "mrdrozdov",
                        "type": "user"
                    },
                    "name": "Andrew Drozdov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-19T03:58:32.721Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-18T17:46:32.000Z",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "summary": "Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking.",
            "upvotes": 7,
            "discussionId": "673c0576955070e4cfbc9601"
        },
        "publishedAt": "2024-11-19T02:07:03.407Z",
        "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11767.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2048e58c43997a60e2dabcfc13112eec.svg",
            "fullname": "Andrew Drozdov",
            "name": "mrdrozdov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.11171",
            "authors": [
                {
                    "_id": "673c3f6c19cbbe3091dc6ad7",
                    "user": {
                        "_id": "6070431e1a4c4d313032558b",
                        "avatarUrl": "/avatars/142102e7e14886a0ff34af7959ca2f8c.svg",
                        "isPro": false,
                        "fullname": "Jan Pfister",
                        "user": "JanPf",
                        "type": "user"
                    },
                    "name": "Jan Pfister",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:38:08.396Z",
                    "hidden": false
                },
                {
                    "_id": "673c3f6c19cbbe3091dc6ad8",
                    "user": {
                        "_id": "61a36cba39c7c71f0bda980e",
                        "avatarUrl": "/avatars/3a351b7ea31e82f350193415cc5b256e.svg",
                        "isPro": false,
                        "fullname": "Julia Wunderle ",
                        "user": "Julia287",
                        "type": "user"
                    },
                    "name": "Julia Wunderle",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:37:46.563Z",
                    "hidden": false
                },
                {
                    "_id": "673c3f6c19cbbe3091dc6ad9",
                    "user": {
                        "_id": "670d2ac02a2ae377cf4aa56e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CDM7_7XmFF-1pCqsUkrBB.png",
                        "isPro": false,
                        "fullname": "Andreas Hotho",
                        "user": "arthur3131",
                        "type": "user"
                    },
                    "name": "Andreas Hotho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:37:40.241Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-17T20:44:34.000Z",
            "title": "LLäMmlein: Compact and Competitive German-Only Language Models from\n  Scratch",
            "summary": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.",
            "upvotes": 6,
            "discussionId": "673c3f6d19cbbe3091dc6b24"
        },
        "publishedAt": "2024-11-19T06:05:44.227Z",
        "title": "LLäMmlein: Compact and Competitive German-Only Language Models from Scratch",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/AjY7XHOmynJmRx4_UXFk8.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11171.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 1855
        }
    },
    {
        "paper": {
            "id": "2411.09944",
            "authors": [
                {
                    "_id": "673c3346955070e4cfcac54f",
                    "user": {
                        "_id": "600e4c4010a64b1c9f81ea94",
                        "avatarUrl": "/avatars/d043adb5a89023eb6ef01c2ec457e8ca.svg",
                        "isPro": false,
                        "fullname": "Thang M. Pham",
                        "user": "pmthangk09",
                        "type": "user"
                    },
                    "name": "Thang M. Pham",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:25:53.374Z",
                    "hidden": false
                },
                {
                    "_id": "673c3346955070e4cfcac550",
                    "name": "Phat T. Nguyen",
                    "hidden": false
                },
                {
                    "_id": "673c3346955070e4cfcac551",
                    "name": "Seunghyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "673c3346955070e4cfcac552",
                    "name": "Viet Dac Lai",
                    "hidden": false
                },
                {
                    "_id": "673c3346955070e4cfcac553",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-19T06:43:47.566Z",
                    "hidden": false
                },
                {
                    "_id": "673c3346955070e4cfcac554",
                    "name": "Trung Bui",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T04:44:34.000Z",
            "title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance",
            "summary": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.",
            "upvotes": 6,
            "discussionId": "673c3348955070e4cfcac614"
        },
        "publishedAt": "2024-11-19T05:12:19.731Z",
        "title": "SlimLM: An Efficient Small Language Model for On-Device Document Assistance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09944.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.10669",
            "authors": [
                {
                    "_id": "673c25ff11b7efeeed69461d",
                    "name": "Jinqiang Long",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed69461e",
                    "user": {
                        "_id": "66cde57cb1fe4c78fe3ab770",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg",
                        "isPro": false,
                        "fullname": "Yanqi Dai",
                        "user": "YanqiDai",
                        "type": "user"
                    },
                    "name": "Yanqi Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:23:30.039Z",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed69461f",
                    "name": "Guoxing Yang",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed694620",
                    "name": "Hongpeng Lin",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed694621",
                    "name": "Nanyi Fei",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed694622",
                    "user": {
                        "_id": "661c96f48921f03a9dae04c3",
                        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
                        "isPro": false,
                        "fullname": "Yizhao Gao",
                        "user": "Retromonic",
                        "type": "user"
                    },
                    "name": "Yizhao Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:23:56.495Z",
                    "hidden": false
                },
                {
                    "_id": "673c25ff11b7efeeed694623",
                    "user": {
                        "_id": "6351e3035b904878f1d36719",
                        "avatarUrl": "/avatars/a7b3a5faacaa483fab1a5b1b95bcc0a4.svg",
                        "isPro": false,
                        "fullname": "Zhiwu Lu",
                        "user": "zewlu",
                        "type": "user"
                    },
                    "name": "Zhiwu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T15:23:50.216Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-16T02:10:14.000Z",
            "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of\n  Experts",
            "summary": "As the research of Multimodal Large Language Models (MLLMs) becomes popular,\nan advancing MLLM model is typically required to handle various textual and\nvisual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for\nreal-world applications. However, due to the significant differences in\nrepresentation and distribution among data from various tasks, simply mixing\ndata of all tasks together leads to the well-known``multi-task conflict\" issue,\nresulting in performance degradation across various tasks. To address this\nissue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture\nsuitable for MLLM, which acquires the multi-task capabilities through multiple\nsparsely activated experts. To speed up the training and inference of\nAwaker2.5-VL, each expert in our model is devised as a low-rank adaptation\n(LoRA) structure. Extensive experiments on multiple latest benchmarks\ndemonstrate the effectiveness of Awaker2.5-VL. The code and model weight are\nreleased in our Project Page: https://github.com/MetabrainAGI/Awaker.",
            "upvotes": 6,
            "discussionId": "673c260011b7efeeed694639"
        },
        "publishedAt": "2024-11-19T04:15:48.495Z",
        "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10669.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5172
        }
    },
    {
        "paper": {
            "id": "2411.10510",
            "authors": [
                {
                    "_id": "673c2472d1794741928fa132",
                    "user": {
                        "_id": "65330e31d690f3012e0557a2",
                        "avatarUrl": "/avatars/c0969c9643a6dc70ff7b3e65c9412e54.svg",
                        "isPro": false,
                        "fullname": "Eng Ann Joseph Liu",
                        "user": "josephliu-roblox",
                        "type": "user"
                    },
                    "name": "Joseph Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-19T05:39:04.787Z",
                    "hidden": false
                },
                {
                    "_id": "673c2472d1794741928fa133",
                    "name": "Joshua Geddes",
                    "hidden": false
                },
                {
                    "_id": "673c2472d1794741928fa134",
                    "user": {
                        "_id": "648cce407f7821f063c140be",
                        "avatarUrl": "/avatars/6a458c35f34bfbceba6be8e5b6fb1437.svg",
                        "isPro": false,
                        "fullname": "Ziyu Guo",
                        "user": "helloterran",
                        "type": "user"
                    },
                    "name": "Ziyu Guo",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-19T05:39:04.787Z",
                    "hidden": false
                },
                {
                    "_id": "673c2472d1794741928fa135",
                    "name": "Haomiao Jiang",
                    "hidden": false
                },
                {
                    "_id": "673c2472d1794741928fa136",
                    "user": {
                        "_id": "667f437aa94430cd2e723346",
                        "avatarUrl": "/avatars/c7d71f6789756082e129b0b9cf9dcfda.svg",
                        "isPro": false,
                        "fullname": "Mahesh Nandwana",
                        "user": "mnandwana",
                        "type": "user"
                    },
                    "name": "Mahesh Kumar Nandwana",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-19T05:39:04.787Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T16:24:02.000Z",
            "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
            "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
            "upvotes": 6,
            "discussionId": "673c2478d1794741928fa2de"
        },
        "publishedAt": "2024-11-19T04:09:13.608Z",
        "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10510.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5172
        }
    },
    {
        "paper": {
            "id": "2411.09213",
            "authors": [
                {
                    "_id": "673c338be17a2c7a97e8b8c3",
                    "user": {
                        "_id": "658a8208c4b2004663d82daf",
                        "avatarUrl": "/avatars/af6bcee06aec82602c2b931f79c008e7.svg",
                        "isPro": false,
                        "fullname": "Nghia Trung Ngo",
                        "user": "ntnghia1811",
                        "type": "user"
                    },
                    "name": "Nghia Trung Ngo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T14:21:03.206Z",
                    "hidden": false
                },
                {
                    "_id": "673c338be17a2c7a97e8b8c4",
                    "name": "Chien Van Nguyen",
                    "hidden": false
                },
                {
                    "_id": "673c338be17a2c7a97e8b8c5",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-19T06:43:48.817Z",
                    "hidden": false
                },
                {
                    "_id": "673c338be17a2c7a97e8b8c6",
                    "user": {
                        "_id": "64804fad8c6a3b8f11f73912",
                        "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
                        "isPro": false,
                        "fullname": "Thien Huu Nguyen",
                        "user": "anoperson",
                        "type": "user"
                    },
                    "name": "Thien Huu Nguyen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T14:21:15.211Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-14T06:19:18.000Z",
            "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering",
            "summary": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
            "upvotes": 5,
            "discussionId": "673c338de17a2c7a97e8b977"
        },
        "publishedAt": "2024-11-19T05:13:27.978Z",
        "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09213.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.11024",
            "authors": [
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef3e",
                    "name": "Weronika Smolak-Dyżewska",
                    "hidden": false
                },
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef3f",
                    "user": {
                        "_id": "6303d84b7373aacccd867994",
                        "avatarUrl": "/avatars/c600374518284ba9c50899dde00bdf28.svg",
                        "isPro": false,
                        "fullname": "Dawid Malarz",
                        "user": "MalarzDawid",
                        "type": "user"
                    },
                    "name": "Dawid Malarz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:26:14.135Z",
                    "hidden": false
                },
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef40",
                    "user": {
                        "_id": "65fc24b804769daf21a98fa6",
                        "avatarUrl": "/avatars/d7faf536940809823d56a442f55e9fb9.svg",
                        "isPro": false,
                        "fullname": "Kornel",
                        "user": "kornelhowil",
                        "type": "user"
                    },
                    "name": "Kornel Howil",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T13:09:06.082Z",
                    "hidden": false
                },
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef41",
                    "name": "Jan Kaczmarczyk",
                    "hidden": false
                },
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef42",
                    "name": "Marcin Mazur",
                    "hidden": false
                },
                {
                    "_id": "673c8d3a44ea7d8aeaa3ef43",
                    "user": {
                        "_id": "670a4a2b64cab942c09769c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mP8o2FNQJHI5BHKr6AsvF.png",
                        "isPro": false,
                        "fullname": "Przemysław Spurek",
                        "user": "przem85",
                        "type": "user"
                    },
                    "name": "Przemysław Spurek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:26:57.859Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-17T10:02:36.000Z",
            "title": "VeGaS: Video Gaussian Splatting",
            "summary": "Implicit Neural Representations (INRs) employ neural networks to approximate\ndiscrete data as continuous functions. In the context of video data, such\nmodels can be utilized to transform the coordinates of pixel locations along\nwith frame occurrence times (or indices) into RGB color values. Although INRs\nfacilitate effective compression, they are unsuitable for editing purposes. One\npotential solution is to use a 3D Gaussian Splatting (3DGS) based model, such\nas the Video Gaussian Representation (VGR), which is capable of encoding video\nas a multitude of 3D Gaussians and is applicable for numerous video processing\noperations, including editing. Nevertheless, in this case, the capacity for\nmodification is constrained to a limited set of basic transformations. To\naddress this issue, we introduce the Video Gaussian Splatting (VeGaS) model,\nwhich enables realistic modifications of video data. To construct VeGaS, we\npropose a novel family of Folded-Gaussian distributions designed to capture\nnonlinear dynamics in a video stream and model consecutive frames by 2D\nGaussians obtained as respective conditional distributions. Our experiments\ndemonstrate that VeGaS outperforms state-of-the-art solutions in frame\nreconstruction tasks and allows realistic modifications of video data. The code\nis available at: https://github.com/gmum/VeGaS.",
            "upvotes": 3,
            "discussionId": "673c8d3c44ea7d8aeaa3f046"
        },
        "publishedAt": "2024-11-19T11:41:47.911Z",
        "title": "VeGaS: Video Gaussian Splatting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11024.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d7faf536940809823d56a442f55e9fb9.svg",
            "fullname": "Kornel",
            "name": "kornelhowil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.09661",
            "authors": [
                {
                    "_id": "673890125ca44c7491b36202",
                    "user": {
                        "_id": "6325db12ddb9ff68f4698292",
                        "avatarUrl": "/avatars/88e9f57a17bc9b41ddad508e7b1d2640.svg",
                        "isPro": false,
                        "fullname": "Shehzaad Dhuliawala",
                        "user": "shehzaadzd",
                        "type": "user"
                    },
                    "name": "Shehzaad Dhuliawala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:29:29.350Z",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36203",
                    "user": {
                        "_id": "65d645913bc37244c55c39d5",
                        "avatarUrl": "/avatars/508372876a654957650ee9429886d10c.svg",
                        "isPro": false,
                        "fullname": "ilia kulikov",
                        "user": "uralik",
                        "type": "user"
                    },
                    "name": "Ilia Kulikov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:29:34.928Z",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36204",
                    "user": {
                        "_id": "646c004e31968a60a022590e",
                        "avatarUrl": "/avatars/349e2ed4feef1be050fd9903dccec1e1.svg",
                        "isPro": false,
                        "fullname": "Ping Yu",
                        "user": "pingyu",
                        "type": "user"
                    },
                    "name": "Ping Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:29:53.452Z",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36205",
                    "name": "Asli Celikyilmaz",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36206",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:30:07.574Z",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36207",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:30:15.503Z",
                    "hidden": false
                },
                {
                    "_id": "673890125ca44c7491b36208",
                    "name": "Jack Lanchantin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-14T18:31:39.000Z",
            "title": "Adaptive Decoding via Latent Preference Optimization",
            "summary": "During language model decoding, it is known that using higher temperature\nsampling gives more creative responses, while lower temperatures are more\nfactually accurate. However, such models are commonly applied to general\ninstruction following, which involves both creative and fact seeking tasks,\nusing a single fixed temperature across all examples and tokens. In this work,\nwe introduce Adaptive Decoding, a layer added to the model to select the\nsampling temperature dynamically at inference time, at either the token or\nexample level, in order to optimize performance. To learn its parameters we\nintroduce Latent Preference Optimization (LPO) a general approach to train\ndiscrete latent variables such as choices of temperature. Our method\noutperforms all fixed decoding temperatures across a range of tasks that\nrequire different temperatures, including UltraFeedback, Creative Story\nWriting, and GSM8K.",
            "upvotes": 3,
            "discussionId": "673890135ca44c7491b36255"
        },
        "publishedAt": "2024-11-19T09:35:55.561Z",
        "title": "Adaptive Decoding via Latent Preference Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09661.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/88e9f57a17bc9b41ddad508e7b1d2640.svg",
            "fullname": "Shehzaad Dhuliawala",
            "name": "shehzaadzd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.11045",
            "authors": [
                {
                    "_id": "673c4bb46864fdcc715c8fd1",
                    "user": {
                        "_id": "6322cae4212b17c7728e7387",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6322cae4212b17c7728e7387/p-hJt0795EO_wlUVY39rL.jpeg",
                        "isPro": false,
                        "fullname": "Chang Liu",
                        "user": "AlonzoLeeeooo",
                        "type": "user"
                    },
                    "name": "Chang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-19T13:09:10.693Z",
                    "hidden": false
                },
                {
                    "_id": "673c4bb46864fdcc715c8fd2",
                    "name": "Rui Li",
                    "hidden": false
                },
                {
                    "_id": "673c4bb46864fdcc715c8fd3",
                    "user": {
                        "_id": "631c42f8aa346997917b562c",
                        "avatarUrl": "/avatars/900209c7c747914baf89175bcdb9f7c3.svg",
                        "isPro": false,
                        "fullname": "Kaidong Zhang",
                        "user": "hitachinsk",
                        "type": "user"
                    },
                    "name": "Kaidong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:34:40.509Z",
                    "hidden": false
                },
                {
                    "_id": "673c4bb46864fdcc715c8fd4",
                    "name": "Yunwei Lan",
                    "hidden": false
                },
                {
                    "_id": "673c4bb46864fdcc715c8fd5",
                    "user": {
                        "_id": "66b5dce0602c26aa8838ad3b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b5dce0602c26aa8838ad3b/rcwOZ_yDyZxR54S5ftKoK.png",
                        "isPro": false,
                        "fullname": "Dong Liu",
                        "user": "DongLiu666",
                        "type": "user"
                    },
                    "name": "Dong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:34:48.363Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-17T11:48:01.000Z",
            "title": "StableV2V: Stablizing Shape Consistency in Video-to-Video Editing",
            "summary": "Recent advancements of generative AI have significantly promoted content\ncreation and editing, where prevailing studies further extend this exciting\nprogress to video editing. In doing so, these studies mainly transfer the\ninherent motion patterns from the source videos to the edited ones, where\nresults with inferior consistency to user prompts are often observed, due to\nthe lack of particular alignments between the delivered motions and edited\ncontents. To address this limitation, we present a shape-consistent video\nediting method, namely StableV2V, in this paper. Our method decomposes the\nentire editing pipeline into several sequential procedures, where it edits the\nfirst video frame, then establishes an alignment between the delivered motions\nand user prompts, and eventually propagates the edited contents to all other\nframes based on such alignment. Furthermore, we curate a testing benchmark,\nnamely DAVIS-Edit, for a comprehensive evaluation of video editing, considering\nvarious types of prompts and difficulties. Experimental results and analyses\nillustrate the outperforming performance, visual consistency, and inference\nefficiency of our method compared to existing state-of-the-art studies.",
            "upvotes": 3,
            "discussionId": "673c4bb96864fdcc715c9167"
        },
        "publishedAt": "2024-11-19T08:07:47.441Z",
        "title": "StableV2V: Stablizing Shape Consistency in Video-to-Video Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11045.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6322cae4212b17c7728e7387/p-hJt0795EO_wlUVY39rL.jpeg",
            "fullname": "Chang Liu",
            "name": "AlonzoLeeeooo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.10499",
            "authors": [
                {
                    "_id": "673c23932b49c38cbe85be38",
                    "user": {
                        "_id": "63b63a470d5913eee48bc2b3",
                        "avatarUrl": "/avatars/9be7857a6e06d6cf99bd2829ded9ef02.svg",
                        "isPro": false,
                        "fullname": "jiang",
                        "user": "BoyuanJiang",
                        "type": "user"
                    },
                    "name": "Boyuan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:38:21.750Z",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be39",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3a",
                    "name": "Donghao Luo",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3b",
                    "user": {
                        "_id": "64799de9587656b38e8dcf7a",
                        "avatarUrl": "/avatars/87069f03d44cc2d9e0458bc17bd0159a.svg",
                        "isPro": false,
                        "fullname": "heqingdong",
                        "user": "hqd",
                        "type": "user"
                    },
                    "name": "Qingdong He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:38:47.038Z",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3c",
                    "user": {
                        "_id": "652fab9d04a34a9282bf29d6",
                        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                        "isPro": false,
                        "fullname": "Chengming Xu",
                        "user": "ChengmingX",
                        "type": "user"
                    },
                    "name": "Chengming Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:38:52.876Z",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3d",
                    "name": "Jinlong Peng",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3e",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be3f",
                    "name": "Chengjie Wang",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be40",
                    "name": "Yunsheng Wu",
                    "hidden": false
                },
                {
                    "_id": "673c23932b49c38cbe85be41",
                    "name": "Yanwei Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T11:02:23.000Z",
            "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity\n  Virtual Try-on",
            "summary": "Although image-based virtual try-on has made considerable progress, emerging\napproaches still encounter challenges in producing high-fidelity and robust\nfitting images across diverse scenarios. These methods often struggle with\nissues such as texture-aware maintenance and size-aware fitting, which hinder\ntheir overall effectiveness. To address these limitations, we propose a novel\ngarment perception enhancement technique, termed FitDiT, designed for\nhigh-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more\nparameters and attention to high-resolution features. First, to further improve\ntexture-aware maintenance, we introduce a garment texture extractor that\nincorporates garment priors evolution to fine-tune garment feature,\nfacilitating to better capture rich details such as stripes, patterns, and\ntext. Additionally, we introduce frequency-domain learning by customizing a\nfrequency distance loss to enhance high-frequency garment details. To tackle\nthe size-aware fitting issue, we employ a dilated-relaxed mask strategy that\nadapts to the correct length of garments, preventing the generation of garments\nthat fill the entire mask area during cross-category try-on. Equipped with the\nabove design, FitDiT surpasses all baselines in both qualitative and\nquantitative evaluations. It excels in producing well-fitting garments with\nphotorealistic and intricate details, while also achieving competitive\ninference times of 4.57 seconds for a single 1024x768 image after DiT structure\nslimming, outperforming existing methods.",
            "upvotes": 3,
            "discussionId": "673c23962b49c38cbe85be53"
        },
        "publishedAt": "2024-11-19T04:05:53.074Z",
        "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10499.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5172
        }
    },
    {
        "paper": {
            "id": "2411.10168",
            "authors": [
                {
                    "_id": "673c87d3dc27d069a3c808f7",
                    "user": {
                        "_id": "65f1e0e65c4e61a391f087de",
                        "avatarUrl": "/avatars/02248cb36255cd18583ceead0839fd6e.svg",
                        "isPro": false,
                        "fullname": "Saskia Redgate",
                        "user": "saskiar",
                        "type": "user"
                    },
                    "name": "Saskia Redgate",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:40:47.383Z",
                    "hidden": false
                },
                {
                    "_id": "673c87d3dc27d069a3c808f8",
                    "user": {
                        "_id": "659bec4728676374f33ef921",
                        "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
                        "isPro": false,
                        "fullname": "Andrew Bean",
                        "user": "ambean",
                        "type": "user"
                    },
                    "name": "Andrew M. Bean",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-19T16:40:55.178Z",
                    "hidden": false
                },
                {
                    "_id": "673c87d3dc27d069a3c808f9",
                    "name": "Adam Mahdi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T13:16:11.000Z",
            "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
            "summary": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas.",
            "upvotes": 1,
            "discussionId": "673c87d4dc27d069a3c8093a"
        },
        "publishedAt": "2024-11-19T11:18:33.656Z",
        "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10168.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
            "fullname": "Andrew Bean",
            "name": "ambean",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]