[
  {
    "paper": {
      "id": "2504.19838",
      "authors": [
        {
          "_id": "6810317e007d579cbf5200ba",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bb",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bc",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bd",
          "name": "Yaxuan Guo",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200be",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200bf",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c0",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c1",
          "name": "Yue Han",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c2",
          "name": "Shuai Ren",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c3",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c4",
          "name": "Xiaoyu Liang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c5",
          "name": "Wenhao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c6",
          "name": "Tianze Wu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c7",
          "name": "Linghao Li",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c8",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200c9",
          "name": "Guanjing Xiong",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200ca",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "6810317e007d579cbf5200cb",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T14:39:25.000Z",
      "submittedOnDailyAt": "2025-04-29T00:30:25.482Z",
      "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
      "submittedOnDailyBy": {
        "_id": "64d761b98ebc40443831f82a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
        "isPro": false,
        "fullname": "lgy0404",
        "user": "lgy0404",
        "type": "user"
      },
      "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.",
      "upvotes": 14,
      "discussionId": "68103184007d579cbf5202d9",
      "projectPage": "https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents"
    },
    "publishedAt": "2025-04-28T10:39:25.000Z",
    "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
    "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19838.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64d761b98ebc40443831f82a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
      "fullname": "lgy0404",
      "name": "lgy0404",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19724",
      "authors": [
        {
          "_id": "68104dd9ec94d9d54ebde2c8",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-29T03:56:15.248Z",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2c9",
          "name": "Yujia Xu",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2ca",
          "name": "Yimeng Li",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cb",
          "name": "Junchen Li",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cc",
          "name": "Chaowei Zhang",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cd",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2ce",
          "name": "Kejia Yang",
          "hidden": false
        },
        {
          "_id": "68104dd9ec94d9d54ebde2cf",
          "name": "Zhibo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T12:19:53.000Z",
      "submittedOnDailyAt": "2025-04-29T02:27:56.443Z",
      "title": "RepText: Rendering Visual Text via Replicating",
      "submittedOnDailyBy": {
        "_id": "637745113a63a2983ffbde13",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
        "isPro": false,
        "fullname": "Haofan Wang",
        "user": "wanghaofan",
        "type": "user"
      },
      "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end.",
      "upvotes": 12,
      "discussionId": "68104ddfec94d9d54ebde3f3",
      "projectPage": "https://reptext.github.io/",
      "githubRepo": "https://github.com/Shakker-Labs/RepText"
    },
    "publishedAt": "2025-04-28T08:19:53.000Z",
    "title": "RepText: Rendering Visual Text via Replicating",
    "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19724.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637745113a63a2983ffbde13",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
      "fullname": "Haofan Wang",
      "name": "wanghaofan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19093",
      "authors": [
        {
          "_id": "6810356ab91a093e4f4cc262",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc263",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc264",
          "name": "Mengyuan Sun",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc265",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc266",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc267",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc268",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc269",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6810356ab91a093e4f4cc26a",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-27T03:41:17.000Z",
      "submittedOnDailyAt": "2025-04-29T04:39:29.218Z",
      "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities.",
      "upvotes": 7,
      "discussionId": "68103574b91a093e4f4cc57a",
      "projectPage": "https://cipherbankeva.github.io/",
      "githubRepo": "https://github.com/Goodman-liyu/CipherBank"
    },
    "publishedAt": "2025-04-26T23:41:17.000Z",
    "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through\n  Cryptography Challenges",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19093.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15780",
      "authors": [
        {
          "_id": "68104f85b442ffc234b670cd",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670ce",
          "name": "Zijun Chen",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670cf",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d0",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d1",
          "name": "Yuan Feng",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d2",
          "name": "Hongbin Zhou",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d3",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d4",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d5",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d6",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d7",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d8",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "68104f85b442ffc234b670d9",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T10:45:23.000Z",
      "submittedOnDailyAt": "2025-04-29T02:36:15.881Z",
      "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
      "upvotes": 4,
      "discussionId": "68104f86b442ffc234b67113"
    },
    "publishedAt": "2025-04-22T06:45:23.000Z",
    "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving",
    "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15780.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17258",
      "authors": [
        {
          "_id": "680edc612488a3b6b9feb9d0",
          "user": {
            "_id": "661e07e02a8496916011c08a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
            "isPro": false,
            "fullname": "Md Ashiqur Rahman",
            "user": "ashiq24",
            "type": "user"
          },
          "name": "Md Ashiqur Rahman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-28T03:56:25.436Z",
          "hidden": false
        },
        {
          "_id": "680edc612488a3b6b9feb9d1",
          "name": "Raymond A. Yeh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_lIAAwBEVKqioepM0Mmfl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/UUaV0uoeUWOTbfJopBnbV.png"
      ],
      "publishedAt": "2025-04-24T05:29:51.000Z",
      "submittedOnDailyAt": "2025-04-29T02:41:40.480Z",
      "title": "Group Downsampling with Equivariant Anti-aliasing",
      "submittedOnDailyBy": {
        "_id": "661e07e02a8496916011c08a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
        "isPro": false,
        "fullname": "Md Ashiqur Rahman",
        "user": "ashiq24",
        "type": "user"
      },
      "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks",
      "upvotes": 3,
      "discussionId": "680edc622488a3b6b9feba0e",
      "projectPage": "https://github.com/ashiq24/Group_Sampling",
      "githubRepo": "https://github.com/ashiq24/Group_Sampling",
      "ai_keywords": [
        "downsampling layers",
        "CNN architectures",
        "receptive field",
        "high-level features",
        "memory/computation",
        "group equivariant architectures",
        "G-CNNs",
        "finite groups",
        "downsampling rate",
        "subgroup",
        "bandlimited-ness",
        "anti-aliasing",
        "classical sampling theory",
        "cyclic group",
        "periodic",
        "ideal low-pass filter",
        "subsampling operation",
        "image classification tasks",
        "equivariance",
        "model size",
        "G-equivariant networks"
      ]
    },
    "publishedAt": "2025-04-24T01:29:51.000Z",
    "title": "Group Downsampling with Equivariant Anti-aliasing",
    "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_lIAAwBEVKqioepM0Mmfl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/UUaV0uoeUWOTbfJopBnbV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661e07e02a8496916011c08a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
      "fullname": "Md Ashiqur Rahman",
      "name": "ashiq24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16083",
      "authors": [
        {
          "_id": "68105d8632d635f02bc2976e",
          "name": "Yucheng Li",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc2976f",
          "user": {
            "_id": "6278bd42541f3d2dfa77ea70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
            "isPro": true,
            "fullname": "Huiqiang Jiang",
            "user": "iofu728",
            "type": "user"
          },
          "name": "Huiqiang Jiang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-29T05:03:03.400Z",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29770",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29771",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29772",
          "name": "Xufang Luo",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29773",
          "name": "Surin Ahn",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29774",
          "name": "Amir H. Abdi",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29775",
          "name": "Dongsheng Li",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29776",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29777",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "68105d8632d635f02bc29778",
          "name": "Lili Qiu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6278bd42541f3d2dfa77ea70/K2pFSyL6PhhzhfK3ar9Ok.jpeg"
      ],
      "publishedAt": "2025-04-22T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-29T03:34:32.012Z",
      "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
      "upvotes": 2,
      "discussionId": "68105d8732d635f02bc297bb"
    },
    "publishedAt": "2025-04-22T13:59:51.000Z",
    "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention",
    "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6278bd42541f3d2dfa77ea70/K2pFSyL6PhhzhfK3ar9Ok.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19395",
      "authors": [
        {
          "_id": "6810767a0f244cf14e5a3060",
          "name": "Zhouxiang Fang",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3061",
          "name": "Aayush Mishra",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3062",
          "name": "Muhan Gao",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3063",
          "name": "Anqi Liu",
          "hidden": false
        },
        {
          "_id": "6810767a0f244cf14e5a3064",
          "name": "Daniel Khashabi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6675c9305eaa9dd299dcdca0/DAhhWMnvctlmAuzzjp97v.png"
      ],
      "publishedAt": "2025-04-28T00:05:29.000Z",
      "submittedOnDailyAt": "2025-04-29T05:22:18.705Z",
      "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
      "submittedOnDailyBy": {
        "_id": "6675c9305eaa9dd299dcdca0",
        "avatarUrl": "/avatars/504a259033605e489809c8f202538d75.svg",
        "isPro": false,
        "fullname": "Zhouxiang Fang",
        "user": "FocusV857",
        "type": "user"
      },
      "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.",
      "upvotes": 1,
      "discussionId": "6810767b0f244cf14e5a30dc",
      "githubRepo": "https://github.com/jhu-CLSP/icl-ciphers"
    },
    "publishedAt": "2025-04-27T20:05:29.000Z",
    "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
    "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6675c9305eaa9dd299dcdca0/DAhhWMnvctlmAuzzjp97v.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6675c9305eaa9dd299dcdca0",
      "avatarUrl": "/avatars/504a259033605e489809c8f202538d75.svg",
      "fullname": "Zhouxiang Fang",
      "name": "FocusV857",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18919",
      "authors": [
        {
          "_id": "681039b2b02c157249d046b0",
          "name": "Andrew M. Bean",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b1",
          "name": "Rebecca Payne",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b2",
          "name": "Guy Parsons",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b3",
          "name": "Hannah Rose Kirk",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b4",
          "name": "Juan Ciro",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b5",
          "name": "Rafael Mosquera",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b6",
          "name": "Sara Hincapi√© Monsalve",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b7",
          "name": "Aruna S. Ekanayaka",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b8",
          "name": "Lionel Tarassenko",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046b9",
          "name": "Luc Rocher",
          "hidden": false
        },
        {
          "_id": "681039b2b02c157249d046ba",
          "name": "Adam Mahdi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T13:32:49.000Z",
      "submittedOnDailyAt": "2025-04-29T01:01:48.840Z",
      "title": "Clinical knowledge in LLMs does not translate to human interactions",
      "submittedOnDailyBy": {
        "_id": "659bec4728676374f33ef921",
        "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
        "isPro": false,
        "fullname": "Andrew Bean",
        "user": "ambean",
        "type": "user"
      },
      "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.",
      "upvotes": 1,
      "discussionId": "681039b5b02c157249d04787"
    },
    "publishedAt": "2025-04-26T09:32:49.000Z",
    "title": "Clinical knowledge in LLMs does not translate to human interactions",
    "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18919.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659bec4728676374f33ef921",
      "avatarUrl": "/avatars/217ae547d6460e65c6d2a23012741830.svg",
      "fullname": "Andrew Bean",
      "name": "ambean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]