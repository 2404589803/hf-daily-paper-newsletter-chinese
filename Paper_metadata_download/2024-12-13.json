[
    "{'paper': {'id': '2412.09596', 'authors': [{'_id': '675ba1de7ce703a00f19b4e4', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4e5', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4e6', 'name': 'Yuhang Cao', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4e7', 'user': {'_id': '63859cf3b2906edaf83af9f0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg', 'isPro': False, 'fullname': 'Yuhang Zang', 'user': 'yuhangzang', 'type': 'user'}, 'name': 'Yuhang Zang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:29.113Z', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4e8', 'name': 'Rui Qian', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4e9', 'user': {'_id': '62eb70462f0f5e54df42f778', 'avatarUrl': '/avatars/456049dba67638d3cdb330cdf383f272.svg', 'isPro': False, 'fullname': 'Xilin Wei', 'user': 'Wiselnn', 'type': 'user'}, 'name': 'Xilin Wei', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:27.497Z', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ea', 'name': 'Lin Chen', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4eb', 'name': 'Yifei Li', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ec', 'name': 'Junbo Niu', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ed', 'name': 'Shuangrui Ding', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ee', 'name': 'Qipeng Guo', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ef', 'name': 'Haodong Duan', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f0', 'name': 'Xin Chen', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f1', 'name': 'Han Lv', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f2', 'name': 'Zheng Nie', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f3', 'name': 'Min Zhang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f4', 'name': 'Bin Wang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f5', 'name': 'Wenwei Zhang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f6', 'name': 'Xinyue Zhang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f7', 'name': 'Jiaye Ge', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f8', 'name': 'Wei Li', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4f9', 'name': 'Jingwen Li', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4fa', 'name': 'Zhongying Tu', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4fb', 'name': 'Conghui He', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4fc', 'name': 'Xingcheng Zhang', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4fd', 'name': 'Kai Chen', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4fe', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b4ff', 'name': 'Dahua Lin', 'hidden': False}, {'_id': '675ba1de7ce703a00f19b500', 'name': 'Jiaqi Wang', 'hidden': False}], 'publishedAt': '2024-12-12T18:58:30.000Z', 'title': 'InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\\n  Long-term Streaming Video and Audio Interactions', 'summary': 'Creating AI systems that can interact with environments over long periods,\\nsimilar to human cognition, has been a longstanding research goal. Recent\\nadvancements in multimodal large language models (MLLMs) have made significant\\nstrides in open-world understanding. However, the challenge of continuous and\\nsimultaneous streaming perception, memory, and reasoning remains largely\\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\\narchitecture, which limits their ability to process inputs and generate\\nresponses simultaneously, akin to being unable to think while perceiving.\\nFurthermore, relying on long contexts to store historical data is impractical\\nfor long-term interactions, as retaining all information becomes costly and\\ninefficient. Therefore, rather than relying on a single foundation model to\\nperform all functions, this project draws inspiration from the concept of the\\nSpecialized Generalist AI and introduces disentangled streaming perception,\\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\\nProcesses multimodal information in real-time, storing key details in memory\\nand triggering reasoning in response to user queries. (2) Multi-modal Long\\nMemory Module: Integrates short-term and long-term memory, compressing\\nshort-term memories into long-term ones for efficient retrieval and improved\\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\\ntasks, coordinating with the perception and memory modules. This project\\nsimulates human-like cognition, enabling multimodal large language models to\\nprovide continuous and adaptive service over time.', 'upvotes': 56, 'discussionId': '675ba1df7ce703a00f19b54f'}, 'publishedAt': '2024-12-12T21:57:19.813Z', 'title': 'InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/cGOOF3CQ5lGqEOdaPh_Rk.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09596.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 10}}",
    "{'paper': {'id': '2412.08905', 'authors': [{'_id': '675b8e0dcda0ebd0345d6ffa', 'name': 'Marah Abdin', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d6ffb', 'name': 'Jyoti Aneja', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d6ffc', 'name': 'Harkirat Behl', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d6ffd', 'name': 'SÃ©bastien Bubeck', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d6ffe', 'name': 'Ronen Eldan', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d6fff', 'name': 'Suriya Gunasekar', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7000', 'name': 'Michael Harrison', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7001', 'name': 'Russell J. Hewett', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7002', 'name': 'Mojan Javaheripi', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7003', 'name': 'Piero Kauffmann', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7004', 'name': 'James R. Lee', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7005', 'name': 'Yin Tat Lee', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7006', 'name': 'Yuanzhi Li', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7007', 'name': 'Weishung Liu', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7008', 'name': 'Caio C. T. Mendes', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7009', 'name': 'Anh Nguyen', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700a', 'name': 'Eric Price', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700b', 'user': {'_id': '6157454831624da88210e627', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1666203761402-6157454831624da88210e627.jpeg', 'isPro': False, 'fullname': 'Gustavo de Rosa', 'user': 'gugarosa', 'type': 'user'}, 'name': 'Gustavo de Rosa', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T16:05:51.994Z', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700c', 'name': 'Olli Saarikivi', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700d', 'name': 'Adil Salim', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700e', 'name': 'Shital Shah', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d700f', 'name': 'Xin Wang', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7010', 'name': 'Rachel Ward', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7011', 'name': 'Yue Wu', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7012', 'name': 'Dingli Yu', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7013', 'name': 'Cyril Zhang', 'hidden': False}, {'_id': '675b8e0dcda0ebd0345d7014', 'name': 'Yi Zhang', 'hidden': False}], 'publishedAt': '2024-12-12T03:37:41.000Z', 'title': 'Phi-4 Technical Report', 'summary': 'We present phi-4, a 14-billion parameter language model developed with a\\ntraining recipe that is centrally focused on data quality. Unlike most language\\nmodels, where pre-training is based primarily on organic data sources such as\\nweb content or code, phi-4 strategically incorporates synthetic data throughout\\nthe training process. While previous models in the Phi family largely distill\\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\\nthat our data-generation and post-training techniques go beyond distillation.\\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\\nperformance relative to its size -- especially on reasoning-focused benchmarks\\n-- due to improved data, training curriculum, and innovations in the\\npost-training scheme.', 'upvotes': 34, 'discussionId': '675b8e0dcda0ebd0345d7042'}, 'publishedAt': '2024-12-12T23:41:26.758Z', 'title': 'Phi-4 Technical Report', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08905.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5363}}",
    "{'paper': {'id': '2412.08737', 'authors': [{'_id': '675bac3048fca25a2189e618', 'user': {'_id': '635b99d47a1656011516bff9', 'avatarUrl': '/avatars/7243c4171ff127ba90631f105881d9d7.svg', 'isPro': False, 'fullname': 'jiarui zhang', 'user': 'jrzhang', 'type': 'user'}, 'name': 'Jiarui Zhang', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-13T03:42:40.590Z', 'hidden': False}, {'_id': '675bac3048fca25a2189e619', 'user': {'_id': '66197a8afeb55cbe39e50ae8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png', 'isPro': False, 'fullname': 'Ollie Liu', 'user': 'oliu-io', 'type': 'user'}, 'name': 'Ollie Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:16.732Z', 'hidden': False}, {'_id': '675bac3048fca25a2189e61a', 'name': 'Tianyu Yu', 'hidden': False}, {'_id': '675bac3048fca25a2189e61b', 'name': 'Jinyi Hu', 'hidden': False}, {'_id': '675bac3048fca25a2189e61c', 'name': 'Willie Neiswanger', 'hidden': False}], 'publishedAt': '2024-12-11T19:12:13.000Z', 'title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity\\n  Visual Descriptions', 'summary': \"Multimodal large language models (MLLMs) have made rapid progress in recent\\nyears, yet continue to struggle with low-level visual perception (LLVP) --\\nparticularly the ability to accurately describe the geometric details of an\\nimage. This capability is crucial for applications in areas such as robotics,\\nmedical image analysis, and manufacturing. In this paper, we first introduce\\nGeoperception, a benchmark designed to evaluate an MLLM's ability to accurately\\ntranscribe 2D geometric information from an image. Using this benchmark, we\\ndemonstrate the limitations of leading MLLMs, and then conduct a comprehensive\\nempirical study to explore strategies for improving their performance on\\ngeometric tasks. Our findings highlight the benefits of certain model\\narchitectures, training techniques, and data strategies, including the use of\\nhigh-fidelity synthetic data and multi-stage training with a data curriculum.\\nNotably, we find that a data curriculum enables models to learn challenging\\ngeometry understanding tasks which they fail to learn from scratch. Leveraging\\nthese insights, we develop Euclid, a family of models specifically optimized\\nfor strong low-level geometric perception. Although purely trained on synthetic\\nmultimodal data, Euclid shows strong generalization ability to novel geometry\\nshapes. For instance, Euclid outperforms the best closed-source model,\\nGemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and\\n10.65% on average across all tasks.\", 'upvotes': 24, 'discussionId': '675bac3248fca25a2189e688'}, 'publishedAt': '2024-12-12T22:40:30.659Z', 'title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08737.png', 'numComments': 1, 'submittedBy': {'_id': '66197a8afeb55cbe39e50ae8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png', 'fullname': 'Ollie Liu', 'name': 'oliu-io', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.08635', 'authors': [{'_id': '675b17aa378af168bbfd66fa', 'name': 'Yutao Sun', 'hidden': False}, {'_id': '675b17aa378af168bbfd66fb', 'name': 'Hangbo Bao', 'hidden': False}, {'_id': '675b17aa378af168bbfd66fc', 'name': 'Wenhui Wang', 'hidden': False}, {'_id': '675b17aa378af168bbfd66fd', 'name': 'Zhiliang Peng', 'hidden': False}, {'_id': '675b17aa378af168bbfd66fe', 'user': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'isPro': False, 'fullname': 'Li Dong', 'user': 'unilm', 'type': 'user'}, 'name': 'Li Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:23:05.164Z', 'hidden': False}, {'_id': '675b17aa378af168bbfd66ff', 'name': 'Shaohan Huang', 'hidden': False}, {'_id': '675b17aa378af168bbfd6700', 'name': 'Jianyong Wang', 'hidden': False}, {'_id': '675b17aa378af168bbfd6701', 'name': 'Furu Wei', 'hidden': False}], 'publishedAt': '2024-12-11T18:57:32.000Z', 'title': 'Multimodal Latent Language Modeling with Next-Token Diffusion', 'summary': 'Multimodal generative models require a unified approach to handle both\\ndiscrete data (e.g., text and code) and continuous data (e.g., image, audio,\\nvideo). In this work, we propose Latent Language Modeling (LatentLM), which\\nseamlessly integrates continuous and discrete data using causal Transformers.\\nSpecifically, we employ a variational autoencoder (VAE) to represent continuous\\ndata as latent vectors and introduce next-token diffusion for autoregressive\\ngeneration of these vectors. Additionally, we develop sigma-VAE to address\\nthe challenges of variance collapse, which is crucial for autoregressive\\nmodeling. Extensive experiments demonstrate the effectiveness of LatentLM\\nacross various modalities. In image generation, LatentLM surpasses Diffusion\\nTransformers in both performance and scalability. When integrated into\\nmultimodal large language models, LatentLM provides a general-purpose interface\\nthat unifies multimodal generation and understanding. Experimental results show\\nthat LatentLM achieves favorable performance compared to Transfusion and vector\\nquantized models in the setting of scaling up training tokens. In\\ntext-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2\\nmodel in speaker similarity and robustness, while requiring 10x fewer decoding\\nsteps. The results establish LatentLM as a highly effective and scalable\\napproach to advance large multimodal models.', 'upvotes': 20, 'discussionId': '675b17ac378af168bbfd6757'}, 'publishedAt': '2024-12-12T22:04:42.280Z', 'title': 'Multimodal Latent Language Modeling with Next-Token Diffusion', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/f2AdfEoZZsYaBOVcXFGT9.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08635.png', 'numComments': 1, 'submittedBy': {'_id': '5df85abada6d0311fd3d5408', 'avatarUrl': '/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg', 'fullname': 'Li Dong', 'name': 'unilm', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 19}}",
    "{'paper': {'id': '2412.09618', 'authors': [{'_id': '675bad005b45ed6ecd315666', 'name': 'Zhuofan Zong', 'hidden': False}, {'_id': '675bad005b45ed6ecd315667', 'user': {'_id': '6349214f8146350b3a4c5cdf', 'avatarUrl': '/avatars/cfd24caac9a87efb528d0f4c375932bc.svg', 'isPro': False, 'fullname': 'Dongzhi Jiang', 'user': 'CaraJ', 'type': 'user'}, 'name': 'Dongzhi Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:14.337Z', 'hidden': False}, {'_id': '675bad005b45ed6ecd315668', 'name': 'Bingqi Ma', 'hidden': False}, {'_id': '675bad005b45ed6ecd315669', 'name': 'Guanglu Song', 'hidden': False}, {'_id': '675bad005b45ed6ecd31566a', 'name': 'Hao Shao', 'hidden': False}, {'_id': '675bad005b45ed6ecd31566b', 'name': 'Dazhong Shen', 'hidden': False}, {'_id': '675bad005b45ed6ecd31566c', 'name': 'Yu Liu', 'hidden': False}, {'_id': '675bad005b45ed6ecd31566d', 'name': 'Hongsheng Li', 'hidden': False}], 'publishedAt': '2024-12-12T18:59:48.000Z', 'title': 'EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\\n  Multimodal LLM', 'summary': \"Significant achievements in personalization of diffusion models have been\\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\\nimages by averaging their image embeddings as the injection condition, but such\\nan image-independent operation cannot perform interaction among images to\\ncapture consistent visual elements within multiple references. Although the\\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\\nelements within multiple images through the training process, it necessitates\\nspecific finetuning for each distinct image group. This paper introduces\\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\\nto be conditioned on multiple reference images and the text prompt. To\\neffectively exploit consistent visual elements within multiple images, we\\nleverage the multi-image comprehension and instruction-following capabilities\\nof the multimodal large language model (MLLM), prompting it to capture\\nconsistent visual elements based on the instruction. Besides, injecting the\\nMLLM's representations into the diffusion process through adapters can easily\\ngeneralize to unseen domains, mining the consistent visual elements within\\nunseen data. To mitigate computational costs and enhance fine-grained detail\\npreservation, we introduce an efficient reference aggregation strategy and a\\nprogressive training scheme. Finally, we introduce MRBench, a new\\nmulti-reference image generation benchmark. Experimental results demonstrate\\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\\ngeneralization across diverse domains.\", 'upvotes': 15, 'discussionId': '675bad065b45ed6ecd3158f2'}, 'publishedAt': '2024-12-12T22:44:37.190Z', 'title': 'EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09618.png', 'numComments': 2, 'submittedBy': {'_id': '6349214f8146350b3a4c5cdf', 'avatarUrl': '/avatars/cfd24caac9a87efb528d0f4c375932bc.svg', 'fullname': 'Dongzhi Jiang', 'name': 'CaraJ', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}}",
    "{'paper': {'id': '2412.09605', 'authors': [{'_id': '675bcc75b497da95e5a4ffd1', 'user': {'_id': '601d29ab913ad3afd7b7ddb8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1620447944896-601d29ab913ad3afd7b7ddb8.jpeg', 'isPro': True, 'fullname': 'Yiheng Xu', 'user': 'ranpox', 'type': 'user'}, 'name': 'Yiheng Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:15:41.823Z', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd2', 'name': 'Dunjie Lu', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd3', 'name': 'Zhennan Shen', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd4', 'user': {'_id': '65f944d5056d465a38f49361', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PWp43VsXeltiPifGxGwrn.jpeg', 'isPro': False, 'fullname': 'Junli Wang', 'user': 'ZeonLap', 'type': 'user'}, 'name': 'Junli Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T08:56:03.169Z', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd5', 'user': {'_id': '656832dfbd65fd41ee7aa8cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg', 'isPro': False, 'fullname': 'Zekun Wang', 'user': 'kugwzk', 'type': 'user'}, 'name': 'Zekun Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:15:40.064Z', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd6', 'name': 'Yuchen Mao', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd7', 'user': {'_id': '649dbcc4e0fff1ed099dc80a', 'avatarUrl': '/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg', 'isPro': False, 'fullname': 'Caiming Xiong', 'user': 'cxiong', 'type': 'user'}, 'name': 'Caiming Xiong', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-13T05:56:07.585Z', 'hidden': False}, {'_id': '675bcc75b497da95e5a4ffd8', 'name': 'Tao Yu', 'hidden': False}], 'publishedAt': '2024-12-12T18:59:27.000Z', 'title': 'AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web\\n  Tutorials', 'summary': 'Graphical User Interface (GUI) agents hold great potential for automating\\ncomplex tasks across diverse digital environments, from web applications to\\ndesktop software. However, the development of such agents is hindered by the\\nlack of high-quality, multi-step trajectory data required for effective\\ntraining. Existing approaches rely on expensive and labor-intensive human\\nannotation, making them unsustainable at scale. To address this challenge, we\\npropose AgentTrek, a scalable data synthesis pipeline that generates\\nhigh-quality GUI agent trajectories by leveraging web tutorials. Our method\\nautomatically gathers tutorial-like texts from the internet, transforms them\\ninto task goals with step-by-step instructions, and employs a visual-language\\nmodel agent to simulate their execution in a real digital environment. A\\nVLM-based evaluator ensures the correctness of the generated trajectories. We\\ndemonstrate that training GUI agents with these synthesized trajectories\\nsignificantly improves their grounding and planning performance over the\\ncurrent models. Moreover, our approach is more cost-efficient compared to\\ntraditional human annotation methods. This work underscores the potential of\\nguided replay with web tutorials as a viable strategy for large-scale GUI agent\\ntraining, paving the way for more capable and autonomous digital agents.', 'upvotes': 13, 'discussionId': '675bcc77b497da95e5a50055'}, 'publishedAt': '2024-12-13T01:12:42.739Z', 'title': 'AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09605.png', 'numComments': 1, 'submittedBy': {'_id': '601d29ab913ad3afd7b7ddb8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1620447944896-601d29ab913ad3afd7b7ddb8.jpeg', 'fullname': 'Yiheng Xu', 'name': 'ranpox', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 23}}",
    "{'paper': {'id': '2412.09593', 'authors': [{'_id': '675b9a4634a23ade5100fde2', 'user': {'_id': '63a5d18a8fb23d08bb30094c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671811420431-noauth.png', 'isPro': False, 'fullname': 'Zexin He', 'user': 'zxhezexin', 'type': 'user'}, 'name': 'Zexin He', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:32.990Z', 'hidden': False}, {'_id': '675b9a4634a23ade5100fde3', 'name': 'Tengfei Wang', 'hidden': False}, {'_id': '675b9a4634a23ade5100fde4', 'name': 'Xin Huang', 'hidden': False}, {'_id': '675b9a4634a23ade5100fde5', 'name': 'Xingang Pan', 'hidden': False}, {'_id': '675b9a4634a23ade5100fde6', 'name': 'Ziwei Liu', 'hidden': False}], 'publishedAt': '2024-12-12T18:58:09.000Z', 'title': 'Neural LightRig: Unlocking Accurate Object Normal and Material\\n  Estimation with Multi-Light Diffusion', 'summary': 'Recovering the geometry and materials of objects from a single image is\\nchallenging due to its under-constrained nature. In this paper, we present\\nNeural LightRig, a novel framework that boosts intrinsic estimation by\\nleveraging auxiliary multi-lighting conditions from 2D diffusion priors.\\nSpecifically, 1) we first leverage illumination priors from large-scale\\ndiffusion models to build our multi-light diffusion model on a synthetic\\nrelighting dataset with dedicated designs. This diffusion model generates\\nmultiple consistent images, each illuminated by point light sources in\\ndifferent directions. 2) By using these varied lighting images to reduce\\nestimation uncertainty, we train a large G-buffer model with a U-Net backbone\\nto accurately predict surface normals and materials. Extensive experiments\\nvalidate that our approach significantly outperforms state-of-the-art methods,\\nenabling accurate surface normal and PBR material estimation with vivid\\nrelighting effects. Code and dataset are available on our project page at\\nhttps://projects.zxhezexin.com/neural-lightrig.', 'upvotes': 12, 'discussionId': '675b9a4a34a23ade5100ff2b'}, 'publishedAt': '2024-12-12T21:41:19.282Z', 'title': 'Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63a5d18a8fb23d08bb30094c/KPQ_maFkSLewuU-8eK64c.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/63a5d18a8fb23d08bb30094c/i4JjkxNKww2BpW7rDTjxz.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09593.png', 'numComments': 3, 'submittedBy': {'_id': '63a5d18a8fb23d08bb30094c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671811420431-noauth.png', 'fullname': 'Zexin He', 'name': 'zxhezexin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 30}}",
    "{'paper': {'id': '2412.09619', 'authors': [{'_id': '675bc9374c7255e45f9085f7', 'user': {'_id': '655d812e668b64adf13d6382', 'avatarUrl': '/avatars/b9497be8dc479af0bd7dff72d3d6de2c.svg', 'isPro': False, 'fullname': 'Dongting HU', 'user': 'timmy11hu', 'type': 'user'}, 'name': 'Dongting Hu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:21:57.830Z', 'hidden': False}, {'_id': '675bc9374c7255e45f9085f8', 'user': {'_id': '65e0d0f417458030d05b5622', 'avatarUrl': '/avatars/bb35db227e946bc8dbfcba871f32730a.svg', 'isPro': False, 'fullname': 'Jierun Chen', 'user': 'JierunChen', 'type': 'user'}, 'name': 'Jierun Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:00.511Z', 'hidden': False}, {'_id': '675bc9374c7255e45f9085f9', 'user': {'_id': '6411a4f39457280bcdd29cf7', 'avatarUrl': '/avatars/b0c85362be1edbfc40035e7e841fcbeb.svg', 'isPro': False, 'fullname': 'Xijie Huang', 'user': 'ScarletAce', 'type': 'user'}, 'name': 'Xijie Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:21:54.717Z', 'hidden': False}, {'_id': '675bc9374c7255e45f9085fa', 'name': 'Huseyin Coskun', 'hidden': False}, {'_id': '675bc9374c7255e45f9085fb', 'name': 'Arpit Sahni', 'hidden': False}, {'_id': '675bc9374c7255e45f9085fc', 'name': 'Aarush Gupta', 'hidden': False}, {'_id': '675bc9374c7255e45f9085fd', 'name': 'Anujraaj Goyal', 'hidden': False}, {'_id': '675bc9374c7255e45f9085fe', 'name': 'Dishani Lahiri', 'hidden': False}, {'_id': '675bc9374c7255e45f9085ff', 'name': 'Rajesh Singh', 'hidden': False}, {'_id': '675bc9374c7255e45f908600', 'name': 'Yerlan Idelbayev', 'hidden': False}, {'_id': '675bc9374c7255e45f908601', 'name': 'Junli Cao', 'hidden': False}, {'_id': '675bc9374c7255e45f908602', 'name': 'Yanyu Li', 'hidden': False}, {'_id': '675bc9374c7255e45f908603', 'name': 'Kwang-Ting Cheng', 'hidden': False}, {'_id': '675bc9374c7255e45f908604', 'name': 'S. -H. Gary Chan', 'hidden': False}, {'_id': '675bc9374c7255e45f908605', 'name': 'Mingming Gong', 'hidden': False}, {'_id': '675bc9374c7255e45f908606', 'name': 'Sergey Tulyakov', 'hidden': False}, {'_id': '675bc9374c7255e45f908607', 'name': 'Anil Kag', 'hidden': False}, {'_id': '675bc9374c7255e45f908608', 'name': 'Yanwu Xu', 'hidden': False}, {'_id': '675bc9374c7255e45f908609', 'name': 'Jian Ren', 'hidden': False}], 'publishedAt': '2024-12-12T18:59:53.000Z', 'title': 'SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices\\n  with Efficient Architectures and Training', 'summary': 'Existing text-to-image (T2I) diffusion models face several limitations,\\nincluding large model sizes, slow runtime, and low-quality generation on mobile\\ndevices. This paper aims to address all of these challenges by developing an\\nextremely small and fast T2I model that generates high-resolution and\\nhigh-quality images on mobile platforms. We propose several techniques to\\nachieve this goal. First, we systematically examine the design choices of the\\nnetwork architecture to reduce model parameters and latency, while ensuring\\nhigh-quality generation. Second, to further improve generation quality, we\\nemploy cross-architecture knowledge distillation from a much larger model,\\nusing a multi-level approach to guide the training of our model from scratch.\\nThird, we enable a few-step generation by integrating adversarial guidance with\\nknowledge distillation. For the first time, our model SnapGen, demonstrates the\\ngeneration of 1024x1024 px images on a mobile device around 1.4 seconds. On\\nImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for\\n256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our\\nmodel with merely 379M parameters, surpasses large-scale models with billions\\nof parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x\\nsmaller than IF-XL).', 'upvotes': 10, 'discussionId': '675bc93b4c7255e45f90876a'}, 'publishedAt': '2024-12-13T01:08:36.213Z', 'title': 'SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09619.png', 'numComments': 1, 'submittedBy': {'_id': '61f19829233c91cbd2f79e70', 'avatarUrl': '/avatars/a0735a94542b4f7cda5aed8bc4be0538.svg', 'fullname': 'Jian Ren', 'name': 'alanspike', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.05994', 'authors': [{'_id': '675a80635f94292180d2f7b9', 'user': {'_id': '675a7d0ecb8625d2f4770e66', 'avatarUrl': '/avatars/a5a5c2cb90d9eca3b04b8dc949135c0f.svg', 'isPro': False, 'fullname': 'Namgyu Kang', 'user': 'kangnamgyu27', 'type': 'user'}, 'name': 'Namgyu Kang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-12T08:19:37.357Z', 'hidden': False}, {'_id': '675a80635f94292180d2f7ba', 'name': 'Jaemin Oh', 'hidden': False}, {'_id': '675a80635f94292180d2f7bb', 'name': 'Youngjoon Hong', 'hidden': False}, {'_id': '675a80635f94292180d2f7bc', 'name': 'Eunbyung Park', 'hidden': False}], 'publishedAt': '2024-12-08T16:58:29.000Z', 'title': 'PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh\\n  Representations', 'summary': 'The approximation of Partial Differential Equations (PDEs) using neural\\nnetworks has seen significant advancements through Physics-Informed Neural\\nNetworks (PINNs). Despite their straightforward optimization framework and\\nflexibility in implementing various PDEs, PINNs often suffer from limited\\naccuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which\\nstruggle to effectively learn high-frequency and non-linear components.\\nRecently, parametric mesh representations in combination with neural networks\\nhave been investigated as a promising approach to eliminate the inductive\\nbiases of neural networks. However, they usually require very high-resolution\\ngrids and a large number of collocation points to achieve high accuracy while\\navoiding overfitting issues. In addition, the fixed positions of the mesh\\nparameters restrict their flexibility, making it challenging to accurately\\napproximate complex PDEs. To overcome these limitations, we propose\\nPhysics-Informed Gaussians (PIGs), which combine feature embeddings using\\nGaussian functions with a lightweight neural network. Our approach uses\\ntrainable parameters for the mean and variance of each Gaussian, allowing for\\ndynamic adjustment of their positions and shapes during training. This\\nadaptability enables our model to optimally approximate PDE solutions, unlike\\nmodels with fixed parameter positions. Furthermore, the proposed approach\\nmaintains the same optimization framework used in PINNs, allowing us to benefit\\nfrom their excellent properties. Experimental results show the competitive\\nperformance of our model across various PDEs, demonstrating its potential as a\\nrobust tool for solving complex PDEs. Our project page is available at\\nhttps://namgyukang.github.io/Physics-Informed-Gaussians/', 'upvotes': 10, 'discussionId': '675a80655f94292180d2f823'}, 'publishedAt': '2024-12-13T00:50:01.557Z', 'title': 'PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05994.png', 'numComments': 1, 'submittedBy': {'_id': '675a7d0ecb8625d2f4770e66', 'avatarUrl': '/avatars/a5a5c2cb90d9eca3b04b8dc949135c0f.svg', 'fullname': 'Namgyu Kang', 'name': 'kangnamgyu27', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.09501', 'authors': [{'_id': '675bcad776d0e555d631381c', 'name': 'Zhisheng Zhong', 'hidden': False}, {'_id': '675bcad776d0e555d631381d', 'user': {'_id': '6423e35b30b0e4ab36dd1b16', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg', 'isPro': False, 'fullname': 'Wang Chengyao', 'user': 'wcy1122', 'type': 'user'}, 'name': 'Chengyao Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:21:46.575Z', 'hidden': False}, {'_id': '675bcad776d0e555d631381e', 'name': 'Yuqi Liu', 'hidden': False}, {'_id': '675bcad776d0e555d631381f', 'name': 'Senqiao Yang', 'hidden': False}, {'_id': '675bcad776d0e555d6313820', 'name': 'Longxiang Tang', 'hidden': False}, {'_id': '675bcad776d0e555d6313821', 'name': 'Yuechen Zhang', 'hidden': False}, {'_id': '675bcad776d0e555d6313822', 'name': 'Jingyao Li', 'hidden': False}, {'_id': '675bcad776d0e555d6313823', 'name': 'Tianyuan Qu', 'hidden': False}, {'_id': '675bcad776d0e555d6313824', 'name': 'Yanwei Li', 'hidden': False}, {'_id': '675bcad776d0e555d6313825', 'name': 'Yukang Chen', 'hidden': False}, {'_id': '675bcad776d0e555d6313826', 'name': 'Shaozuo Yu', 'hidden': False}, {'_id': '675bcad776d0e555d6313827', 'name': 'Sitong Wu', 'hidden': False}, {'_id': '675bcad776d0e555d6313828', 'name': 'Eric Lo', 'hidden': False}, {'_id': '675bcad776d0e555d6313829', 'name': 'Shu Liu', 'hidden': False}, {'_id': '675bcad776d0e555d631382a', 'name': 'Jiaya Jia', 'hidden': False}], 'publishedAt': '2024-12-12T17:50:39.000Z', 'title': 'Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition', 'summary': 'As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond\\nsingle-domain capabilities is essential to meet the demands for more versatile\\nand efficient AI. However, previous omni-models have insufficiently explored\\nspeech, neglecting its integration with multi-modality. We introduce Lyra, an\\nefficient MLLM that enhances multimodal abilities, including advanced\\nlong-speech comprehension, sound understanding, cross-modality efficiency, and\\nseamless speech interaction. To achieve efficiency and speech-centric\\ncapabilities, Lyra employs three strategies: (1) leveraging existing\\nopen-source large models and a proposed multi-modality LoRA to reduce training\\ncosts and data requirements; (2) using a latent multi-modality regularizer and\\nextractor to strengthen the relationship between speech and other modalities,\\nthereby enhancing model performance; and (3) constructing a high-quality,\\nextensive dataset that includes 1.5M multi-modal (language, vision, audio) data\\nsamples and 12K long speech samples, enabling Lyra to handle complex long\\nspeech inputs and achieve more robust omni-cognition. Compared to other\\nomni-methods, Lyra achieves state-of-the-art performance on various\\nvision-language, vision-speech, and speech-language benchmarks, while also\\nusing fewer computational resources and less training data.', 'upvotes': 8, 'discussionId': '675bcad876d0e555d6313861'}, 'publishedAt': '2024-12-13T00:58:45.187Z', 'title': 'Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6423e35b30b0e4ab36dd1b16/QQC4kzQ5cQ9oPsd8m3R0P.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09501.png', 'numComments': 1, 'submittedBy': {'_id': '6423e35b30b0e4ab36dd1b16', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg', 'fullname': 'Wang Chengyao', 'name': 'wcy1122', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 9}}",
    "{'paper': {'id': '2412.08972', 'authors': [{'_id': '675bca99935c1403e2c788a2', 'name': 'Ruiwen Zhou', 'hidden': False}, {'_id': '675bca99935c1403e2c788a3', 'name': 'Wenyue Hua', 'hidden': False}, {'_id': '675bca99935c1403e2c788a4', 'name': 'Liangming Pan', 'hidden': False}, {'_id': '675bca99935c1403e2c788a5', 'name': 'Sitao Cheng', 'hidden': False}, {'_id': '675bca99935c1403e2c788a6', 'name': 'Xiaobao Wu', 'hidden': False}, {'_id': '675bca99935c1403e2c788a7', 'name': 'En Yu', 'hidden': False}, {'_id': '675bca99935c1403e2c788a8', 'name': 'William Yang Wang', 'hidden': False}], 'publishedAt': '2024-12-12T06:08:46.000Z', 'title': 'RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\\n  Scenarios', 'summary': \"This paper introduces RuleArena, a novel and challenging benchmark designed\\nto evaluate the ability of large language models (LLMs) to follow complex,\\nreal-world rules in reasoning. Covering three practical domains -- airline\\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\\nproficiency in handling intricate natural language instructions that demand\\nlong-context understanding, logical reasoning, and accurate mathematical\\ncomputation. Two key attributes distinguish RuleArena from traditional\\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\\nlogic representations, and (2) it is grounded in authentic, practical\\nscenarios, providing insights into the suitability and reliability of LLMs for\\nreal-world applications. Our findings reveal several notable limitations in\\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\\nbecoming confused by similar but distinct regulations, (2) they cannot\\nconsistently perform accurate mathematical computations, even when they\\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\\nin the benchmark. These results highlight significant challenges in advancing\\nLLMs' rule-guided reasoning capabilities in real-life applications.\", 'upvotes': 7, 'discussionId': '675bca9a935c1403e2c788cd'}, 'publishedAt': '2024-12-13T00:48:32.498Z', 'title': 'RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08972.png', 'numComments': 1, 'submittedBy': {'_id': '639a25aba2b0b1c9d85a51e8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/639a25aba2b0b1c9d85a51e8/pphz-MK62hPNbBkMHAkeR.jpeg', 'fullname': 'Wenyue Hua', 'name': 'wenyueH', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.09405', 'authors': [{'_id': '675badf8b46c607ac7ab809d', 'user': {'_id': '63213080d2d45f3151837eba', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63213080d2d45f3151837eba/aBhKfY-0gZhKGQmb_Gwi2.png', 'isPro': True, 'fullname': 'Dan Jacobellis', 'user': 'danjacobellis', 'type': 'user'}, 'name': 'Dan Jacobellis', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:11.962Z', 'hidden': False}, {'_id': '675badf8b46c607ac7ab809e', 'name': 'Neeraja J. Yadwadkar', 'hidden': False}], 'publishedAt': '2024-12-12T16:09:57.000Z', 'title': 'Learned Compression for Compressed Learning', 'summary': \"Modern sensors produce increasingly rich streams of high-resolution data. Due\\nto resource constraints, machine learning systems discard the vast majority of\\nthis information via resolution reduction. Compressed-domain learning allows\\nmodels to operate on compact latent representations, allowing higher effective\\nresolution for the same budget. However, existing compression systems are not\\nideal for compressed learning. Linear transform coding and end-to-end learned\\ncompression systems reduce bitrate, but do not uniformly reduce dimensionality;\\nthus, they do not meaningfully increase efficiency. Generative autoencoders\\nreduce dimensionality, but their adversarial or perceptual objectives lead to\\nsignificant information loss. To address these limitations, we introduce WaLLoC\\n(Wavelet Learned Lossy Compression), a neural codec architecture that combines\\nlinear transform coding with nonlinear dimensionality-reducing autoencoders.\\nWaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck\\nbetween an invertible wavelet packet transform. Across several key metrics,\\nWaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion\\nmodels. WaLLoC does not require perceptual or adversarial losses to represent\\nhigh-frequency detail, providing compatibility with modalities beyond RGB\\nimages and stereo audio. WaLLoC's encoder consists almost entirely of linear\\noperations, making it exceptionally efficient and suitable for mobile\\ncomputing, remote sensing, and learning directly from compressed data. We\\ndemonstrate WaLLoC's capability for compressed-domain learning across several\\ntasks, including image classification, colorization, document understanding,\\nand music source separation. Our code, experiments, and pre-trained audio and\\nimage codecs are available at https://ut-sysml.org/walloc\", 'upvotes': 7, 'discussionId': '675badfab46c607ac7ab812e'}, 'publishedAt': '2024-12-12T23:09:53.405Z', 'title': 'Learned Compression for Compressed Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09405.png', 'numComments': 1, 'submittedBy': {'_id': '63213080d2d45f3151837eba', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63213080d2d45f3151837eba/aBhKfY-0gZhKGQmb_Gwi2.png', 'fullname': 'Dan Jacobellis', 'name': 'danjacobellis', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.09569', 'authors': [{'_id': '675bdf1ba301c0203a58d39d', 'user': {'_id': '6415ebd2107962562e9a0712', 'avatarUrl': '/avatars/dfd2e6d42c6110bb64b6a73c96eeb3f3.svg', 'isPro': False, 'fullname': 'Ariel Gera', 'user': 'arielgera', 'type': 'user'}, 'name': 'Ariel Gera', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:15:38.275Z', 'hidden': False}, {'_id': '675bdf1ba301c0203a58d39e', 'name': 'Odellia Boni', 'hidden': False}, {'_id': '675bdf1ba301c0203a58d39f', 'name': 'Yotam Perlitz', 'hidden': False}, {'_id': '675bdf1ba301c0203a58d3a0', 'name': 'Roy Bar-Haim', 'hidden': False}, {'_id': '675bdf1ba301c0203a58d3a1', 'name': 'Lilach Eden', 'hidden': False}, {'_id': '675bdf1ba301c0203a58d3a2', 'name': 'Asaf Yehudai', 'hidden': False}], 'publishedAt': '2024-12-12T18:51:13.000Z', 'title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'summary': \"Given the rapid progress of generative AI, there is a pressing need to\\nsystematically compare and choose between the numerous models and\\nconfigurations available. The scale and versatility of such evaluations make\\nthe use of LLM-based judges a compelling solution for this challenge.\\nCrucially, this approach requires first to validate the quality of the LLM\\njudge itself. Previous work has focused on instance-based assessment of LLM\\njudges, where a judge is evaluated over a set of responses, or response pairs,\\nwhile being agnostic to their source systems. We argue that this setting\\noverlooks critical factors affecting system-level ranking, such as a judge's\\npositive or negative bias towards certain systems. To address this gap, we\\nconduct the first large-scale study of LLM judges as system rankers. System\\nscores are generated by aggregating judgment scores over multiple system\\noutputs, and the judge's quality is assessed by comparing the resulting system\\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\\nprovides a fine-grained characterization of judge behavior, including their\\ndecisiveness and bias.\", 'upvotes': 6, 'discussionId': '675bdf1ca301c0203a58d3f1'}, 'publishedAt': '2024-12-13T04:00:51.092Z', 'title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09569.png', 'numComments': 2, 'submittedBy': {'_id': '6415ebd2107962562e9a0712', 'avatarUrl': '/avatars/dfd2e6d42c6110bb64b6a73c96eeb3f3.svg', 'fullname': 'Ariel Gera', 'name': 'arielgera', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.09349', 'authors': [{'_id': '675ba7567a3e15dbc1834a8a', 'name': 'Hongxiang Li', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a8b', 'name': 'Yaowei Li', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a8c', 'name': 'Yuhang Yang', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a8d', 'name': 'Junjie Cao', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a8e', 'name': 'Zhihong Zhu', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a8f', 'name': 'Xuxin Cheng', 'hidden': False}, {'_id': '675ba7567a3e15dbc1834a90', 'name': 'Long Chen', 'hidden': False}], 'publishedAt': '2024-12-12T15:15:59.000Z', 'title': 'DisPose: Disentangling Pose Guidance for Controllable Human Image\\n  Animation', 'summary': 'Controllable human image animation aims to generate videos from reference\\nimages using driving videos. Due to the limited control signals provided by\\nsparse guidance (e.g., skeleton pose), recent works have attempted to introduce\\nadditional dense conditions (e.g., depth map) to ensure motion alignment.\\nHowever, such strict dense guidance impairs the quality of the generated video\\nwhen the body shape of the reference character differs significantly from that\\nof the driving video. In this paper, we present DisPose to mine more\\ngeneralizable and effective control signals without additional dense input,\\nwhich disentangles the sparse skeleton pose in human image animation into\\nmotion field guidance and keypoint correspondence. Specifically, we generate a\\ndense motion field from a sparse motion field and the reference image, which\\nprovides region-level dense guidance while maintaining the generalization of\\nthe sparse pose control. We also extract diffusion features corresponding to\\npose keypoints from the reference image, and then these point features are\\ntransferred to the target pose to provide distinct identity information. To\\nseamlessly integrate into existing models, we propose a plug-and-play hybrid\\nControlNet that improves the quality and consistency of generated videos while\\nfreezing the existing model parameters. Extensive qualitative and quantitative\\nexperiments demonstrate the superiority of DisPose compared to current methods.\\nCode:\\nhttps://github.com/lihxxx/DisPose{https://github.com/lihxxx/DisPose}.', 'upvotes': 5, 'discussionId': '675ba75a7a3e15dbc1834c2a'}, 'publishedAt': '2024-12-13T03:50:30.137Z', 'title': 'DisPose: Disentangling Pose Guidance for Controllable Human Image Animation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09349.png', 'numComments': 1, 'submittedBy': {'_id': '6362801380c1a705a6ea54ac', 'avatarUrl': '/avatars/041ad5abf9be42e336938f51ebb8746c.svg', 'fullname': 'Yaowei Li', 'name': 'Yw22', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.09586', 'authors': [{'_id': '675ba24cc7ceaec7e65066d0', 'user': {'_id': '656a4b89e8bf55919a313920', 'avatarUrl': '/avatars/c903ec6e1c7952c5a200edd9f8cfcee7.svg', 'isPro': False, 'fullname': 'Fiona Ryan', 'user': 'fkryan', 'type': 'user'}, 'name': 'Fiona Ryan', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:25.678Z', 'hidden': False}, {'_id': '675ba24cc7ceaec7e65066d1', 'name': 'Ajay Bati', 'hidden': False}, {'_id': '675ba24cc7ceaec7e65066d2', 'name': 'Sangmin Lee', 'hidden': False}, {'_id': '675ba24cc7ceaec7e65066d3', 'name': 'Daniel Bolya', 'hidden': False}, {'_id': '675ba24cc7ceaec7e65066d4', 'name': 'Judy Hoffman', 'hidden': False}, {'_id': '675ba24cc7ceaec7e65066d5', 'name': 'James M. Rehg', 'hidden': False}], 'publishedAt': '2024-12-12T18:55:30.000Z', 'title': 'Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders', 'summary': \"We address the problem of gaze target estimation, which aims to predict where\\na person is looking in a scene. Predicting a person's gaze target requires\\nreasoning both about the person's appearance and the contents of the scene.\\nPrior works have developed increasingly complex, hand-crafted pipelines for\\ngaze target estimation that carefully fuse features from separate scene\\nencoders, head encoders, and auxiliary models for signals like depth and pose.\\nMotivated by the success of general-purpose feature extractors on a variety of\\nvisual tasks, we propose Gaze-LLE, a novel transformer framework that\\nstreamlines gaze target estimation by leveraging features from a frozen DINOv2\\nencoder. We extract a single feature representation for the scene, and apply a\\nperson-specific positional prompt to decode gaze with a lightweight module. We\\ndemonstrate state-of-the-art performance across several gaze benchmarks and\\nprovide extensive analysis to validate our design choices. Our code is\\navailable at: http://github.com/fkryan/gazelle .\", 'upvotes': 5, 'discussionId': '675ba24ec7ceaec7e65068b3'}, 'publishedAt': '2024-12-12T22:08:34.270Z', 'title': 'Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09586.png', 'numComments': 1, 'submittedBy': {'_id': '623dfe96dcda6a715304cbca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg', 'fullname': 'Jitesh Jain', 'name': 'praeclarumjj3', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.09622', 'authors': [{'_id': '675bd7f6a6ba99158dd02ad4', 'user': {'_id': '63412f2add8853dc7e306a4f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnPbjO1jAvycUkeooUxHD.png', 'isPro': False, 'fullname': 'Enis Simsar', 'user': 'enisimsar', 'type': 'user'}, 'name': 'Enis Simsar', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T08:55:57.355Z', 'hidden': False}, {'_id': '675bd7f6a6ba99158dd02ad5', 'name': 'Thomas Hofmann', 'hidden': False}, {'_id': '675bd7f6a6ba99158dd02ad6', 'name': 'Federico Tombari', 'hidden': False}, {'_id': '675bd7f6a6ba99158dd02ad7', 'name': 'Pinar Yanardag', 'hidden': False}], 'publishedAt': '2024-12-12T18:59:55.000Z', 'title': 'LoRACLR: Contrastive Adaptation for Customization of Diffusion Models', 'summary': 'Recent advances in text-to-image customization have enabled high-fidelity,\\ncontext-rich generation of personalized images, allowing specific concepts to\\nappear in a variety of scenarios. However, current methods struggle with\\ncombining multiple personalized models, often leading to attribute entanglement\\nor requiring separate training to preserve concept distinctiveness. We present\\nLoRACLR, a novel approach for multi-concept image generation that merges\\nmultiple LoRA models, each fine-tuned for a distinct concept, into a single,\\nunified model without additional individual fine-tuning. LoRACLR uses a\\ncontrastive objective to align and merge the weight spaces of these models,\\nensuring compatibility while minimizing interference. By enforcing distinct yet\\ncohesive representations for each concept, LoRACLR enables efficient, scalable\\nmodel composition for high-quality, multi-concept image synthesis. Our results\\nhighlight the effectiveness of LoRACLR in accurately merging multiple concepts,\\nadvancing the capabilities of personalized image generation.', 'upvotes': 4, 'discussionId': '675bd7fba6ba99158dd02b86'}, 'publishedAt': '2024-12-13T02:21:32.949Z', 'title': 'LoRACLR: Contrastive Adaptation for Customization of Diffusion Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09622.png', 'numComments': 1, 'submittedBy': {'_id': '63412f2add8853dc7e306a4f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnPbjO1jAvycUkeooUxHD.png', 'fullname': 'Enis Simsar', 'name': 'enisimsar', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.09585', 'authors': [{'_id': '675ba1257950eafb111baff3', 'user': {'_id': '623dfe96dcda6a715304cbca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg', 'isPro': False, 'fullname': 'Jitesh Jain', 'user': 'praeclarumjj3', 'type': 'user'}, 'name': 'Jitesh Jain', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:31.309Z', 'hidden': False}, {'_id': '675ba1257950eafb111baff4', 'name': 'Zhengyuan Yang', 'hidden': False}, {'_id': '675ba1257950eafb111baff5', 'name': 'Humphrey Shi', 'hidden': False}, {'_id': '675ba1257950eafb111baff6', 'name': 'Jianfeng Gao', 'hidden': False}, {'_id': '675ba1257950eafb111baff7', 'name': 'Jianwei Yang', 'hidden': False}], 'publishedAt': '2024-12-12T18:55:18.000Z', 'title': 'OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\\n  Embedding Distillation', 'summary': \"The standard practice for developing contemporary MLLMs is to feed features\\nfrom vision encoder(s) into the LLM and train with natural language\\nsupervision. In this work, we posit an overlooked opportunity to optimize the\\nintermediate LLM representations through a vision perspective (objective),\\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\\ndistilling knowledge into the LLM's hidden representations from a set of target\\nvisual representations. Firstly, we formulate the objective during the\\npretraining stage in MLLMs as a coupled optimization of predictive visual\\nembedding and next text-token prediction. Secondly, we investigate MLLMs\\ntrained solely with natural language supervision and identify a positive\\ncorrelation between the quality of visual representations within these models\\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\\nobserve improved representation quality owing to the embedding optimization.\\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\\nmulti-encoder baselines, proving our approach's superiority over explicitly\\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\\nperformance by an average margin of up to 2.5% on various benchmarks, with a\\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM .\", 'upvotes': 4, 'discussionId': '675ba1277950eafb111bb072'}, 'publishedAt': '2024-12-12T21:53:04.132Z', 'title': 'OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09585.png', 'numComments': 1, 'submittedBy': {'_id': '623dfe96dcda6a715304cbca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg', 'fullname': 'Jitesh Jain', 'name': 'praeclarumjj3', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.09460', 'authors': [{'_id': '675be4b8f005d9ae8b86a802', 'user': {'_id': '5ef3829e518622264685b0cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1593016943046-noauth.jpeg', 'isPro': False, 'fullname': 'Javier de la Rosa', 'user': 'versae', 'type': 'user'}, 'name': 'Javier de la Rosa', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:15:36.269Z', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a803', 'name': 'Vladislav Mikhailov', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a804', 'name': 'Lemei Zhang', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a805', 'name': 'Freddy Wetjen', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a806', 'name': 'David Samuel', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a807', 'name': 'Peng Liu', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a808', 'name': 'Rolv-Arild Braaten', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a809', 'name': 'Petter MÃ¦hlum', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80a', 'name': 'Magnus Breder Birkenes', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80b', 'name': 'Andrey Kutuzov', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80c', 'name': 'Tita Enstad', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80d', 'name': 'Svein Arne Brygfjeld', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80e', 'name': 'Jon Atle Gulla', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a80f', 'name': 'Stephan Oepen', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a810', 'name': 'Erik Velldal', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a811', 'name': 'Wilfred Ãstgulen', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a812', 'name': 'Liljia Ãvrelid', 'hidden': False}, {'_id': '675be4b8f005d9ae8b86a813', 'name': 'Aslak Sira Myhre', 'hidden': False}], 'publishedAt': '2024-12-12T17:11:22.000Z', 'title': 'The Impact of Copyrighted Material on Large Language Models: A Norwegian\\n  Perspective', 'summary': 'The use of copyrighted materials in training generative language models\\nraises critical legal and ethical questions. This paper presents a framework\\nfor and the results of empirically assessing the impact of copyrighted\\nmaterials on the performance of large language models (LLMs) for Norwegian. We\\nfound that both books and newspapers contribute positively when the models are\\nevaluated on a diverse set of Norwegian benchmarks, while fiction works\\npossibly lead to decreased performance. Our experiments could inform the\\ncreation of a compensation scheme for authors whose works contribute to AI\\ndevelopment.', 'upvotes': 3, 'discussionId': '675be4bbf005d9ae8b86a925'}, 'publishedAt': '2024-12-13T05:18:48.617Z', 'title': 'The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09460.png', 'numComments': 1, 'submittedBy': {'_id': '5ef3829e518622264685b0cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1593016943046-noauth.jpeg', 'fullname': 'Javier de la Rosa', 'name': 'versae', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 27}}",
    "{'paper': {'id': '2412.09013', 'authors': [{'_id': '675bc39259bc9663aeea8ab0', 'name': 'Zongsheng Yue', 'hidden': False}, {'_id': '675bc39259bc9663aeea8ab1', 'name': 'Kang Liao', 'hidden': False}, {'_id': '675bc39259bc9663aeea8ab2', 'name': 'Chen Change Loy', 'hidden': False}], 'publishedAt': '2024-12-12T07:24:13.000Z', 'title': 'Arbitrary-steps Image Super-resolution via Diffusion Inversion', 'summary': 'This study presents a new image super-resolution (SR) technique based on\\ndiffusion inversion, aiming at harnessing the rich image priors encapsulated in\\nlarge pre-trained diffusion models to improve SR performance. We design a\\nPartial noise Prediction strategy to construct an intermediate state of the\\ndiffusion model, which serves as the starting sampling point. Central to our\\napproach is a deep noise predictor to estimate the optimal noise maps for the\\nforward diffusion process. Once trained, this noise predictor can be used to\\ninitialize the sampling process partially along the diffusion trajectory,\\ngenerating the desirable high-resolution result. Compared to existing\\napproaches, our method offers a flexible and efficient sampling mechanism that\\nsupports an arbitrary number of sampling steps, ranging from one to five. Even\\nwith a single sampling step, our method demonstrates superior or comparable\\nperformance to recent state-of-the-art approaches. The code and model are\\npublicly available at https://github.com/zsyOAOA/InvSR.', 'upvotes': 3, 'discussionId': '675bc39559bc9663aeea8b90'}, 'publishedAt': '2024-12-13T02:49:51.778Z', 'title': 'Arbitrary-steps Image Super-resolution via Diffusion Inversion', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09013.png', 'numComments': 1, 'submittedBy': {'_id': '630ad0dd2ff113e0fb31c6b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671174653229-630ad0dd2ff113e0fb31c6b0.jpeg', 'fullname': 'Zongsheng Yue', 'name': 'OAOA', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.09370', 'authors': [{'_id': '675bc870491d8c264d4c3520', 'user': {'_id': '633ee9704be90e06da1eb774', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/633ee9704be90e06da1eb774/xU6gXNHkBub0hAQb9RRsR.jpeg', 'isPro': False, 'fullname': 'Andrei Stefan Bejgu', 'user': 'andreim14', 'type': 'user'}, 'name': 'Andrei Stefan Bejgu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-13T15:22:04.077Z', 'hidden': False}, {'_id': '675bc870491d8c264d4c3521', 'name': 'Edoardo Barba', 'hidden': False}, {'_id': '675bc870491d8c264d4c3522', 'name': 'Luigi Procopio', 'hidden': False}, {'_id': '675bc870491d8c264d4c3523', 'name': 'Alberte FernÃ¡ndez-Castro', 'hidden': False}, {'_id': '675bc870491d8c264d4c3524', 'name': 'Roberto Navigli', 'hidden': False}], 'publishedAt': '2024-12-12T15:38:34.000Z', 'title': 'Word Sense Linking: Disambiguating Outside the Sandbox', 'summary': 'Word Sense Disambiguation (WSD) is the task of associating a word in a given\\ncontext with its most suitable meaning among a set of possible candidates.\\nWhile the task has recently witnessed renewed interest, with systems achieving\\nperformances above the estimated inter-annotator agreement, at the time of\\nwriting it still struggles to find downstream applications. We argue that one\\nof the reasons behind this is the difficulty of applying WSD to plain text.\\nIndeed, in the standard formulation, models work under the assumptions that a)\\nall the spans to disambiguate have already been identified, and b) all the\\npossible candidate senses of each span are provided, both of which are\\nrequirements that are far from trivial. In this work, we present a new task\\ncalled Word Sense Linking (WSL) where, given an input text and a reference\\nsense inventory, systems have to both identify which spans to disambiguate and\\nthen link them to their most suitable meaning.We put forward a\\ntransformer-based architecture for the task and thoroughly evaluate both its\\nperformance and those of state-of-the-art WSD systems scaled to WSL,\\niteratively relaxing the assumptions of WSD. We hope that our work will foster\\neasier integration of lexical semantics into downstream applications.', 'upvotes': 3, 'discussionId': '675bc872491d8c264d4c35d5'}, 'publishedAt': '2024-12-13T00:46:06.836Z', 'title': 'Word Sense Linking: Disambiguating Outside the Sandbox', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/633ee9704be90e06da1eb774/bmjzMKyo8pW10JCGJaZRK.jpeg'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09370.png', 'numComments': 1, 'submittedBy': {'_id': '633ee9704be90e06da1eb774', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/633ee9704be90e06da1eb774/xU6gXNHkBub0hAQb9RRsR.jpeg', 'fullname': 'Andrei Stefan Bejgu', 'name': 'andreim14', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}}",
    "{'paper': {'id': '2412.09573', 'authors': [{'_id': '675b9f587a3e15dbc1811e95', 'name': 'Jiale Xu', 'hidden': False}, {'_id': '675b9f587a3e15dbc1811e96', 'name': 'Shenghua Gao', 'hidden': False}, {'_id': '675b9f587a3e15dbc1811e97', 'name': 'Ying Shan', 'hidden': False}], 'publishedAt': '2024-12-12T18:52:53.000Z', 'title': 'FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D\\n  Reconstruction', 'summary': \"Existing sparse-view reconstruction models heavily rely on accurate known\\ncamera poses. However, deriving camera extrinsics and intrinsics from\\nsparse-view images presents significant challenges. In this work, we present\\nFreeSplatter, a highly scalable, feed-forward reconstruction framework capable\\nof generating high-quality 3D Gaussians from uncalibrated sparse-view images\\nand recovering their camera parameters in mere seconds. FreeSplatter is built\\nupon a streamlined transformer architecture, comprising sequential\\nself-attention blocks that facilitate information exchange among multi-view\\nimage tokens and decode them into pixel-wise 3D Gaussian primitives. The\\npredicted Gaussian primitives are situated in a unified reference frame,\\nallowing for high-fidelity 3D modeling and instant camera parameter estimation\\nusing off-the-shelf solvers. To cater to both object-centric and scene-level\\nreconstruction, we train two model variants of FreeSplatter on extensive\\ndatasets. In both scenarios, FreeSplatter outperforms state-of-the-art\\nbaselines in terms of reconstruction quality and pose estimation accuracy.\\nFurthermore, we showcase FreeSplatter's potential in enhancing the productivity\\nof downstream applications, such as text/image-to-3D content creation.\", 'upvotes': 3, 'discussionId': '675b9f5a7a3e15dbc1811f4a'}, 'publishedAt': '2024-12-12T21:49:49.976Z', 'title': 'FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09573.png', 'numComments': 2, 'submittedBy': {'_id': '62c695829db11473f08af1cd', 'avatarUrl': '/avatars/cacb54077892a44aef81454dc107df4f.svg', 'fullname': 'Jiale Xu', 'name': 'bluestyle97', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.09025', 'authors': [{'_id': '675baa5ccd0640cdf19e6784', 'user': {'_id': '62fa1718c95d426ff8eeec2a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1718c95d426ff8eeec2a/pWb33DlH6NPtl9Avss0gA.png', 'isPro': False, 'fullname': 'Advait Joglekar', 'user': 'rumourscape', 'type': 'user'}, 'name': 'Advait Joglekar', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-13T05:30:34.734Z', 'hidden': False}, {'_id': '675baa5ccd0640cdf19e6785', 'name': 'Srinivasan Umesh', 'hidden': False}], 'publishedAt': '2024-12-12T07:40:55.000Z', 'title': 'Shiksha: A Technical Domain focused Translation Dataset and Model for\\n  Indian Languages', 'summary': 'Neural Machine Translation (NMT) models are typically trained on datasets\\nwith limited exposure to Scientific, Technical and Educational domains.\\nTranslation models thus, in general, struggle with tasks that involve\\nscientific understanding or technical jargon. Their performance is found to be\\neven worse for low-resource Indian languages. Finding a translation dataset\\nthat tends to these domains in particular, poses a difficult challenge. In this\\npaper, we address this by creating a multilingual parallel corpus containing\\nmore than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality\\ntranslation pairs across 8 Indian languages. We achieve this by bitext mining\\nhuman-translated transcriptions of NPTEL video lectures. We also finetune and\\nevaluate NMT models using this corpus and surpass all other publicly available\\nmodels at in-domain tasks. We also demonstrate the potential for generalizing\\nto out-of-domain translation tasks by improving the baseline by over 2 BLEU on\\naverage for these Indian languages on the Flores+ benchmark. We are pleased to\\nrelease our model and dataset via this link: https://huggingface.co/SPRINGLab.', 'upvotes': 2, 'discussionId': '675baa5fcd0640cdf19e6830'}, 'publishedAt': '2024-12-12T22:31:53.526Z', 'title': 'Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09025.png', 'numComments': 1, 'submittedBy': {'_id': '62fa1718c95d426ff8eeec2a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1718c95d426ff8eeec2a/pWb33DlH6NPtl9Avss0gA.png', 'fullname': 'Advait Joglekar', 'name': 'rumourscape', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.05552', 'authors': [{'_id': '675b9d8aa86e549855482d11', 'name': 'Gengze Zhou', 'hidden': False}, {'_id': '675b9d8aa86e549855482d12', 'name': 'Yicong Hong', 'hidden': False}, {'_id': '675b9d8aa86e549855482d13', 'name': 'Zun Wang', 'hidden': False}, {'_id': '675b9d8aa86e549855482d14', 'name': 'Chongyang Zhao', 'hidden': False}, {'_id': '675b9d8aa86e549855482d15', 'name': 'Mohit Bansal', 'hidden': False}, {'_id': '675b9d8aa86e549855482d16', 'name': 'Qi Wu', 'hidden': False}], 'publishedAt': '2024-12-07T06:12:53.000Z', 'title': 'SAME: Learning Generic Language-Guided Visual Navigation with\\n  State-Adaptive Mixture of Experts', 'summary': 'The academic field of learning instruction-guided visual navigation can be\\ngenerally categorized into high-level category-specific search and low-level\\nlanguage-guided navigation, depending on the granularity of language\\ninstruction, in which the former emphasizes the exploration process, while the\\nlatter concentrates on following detailed textual commands. Despite the\\ndiffering focuses of these tasks, the underlying requirements of interpreting\\ninstructions, comprehending the surroundings, and inferring action decisions\\nremain consistent. This paper consolidates diverse navigation tasks into a\\nunified and generic framework -- we investigate the core difficulties of\\nsharing general knowledge and exploiting task-specific capabilities in learning\\nnavigation and propose a novel State-Adaptive Mixture of Experts (SAME) model\\nthat effectively enables an agent to infer decisions based on\\ndifferent-granularity language and dynamic observations. Powered by SAME, we\\npresent a versatile agent capable of addressing seven navigation tasks\\nsimultaneously that outperforms or achieves highly comparable performance to\\ntask-specific agents.', 'upvotes': 2, 'discussionId': '675b9d8ba86e549855482d80'}, 'publishedAt': '2024-12-12T21:41:15.650Z', 'title': 'SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05552.png', 'numComments': 1, 'submittedBy': {'_id': '6450bcd3673b2bcfaf8681af', 'avatarUrl': '/avatars/f5f93d780562d0772ec5dc1728945fcf.svg', 'fullname': 'Gengze Zhou', 'name': 'ZGZzz', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}"
]