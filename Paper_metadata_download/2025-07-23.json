[
  {
    "paper": {
      "id": "2507.16784",
      "authors": [
        {
          "_id": "68805f8800f4b5a05f2fbe76",
          "name": "Hongyin Luo",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe77",
          "name": "Nathaniel Morgan",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe78",
          "name": "Tina Li",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe79",
          "name": "Derek Zhao",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7a",
          "name": "Ai Vy Ngo",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7b",
          "name": "Philip Schroeder",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7c",
          "name": "Lijie Yang",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7d",
          "name": "Assaf Ben-Kish",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7e",
          "name": "Jack O'Brien",
          "hidden": false
        },
        {
          "_id": "68805f8800f4b5a05f2fbe7f",
          "name": "James Glass",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62bc495016897fa76cdcda18/ovSQjkKwaBq1NNegEB9VX.jpeg"
      ],
      "publishedAt": "2025-07-22T17:30:04.000Z",
      "submittedOnDailyAt": "2025-07-23T02:54:20.560Z",
      "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
      "submittedOnDailyBy": {
        "_id": "62bc495016897fa76cdcda18",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656506661693-noauth.jpeg",
        "isPro": false,
        "fullname": "Hongyin Luo",
        "user": "luohy",
        "type": "user"
      },
      "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
      "upvotes": 32,
      "discussionId": "68805f8800f4b5a05f2fbe80",
      "ai_summary": "A Thread Inference Model (TIM) and its runtime (TIMRUN) enable long-horizon reasoning in LLMs by using reasoning trees and key-value state retention, overcoming context and memory limitations.",
      "ai_keywords": [
        "Thread Inference Model",
        "TIMRUN",
        "recursive problem solving",
        "decompositional problem solving",
        "long-horizon structured reasoning",
        "working memory",
        "multi-hop tool calls",
        "reasoning trees",
        "key-value states",
        "subtask-pruning mechanism",
        "positional embeddings",
        "GPU-memory bottlenecks",
        "inference throughput",
        "mathematical tasks",
        "information retrieval"
      ]
    },
    "publishedAt": "2025-07-22T13:30:04.000Z",
    "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
    "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62bc495016897fa76cdcda18/ovSQjkKwaBq1NNegEB9VX.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16784.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc495016897fa76cdcda18",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656506661693-noauth.jpeg",
      "fullname": "Hongyin Luo",
      "name": "luohy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16632",
      "authors": [
        {
          "_id": "6880532600f4b5a05f2fbde6",
          "name": "Boyong Wu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbde7",
          "name": "Chao Yan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbde8",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbde9",
          "name": "Cheng Yi",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdea",
          "name": "Chengli Feng",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdeb",
          "name": "Fei Tian",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdec",
          "name": "Feiyu Shen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbded",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdee",
          "name": "Haoyang Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdef",
          "name": "Jingbei Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf0",
          "name": "Mingrui Chen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf1",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf2",
          "name": "Wang You",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf3",
          "name": "Xiangyu Tony Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf4",
          "name": "Xingyuan Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf5",
          "name": "Xuerui Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf6",
          "name": "Yayue Deng",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf7",
          "name": "Yechang Huang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf8",
          "name": "Yuxin Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdf9",
          "name": "Yuxin Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdfa",
          "name": "Zhao You",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdfb",
          "name": "Brian Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdfc",
          "name": "Changyi Wan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdfd",
          "name": "Hanpeng Hu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdfe",
          "name": "Jiangjie Zhen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbdff",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe00",
          "name": "Song Yuan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe01",
          "name": "Xuelin Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe02",
          "name": "Yimin Jiang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe03",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe04",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe05",
          "name": "Bingxin Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe06",
          "name": "Buyun Ma",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe07",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe08",
          "name": "Dongqing Pang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe09",
          "name": "Guoqiang Hu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0a",
          "name": "Haiyang Sun",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0b",
          "name": "Kang An",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0c",
          "name": "Na Wang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0d",
          "name": "Shuli Gao",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0e",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe0f",
          "name": "Wen Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe10",
          "name": "Wen Sun",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe11",
          "name": "Xuan Wen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe12",
          "name": "Yong Ren",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe13",
          "name": "Yuankai Ma",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe14",
          "name": "Yufan Lu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe15",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe16",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe17",
          "name": "Changxin Miao",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe18",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe19",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1a",
          "name": "Dapeng Shi",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1b",
          "name": "Dingyuan Hu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1c",
          "name": "Donghang Wu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1d",
          "name": "Enle Liu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1e",
          "name": "Guanzhe Huang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe1f",
          "name": "Gulin Yan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe20",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe21",
          "name": "Hao Nie",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe22",
          "name": "Haonan Jia",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe23",
          "name": "Hongyu Zhou",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe24",
          "name": "Jianjian Sun",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe25",
          "name": "Jiaoren Wu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe26",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe27",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe28",
          "name": "Jin Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe29",
          "name": "Junzhe Lin",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2a",
          "name": "Kaixiang Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2b",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2c",
          "name": "Liying Shi",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2d",
          "name": "Li Zhou",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2e",
          "name": "Longlong Gu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe2f",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe30",
          "name": "Mingliang Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe31",
          "name": "Mingxiao Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe32",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe33",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe34",
          "name": "Qinyuan Tan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe35",
          "name": "Shaoliang Pang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe36",
          "name": "Shengjie Fan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe37",
          "name": "Siqi Liu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe38",
          "name": "Tiancheng Cao",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe39",
          "name": "Wanying Lu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3a",
          "name": "Wenqing He",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3b",
          "name": "Wuxun Xie",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3c",
          "name": "Xu Zhao",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3d",
          "name": "Xueqi Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3e",
          "name": "Yanbo Yu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe3f",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe40",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe41",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe42",
          "name": "Yilei Wang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe43",
          "name": "Yuanhao Ding",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe44",
          "name": "Yuanwei Liang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe45",
          "name": "Yuanwei Lu",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe46",
          "name": "Yuchu Luo",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe47",
          "name": "Yuhe Yin",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe48",
          "name": "Yumeng Zhan",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe49",
          "name": "Yuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4a",
          "name": "Zidong Yang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4b",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4c",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4d",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4e",
          "name": "Heung-Yeung Shum",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe4f",
          "name": "Jiansheng Chen",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe50",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe51",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6880532600f4b5a05f2fbe52",
          "name": "Yibo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T14:23:55.000Z",
      "submittedOnDailyAt": "2025-07-23T01:43:38.121Z",
      "title": "Step-Audio 2 Technical Report",
      "submittedOnDailyBy": {
        "_id": "63417332c5565a4b8d43a0d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
        "isPro": false,
        "fullname": "Gang Yu",
        "user": "skicy",
        "type": "user"
      },
      "summary": "This paper presents Step-Audio~2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information.",
      "upvotes": 21,
      "discussionId": "6880532700f4b5a05f2fbe53",
      "githubRepo": "https://github.com/stepfun-ai/Step-Audio2",
      "githubStars": 68
    },
    "publishedAt": "2025-07-22T10:23:55.000Z",
    "title": "Step-Audio 2 Technical Report",
    "summary": "This paper presents Step-Audio~2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63417332c5565a4b8d43a0d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
      "fullname": "Gang Yu",
      "name": "skicy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16812",
      "authors": [
        {
          "_id": "688037ee00f4b5a05f2fbda9",
          "name": "Run-Ze Fan",
          "hidden": false
        },
        {
          "_id": "688037ee00f4b5a05f2fbdaa",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "688037ee00f4b5a05f2fbdab",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/7o5TZQfu85o-FOJofCLul.png",
        "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/L24hJQbCMi6F8Uei_GBlW.png"
      ],
      "publishedAt": "2025-07-22T17:59:03.000Z",
      "submittedOnDailyAt": "2025-07-23T00:20:14.145Z",
      "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "616bfc2b40e2f69baa1c7add",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
        "isPro": false,
        "fullname": "Run-Ze Fan",
        "user": "Vfrz",
        "type": "user"
      },
      "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
      "upvotes": 18,
      "discussionId": "6880381f00f4b5a05f2fbdac",
      "projectPage": "https://huggingface.co/MegaScience",
      "githubRepo": "https://github.com/GAIR-NLP/MegaScience",
      "ai_summary": "MegaScience, a large-scale dataset of scientific reasoning questions, enhances the performance and training efficiency of AI models compared to existing datasets.",
      "ai_keywords": [
        "TextbookReasoning",
        "MegaScience",
        "reasoning questions",
        "scientific disciplines",
        "data selection methodologies",
        "evaluation system",
        "Llama3.1",
        "Qwen2.5",
        "Qwen3",
        "scientific tuning"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-22T13:59:03.000Z",
    "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
    "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/7o5TZQfu85o-FOJofCLul.png",
      "https://cdn-uploads.huggingface.co/production/uploads/616bfc2b40e2f69baa1c7add/L24hJQbCMi6F8Uei_GBlW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616bfc2b40e2f69baa1c7add",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
      "fullname": "Run-Ze Fan",
      "name": "Vfrz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08422",
      "authors": [
        {
          "_id": "6880820200f4b5a05f2fbeb1",
          "name": "Wongi Jeong",
          "hidden": false
        },
        {
          "_id": "6880820200f4b5a05f2fbeb2",
          "name": "Kyungryeol Lee",
          "hidden": false
        },
        {
          "_id": "6880820200f4b5a05f2fbeb3",
          "name": "Hoigi Seo",
          "hidden": false
        },
        {
          "_id": "6880820200f4b5a05f2fbeb4",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T09:07:43.000Z",
      "submittedOnDailyAt": "2025-07-23T05:06:19.468Z",
      "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0times speed-up on FLUX and 3.0times\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
      "upvotes": 16,
      "discussionId": "6880820200f4b5a05f2fbeb5"
    },
    "publishedAt": "2025-07-11T05:07:43.000Z",
    "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
    "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0times speed-up on FLUX and 3.0times\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16814",
      "authors": [
        {
          "_id": "68805e7e00f4b5a05f2fbe6a",
          "name": "Junhao Shen",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe6b",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe6c",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe6d",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe6e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe6f",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe70",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe71",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe72",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "68805e7e00f4b5a05f2fbe73",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T17:59:34.000Z",
      "submittedOnDailyAt": "2025-07-23T02:32:32.526Z",
      "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.",
      "upvotes": 13,
      "discussionId": "68805e7e00f4b5a05f2fbe74"
    },
    "publishedAt": "2025-07-22T13:59:34.000Z",
    "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning",
    "summary": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16815",
      "authors": [
        {
          "_id": "688044e800f4b5a05f2fbdd8",
          "name": "Chi-Pin Huang",
          "hidden": false
        },
        {
          "_id": "688044e800f4b5a05f2fbdd9",
          "name": "Yueh-Hua Wu",
          "hidden": false
        },
        {
          "_id": "688044e800f4b5a05f2fbdda",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "688044e800f4b5a05f2fbddb",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "688044e800f4b5a05f2fbddc",
          "name": "Fu-En Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T17:59:46.000Z",
      "submittedOnDailyAt": "2025-07-23T00:44:28.338Z",
      "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
      "submittedOnDailyBy": {
        "_id": "64705d224be5cf1f3348d6bc",
        "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg",
        "isPro": false,
        "fullname": "Chi-Pin Huang",
        "user": "jasper0314-huang",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
      "upvotes": 11,
      "discussionId": "688044e800f4b5a05f2fbddd",
      "ai_summary": "ThinkAct, a dual-system framework, uses reinforced visual latent planning to enable high-level reasoning and robust action execution in vision-language-action tasks.",
      "ai_keywords": [
        "multimodal LLM",
        "embodied reasoning plans",
        "reinforced action-aligned visual rewards",
        "visual plan latent",
        "downstream action model",
        "few-shot adaptation",
        "long-horizon planning",
        "self-correction behaviors"
      ]
    },
    "publishedAt": "2025-07-22T13:59:46.000Z",
    "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
    "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16815.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64705d224be5cf1f3348d6bc",
      "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg",
      "fullname": "Chi-Pin Huang",
      "name": "jasper0314-huang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16813",
      "authors": [
        {
          "_id": "68803aa000f4b5a05f2fbdb4",
          "name": "Dong Liang",
          "hidden": false
        },
        {
          "_id": "68803aa000f4b5a05f2fbdb5",
          "name": "Jinyuan Jia",
          "hidden": false
        },
        {
          "_id": "68803aa000f4b5a05f2fbdb6",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68803aa000f4b5a05f2fbdb7",
          "name": "Rynson W. H. Lau",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6351463b8445bbe32e944f6c/wr4t2EY1Jx56UA6EUPfYl.png"
      ],
      "publishedAt": "2025-07-22T17:59:21.000Z",
      "submittedOnDailyAt": "2025-07-23T00:10:46.593Z",
      "title": "HOComp: Interaction-Aware Human-Object Composition",
      "submittedOnDailyBy": {
        "_id": "6351463b8445bbe32e944f6c",
        "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
        "isPro": false,
        "fullname": "Yuhao Liu",
        "user": "LeoLau",
        "type": "user"
      },
      "summary": "While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.",
      "upvotes": 9,
      "discussionId": "68803aa000f4b5a05f2fbdb8",
      "projectPage": "https://dliang293.github.io/HOComp-project",
      "ai_summary": "HOComp uses MLLMs and attention mechanisms to achieve seamless human-object interactions with consistent appearances in image compositing.",
      "ai_keywords": [
        "MLLMs",
        "Region-based Pose Guidance",
        "MRPG",
        "Detail-Consistent Appearance Preservation",
        "DCAP",
        "shape-aware attention modulation",
        "multi-view appearance loss",
        "background consistency loss",
        "Interaction-aware Human-Object Composition",
        "IHOC"
      ]
    },
    "publishedAt": "2025-07-22T13:59:21.000Z",
    "title": "HOComp: Interaction-Aware Human-Object Composition",
    "summary": "While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6351463b8445bbe32e944f6c/wr4t2EY1Jx56UA6EUPfYl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16813.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6351463b8445bbe32e944f6c",
      "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
      "fullname": "Yuhao Liu",
      "name": "LeoLau",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16746",
      "authors": [
        {
          "_id": "68803f5300f4b5a05f2fbdba",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdbb",
          "name": "Charles Wang",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdbc",
          "name": "Kaiyu Yue",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdbd",
          "name": "Zikui Cai",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdbe",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdbf",
          "name": "Deqing Fu",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc0",
          "name": "Peng Guo",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc1",
          "name": "Wang Bill Zhu",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc2",
          "name": "Vatsal Sharan",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc3",
          "name": "Robin Jia",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc4",
          "name": "Willie Neiswanger",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc5",
          "name": "Furong Huang",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc6",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68803f5300f4b5a05f2fbdc7",
          "name": "Micah Goldblum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T16:35:36.000Z",
      "submittedOnDailyAt": "2025-07-23T00:22:07.469Z",
      "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
      "submittedOnDailyBy": {
        "_id": "63c8454e46421a2efe82709d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
        "isPro": true,
        "fullname": "Deqing Fu",
        "user": "deqing",
        "type": "user"
      },
      "summary": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce Zebra-CoT, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
      "upvotes": 7,
      "discussionId": "68803f5400f4b5a05f2fbdc8",
      "githubRepo": "https://github.com/multimodal-reasoning-lab/Bagel-Zebra-CoT",
      "githubStars": 11
    },
    "publishedAt": "2025-07-22T12:35:36.000Z",
    "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
    "summary": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce Zebra-CoT, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16746.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c8454e46421a2efe82709d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
      "fullname": "Deqing Fu",
      "name": "deqing",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15024",
      "authors": [
        {
          "_id": "68805b3600f4b5a05f2fbe55",
          "name": "Qiaoyu Tang",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe56",
          "name": "Hao Xiang",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe57",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe58",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe59",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe5a",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe5b",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe5c",
          "name": "Le Sun",
          "hidden": false
        },
        {
          "_id": "68805b3600f4b5a05f2fbe5d",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T16:19:51.000Z",
      "submittedOnDailyAt": "2025-07-23T02:18:48.884Z",
      "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement\n  Feedback",
      "submittedOnDailyBy": {
        "_id": "63f33d500be81bdc5d902356",
        "avatarUrl": "/avatars/125812b9a86c3379b34ebfa8026f1a7f.svg",
        "isPro": false,
        "fullname": "xianghao",
        "user": "xiangh",
        "type": "user"
      },
      "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.",
      "upvotes": 5,
      "discussionId": "68805b3700f4b5a05f2fbe5e"
    },
    "publishedAt": "2025-07-20T12:19:51.000Z",
    "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement\n  Feedback",
    "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15024.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f33d500be81bdc5d902356",
      "avatarUrl": "/avatars/125812b9a86c3379b34ebfa8026f1a7f.svg",
      "fullname": "xianghao",
      "name": "xiangh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15974",
      "authors": [
        {
          "_id": "688061c500f4b5a05f2fbe89",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8a",
          "name": "Chong Xiang",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8b",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8c",
          "name": "Weichen Yu",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8d",
          "name": "Chawin Sitawarin",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8e",
          "name": "Vikash Sehwag",
          "hidden": false
        },
        {
          "_id": "688061c500f4b5a05f2fbe8f",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62fae9328e137d7c4b896498/i5eL8D7U6vtMZfTVQwp9_.png"
      ],
      "publishedAt": "2025-07-21T18:08:38.000Z",
      "submittedOnDailyAt": "2025-07-23T02:51:38.507Z",
      "title": "Does More Inference-Time Compute Really Help Robustness?",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications.",
      "upvotes": 2,
      "discussionId": "688061c500f4b5a05f2fbe90"
    },
    "publishedAt": "2025-07-21T14:08:38.000Z",
    "title": "Does More Inference-Time Compute Really Help Robustness?",
    "summary": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62fae9328e137d7c4b896498/i5eL8D7U6vtMZfTVQwp9_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15245",
      "authors": [
        {
          "_id": "688042e900f4b5a05f2fbdd0",
          "name": "Xiaofeng Shi",
          "hidden": false
        },
        {
          "_id": "688042e900f4b5a05f2fbdd1",
          "name": "Yuduo Li",
          "hidden": false
        },
        {
          "_id": "688042e900f4b5a05f2fbdd2",
          "name": "Qian Kou",
          "hidden": false
        },
        {
          "_id": "688042e900f4b5a05f2fbdd3",
          "name": "Longbin Yu",
          "hidden": false
        },
        {
          "_id": "688042e900f4b5a05f2fbdd4",
          "name": "Jinxin Xie",
          "hidden": false
        },
        {
          "_id": "688042e900f4b5a05f2fbdd5",
          "name": "Hua Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T05:06:53.000Z",
      "submittedOnDailyAt": "2025-07-23T00:35:53.723Z",
      "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search",
      "submittedOnDailyBy": {
        "_id": "642f6c64f945a8a5c9ee5b5d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f6c64f945a8a5c9ee5b5d/V_4S_39gZc3ttiO4rXccj.png",
        "isPro": false,
        "fullname": "XiaofengShi",
        "user": "MonteXiaofeng",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR",
      "upvotes": 2,
      "discussionId": "688042ea00f4b5a05f2fbdd6",
      "githubRepo": "https://github.com/xiaofengShi/SPAR",
      "githubStars": 2
    },
    "publishedAt": "2025-07-21T01:06:53.000Z",
    "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search",
    "summary": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f6c64f945a8a5c9ee5b5d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f6c64f945a8a5c9ee5b5d/V_4S_39gZc3ttiO4rXccj.png",
      "fullname": "XiaofengShi",
      "name": "MonteXiaofeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16795",
      "authors": [
        {
          "_id": "68806b4b00f4b5a05f2fbe92",
          "name": "Helena Casademunt",
          "hidden": false
        },
        {
          "_id": "68806b4b00f4b5a05f2fbe93",
          "name": "Caden Juang",
          "hidden": false
        },
        {
          "_id": "68806b4b00f4b5a05f2fbe94",
          "name": "Adam Karvonen",
          "hidden": false
        },
        {
          "_id": "68806b4b00f4b5a05f2fbe95",
          "name": "Samuel Marks",
          "hidden": false
        },
        {
          "_id": "68806b4b00f4b5a05f2fbe96",
          "name": "Senthooran Rajamanoharan",
          "hidden": false
        },
        {
          "_id": "68806b4b00f4b5a05f2fbe97",
          "name": "Neel Nanda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T17:45:04.000Z",
      "submittedOnDailyAt": "2025-07-23T03:26:09.816Z",
      "title": "Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "62ebf45a341b2cd73454e796",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ebf45a341b2cd73454e796/kmJkeT7L3-c5XAF5suS-p.png",
        "isPro": false,
        "fullname": "Caden Juang",
        "user": "kh4dien",
        "type": "user"
      },
      "summary": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.",
      "upvotes": 1,
      "discussionId": "68806b4b00f4b5a05f2fbe98",
      "ai_summary": "Concept Ablation Fine-Tuning (CAFT) uses interpretability tools to steer LLM generalization away from unintended concepts without altering training data.",
      "ai_keywords": [
        "fine-tuning",
        "large language models (LLMs)",
        "out-of-distribution generalization",
        "Concept Ablation Fine-Tuning (CAFT)",
        "latent space",
        "linear projections",
        "emergent misalignment",
        "misaligned responses"
      ]
    },
    "publishedAt": "2025-07-22T13:45:04.000Z",
    "title": "Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning",
    "summary": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ebf45a341b2cd73454e796",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ebf45a341b2cd73454e796/kmJkeT7L3-c5XAF5suS-p.png",
      "fullname": "Caden Juang",
      "name": "kh4dien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15454",
      "authors": [
        {
          "_id": "68805c4300f4b5a05f2fbe60",
          "name": "Ruijie Zhu",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe61",
          "name": "Mulin Yu",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe62",
          "name": "Linning Xu",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe63",
          "name": "Lihan Jiang",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe64",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe65",
          "name": "Tianzhu Zhang",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe66",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68805c4300f4b5a05f2fbe67",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/ckv7HUkaPz3blmDV_dqz_.jpeg"
      ],
      "publishedAt": "2025-07-21T10:06:23.000Z",
      "submittedOnDailyAt": "2025-07-23T02:25:15.789Z",
      "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "6697ac8427e4e21a3a92da27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
        "isPro": false,
        "fullname": "Ruijie Zhu",
        "user": "RuijieZhu",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
      "upvotes": 1,
      "discussionId": "68805c4400f4b5a05f2fbe68",
      "ai_summary": "ObjectGS combines 3D scene reconstruction with semantic understanding by modeling individual objects as neural Gaussians, achieving superior performance in segmentation and integration with applications like mesh extraction and scene editing.",
      "ai_keywords": [
        "Gaussian Splatting",
        "object-aware framework",
        "neural Gaussians",
        "one-hot ID encoding",
        "classification loss",
        "open-vocabulary segmentation",
        "panoptic segmentation",
        "mesh extraction",
        "scene editing"
      ]
    },
    "publishedAt": "2025-07-21T06:06:23.000Z",
    "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting",
    "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/ckv7HUkaPz3blmDV_dqz_.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6697ac8427e4e21a3a92da27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
      "fullname": "Ruijie Zhu",
      "name": "RuijieZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]