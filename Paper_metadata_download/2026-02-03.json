[
  {
    "paper": {
      "id": "2602.02276",
      "authors": [
        {
          "_id": "69817e2cce18b1862809615b",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809615c",
          "name": "Tongtong Bai",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809615d",
          "name": "Yifan Bai",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809615e",
          "name": "Yiping Bao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809615f",
          "name": "S. H. Cai",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096160",
          "name": "Yuan Cao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096161",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096162",
          "name": "H. S. Che",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096163",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096164",
          "name": "Guanduo Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096165",
          "name": "Huarong Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096166",
          "name": "Jia Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096167",
          "name": "Jiahao Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096168",
          "name": "Jianlong Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096169",
          "name": "Jun Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616a",
          "name": "Kefan Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616b",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616c",
          "name": "Ruijue Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616d",
          "name": "Xinhao Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616e",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809616f",
          "name": "Yanxu Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096170",
          "name": "Yicun Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096171",
          "name": "Yimin Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096172",
          "name": "Yingjiang Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096173",
          "name": "Yuankun Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096174",
          "name": "Yujie Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096175",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096176",
          "name": "Zhirong Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096177",
          "name": "Ziwei Chen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096178",
          "name": "Dazhi Cheng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096179",
          "name": "Minghan Chu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617a",
          "name": "Jialei Cui",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617b",
          "name": "Jiaqi Deng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617c",
          "name": "Muxi Diao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617d",
          "name": "Hao Ding",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617e",
          "name": "Mengfan Dong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809617f",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096180",
          "name": "Yuxin Dong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096181",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096182",
          "name": "Angang Du",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096183",
          "name": "Chenzhuang Du",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096184",
          "name": "Dikang Du",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096185",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096186",
          "name": "Yulun Du",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096187",
          "name": "Yu Fan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096188",
          "name": "Shengjun Fang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096189",
          "name": "Qiulin Feng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618a",
          "name": "Yichen Feng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618b",
          "name": "Garimugai Fu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618c",
          "name": "Kelin Fu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618d",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618e",
          "name": "Tong Gao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809618f",
          "name": "Yuyao Ge",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096190",
          "name": "Shangyi Geng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096191",
          "name": "Chengyang Gong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096192",
          "name": "Xiaochen Gong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096193",
          "name": "Zhuoma Gongque",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096194",
          "name": "Qizheng Gu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096195",
          "name": "Xinran Gu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096196",
          "name": "Yicheng Gu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096197",
          "name": "Longyu Guan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096198",
          "name": "Yuanying Guo",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096199",
          "name": "Xiaoru Hao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619a",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619b",
          "name": "Wenyang He",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619c",
          "name": "Yunjia He",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619d",
          "name": "Chao Hong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619e",
          "name": "Hao Hu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809619f",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a0",
          "name": "Yangyang Hu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a1",
          "name": "Zhenxing Hu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a2",
          "name": "Ke Huang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a3",
          "name": "Ruiyuan Huang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a4",
          "name": "Weixiao Huang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a5",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a6",
          "name": "Tao Jiang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a7",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a8",
          "name": "Xinyi Jin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961a9",
          "name": "Yu Jing",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961aa",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ab",
          "name": "Aidi Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ac",
          "name": "C. Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ad",
          "name": "Cheng Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ae",
          "name": "Fang Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961af",
          "name": "Guanghe Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b0",
          "name": "Guanyu Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b1",
          "name": "Haitao Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b2",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b3",
          "name": "Jia Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b4",
          "name": "Jingwei Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b5",
          "name": "Junxiong Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b6",
          "name": "Lincan Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b7",
          "name": "Mo Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b8",
          "name": "Weihong Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961b9",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ba",
          "name": "Xinhang Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961bb",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961bc",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961bd",
          "name": "Yanhao Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961be",
          "name": "Yiwei Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961bf",
          "name": "Yuxiao Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c0",
          "name": "Zhaowei Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c1",
          "name": "Zheming Li",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c2",
          "name": "Weilong Liao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c3",
          "name": "Jiawei Lin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c4",
          "name": "Xiaohan Lin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c5",
          "name": "Zhishan Lin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c6",
          "name": "Zichao Lin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c7",
          "name": "Cheng Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c8",
          "name": "Chenyu Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961c9",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ca",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961cb",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961cc",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961cd",
          "name": "Shuran Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ce",
          "name": "Tianwei Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961cf",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d0",
          "name": "Weizhou Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d1",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d2",
          "name": "Yangyang Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d3",
          "name": "Yanming Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d4",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d5",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d6",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d7",
          "name": "Zhengying Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d8",
          "name": "Zhongnuo Liu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961d9",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961da",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961db",
          "name": "Zhiyuan Lu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961dc",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961dd",
          "name": "Tongxu Luo",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961de",
          "name": "Yashuo Luo",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961df",
          "name": "Long Ma",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e0",
          "name": "Yingwei Ma",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e1",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e2",
          "name": "Yuan Mei",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e3",
          "name": "Xin Men",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e4",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e5",
          "name": "Zhiyong Meng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e6",
          "name": "Yibo Miao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e7",
          "name": "Minqing Ni",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e8",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961e9",
          "name": "Siyuan Pan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ea",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961eb",
          "name": "Yuchao Qian",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ec",
          "name": "Ruoyu Qin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ed",
          "name": "Zeyu Qin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ee",
          "name": "Jiezhong Qiu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ef",
          "name": "Bowen Qu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f0",
          "name": "Zeyu Shang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f1",
          "name": "Youbo Shao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f2",
          "name": "Tianxiao Shen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f3",
          "name": "Zhennan Shen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f4",
          "name": "Juanfeng Shi",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f5",
          "name": "Lidong Shi",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f6",
          "name": "Shengyuan Shi",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f7",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f8",
          "name": "Pengwei Song",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961f9",
          "name": "Tianhui Song",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961fa",
          "name": "Xiaoxi Song",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961fb",
          "name": "Hongjin Su",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961fc",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961fd",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961fe",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280961ff",
          "name": "Jinsong Sun",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096200",
          "name": "Junyao Sun",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096201",
          "name": "Tongyu Sun",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096202",
          "name": "Flood Sung",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096203",
          "name": "Yunpeng Tai",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096204",
          "name": "Chuning Tang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096205",
          "name": "Heyi Tang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096206",
          "name": "Xiaojuan Tang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096207",
          "name": "Zhengyang Tang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096208",
          "name": "Jiawen Tao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096209",
          "name": "Shiyuan Teng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620a",
          "name": "Chaoran Tian",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620b",
          "name": "Pengfei Tian",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620c",
          "name": "Ao Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620d",
          "name": "Bowen Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620e",
          "name": "Chensi Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809620f",
          "name": "Chuang Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096210",
          "name": "Congcong Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096211",
          "name": "Dingkun Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096212",
          "name": "Dinglu Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096213",
          "name": "Dongliang Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096214",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096215",
          "name": "Hailong Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096216",
          "name": "Haiming Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096217",
          "name": "Hengzhi Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096218",
          "name": "Huaqing Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096219",
          "name": "Hui Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621a",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621b",
          "name": "Jinhong Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621c",
          "name": "Jiuzheng Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621d",
          "name": "Kaixin Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621e",
          "name": "Linian Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809621f",
          "name": "Qibin Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096220",
          "name": "Shengjie Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096221",
          "name": "Shuyi Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096222",
          "name": "Si Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096223",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096224",
          "name": "Xiaochen Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096225",
          "name": "Xinyuan Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096226",
          "name": "Yao Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096227",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096228",
          "name": "Yipu Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096229",
          "name": "Yiqin Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622a",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622b",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622c",
          "name": "Zhaoji Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622d",
          "name": "Zhaowei Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622e",
          "name": "Zhengtao Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809622f",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096230",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096231",
          "name": "Zizhe Wang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096232",
          "name": "Chu Wei",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096233",
          "name": "Ming Wei",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096234",
          "name": "Chuan Wen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096235",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096236",
          "name": "Chengjie Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096237",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096238",
          "name": "Junyan Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096239",
          "name": "Rucong Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623a",
          "name": "Wenhao Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623b",
          "name": "Yuefeng Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623c",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623d",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623e",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809623f",
          "name": "Chenjun Xiao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096240",
          "name": "Jin Xie",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096241",
          "name": "Xiaotong Xie",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096242",
          "name": "Yuchong Xie",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096243",
          "name": "Yifei Xin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096244",
          "name": "Bowei Xing",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096245",
          "name": "Boyu Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096246",
          "name": "Jianfan Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096247",
          "name": "Jing Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096248",
          "name": "Jinjing Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096249",
          "name": "L. H. Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624a",
          "name": "Lin Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624b",
          "name": "Suting Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624c",
          "name": "Weixin Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624d",
          "name": "Xinbo Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624e",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809624f",
          "name": "Yangchuan Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096250",
          "name": "Yichang Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096251",
          "name": "Yuemeng Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096252",
          "name": "Zelai Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096253",
          "name": "Ziyao Xu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096254",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096255",
          "name": "Yuzi Yan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096256",
          "name": "Guangyao Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096257",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096258",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096259",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625a",
          "name": "Ningyuan Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625b",
          "name": "Ruihan Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625c",
          "name": "Xiaofei Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625d",
          "name": "Xinlong Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625e",
          "name": "Ying Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809625f",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096260",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096261",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096262",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096263",
          "name": "Zonghan Yang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096264",
          "name": "Haotian Yao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096265",
          "name": "Dan Ye",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096266",
          "name": "Wenjie Ye",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096267",
          "name": "Zhuorui Ye",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096268",
          "name": "Bohong Yin",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096269",
          "name": "Chengzhen Yu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626a",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626c",
          "name": "Tianxiang Yu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626d",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626e",
          "name": "Mengjie Yuan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809626f",
          "name": "Xiaokun Yuan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096270",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096271",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096272",
          "name": "Dunyuan Zha",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096273",
          "name": "Haobing Zhan",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096274",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096275",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096276",
          "name": "Jin Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096277",
          "name": "Puqi Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096278",
          "name": "Qiao Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096279",
          "name": "Rui Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627a",
          "name": "Xiaobin Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627b",
          "name": "Y. Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627c",
          "name": "Yadong Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627d",
          "name": "Yangkun Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627e",
          "name": "Yichi Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809627f",
          "name": "Yizhi Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096280",
          "name": "Yongting Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096281",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096282",
          "name": "Yushun Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096283",
          "name": "Yutao Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096284",
          "name": "Yutong Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096285",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096286",
          "name": "Chenguang Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096287",
          "name": "Feifan Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096288",
          "name": "Jinxiang Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096289",
          "name": "Shuai Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628a",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628b",
          "name": "Yikai Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628c",
          "name": "Zijia Zhao",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628d",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628e",
          "name": "Ruihan Zheng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809628f",
          "name": "Shaojie Zheng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096290",
          "name": "Tengyang Zheng",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096291",
          "name": "Junfeng Zhong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096292",
          "name": "Longguang Zhong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096293",
          "name": "Weiming Zhong",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096294",
          "name": "M. Zhou",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096295",
          "name": "Runjie Zhou",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096296",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096297",
          "name": "Zaida Zhou",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096298",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b18628096299",
          "name": "Liya Zhu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629a",
          "name": "Xinhao Zhu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629b",
          "name": "Yuxuan Zhu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629c",
          "name": "Zhen Zhu",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629d",
          "name": "Jingze Zhuang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629e",
          "name": "Weiyu Zhuang",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b1862809629f",
          "name": "Ying Zou",
          "hidden": false
        },
        {
          "_id": "69817e2cce18b186280962a0",
          "name": "Xinxing Zu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T16:17:38.000Z",
      "submittedOnDailyAt": "2026-02-03T02:18:48.721Z",
      "title": "Kimi K2.5: Visual Agentic Intelligence",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "upvotes": 77,
      "discussionId": "69817e2cce18b186280962a1",
      "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5",
      "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "ai_keywords": [
        "multimodal agentic model",
        "joint text-vision pre-training",
        "zero-vision SFT",
        "joint text-vision reinforcement learning",
        "Agent Swarm",
        "self-directed parallel agent orchestration framework",
        "heterogeneous sub-problems"
      ],
      "organization": {
        "_id": "6425a114812813f8f4a9b02c",
        "name": "moonshotai",
        "fullname": "Moonshot AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"
      }
    },
    "publishedAt": "2026-02-02T11:17:38.000Z",
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 226,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6425a114812813f8f4a9b02c",
      "name": "moonshotai",
      "fullname": "Moonshot AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.00919",
      "authors": [
        {
          "_id": "698186fdce18b1862809633b",
          "name": "I. Apanasevich",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809633c",
          "name": "M. Artemyev",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809633d",
          "name": "R. Babakyan",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809633e",
          "name": "P. Fedotova",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809633f",
          "name": "D. Grankin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096340",
          "name": "E. Kupryashin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096341",
          "name": "A. Misailidi",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096342",
          "name": "D. Nerus",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096343",
          "name": "A. Nutalapati",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096344",
          "name": "G. Sidorov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096345",
          "name": "I. Efremov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096346",
          "name": "M. Gerasyov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096347",
          "name": "D. Pikurov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096348",
          "name": "Y. Senchenko",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096349",
          "name": "S. Davidenko",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634a",
          "name": "D. Kulikov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634b",
          "name": "M. Sultankin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634c",
          "name": "K. Askarbek",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634d",
          "name": "O. Shamanin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634e",
          "name": "D. Statovoy",
          "hidden": false
        },
        {
          "_id": "698186fdce18b1862809634f",
          "name": "E. Zalyaev",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096350",
          "name": "I. Zorin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096351",
          "name": "A. Letkin",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096352",
          "name": "E. Rusakov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096353",
          "name": "A. Silchenko",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096354",
          "name": "V. Vorobyov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096355",
          "name": "S. Sobolnikov",
          "hidden": false
        },
        {
          "_id": "698186fdce18b18628096356",
          "name": "A. Postnikov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"
      ],
      "publishedAt": "2026-01-31T22:13:23.000Z",
      "submittedOnDailyAt": "2026-02-03T03:13:09.153Z",
      "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
      "submittedOnDailyBy": {
        "_id": "642694c721d5f027bec07c71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg",
        "isPro": false,
        "fullname": "Polina Fedotova",
        "user": "2pd",
        "type": "user"
      },
      "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "upvotes": 56,
      "discussionId": "698186fece18b18628096357",
      "projectPage": "https://greenvla.github.io",
      "githubRepo": "https://github.com/greenvla/GreenVLA",
      "githubRepoAddedBy": "user",
      "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "ai_keywords": [
        "Vision-Language-Action",
        "multimodal grounding",
        "multi-embodiment pretraining",
        "embodiment-specific adaptation",
        "reinforcement-learning",
        "episode-progress prediction",
        "out-of-distribution detection",
        "joint-prediction-based guidance"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "6973998bee83f4964edef012",
        "name": "SberRoboticsCenter",
        "fullname": "Sber Robotics Center",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/BJuUJyhw51VXtN3bQhvYV.png"
      }
    },
    "publishedAt": "2026-01-31T17:13:23.000Z",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642694c721d5f027bec07c71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg",
      "fullname": "Polina Fedotova",
      "name": "2pd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6973998bee83f4964edef012",
      "name": "SberRoboticsCenter",
      "fullname": "Sber Robotics Center",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/BJuUJyhw51VXtN3bQhvYV.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02437",
      "authors": [
        {
          "_id": "6981756ace18b18628096056",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6981756ace18b18628096057",
          "name": "Chaofan Ma",
          "hidden": false
        },
        {
          "_id": "6981756ace18b18628096058",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "6981756ace18b18628096059",
          "name": "Size Wu",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605a",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605b",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605c",
          "name": "Zhixiong Zhang",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605d",
          "name": "Tianhang Wang",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605e",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6981756ace18b1862809605f",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "6981756ace18b18628096060",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T18:34:35.000Z",
      "submittedOnDailyAt": "2026-02-03T02:26:49.217Z",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
      "upvotes": 53,
      "discussionId": "6981756ace18b18628096061",
      "ai_summary": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.",
      "ai_keywords": [
        "text-to-image generation",
        "image editing",
        "dual reasoning paradigm",
        "world knowledge-enhanced planning",
        "visual self-correction",
        "shared representation",
        "reasoning-intensive benchmarks",
        "WISE",
        "KrisBench",
        "UniREditBench"
      ]
    },
    "publishedAt": "2026-02-02T13:34:35.000Z",
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02437.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 226,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02084",
      "authors": [
        {
          "_id": "6981767dce18b186280960af",
          "name": "Jane Luo",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b0",
          "name": "Chengyu Yin",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b1",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b2",
          "name": "Qingtao Li",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b3",
          "name": "Steven Liu",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b4",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b5",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b6",
          "name": "Hao Liu",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b7",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b8",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960b9",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960ba",
          "name": "Ying Xin",
          "hidden": false
        },
        {
          "_id": "6981767dce18b186280960bb",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T13:30:00.000Z",
      "submittedOnDailyAt": "2026-02-03T01:47:04.655Z",
      "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
      "submittedOnDailyBy": {
        "_id": "66adf5cc0c6056d9f4dc308f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg",
        "isPro": false,
        "fullname": "Jane Luo",
        "user": "Luo2003",
        "type": "user"
      },
      "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
      "upvotes": 35,
      "discussionId": "6981767dce18b186280960bc",
      "projectPage": "https://ayanami2003.github.io/RPG-Encoder/",
      "githubRepo": "https://github.com/microsoft/RPG-ZeroRepo",
      "githubRepoAddedBy": "user",
      "ai_summary": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.",
      "ai_keywords": [
        "Repository Planning Graph",
        "RPG-Encoder",
        "code representation",
        "semantic features",
        "code dependencies",
        "incremental topology evolution",
        "structure-aware navigation",
        "fine-grained localization",
        "repository understanding",
        "codebase reconstruction"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-02-02T08:30:00.000Z",
    "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
    "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66adf5cc0c6056d9f4dc308f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg",
      "fullname": "Jane Luo",
      "name": "Luo2003",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02053",
      "authors": [
        {
          "_id": "698175bdce18b18628096063",
          "name": "Pengyu Wang",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096064",
          "name": "Benfeng Xu",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096065",
          "name": "Licheng Zhang",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096066",
          "name": "Shaohan Wang",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096067",
          "name": "Mingxuan Du",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096068",
          "name": "Chiwei Zhu",
          "hidden": false
        },
        {
          "_id": "698175bdce18b18628096069",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T12:55:29.000Z",
      "submittedOnDailyAt": "2026-02-03T01:43:37.040Z",
      "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
      "submittedOnDailyBy": {
        "_id": "646dbba74ad7f907279dd486",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png",
        "isPro": false,
        "fullname": "Mingxuan Du",
        "user": "Ayanami0730",
        "type": "user"
      },
      "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
      "upvotes": 30,
      "discussionId": "698175bece18b1862809606a",
      "ai_summary": "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.",
      "ai_keywords": [
        "GraphRAG",
        "retrieval-augmented generation",
        "hierarchical graph",
        "external knowledge",
        "long contexts",
        "heterogeneous documents",
        "Wikipedia",
        "retrieval corpus",
        "citation-linked statements",
        "multi-fact aggregation",
        "summarization"
      ],
      "organization": {
        "_id": "6912993f45f02a20f4c50b8a",
        "name": "muset-ai",
        "fullname": "muset.ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
      }
    },
    "publishedAt": "2026-02-02T07:55:29.000Z",
    "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
    "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02053.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "646dbba74ad7f907279dd486",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png",
      "fullname": "Mingxuan Du",
      "name": "Ayanami0730",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6912993f45f02a20f4c50b8a",
      "name": "muset-ai",
      "fullname": "muset.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01566",
      "authors": [
        {
          "_id": "69816c7ace18b18628095fb1",
          "name": "Chiwei Zhu",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb2",
          "name": "Benfeng Xu",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb3",
          "name": "Mingxuan Du",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb4",
          "name": "Shaohan Wang",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb5",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb6",
          "name": "Zhendong Mao",
          "hidden": false
        },
        {
          "_id": "69816c7ace18b18628095fb7",
          "name": "Yongdong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T03:00:19.000Z",
      "submittedOnDailyAt": "2026-02-03T01:16:46.610Z",
      "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
      "submittedOnDailyBy": {
        "_id": "663b22a80966eef8686aadaf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
        "isPro": false,
        "fullname": "Chiwei Zhu",
        "user": "IgnoraZ",
        "type": "user"
      },
      "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.",
      "upvotes": 29,
      "discussionId": "69816c7bce18b18628095fb8",
      "ai_summary": "A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.",
      "ai_keywords": [
        "large language model agents",
        "context window",
        "file system",
        "dual-agent framework",
        "persistent workspace",
        "external memory",
        "test-time scaling",
        "deep research",
        "knowledge base",
        "structured notes"
      ],
      "organization": {
        "_id": "6912993f45f02a20f4c50b8a",
        "name": "muset-ai",
        "fullname": "muset.ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
      }
    },
    "publishedAt": "2026-02-01T22:00:19.000Z",
    "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
    "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01566.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663b22a80966eef8686aadaf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
      "fullname": "Chiwei Zhu",
      "name": "IgnoraZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6912993f45f02a20f4c50b8a",
      "name": "muset-ai",
      "fullname": "muset.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01590",
      "authors": [
        {
          "_id": "69817335ce18b1862809603c",
          "name": "Shaohan Wang",
          "hidden": false
        },
        {
          "_id": "69817335ce18b1862809603d",
          "name": "Benfeng Xu",
          "hidden": false
        },
        {
          "_id": "69817335ce18b1862809603e",
          "name": "Licheng Zhang",
          "hidden": false
        },
        {
          "_id": "69817335ce18b1862809603f",
          "name": "Mingxuan Du",
          "hidden": false
        },
        {
          "_id": "69817335ce18b18628096040",
          "name": "Chiwei Zhu",
          "hidden": false
        },
        {
          "_id": "69817335ce18b18628096041",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "69817335ce18b18628096042",
          "name": "Zhendong Mao",
          "hidden": false
        },
        {
          "_id": "69817335ce18b18628096043",
          "name": "Yongdong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T03:30:13.000Z",
      "submittedOnDailyAt": "2026-02-03T01:48:24.665Z",
      "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
      "submittedOnDailyBy": {
        "_id": "646dbba74ad7f907279dd486",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png",
        "isPro": false,
        "fullname": "Mingxuan Du",
        "user": "Ayanami0730",
        "type": "user"
      },
      "summary": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge",
      "upvotes": 24,
      "discussionId": "69817335ce18b18628096044",
      "githubRepo": "https://github.com/WangShao2000/Wiki_Live_Challenge",
      "githubRepoAddedBy": "user",
      "ai_summary": "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.",
      "ai_keywords": [
        "Deep Research Agents",
        "Wikipedia Good Articles",
        "live benchmark",
        "evaluation framework",
        "fine-grained evaluation",
        "factual verifiability",
        "agent research"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6912993f45f02a20f4c50b8a",
        "name": "muset-ai",
        "fullname": "muset.ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
      }
    },
    "publishedAt": "2026-02-01T22:30:13.000Z",
    "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
    "summary": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01590.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646dbba74ad7f907279dd486",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png",
      "fullname": "Mingxuan Du",
      "name": "Ayanami0730",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6912993f45f02a20f4c50b8a",
      "name": "muset-ai",
      "fullname": "muset.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02185",
      "authors": [
        {
          "_id": "69817bb2ce18b18628096119",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611b",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611c",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611d",
          "name": "Yufan Shen",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611e",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b1862809611f",
          "name": "Xiaoman Wang",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096120",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096121",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096122",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096123",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096124",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096125",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096126",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096127",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "69817bb2ce18b18628096128",
          "name": "Shaosheng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T14:53:11.000Z",
      "submittedOnDailyAt": "2026-02-03T02:09:42.355Z",
      "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "665d652e0f35c005de892108",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
        "isPro": false,
        "fullname": "Yu Zeng",
        "user": "YuZeng260",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "upvotes": 23,
      "discussionId": "69817bb3ce18b18628096129",
      "projectPage": "https://osilly.github.io/Vision-DeepResearch/",
      "ai_summary": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Vision-DeepResearch",
        "visual-textual fact-finding",
        "VQA",
        "vision search",
        "text-search",
        "visual retrieval",
        "multimodal deep-research systems"
      ]
    },
    "publishedAt": "2026-02-02T09:53:11.000Z",
    "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665d652e0f35c005de892108",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
      "fullname": "Yu Zeng",
      "name": "YuZeng260",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22060",
      "authors": [
        {
          "_id": "69817968ce18b186280960f0",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f1",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f2",
          "name": "Qiuchen Wang",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f3",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f4",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f5",
          "name": "Zheng Chu",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f6",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f7",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f8",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960f9",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960fa",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960fb",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960fc",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960fd",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "69817968ce18b186280960fe",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"
      ],
      "publishedAt": "2026-01-29T17:58:40.000Z",
      "submittedOnDailyAt": "2026-02-03T02:05:47.568Z",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "665d652e0f35c005de892108",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
        "isPro": false,
        "fullname": "Yu Zeng",
        "user": "YuZeng260",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "upvotes": 23,
      "discussionId": "69817968ce18b186280960ff",
      "projectPage": "https://osilly.github.io/Vision-DeepResearch/",
      "githubRepo": "https://github.com/Osilly/Vision-DeepResearch",
      "githubRepoAddedBy": "user",
      "ai_summary": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.",
      "ai_keywords": [
        "multimodal large language models",
        "visual and textual search engines",
        "reasoning-then-tool-call",
        "multimodal deep-research",
        "multi-turn search",
        "multi-entity search",
        "multi-scale search",
        "cold-start supervision",
        "reinforcement learning",
        "end-to-end multimodal deep-research"
      ],
      "githubStars": 18
    },
    "publishedAt": "2026-01-29T12:58:40.000Z",
    "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22060.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665d652e0f35c005de892108",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
      "fullname": "Yu Zeng",
      "name": "YuZeng260",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02488",
      "authors": [
        {
          "_id": "698179b8ce18b18628096104",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "698179b8ce18b18628096105",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "698179b8ce18b18628096106",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "698179b8ce18b18628096107",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "698179b8ce18b18628096108",
          "name": "Ling Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T18:59:04.000Z",
      "submittedOnDailyAt": "2026-02-03T02:02:10.235Z",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "upvotes": 21,
      "discussionId": "698179b8ce18b18628096109",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/open-agentrl",
      "githubRepo": "https://github.com/Gen-Verse/Open-AgentRL",
      "githubRepoAddedBy": "user",
      "ai_summary": "RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.",
      "ai_keywords": [
        "reinforcement learning",
        "environment modeling",
        "policy models",
        "reward models",
        "closed-loop optimization",
        "step-wise signals",
        "outcome signals",
        "consistency feedback",
        "critic feedback",
        "automatic environment adaptation",
        "Qwen3-VL-8B-Thinking",
        "Qwen2.5-7B-Instruct",
        "OSWorld",
        "AlfWorld",
        "LiveBench"
      ],
      "githubStars": 161,
      "organization": {
        "_id": "69081a9c8b3b900d6e63602f",
        "name": "princeton-ai",
        "fullname": "Princeton AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/Xh9rZKOFsWasVQXJwjmVt.jpeg"
      }
    },
    "publishedAt": "2026-02-02T13:59:04.000Z",
    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
    "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02488.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69081a9c8b3b900d6e63602f",
      "name": "princeton-ai",
      "fullname": "Princeton AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/Xh9rZKOFsWasVQXJwjmVt.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01624",
      "authors": [
        {
          "_id": "69816f55ce18b18628095fd8",
          "name": "Minh-Quan Le",
          "hidden": false
        },
        {
          "_id": "69816f55ce18b18628095fd9",
          "name": "Gaurav Mittal",
          "hidden": false
        },
        {
          "_id": "69816f55ce18b18628095fda",
          "name": "Cheng Zhao",
          "hidden": false
        },
        {
          "_id": "69816f55ce18b18628095fdb",
          "name": "David Gu",
          "hidden": false
        },
        {
          "_id": "69816f55ce18b18628095fdc",
          "name": "Dimitris Samaras",
          "hidden": false
        },
        {
          "_id": "69816f55ce18b18628095fdd",
          "name": "Mei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6618413e625ae15f64769476/HjXIn53g9ZJMyBci6wxBd.mp4"
      ],
      "publishedAt": "2026-02-02T04:37:11.000Z",
      "submittedOnDailyAt": "2026-02-03T01:27:52.123Z",
      "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
      "submittedOnDailyBy": {
        "_id": "6618413e625ae15f64769476",
        "avatarUrl": "/avatars/1585703eab341f7808e7bf703ffef9dc.svg",
        "isPro": false,
        "fullname": "Minh-Quan Le",
        "user": "lmquan",
        "type": "user"
      },
      "summary": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.",
      "upvotes": 19,
      "discussionId": "69816f56ce18b18628095fde",
      "ai_summary": "PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.",
      "ai_keywords": [
        "text-to-video generation",
        "reward-based post-training",
        "optimal transport",
        "Dual Optimal Transport",
        "OT-aligned Rewards",
        "distributional OT alignment",
        "discrete token-level OT alignment",
        "semantic alignment",
        "temporal consistency",
        "visual quality",
        "reinforcement learning fine-tuning",
        "direct backpropagation"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-02-01T23:37:11.000Z",
    "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
    "summary": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6618413e625ae15f64769476/HjXIn53g9ZJMyBci6wxBd.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01624.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6618413e625ae15f64769476",
      "avatarUrl": "/avatars/1585703eab341f7808e7bf703ffef9dc.svg",
      "fullname": "Minh-Quan Le",
      "name": "lmquan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02361",
      "authors": [
        {
          "_id": "69818982ce18b18628096375",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096376",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096377",
          "name": "Yunlong Feng",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096378",
          "name": "Xuwu Wang",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096379",
          "name": "Wenting Zhao",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637a",
          "name": "Ruisheng Cao",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637b",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637c",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637d",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637e",
          "name": "Zeyao Ma",
          "hidden": false
        },
        {
          "_id": "69818982ce18b1862809637f",
          "name": "Hao Ge",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096380",
          "name": "Zongmeng Zhang",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096381",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096382",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096383",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096384",
          "name": "Jianling Sun",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096385",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "69818982ce18b18628096386",
          "name": "Binyuan Hui",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T17:20:30.000Z",
      "submittedOnDailyAt": "2026-02-03T03:11:46.660Z",
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "submittedOnDailyBy": {
        "_id": "66d58df54b87a685ccb8e4a0",
        "avatarUrl": "/avatars/2566c20d79088ba761215b9a0197cb8e.svg",
        "isPro": false,
        "fullname": "Mouxiang Chen",
        "user": "chenmouxiang",
        "type": "user"
      },
      "summary": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "upvotes": 18,
      "discussionId": "69818983ce18b18628096387",
      "ai_summary": "A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.",
      "ai_keywords": [
        "building agent",
        "self-verification",
        "in-loop hacking detection",
        "real-world software engineering",
        "GitHub pull requests",
        "custom-trained model",
        "verifiable environments",
        "agentic mid-training",
        "reinforcement learning",
        "SWE-Bench Verified"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-02-02T12:20:30.000Z",
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "summary": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d58df54b87a685ccb8e4a0",
      "avatarUrl": "/avatars/2566c20d79088ba761215b9a0197cb8e.svg",
      "fullname": "Mouxiang Chen",
      "name": "chenmouxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01756",
      "authors": [
        {
          "_id": "69817073ce18b18628095ff5",
          "name": "Jun He",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ff6",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ff7",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ff8",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ff9",
          "name": "Chenjue Zhang",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ffa",
          "name": "Leqi Zhu",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ffb",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ffc",
          "name": "Xiang Zhang",
          "hidden": false
        },
        {
          "_id": "69817073ce18b18628095ffd",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6487e158f675b4a7867f45fa/ijH6gMLzup8AxdjVATsSq.mp4"
      ],
      "publishedAt": "2026-02-02T07:42:13.000Z",
      "submittedOnDailyAt": "2026-02-03T02:09:21.436Z",
      "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
      "submittedOnDailyBy": {
        "_id": "6487e158f675b4a7867f45fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
        "isPro": false,
        "fullname": "Zilong Huang",
        "user": "SereinH",
        "type": "user"
      },
      "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.",
      "upvotes": 17,
      "discussionId": "69817073ce18b18628095ffe",
      "githubRepo": "https://github.com/PicoTrex/Mind-Brush",
      "githubRepoAddedBy": "user",
      "ai_summary": "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.",
      "ai_keywords": [
        "text-to-image generation",
        "unified understanding-generation models",
        "implicit user intentions",
        "multimodal evidence",
        "reasoning tools",
        "dynamic workflow",
        "agentic framework",
        "knowledge-driven workflow",
        "out-of-distribution concepts",
        "visual constraints"
      ],
      "githubStars": 7
    },
    "publishedAt": "2026-02-02T02:42:13.000Z",
    "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
    "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6487e158f675b4a7867f45fa/ijH6gMLzup8AxdjVATsSq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6487e158f675b4a7867f45fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
      "fullname": "Zilong Huang",
      "name": "SereinH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02214",
      "authors": [
        {
          "_id": "69817f0fce18b186280962a3",
          "name": "Hongzhou Zhu",
          "hidden": false
        },
        {
          "_id": "69817f0fce18b186280962a4",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "69817f0fce18b186280962a5",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "69817f0fce18b186280962a6",
          "name": "Hang Su",
          "hidden": false
        },
        {
          "_id": "69817f0fce18b186280962a7",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "69817f0fce18b186280962a8",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c269a52d73768f07ac266c/X-owg4eHQoOPFuw4OydU4.mp4"
      ],
      "publishedAt": "2026-02-02T15:19:22.000Z",
      "submittedOnDailyAt": "2026-02-03T02:22:41.346Z",
      "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
      "submittedOnDailyBy": {
        "_id": "64c269a52d73768f07ac266c",
        "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
        "isPro": false,
        "fullname": "Zhu Hongzhou",
        "user": "zhuhz22",
        "type": "user"
      },
      "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}",
      "upvotes": 13,
      "discussionId": "69817f0fce18b186280962a9",
      "projectPage": "https://thu-ml.github.io/CausalForcing.github.io/",
      "githubRepo": "https://github.com/thu-ml/Causal-Forcing",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.",
      "ai_keywords": [
        "video diffusion models",
        "autoregressive models",
        "causal attention",
        "bidirectional attention",
        "ODE distillation",
        "frame-level injectivity",
        "PF-ODE",
        "conditional-expectation solution",
        "Self Forcing"
      ],
      "githubStars": 34,
      "organization": {
        "_id": "640d3084536d9fe0f005cac3",
        "name": "thu-ml",
        "fullname": "Tsinghua Machine Learning Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
      }
    },
    "publishedAt": "2026-02-02T10:19:22.000Z",
    "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
    "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c269a52d73768f07ac266c/X-owg4eHQoOPFuw4OydU4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c269a52d73768f07ac266c",
      "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
      "fullname": "Zhu Hongzhou",
      "name": "zhuhz22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "640d3084536d9fe0f005cac3",
      "name": "thu-ml",
      "fullname": "Tsinghua Machine Learning Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01851",
      "authors": [
        {
          "_id": "69816b37ce18b18628095fa3",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa4",
          "name": "Xuehai Bai",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa5",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa6",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa7",
          "name": "Haochen Tian",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa8",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fa9",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095faa",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fab",
          "name": "Anna Korhonen",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fac",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fad",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "69816b37ce18b18628095fae",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T09:24:45.000Z",
      "submittedOnDailyAt": "2026-02-03T01:23:51.715Z",
      "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
      "submittedOnDailyBy": {
        "_id": "65e816bbcfd12cd15b052a0e",
        "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
        "isPro": false,
        "fullname": "Huanyu_Zhang",
        "user": "huanyu112",
        "type": "user"
      },
      "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.",
      "upvotes": 13,
      "discussionId": "69816b37ce18b18628095faf",
      "ai_summary": "Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.",
      "ai_keywords": [
        "generative models",
        "image editing",
        "visual instruction following",
        "deictic grounding",
        "morphological manipulation",
        "causal reasoning",
        "LMM-as-a-judge evaluation framework",
        "task-specific metrics",
        "visual instruction benchmark"
      ]
    },
    "publishedAt": "2026-02-02T04:24:45.000Z",
    "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
    "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e816bbcfd12cd15b052a0e",
      "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
      "fullname": "Huanyu_Zhang",
      "name": "huanyu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01538",
      "authors": [
        {
          "_id": "6981707cce18b18628096000",
          "name": "Youliang Zhang",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096001",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096002",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096003",
          "name": "Ziyao Huang",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096004",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096005",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096006",
          "name": "Guozhen Zhang",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096007",
          "name": "Ziqiao Peng",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096008",
          "name": "Shunkai Li",
          "hidden": false
        },
        {
          "_id": "6981707cce18b18628096009",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "6981707cce18b1862809600a",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "6981707cce18b1862809600b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6981707cce18b1862809600c",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6981707cce18b1862809600d",
          "name": "Xiu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T02:12:09.000Z",
      "submittedOnDailyAt": "2026-02-03T01:47:19.001Z",
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "submittedOnDailyBy": {
        "_id": "653e5d31ffd60206c8b64bb5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/bgztraPC27L6culMlJw4s.png",
        "isPro": false,
        "fullname": "Xinchen Zhang",
        "user": "comin",
        "type": "user"
      },
      "summary": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io",
      "upvotes": 12,
      "discussionId": "6981707cce18b1862809600e",
      "ai_summary": "A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.",
      "ai_keywords": [
        "dual-stream framework",
        "perception and interaction module",
        "audio-interaction aware generation module",
        "motion-to-video aligner",
        "grounded human-object interaction",
        "talking avatars"
      ]
    },
    "publishedAt": "2026-02-01T21:12:09.000Z",
    "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
    "summary": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01538.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653e5d31ffd60206c8b64bb5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/bgztraPC27L6culMlJw4s.png",
      "fullname": "Xinchen Zhang",
      "name": "comin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02493",
      "authors": [
        {
          "_id": "69818001ce18b186280962be",
          "name": "Zehong Ma",
          "hidden": false
        },
        {
          "_id": "69818001ce18b186280962bf",
          "name": "Ruihan Xu",
          "hidden": false
        },
        {
          "_id": "69818001ce18b186280962c0",
          "name": "Shiliang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65d851096769b3a9c9376134/swcxkLWLfBFXKlI_lySBK.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/65d851096769b3a9c9376134/lLCDig4-nr-jeQlxcEiXv.jpeg"
      ],
      "publishedAt": "2026-02-02T18:59:42.000Z",
      "submittedOnDailyAt": "2026-02-03T02:36:29.264Z",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "submittedOnDailyBy": {
        "_id": "65d851096769b3a9c9376134",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg",
        "isPro": false,
        "fullname": "ZehongMa",
        "user": "zehongma",
        "type": "user"
      },
      "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "upvotes": 11,
      "discussionId": "69818002ce18b186280962c1",
      "projectPage": "https://zehong-ma.github.io/PixelGen/",
      "githubRepo": "https://github.com/Zehong-Ma/PixelGen",
      "githubRepoAddedBy": "user",
      "ai_summary": "PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.",
      "ai_keywords": [
        "pixel diffusion",
        "perceptual supervision",
        "LPIPS loss",
        "DINO-based perceptual loss",
        "diffusion model",
        "image generation",
        "VAE",
        "latent representations"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-02-02T13:59:42.000Z",
    "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
    "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65d851096769b3a9c9376134/swcxkLWLfBFXKlI_lySBK.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65d851096769b3a9c9376134/lLCDig4-nr-jeQlxcEiXv.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d851096769b3a9c9376134",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg",
      "fullname": "ZehongMa",
      "name": "zehongma",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01541",
      "authors": [
        {
          "_id": "6981765bce18b18628096092",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096093",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096094",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096095",
          "name": "Yifan Xu",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096096",
          "name": "Jiateng Liu",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096097",
          "name": "Xinzhuo Li",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096098",
          "name": "Zhengyuan Li",
          "hidden": false
        },
        {
          "_id": "6981765bce18b18628096099",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609a",
          "name": "Yunhan Zhong",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609b",
          "name": "Fangzhou Lan",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609c",
          "name": "Jianguo Cao",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609d",
          "name": "James M. Rehg",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609e",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "6981765bce18b1862809609f",
          "name": "Ismini Lourentzou",
          "hidden": false
        },
        {
          "_id": "6981765bce18b186280960a0",
          "name": "Xu Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T02:19:50.000Z",
      "submittedOnDailyAt": "2026-02-03T01:46:12.649Z",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
      "upvotes": 11,
      "discussionId": "6981765bce18b186280960a1",
      "projectPage": "https://pediamedai.com/Cognition-MLLM/cogsense/",
      "githubRepo": "https://github.com/PediaMedAI/Cognition-MLLM",
      "githubRepoAddedBy": "user",
      "ai_summary": "MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Chain-of-Thought reasoning",
        "visual reasoning",
        "Latent Visual Imagery Prediction",
        "reinforcement learning",
        "visual cognitive latent embeddings",
        "vision-based internal reasoning chains",
        "CogSense-Bench",
        "visual question answering"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-01T21:19:50.000Z",
    "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01335",
      "authors": [
        {
          "_id": "6981747ece18b1862809604c",
          "name": "Yu Xu",
          "hidden": false
        },
        {
          "_id": "6981747ece18b1862809604d",
          "name": "Yuxin Zhang",
          "hidden": false
        },
        {
          "_id": "6981747ece18b1862809604e",
          "name": "Juan Cao",
          "hidden": false
        },
        {
          "_id": "6981747ece18b1862809604f",
          "name": "Lin Gao",
          "hidden": false
        },
        {
          "_id": "6981747ece18b18628096050",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "6981747ece18b18628096051",
          "name": "Oliver Deussen",
          "hidden": false
        },
        {
          "_id": "6981747ece18b18628096052",
          "name": "Tong-Yee Lee",
          "hidden": false
        },
        {
          "_id": "6981747ece18b18628096053",
          "name": "Fan Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T17:01:36.000Z",
      "submittedOnDailyAt": "2026-02-03T01:41:31.533Z",
      "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
      "submittedOnDailyBy": {
        "_id": "63733e12c1142685da81d167",
        "avatarUrl": "/avatars/20a1d0a0cc8ed180a5609c6fe7fe51a1.svg",
        "isPro": false,
        "fullname": "Yu Xu",
        "user": "YUXU915",
        "type": "user"
      },
      "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.",
      "upvotes": 11,
      "discussionId": "6981747fce18b18628096054",
      "ai_summary": "Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.",
      "ai_keywords": [
        "visual metaphor transfer",
        "conceptual blending theory",
        "schema grammar",
        "multi-agent framework",
        "cross-domain semantic fusion",
        "abstract logic",
        "creative essence",
        "perception agent",
        "transfer agent",
        "generation agent",
        "diagnostic agent"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2026-02-01T12:01:36.000Z",
    "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
    "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63733e12c1142685da81d167",
      "avatarUrl": "/avatars/20a1d0a0cc8ed180a5609c6fe7fe51a1.svg",
      "fullname": "Yu Xu",
      "name": "YUXU915",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02486",
      "authors": [
        {
          "_id": "69817938ce18b186280960da",
          "name": "Jialiang Zhu",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960db",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960dc",
          "name": "Xiaolong Ma",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960dd",
          "name": "Lin Xu",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960de",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960df",
          "name": "Ruiqi Yang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e0",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e1",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e2",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e3",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e4",
          "name": "Ruichun Ma",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e5",
          "name": "Bei Liu",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e6",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e7",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e8",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960e9",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960ea",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960eb",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960ec",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "69817938ce18b186280960ed",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T18:58:07.000Z",
      "submittedOnDailyAt": "2026-02-03T02:29:31.317Z",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "submittedOnDailyBy": {
        "_id": "64ba761efec50c890051c903",
        "avatarUrl": "/avatars/4660b45d122f7430ec98267daccede87.svg",
        "isPro": false,
        "fullname": "Jialiang Zhu",
        "user": "JialiangZhu",
        "type": "user"
      },
      "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "upvotes": 9,
      "discussionId": "69817938ce18b186280960ee",
      "githubRepo": "https://github.com/microsoft/InfoAgent",
      "githubRepoAddedBy": "user",
      "ai_summary": "Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.",
      "ai_keywords": [
        "ReAct framework",
        "cross-trajectory exploration",
        "structured state representation",
        "iterative reflection",
        "globally informed planning",
        "supervised fine-tuning",
        "tool calls",
        "token usage"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-02T13:58:07.000Z",
    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
    "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02486.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba761efec50c890051c903",
      "avatarUrl": "/avatars/4660b45d122f7430ec98267daccede87.svg",
      "fullname": "Jialiang Zhu",
      "name": "JialiangZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02092",
      "authors": [
        {
          "_id": "698180bdce18b186280962c3",
          "name": "FSVideo Team",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c4",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c5",
          "name": "Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c6",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c7",
          "name": "Xinwei Huang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c8",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962c9",
          "name": "Minxuan Lin",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962ca",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962cb",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962cc",
          "name": "Chongyang Ma",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962cd",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962ce",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962cf",
          "name": "Yaojie Shen",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d0",
          "name": "Fuwen Tan",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d1",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d2",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d3",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d4",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d5",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "698180bdce18b186280962d6",
          "name": "Yuxin Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hj-L8mfMSkr7frxzS3GaE.mp4"
      ],
      "publishedAt": "2026-02-02T13:37:38.000Z",
      "submittedOnDailyAt": "2026-02-03T02:33:54.492Z",
      "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space (64times64times4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.",
      "upvotes": 9,
      "discussionId": "698180bdce18b186280962d7",
      "ai_summary": "FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.",
      "ai_keywords": [
        "video autoencoder",
        "diffusion transformer",
        "DIT",
        "layer memory design",
        "inter-layer information flow",
        "context reuse",
        "multi-resolution generation",
        "DIT upsampler",
        "image-to-video diffusion"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-02-02T08:37:38.000Z",
    "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
    "summary": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space (64times64times4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hj-L8mfMSkr7frxzS3GaE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02092.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 226,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02472",
      "authors": [
        {
          "_id": "69819059ce18b186280963b8",
          "name": "Qifan Yu",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963b9",
          "name": "Xinyu Ma",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963ba",
          "name": "Zhijian Zhuo",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963bb",
          "name": "Minrui Wang",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963bc",
          "name": "Deyi Liu",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963bd",
          "name": "Shiyi Zhan",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963be",
          "name": "Yiyuan Ma",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963bf",
          "name": "Liang Xiang",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963c0",
          "name": "Xingyan Bin",
          "hidden": false
        },
        {
          "_id": "69819059ce18b186280963c1",
          "name": "Di He",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T18:52:52.000Z",
      "submittedOnDailyAt": "2026-02-03T03:37:34.840Z",
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "submittedOnDailyBy": {
        "_id": "64b8ca3c5067873176d4b436",
        "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
        "isPro": false,
        "fullname": "AngLv",
        "user": "AngLv",
        "type": "user"
      },
      "summary": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion.",
      "upvotes": 8,
      "discussionId": "69819059ce18b186280963c2",
      "ai_summary": "SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.",
      "ai_keywords": [
        "progressive learning",
        "width expansion",
        "training instabilities",
        "activation statistics",
        "loss spikes",
        "gradient symmetry",
        "signal preservation",
        "RMS-scale consistency",
        "asymmetric optimizer state resetting",
        "learning rate re-warmup",
        "Mixture-of-Experts",
        "computational savings"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-02-02T13:52:52.000Z",
    "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
    "summary": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02472.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02227",
      "authors": [
        {
          "_id": "69817830ce18b186280960c7",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960c8",
          "name": "Xinxiang Yin",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960c9",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960ca",
          "name": "Hongfei Zhang",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960cb",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960cc",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960cd",
          "name": "Litao Guo",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960ce",
          "name": "Qifeng Chen",
          "hidden": false
        },
        {
          "_id": "69817830ce18b186280960cf",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T15:29:48.000Z",
      "submittedOnDailyAt": "2026-02-03T02:48:40.430Z",
      "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
      "submittedOnDailyBy": {
        "_id": "6581a9e2e4bcbca0322e3608",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png",
        "isPro": false,
        "fullname": "Xianfeng Wu",
        "user": "Beckham808",
        "type": "user"
      },
      "summary": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by 44% and token consumption by 51%; and (IV) exhibits 71% cognitive alignment with human intuition on reasoning invocation.",
      "upvotes": 8,
      "discussionId": "69817830ce18b186280960d0",
      "ai_summary": "LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.",
      "ai_keywords": [
        "text-to-image generation",
        "latent reasoning",
        "visual memory",
        "latent thoughts",
        "actionable guidance",
        "image token predictions",
        "reinforcement learning",
        "cognitive alignment",
        "Janus-Pro",
        "GenEval",
        "T2I-CompBench",
        "WISE",
        "IPV-Txt",
        "TwiG"
      ],
      "organization": {
        "_id": "63355133edc1a61aecf74b0e",
        "name": "HKUST",
        "fullname": "HKUST"
      }
    },
    "publishedAt": "2026-02-02T10:29:48.000Z",
    "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
    "summary": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by 44% and token consumption by 51%; and (IV) exhibits 71% cognitive alignment with human intuition on reasoning invocation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6581a9e2e4bcbca0322e3608",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png",
      "fullname": "Xianfeng Wu",
      "name": "Beckham808",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63355133edc1a61aecf74b0e",
      "name": "HKUST",
      "fullname": "HKUST"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01479",
      "authors": [
        {
          "_id": "69816fa4ce18b18628095fe0",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe1",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe2",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe3",
          "name": "Mingzi Song",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe4",
          "name": "Mingyang Jiang",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe5",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe6",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe7",
          "name": "Taiki Hara",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe8",
          "name": "Yuqing Guo",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fe9",
          "name": "Jimin Huang",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095fea",
          "name": "Junichi Tsujii",
          "hidden": false
        },
        {
          "_id": "69816fa4ce18b18628095feb",
          "name": "Sophia Ananiadou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T23:02:21.000Z",
      "submittedOnDailyAt": "2026-02-03T01:18:19.635Z",
      "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
      "submittedOnDailyBy": {
        "_id": "63a0c0803c8841cfe2cd1f15",
        "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
        "isPro": false,
        "fullname": "Xueqing Peng",
        "user": "Xueqing",
        "type": "user"
      },
      "summary": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
      "upvotes": 7,
      "discussionId": "69816fa4ce18b18628095fec",
      "ai_summary": "A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities.",
      "ai_keywords": [
        "implicit commitment",
        "refusal recognition",
        "investor-facing Q&A",
        "hierarchical extraction",
        "nested financial terminology",
        "professional disclosures",
        "language model evaluation",
        "linguistic structure",
        "cultural norms",
        "high-context communication"
      ],
      "organization": {
        "_id": "658f4413674349122c0708e9",
        "name": "TheFinAI",
        "fullname": "The Fin AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
      }
    },
    "publishedAt": "2026-02-01T18:02:21.000Z",
    "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
    "summary": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a0c0803c8841cfe2cd1f15",
      "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
      "fullname": "Xueqing Peng",
      "name": "Xueqing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "658f4413674349122c0708e9",
      "name": "TheFinAI",
      "fullname": "The Fin AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02156",
      "authors": [
        {
          "_id": "69817c5ece18b18628096138",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "69817c5ece18b18628096139",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "69817c5ece18b1862809613a",
          "name": "Rui-Jie Zhu",
          "hidden": false
        },
        {
          "_id": "69817c5ece18b1862809613b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "69817c5ece18b1862809613c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "69817c5ece18b1862809613d",
          "name": "Harry Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T14:32:57.000Z",
      "submittedOnDailyAt": "2026-02-03T02:38:39.540Z",
      "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
      "submittedOnDailyBy": {
        "_id": "6570450a78d7aca0c361a177",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
        "isPro": false,
        "fullname": "Harold Chen",
        "user": "Harold328",
        "type": "user"
      },
      "summary": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
      "upvotes": 5,
      "discussionId": "69817c5ece18b1862809613e",
      "ai_summary": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.",
      "ai_keywords": [
        "vision transformers",
        "ARC-AGI benchmark",
        "feed-forward architecture",
        "weight-tied recurrence",
        "Hybrid Block",
        "local convolutions",
        "global attention",
        "latent chain of thought",
        "predictive entropy",
        "Dynamic Exit mechanism",
        "adaptive iterative computation"
      ]
    },
    "publishedAt": "2026-02-02T09:32:57.000Z",
    "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
    "summary": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6570450a78d7aca0c361a177",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/MX7jHhTQwLs-BvYIu5rqb.jpeg",
      "fullname": "Harold Chen",
      "name": "Harold328",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01660",
      "authors": [
        {
          "_id": "6981725bce18b1862809602a",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6981725bce18b1862809602b",
          "name": "Caijun Xu",
          "hidden": false
        },
        {
          "_id": "6981725bce18b1862809602c",
          "name": "Changyi Xiao",
          "hidden": false
        },
        {
          "_id": "6981725bce18b1862809602d",
          "name": "Shibo Hong",
          "hidden": false
        },
        {
          "_id": "6981725bce18b1862809602e",
          "name": "Eli Zhang",
          "hidden": false
        },
        {
          "_id": "6981725bce18b1862809602f",
          "name": "Stephen Huang",
          "hidden": false
        },
        {
          "_id": "6981725bce18b18628096030",
          "name": "Yixin Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T05:28:26.000Z",
      "submittedOnDailyAt": "2026-02-03T02:23:33.137Z",
      "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
      "submittedOnDailyBy": {
        "_id": "63299f93688ad82b783aaf20",
        "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
        "isPro": false,
        "fullname": "zhongyuan peng",
        "user": "happzy2633",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.",
      "upvotes": 4,
      "discussionId": "6981725cce18b18628096031",
      "githubRepo": "https://github.com/ALEX-nlp/CoDiQ",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.",
      "ai_keywords": [
        "Large Reasoning Models",
        "test-time scaling",
        "controllable difficulty control",
        "CoDiQ-Generator",
        "CoDiQ-Corpus",
        "question generation",
        "reasoning performance",
        "competition-level questions"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "6384ee7fdfffab482400b938",
        "name": "m-a-p",
        "fullname": "Multimodal Art Projection",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6382252f54421460665ec501/oNH4MDqpSiMWJpxbaSLOv.png"
      }
    },
    "publishedAt": "2026-02-02T00:28:26.000Z",
    "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
    "summary": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01660.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63299f93688ad82b783aaf20",
      "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
      "fullname": "zhongyuan peng",
      "name": "happzy2633",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6384ee7fdfffab482400b938",
      "name": "m-a-p",
      "fullname": "Multimodal Art Projection",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6382252f54421460665ec501/oNH4MDqpSiMWJpxbaSLOv.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01382",
      "authors": [
        {
          "_id": "698170b6ce18b1862809601d",
          "name": "Fu-Yun Wang",
          "hidden": false
        },
        {
          "_id": "698170b6ce18b1862809601e",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "698170b6ce18b1862809601f",
          "name": "Michael Gharbi",
          "hidden": false
        },
        {
          "_id": "698170b6ce18b18628096020",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "698170b6ce18b18628096021",
          "name": "Taesung Park",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T18:31:06.000Z",
      "submittedOnDailyAt": "2026-02-03T01:27:31.305Z",
      "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
      "submittedOnDailyBy": {
        "_id": "63e9e92f20c109718713f5eb",
        "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
        "isPro": false,
        "fullname": "Fu-Yun Wang",
        "user": "wangfuyun",
        "type": "user"
      },
      "summary": "Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.\n  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.",
      "upvotes": 4,
      "discussionId": "698170b6ce18b18628096022",
      "githubRepo": "https://github.com/G-U-N/UniRL",
      "githubRepoAddedBy": "user",
      "ai_summary": "Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.",
      "ai_keywords": [
        "flow matching models",
        "text-to-image generation",
        "reinforcement learning",
        "prompt overfitting",
        "language models",
        "prompt refinement",
        "flow-based RL optimization",
        "GenEval",
        "OCR accuracy",
        "PickScore",
        "FLUX.1-Kontext",
        "EditReward",
        "UniRL"
      ],
      "githubStars": 42,
      "organization": {
        "_id": "62a9ed9212b1efd0454bc4ce",
        "name": "CUHK",
        "fullname": "CUHK"
      }
    },
    "publishedAt": "2026-02-01T13:31:06.000Z",
    "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
    "summary": "Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.\n  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e9e92f20c109718713f5eb",
      "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
      "fullname": "Fu-Yun Wang",
      "name": "wangfuyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2892,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62a9ed9212b1efd0454bc4ce",
      "name": "CUHK",
      "fullname": "CUHK"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01058",
      "authors": [
        {
          "_id": "69816dd5ce18b18628095fc9",
          "name": "Dylan Zhang",
          "hidden": false
        },
        {
          "_id": "69816dd5ce18b18628095fca",
          "name": "Yufeng Xu",
          "hidden": false
        },
        {
          "_id": "69816dd5ce18b18628095fcb",
          "name": "Haojin Wang",
          "hidden": false
        },
        {
          "_id": "69816dd5ce18b18628095fcc",
          "name": "Qingzhi Chen",
          "hidden": false
        },
        {
          "_id": "69816dd5ce18b18628095fcd",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T06:53:45.000Z",
      "submittedOnDailyAt": "2026-02-03T02:43:14.735Z",
      "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "642b8add48f67b6f21d4eb20",
        "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
        "isPro": true,
        "fullname": "Dylan",
        "user": "shizhuo2",
        "type": "user"
      },
      "summary": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.",
      "upvotes": 4,
      "discussionId": "69816dd6ce18b18628095fce",
      "ai_summary": "Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.",
      "ai_keywords": [
        "supervised fine-tuning",
        "reinforcement learning",
        "policy evaluation",
        "importance sampling",
        "loss re-weighting",
        "post-training",
        "distribution mismatch",
        "verifiable reasoning",
        "mathematical reasoning",
        "Qwen",
        "DeepSeek"
      ]
    },
    "publishedAt": "2026-02-01T01:53:45.000Z",
    "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
    "summary": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b8add48f67b6f21d4eb20",
      "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
      "fullname": "Dylan",
      "name": "shizhuo2",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.20613",
      "authors": [
        {
          "_id": "697b7fb501afa04094ca0877",
          "name": "Kaiyuan Chen",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0878",
          "name": "Qimin Wu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0879",
          "name": "Taiyu Hou",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087a",
          "name": "Tianhao Tang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087b",
          "name": "Xueyu Hu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087c",
          "name": "Yuchen Hou",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087d",
          "name": "Bikun Li",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087e",
          "name": "Chengming Qian",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca087f",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0880",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0881",
          "name": "Haotong Tian",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0882",
          "name": "Haoye Zhang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0883",
          "name": "Haoyu Bian",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0884",
          "name": "Hongbing Pan",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0885",
          "name": "Hongkang Zhang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0886",
          "name": "Hongyi Zhou",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0887",
          "name": "Jiaqi Cai",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0888",
          "name": "Jiewu Rao",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0889",
          "name": "Jiyuan Ren",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088a",
          "name": "Keduan Huang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088b",
          "name": "Lucia Zhu Huang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088c",
          "name": "Mingyu Yuan",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088d",
          "name": "Naixu Guo",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088e",
          "name": "Qicheng Tang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca088f",
          "name": "Qinyan Zhang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0890",
          "name": "Shuai Chen",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0891",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0892",
          "name": "Ting Ting Li",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0893",
          "name": "Xiaoxing Guo",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0894",
          "name": "Yaocheng Zuo",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0895",
          "name": "Yaoqi Guo",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0896",
          "name": "Yinan Wang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0897",
          "name": "Yinzhou Yu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0898",
          "name": "Yize Wang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca0899",
          "name": "Yuan Jiang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089a",
          "name": "Yuan Tian",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089b",
          "name": "Yuanshuo Zhang",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089d",
          "name": "Yvette Yan Zeng",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089e",
          "name": "Zenyu Shan",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca089f",
          "name": "Zihan Yin",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca08a0",
          "name": "Xiaobo Hu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca08a1",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca08a2",
          "name": "Yixin Ren",
          "hidden": false
        },
        {
          "_id": "697b7fb501afa04094ca08a3",
          "name": "Yuan Gong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-28T13:49:18.000Z",
      "submittedOnDailyAt": "2026-02-03T04:38:42.309Z",
      "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
      "submittedOnDailyBy": {
        "_id": "65897684f8b453e1f57cdb26",
        "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
        "isPro": false,
        "fullname": "huxueyu",
        "user": "huxueyu",
        "type": "user"
      },
      "summary": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
      "upvotes": 4,
      "discussionId": "697b7fb601afa04094ca08a4",
      "ai_summary": "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.",
      "ai_keywords": [
        "AI agents",
        "natural language instructions",
        "agent-based evaluation",
        "Open Workflow Execution",
        "Latent Instruction",
        "Iterative Refinement",
        "LLM-based verification",
        "human judgment alignment",
        "agent RL",
        "API-based agents",
        "ChatGPT agents",
        "LLM APIs",
        "open-source models",
        "agentic capabilities"
      ]
    },
    "publishedAt": "2026-01-28T08:49:18.000Z",
    "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
    "summary": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65897684f8b453e1f57cdb26",
      "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
      "fullname": "huxueyu",
      "name": "huxueyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01511",
      "authors": [
        {
          "_id": "698183a2ce18b18628096306",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b18628096307",
          "name": "Tianci Liu",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b18628096308",
          "name": "Zihan Dong",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b18628096309",
          "name": "Tony You",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b1862809630a",
          "name": "Ilgee Hong",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b1862809630b",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b1862809630c",
          "name": "Linjun Zhang",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b1862809630d",
          "name": "Tao Zhao",
          "hidden": false
        },
        {
          "_id": "698183a2ce18b1862809630e",
          "name": "Haoyu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T00:50:53.000Z",
      "submittedOnDailyAt": "2026-02-03T02:42:36.655Z",
      "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
      "submittedOnDailyBy": {
        "_id": "64bf811d76a6e2efcceabc00",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bf811d76a6e2efcceabc00/0p3zSIVqzoME25Zmfh7SD.png",
        "isPro": false,
        "fullname": "Tianci Liu",
        "user": "lliutianc",
        "type": "user"
      },
      "summary": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
      "upvotes": 3,
      "discussionId": "698183a2ce18b1862809630f",
      "ai_summary": "Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.",
      "ai_keywords": [
        "reward models",
        "reinforcement learning",
        "preference feedback",
        "rubric generator",
        "judge",
        "alternating optimization",
        "non-stationarity",
        "gradient variance",
        "policy alignment",
        "offline reinforcement learning",
        "online reinforcement learning"
      ],
      "organization": {
        "_id": "68e706da311f55603f9b6f2f",
        "name": "OpenRubrics",
        "fullname": "OpenRubrics"
      }
    },
    "publishedAt": "2026-02-01T19:50:53.000Z",
    "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
    "summary": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bf811d76a6e2efcceabc00",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bf811d76a6e2efcceabc00/0p3zSIVqzoME25Zmfh7SD.png",
      "fullname": "Tianci Liu",
      "name": "lliutianc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68e706da311f55603f9b6f2f",
      "name": "OpenRubrics",
      "fullname": "OpenRubrics"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01296",
      "authors": [
        {
          "_id": "69817aa8ce18b18628096112",
          "name": "Zeran Ke",
          "hidden": false
        },
        {
          "_id": "69817aa8ce18b18628096113",
          "name": "Bin Tan",
          "hidden": false
        },
        {
          "_id": "69817aa8ce18b18628096114",
          "name": "Gui-Song Xia",
          "hidden": false
        },
        {
          "_id": "69817aa8ce18b18628096115",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "69817aa8ce18b18628096116",
          "name": "Nan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T15:52:55.000Z",
      "submittedOnDailyAt": "2026-02-03T02:04:47.295Z",
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "submittedOnDailyBy": {
        "_id": "6485ce5ec7f19728a49df17a",
        "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
        "isPro": true,
        "fullname": "Nan",
        "user": "cherubicxn",
        "type": "user"
      },
      "summary": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.",
      "upvotes": 3,
      "discussionId": "69817aa8ce18b18628096117",
      "githubRepo": "https://github.com/calmke/LiPMAP",
      "githubRepoAddedBy": "user",
      "ai_summary": "LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.",
      "ai_keywords": [
        "3D line mapping",
        "planar patch",
        "line-plane joint optimization",
        "learnable primitives",
        "structured reconstruction",
        "visual localization"
      ],
      "githubStars": 6
    },
    "publishedAt": "2026-02-01T10:52:55.000Z",
    "title": "Interacted Planes Reveal 3D Line Mapping",
    "summary": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01296.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485ce5ec7f19728a49df17a",
      "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
      "fullname": "Nan",
      "name": "cherubicxn",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01077",
      "authors": [
        {
          "_id": "69818c43ce18b186280963a7",
          "name": "Haopeng Li",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963a8",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963a9",
          "name": "Wenliang Zhong",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963aa",
          "name": "Zikai Zhou",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963ab",
          "name": "Lichen Bai",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963ac",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "69818c43ce18b186280963ad",
          "name": "Zeke Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T07:47:06.000Z",
      "submittedOnDailyAt": "2026-02-03T03:21:01.987Z",
      "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64e86fbd0c2413c3571ef7a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e86fbd0c2413c3571ef7a6/KDpJ0UpjICfQKUr14ekcR.png",
        "isPro": false,
        "fullname": "Haopeng Li",
        "user": "hp-l33",
        "type": "user"
      },
      "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.",
      "upvotes": 3,
      "discussionId": "69818c43ce18b186280963ae",
      "githubRepo": "https://github.com/xie-lab-ml/piecewise-sparse-attention",
      "githubRepoAddedBy": "user",
      "ai_summary": "PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.",
      "ai_keywords": [
        "diffusion transformers",
        "attention",
        "block sparse attention",
        "sparse attention",
        "piecewise sparse attention",
        "block-wise Taylor expansion",
        "attention scores",
        "quadratic complexity",
        "computational efficiency",
        "visual quality"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "65ad19cac14c3cf579ad9b68",
        "name": "HKUSTGZ",
        "fullname": "HKUSTGZ"
      }
    },
    "publishedAt": "2026-02-01T02:47:06.000Z",
    "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
    "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e86fbd0c2413c3571ef7a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e86fbd0c2413c3571ef7a6/KDpJ0UpjICfQKUr14ekcR.png",
      "fullname": "Haopeng Li",
      "name": "hp-l33",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65ad19cac14c3cf579ad9b68",
      "name": "HKUSTGZ",
      "fullname": "HKUSTGZ"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.02110",
      "authors": [
        {
          "_id": "69817ffdce18b186280962b6",
          "name": "Zhongqian Fu",
          "hidden": false
        },
        {
          "_id": "69817ffdce18b186280962b7",
          "name": "Tianyi Zhao",
          "hidden": false
        },
        {
          "_id": "69817ffdce18b186280962b8",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "69817ffdce18b186280962b9",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "69817ffdce18b186280962ba",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "69817ffdce18b186280962bb",
          "name": "Yunhe Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T13:54:03.000Z",
      "submittedOnDailyAt": "2026-02-03T04:04:17.992Z",
      "title": "An Empirical Study of World Model Quantization",
      "submittedOnDailyBy": {
        "_id": "65e52e7d27dc8aa470a640e3",
        "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg",
        "isPro": false,
        "fullname": "hankai",
        "user": "hankaixyz",
        "type": "user"
      },
      "summary": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.",
      "upvotes": 2,
      "discussionId": "69817ffdce18b186280962bc",
      "githubRepo": "https://github.com/huawei-noah/noah-research/tree/master/QuantWM",
      "githubRepoAddedBy": "user",
      "ai_summary": "Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.",
      "ai_keywords": [
        "world models",
        "post-training quantization",
        "DINO-WM",
        "weight-only quantization",
        "joint weight-activation quantization",
        "quantization granularity",
        "planning horizon",
        "encoder module",
        "predictor module",
        "quantization sensitivity",
        "low-bit rollouts"
      ],
      "githubStars": 929,
      "organization": {
        "_id": "5f83c275f0801648bf88454a",
        "name": "huawei-noah",
        "fullname": "HUAWEI Noah's Ark Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
      }
    },
    "publishedAt": "2026-02-02T08:54:03.000Z",
    "title": "An Empirical Study of World Model Quantization",
    "summary": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e52e7d27dc8aa470a640e3",
      "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg",
      "fullname": "hankai",
      "name": "hankaixyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5f83c275f0801648bf88454a",
      "name": "huawei-noah",
      "fullname": "HUAWEI Noah's Ark Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.00759",
      "authors": [
        {
          "_id": "69816d99ce18b18628095fba",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "69816d99ce18b18628095fbb",
          "name": "Xiaobo Qin",
          "hidden": false
        },
        {
          "_id": "69816d99ce18b18628095fbc",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "69816d99ce18b18628095fbd",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "69816d99ce18b18628095fbe",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-31T14:48:23.000Z",
      "submittedOnDailyAt": "2026-02-03T01:11:48.137Z",
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "629b765ce1af194c641fcbc6",
        "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
        "isPro": false,
        "fullname": "Zhipeng Chen",
        "user": "TimothyCzp",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.",
      "upvotes": 2,
      "discussionId": "69816d99ce18b18628095fbf",
      "ai_summary": "Adaptive Ability Decomposing (AD) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "large language models",
        "blind exploration",
        "adaptive ability decomposing",
        "decomposer",
        "sub-questions",
        "reasoner",
        "plug-and-play module",
        "exploration",
        "exploitation"
      ],
      "organization": {
        "_id": "6704ef33935b1a7c59795566",
        "name": "RUC-AIBOX",
        "fullname": "RUC-AIBOX",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"
      }
    },
    "publishedAt": "2026-01-31T09:48:23.000Z",
    "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629b765ce1af194c641fcbc6",
      "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
      "fullname": "Zhipeng Chen",
      "name": "TimothyCzp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6704ef33935b1a7c59795566",
      "name": "RUC-AIBOX",
      "fullname": "RUC-AIBOX",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22588",
      "authors": [
        {
          "_id": "698170a6ce18b18628096010",
          "name": "Zhuochun Li",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096011",
          "name": "Yong Zhang",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096012",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096013",
          "name": "Yuelyu Ji",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096014",
          "name": "Yiming Zeng",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096015",
          "name": "Ning Cheng",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096016",
          "name": "Yun Zhu",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096017",
          "name": "Yanmeng Wang",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096018",
          "name": "Shaojun Wang",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b18628096019",
          "name": "Jing Xiao",
          "hidden": false
        },
        {
          "_id": "698170a6ce18b1862809601a",
          "name": "Daqing He",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T05:34:24.000Z",
      "submittedOnDailyAt": "2026-02-03T01:22:01.358Z",
      "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
      "submittedOnDailyBy": {
        "_id": "65543181ace6c512cf829cf9",
        "avatarUrl": "/avatars/8360cb3e43ed25dc55d77c900ece1a0a.svg",
        "isPro": false,
        "fullname": "ZHANG YONG",
        "user": "ReRaWo",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \"LLM-as-a-Judge\" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.",
      "upvotes": 2,
      "discussionId": "698170a6ce18b1862809601b",
      "githubRepo": "https://github.com/zhuochunli/Representation-as-a-judge",
      "githubRepoAddedBy": "user",
      "ai_summary": "Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.",
      "ai_keywords": [
        "large language models",
        "small language models",
        "hidden states",
        "semantic capacity",
        "representation learning",
        "LLM-as-a-Judge",
        "Representation-as-a-Judge",
        "probing-based framework",
        "aspect-level evaluation",
        "internal representations"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-01-30T00:34:24.000Z",
    "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
    "summary": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \"LLM-as-a-Judge\" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65543181ace6c512cf829cf9",
      "avatarUrl": "/avatars/8360cb3e43ed25dc55d77c900ece1a0a.svg",
      "fullname": "ZHANG YONG",
      "name": "ReRaWo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.00130",
      "authors": [
        {
          "_id": "6981796cce18b18628096101",
          "name": "Sumit Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-28T04:33:41.000Z",
      "submittedOnDailyAt": "2026-02-03T01:59:14.260Z",
      "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
      "submittedOnDailyBy": {
        "_id": "60efd27e75db42c7e471b5d2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667483621462-60efd27e75db42c7e471b5d2.jpeg",
        "isPro": false,
        "fullname": "Sumit Yadav",
        "user": "rockerritesh",
        "type": "user"
      },
      "summary": "We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.",
      "upvotes": 2,
      "discussionId": "6981796dce18b18628096102",
      "ai_summary": "Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.",
      "ai_keywords": [
        "effective dimension",
        "representation geometry",
        "neural network performance",
        "pretrained ImageNet models",
        "architecture families",
        "unsupervised geometric metric",
        "output effective dimension",
        "total compression",
        "causal relationship",
        "noise types",
        "PCA"
      ]
    },
    "publishedAt": "2026-01-27T23:33:41.000Z",
    "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
    "summary": "We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60efd27e75db42c7e471b5d2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667483621462-60efd27e75db42c7e471b5d2.jpeg",
      "fullname": "Sumit Yadav",
      "name": "rockerritesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01983",
      "authors": [
        {
          "_id": "69816ddece18b18628095fd0",
          "name": "Xintian Shen",
          "hidden": false
        },
        {
          "_id": "69816ddece18b18628095fd1",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "69816ddece18b18628095fd2",
          "name": "Lihao Zheng",
          "hidden": false
        },
        {
          "_id": "69816ddece18b18628095fd3",
          "name": "Hao Ma",
          "hidden": false
        },
        {
          "_id": "69816ddece18b18628095fd4",
          "name": "Tao Wei",
          "hidden": false
        },
        {
          "_id": "69816ddece18b18628095fd5",
          "name": "Kun Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T11:37:45.000Z",
      "submittedOnDailyAt": "2026-02-03T04:03:08.159Z",
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "65dc57d684ea4abda9712710",
        "avatarUrl": "/avatars/4c7dc054e587a30c99afccf9c4cc5c0d.svg",
        "isPro": false,
        "fullname": "Jiawei Chen",
        "user": "Lost-Cloud",
        "type": "user"
      },
      "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
      "upvotes": 1,
      "discussionId": "69816ddece18b18628095fd6",
      "ai_summary": "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.",
      "ai_keywords": [
        "Tool-Integrated Reasoning",
        "language model agents",
        "tool creation",
        "self-optimization",
        "reasoning traces",
        "memory consolidation",
        "automated tool construction",
        "self-evolving capability"
      ],
      "organization": {
        "_id": "692fb0e4ffb801494bdcc1ea",
        "name": "LiAuto-Foundation-Model",
        "fullname": "LiAuto Foundation Model",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692fa56f0c98923a5c8e3357/GRB_WF6DPgrC8O3cNcdTU.png"
      }
    },
    "publishedAt": "2026-02-02T06:37:45.000Z",
    "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
    "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65dc57d684ea4abda9712710",
      "avatarUrl": "/avatars/4c7dc054e587a30c99afccf9c4cc5c0d.svg",
      "fullname": "Jiawei Chen",
      "name": "Lost-Cloud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "692fb0e4ffb801494bdcc1ea",
      "name": "LiAuto-Foundation-Model",
      "fullname": "LiAuto Foundation Model",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692fa56f0c98923a5c8e3357/GRB_WF6DPgrC8O3cNcdTU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01842",
      "authors": [
        {
          "_id": "69817c1cce18b1862809612b",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b1862809612c",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b1862809612d",
          "name": "Yuchen Zhu",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b1862809612e",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b1862809612f",
          "name": "Qingyu Shi",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096130",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096131",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096132",
          "name": "Molei Tao",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096133",
          "name": "Jianru Xue",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096134",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "69817c1cce18b18628096135",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T09:14:51.000Z",
      "submittedOnDailyAt": "2026-02-03T02:11:03.136Z",
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
      "upvotes": 1,
      "discussionId": "69817c1cce18b18628096136",
      "githubRepo": "https://github.com/viiika/Prism",
      "githubRepoAddedBy": "user",
      "ai_summary": "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.",
      "ai_keywords": [
        "discrete diffusion language models",
        "test-time scaling",
        "autoregressive decoding",
        "denoising window",
        "hierarchical trajectory search",
        "local branching",
        "partial remasking",
        "self-verified feedback",
        "mathematical reasoning",
        "code generation"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-02-02T04:14:51.000Z",
    "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
    "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22674",
      "authors": [
        {
          "_id": "69803d8d6676f9332270666c",
          "user": {
            "_id": "652d3023cdb2a91205709b6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652d3023cdb2a91205709b6a/x-MaRkBwSdZ0xIYDJg09u.jpeg",
            "isPro": false,
            "fullname": "Hanxun Yu",
            "user": "JonnyYu828",
            "type": "user"
          },
          "name": "Hanxun Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T16:52:23.428Z",
          "hidden": false
        },
        {
          "_id": "69803d8d6676f9332270666d",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "69803d8d6676f9332270666e",
          "name": "Xuan Qu",
          "hidden": false
        },
        {
          "_id": "69803d8d6676f9332270666f",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "69803d8d6676f93322706670",
          "name": "Junbo Chen",
          "hidden": false
        },
        {
          "_id": "69803d8d6676f93322706671",
          "name": "Jianke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T07:45:48.000Z",
      "submittedOnDailyAt": "2026-02-03T04:37:20.898Z",
      "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
      "submittedOnDailyBy": {
        "_id": "652d3023cdb2a91205709b6a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652d3023cdb2a91205709b6a/x-MaRkBwSdZ0xIYDJg09u.jpeg",
        "isPro": false,
        "fullname": "Hanxun Yu",
        "user": "JonnyYu828",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
      "upvotes": 1,
      "discussionId": "69803d8d6676f93322706672",
      "ai_summary": "VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.",
      "ai_keywords": [
        "Multimodal large language models",
        "visual tokens",
        "token reduction",
        "Dominant Vision Token Selection",
        "Text-Guided Vision Complement",
        "training-free",
        "multimodal benchmarks"
      ],
      "organization": {
        "_id": "6345aadf5efccdc07f1365a5",
        "name": "ZhejiangUniversity",
        "fullname": "Zhejiang University"
      }
    },
    "publishedAt": "2026-01-30T02:45:48.000Z",
    "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
    "summary": "Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652d3023cdb2a91205709b6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652d3023cdb2a91205709b6a/x-MaRkBwSdZ0xIYDJg09u.jpeg",
      "fullname": "Hanxun Yu",
      "name": "JonnyYu828",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.21968",
      "authors": [
        {
          "_id": "69818b62ce18b1862809639b",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b1862809639c",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b1862809639d",
          "name": "Shansan Gong",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b1862809639e",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b1862809639f",
          "name": "Jianghan Shen",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b186280963a0",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b186280963a1",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b186280963a2",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b186280963a3",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "69818b62ce18b186280963a4",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T16:48:14.000Z",
      "submittedOnDailyAt": "2026-02-03T03:15:43.476Z",
      "title": "OVD: On-policy Verbal Distillation",
      "submittedOnDailyBy": {
        "_id": "60851545a5da133ac6c38686",
        "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
        "isPro": false,
        "fullname": "Jing Xiong",
        "user": "menik1126",
        "type": "user"
      },
      "summary": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
      "upvotes": 1,
      "discussionId": "69818b63ce18b186280963a5",
      "ai_summary": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.",
      "ai_keywords": [
        "knowledge distillation",
        "token-level probability matching",
        "trajectory matching",
        "discrete verbal scores",
        "on-policy distillation",
        "memory efficiency",
        "reinforcement learning",
        "token-level alignment",
        "student models",
        "teacher models"
      ]
    },
    "publishedAt": "2026-01-29T11:48:14.000Z",
    "title": "OVD: On-policy Verbal Distillation",
    "summary": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21968.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60851545a5da133ac6c38686",
      "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
      "fullname": "Jing Xiong",
      "name": "menik1126",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21759",
      "authors": [
        {
          "_id": "697c374ba67238fac88cc1ab",
          "user": {
            "_id": "649e9dbdb24e556c33b9e508",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
            "isPro": false,
            "fullname": "Meet Doshi",
            "user": "meetdoshi90",
            "type": "user"
          },
          "name": "Meet Doshi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-30T09:35:37.417Z",
          "hidden": false
        },
        {
          "_id": "697c374ba67238fac88cc1ac",
          "user": {
            "_id": "62c45be1143622c9278f1ae2",
            "avatarUrl": "/avatars/0ddc94e2e8428334f85f13e6f08a66c2.svg",
            "isPro": false,
            "fullname": "vishwajeet kumar",
            "user": "vishwajeetkumar",
            "type": "user"
          },
          "name": "Vishwajeet Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T16:59:33.053Z",
          "hidden": false
        },
        {
          "_id": "697c374ba67238fac88cc1ad",
          "name": "Yulong Li",
          "hidden": false
        },
        {
          "_id": "697c374ba67238fac88cc1ae",
          "name": "Jaydeep Sen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T14:14:29.000Z",
      "submittedOnDailyAt": "2026-02-03T03:36:18.961Z",
      "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
      "submittedOnDailyBy": {
        "_id": "649e9dbdb24e556c33b9e508",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
        "isPro": false,
        "fullname": "Meet Doshi",
        "user": "meetdoshi90",
        "type": "user"
      },
      "summary": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.",
      "upvotes": 1,
      "discussionId": "697c374ba67238fac88cc1af",
      "ai_summary": "An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.",
      "ai_keywords": [
        "reinforcement learning",
        "sampling framework",
        "influence-based reward signals",
        "embedding models",
        "dense retrieval systems",
        "multilingual bge-m3",
        "all-MiniLM-L6-v2",
        "NDCG@10"
      ]
    },
    "publishedAt": "2026-01-29T09:14:29.000Z",
    "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
    "summary": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21759.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "649e9dbdb24e556c33b9e508",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
      "fullname": "Meet Doshi",
      "name": "meetdoshi90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01997",
      "authors": [
        {
          "_id": "6981a43ece18b18628096410",
          "name": "Safal Shrestha",
          "hidden": false
        },
        {
          "_id": "6981a43ece18b18628096411",
          "name": "Anubhav Shrestha",
          "hidden": false
        },
        {
          "_id": "6981a43ece18b18628096412",
          "name": "Aadim Nepal",
          "hidden": false
        },
        {
          "_id": "6981a43ece18b18628096413",
          "name": "Minwu Kim",
          "hidden": false
        },
        {
          "_id": "6981a43ece18b18628096414",
          "name": "Keith Ross",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T11:57:22.000Z",
      "submittedOnDailyAt": "2026-02-03T05:02:22.053Z",
      "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
      "submittedOnDailyBy": {
        "_id": "64cb922ec7f30fbf7b91a9a7",
        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
        "isPro": false,
        "fullname": "Safal Shrestha",
        "user": "safal312",
        "type": "user"
      },
      "summary": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
      "upvotes": 0,
      "discussionId": "6981a43fce18b18628096415",
      "githubRepo": "https://github.com/safal312/on-the-limits-of-layer-pruning",
      "githubRepoAddedBy": "user",
      "ai_summary": "Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.",
      "ai_keywords": [
        "layer pruning",
        "large language models",
        "classification benchmarks",
        "generative reasoning tasks",
        "multi-step reasoning",
        "arithmetic computation",
        "balanced parenthesis generation",
        "post-training",
        "supervised finetuning",
        "Self-Generated Responses"
      ],
      "organization": {
        "_id": "691d8e884bbe8df0d99462e2",
        "name": "newyorkuniversity",
        "fullname": "New York University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"
      }
    },
    "publishedAt": "2026-02-02T06:57:22.000Z",
    "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
    "summary": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb922ec7f30fbf7b91a9a7",
      "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
      "fullname": "Safal Shrestha",
      "name": "safal312",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "691d8e884bbe8df0d99462e2",
      "name": "newyorkuniversity",
      "fullname": "New York University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01815",
      "authors": [
        {
          "_id": "69819c02ce18b186280963e1",
          "name": "Yunhui Jang",
          "hidden": false
        },
        {
          "_id": "69819c02ce18b186280963e2",
          "name": "Seonghyun Park",
          "hidden": false
        },
        {
          "_id": "69819c02ce18b186280963e3",
          "name": "Jaehyung Kim",
          "hidden": false
        },
        {
          "_id": "69819c02ce18b186280963e4",
          "name": "Sungsoo Ahn",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T08:47:36.000Z",
      "submittedOnDailyAt": "2026-02-03T04:27:57.185Z",
      "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
      "submittedOnDailyBy": {
        "_id": "668785136c2f7efac100ffba",
        "avatarUrl": "/avatars/8dd6fe11c8a2f6da5b2486b8381944f0.svg",
        "isPro": false,
        "fullname": "Yunhui Jang",
        "user": "yunhuijang",
        "type": "user"
      },
      "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
      "upvotes": 0,
      "discussionId": "69819c03ce18b186280963e5",
      "ai_summary": "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.",
      "ai_keywords": [
        "multi-agent systems",
        "molecular discovery",
        "individualized scientist profiles",
        "publication history",
        "molecular history",
        "multi-turn debate",
        "proposal",
        "critique",
        "voting phases"
      ]
    },
    "publishedAt": "2026-02-02T03:47:36.000Z",
    "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
    "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01815.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668785136c2f7efac100ffba",
      "avatarUrl": "/avatars/8dd6fe11c8a2f6da5b2486b8381944f0.svg",
      "fullname": "Yunhui Jang",
      "name": "yunhuijang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01618",
      "authors": [
        {
          "_id": "6981704ece18b18628095fee",
          "name": "Panuthep Tasawong",
          "hidden": false
        },
        {
          "_id": "6981704ece18b18628095fef",
          "name": "Jian Gang Ngui",
          "hidden": false
        },
        {
          "_id": "6981704ece18b18628095ff0",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "6981704ece18b18628095ff1",
          "name": "Trevor Cohn",
          "hidden": false
        },
        {
          "_id": "6981704ece18b18628095ff2",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d54d4de5837f38ce8307c7/WMv8OGfLGWysGuWS1Rbfv.png"
      ],
      "publishedAt": "2026-02-02T04:20:35.000Z",
      "submittedOnDailyAt": "2026-02-03T02:12:24.094Z",
      "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
      "submittedOnDailyBy": {
        "_id": "66d54d4de5837f38ce8307c7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d54d4de5837f38ce8307c7/YRevAYHxMSpYbcP4HNhfS.jpeg",
        "isPro": false,
        "fullname": "peerat limkonchotiwat",
        "user": "mrpeerat",
        "type": "user"
      },
      "summary": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.",
      "upvotes": 0,
      "discussionId": "6981704ece18b18628095ff3",
      "ai_summary": "Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.",
      "ai_keywords": [
        ""
      ],
      "organization": {
        "_id": "62ff1dd7a92770ccd0dcd2cb",
        "name": "aisingapore",
        "fullname": "AI Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1660886913493-62ff1d57419904a14287ef84.png"
      }
    },
    "publishedAt": "2026-02-01T23:20:35.000Z",
    "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
    "summary": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d54d4de5837f38ce8307c7/WMv8OGfLGWysGuWS1Rbfv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d54d4de5837f38ce8307c7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d54d4de5837f38ce8307c7/YRevAYHxMSpYbcP4HNhfS.jpeg",
      "fullname": "peerat limkonchotiwat",
      "name": "mrpeerat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62ff1dd7a92770ccd0dcd2cb",
      "name": "aisingapore",
      "fullname": "AI Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1660886913493-62ff1d57419904a14287ef84.png"
    },
    "isAuthorParticipating": false
  }
]