[
  {
    "paper": {
      "id": "2503.24379",
      "authors": [
        {
          "_id": "67ec0a7262144ec35d0e571d",
          "name": "Shengqiong Wu",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571e",
          "name": "Weicai Ye",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571f",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5720",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5721",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5722",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5723",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5724",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5725",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5726",
          "name": "Hao Fei",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5727",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-02T01:29:41.083Z",
      "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "64c139d867eff857ea51caa8",
        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
        "isPro": false,
        "fullname": "Shengqiong Wu",
        "user": "ChocoWu",
        "type": "user"
      },
      "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
      "upvotes": 35,
      "discussionId": "67ec0a7562144ec35d0e57fc",
      "projectPage": "https://sqwu.top/Any2Cap/",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "dense, structured captions",
        "region",
        "motion",
        "camera poses",
        "any-condition-to-caption instruction tuning"
      ]
    },
    "publishedAt": "2025-03-31T13:59:01.000Z",
    "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
    "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24379.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64c139d867eff857ea51caa8",
      "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
      "fullname": "Shengqiong Wu",
      "name": "ChocoWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23145",
      "authors": [
        {
          "_id": "67eb710f1f8a09c48b2e3ba1",
          "user": {
            "_id": "674286496efe2b931f7ce354",
            "avatarUrl": "/avatars/8f920618b777dbff5f2a117ebd9e9caa.svg",
            "isPro": false,
            "fullname": "Anjiang Wei",
            "user": "anjiangwei",
            "type": "user"
          },
          "name": "Anjiang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:42.109Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba2",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba3",
          "name": "Jiannan Cao",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba4",
          "name": "Naveen Kannan",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba5",
          "name": "Yuheng Wu",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba6",
          "name": "Kai Yan",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba7",
          "name": "Thiago S. F. X. Teixeira",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba8",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba9",
          "name": "Alex Aiken",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T16:50:39.000Z",
      "submittedOnDailyAt": "2025-04-02T04:28:51.051Z",
      "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
      "upvotes": 18,
      "discussionId": "67eb71101f8a09c48b2e3bee",
      "ai_keywords": [
        "inductive program synthesis",
        "programming by example",
        "function synthesis",
        "input-output examples",
        "large language model agents",
        "natural language",
        "evaluation protocols",
        "feedback mechanism",
        "real-world scenarios",
        "reverse engineering",
        "CodeARC",
        "Code Abstraction and Reasoning Challenge",
        "hidden target function",
        "querying",
        "candidate functions",
        "differential testing oracle",
        "interactive setting",
        "function calls",
        "self-correction",
        "large-scale benchmark",
        "general-purpose inductive program synthesis",
        "fine-tuning",
        "LLaMA-3.1-8B-Instruct",
        "synthesis traces",
        "relative performance gain"
      ]
    },
    "publishedAt": "2025-03-29T12:50:39.000Z",
    "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
    "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24376",
      "authors": [
        {
          "_id": "67eca2b8351721d62aa537df",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e0",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e1",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e2",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e3",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e4",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e5",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:55:23.000Z",
      "submittedOnDailyAt": "2025-04-02T01:06:48.435Z",
      "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
      "upvotes": 17,
      "discussionId": "67eca2b9351721d62aa53822",
      "githubRepo": "https://github.com/TencentARC/SEED-Bench-R1",
      "ai_keywords": [
        "Chain of Thought (COT)",
        "Large Language Models (LLMs)",
        "reinforcement learning (RL)",
        "Multimodal Large Language Models (MLLMs)",
        "SEED-Bench-R1",
        "video understanding",
        "multiple-choice questions",
        "in-distribution",
        "cross-environment",
        "cross-environment-task scenarios",
        "Qwen2-VL-Instruct-7B",
        "supervised fine-tuning (SFT)",
        "LongVideoBench",
        "visual perception",
        "reasoning chains",
        "inconsistent reasoning",
        "reward modeling"
      ]
    },
    "publishedAt": "2025-03-31T13:55:23.000Z",
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00050",
      "authors": [
        {
          "_id": "67ec9bf3e58745dc7d652587",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652588",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652589",
          "name": "Qingyun Zou",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258a",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258b",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258c",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258d",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T02:18:51.000Z",
      "submittedOnDailyAt": "2025-04-02T04:52:20.239Z",
      "title": "JudgeLRM: Large Reasoning Models as a Judge",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
      "upvotes": 13,
      "discussionId": "67ec9bf4e58745dc7d6525c1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Supervised Fine-Tuning (SFT)",
        "reasoning capabilities",
        "reinforcement learning (RL)",
        "judge-wise, outcome-driven rewards",
        "JudgeLRM",
        "GPT-4",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-03-30T22:18:51.000Z",
    "title": "JudgeLRM: Large Reasoning Models as a Judge",
    "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00050.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00906",
      "authors": [
        {
          "_id": "67ec9340913c072638c16bbf",
          "name": "Saaket Agashe",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc0",
          "name": "Kyle Wong",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc1",
          "name": "Vincent Tu",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc2",
          "name": "Jiachen Yang",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc3",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc4",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:40:27.000Z",
      "submittedOnDailyAt": "2025-04-02T00:02:12.706Z",
      "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
      "upvotes": 10,
      "discussionId": "67ec9343913c072638c16c5f",
      "projectPage": "https://www.simular.ai/articles/agent-s2-technical-review",
      "githubRepo": "https://github.com/simular-ai/Agent-S",
      "ai_keywords": [
        "Mixture-of-Grounding",
        "Proactive Hierarchical Planning",
        "compositional framework",
        "GUI localization",
        "action plans",
        "state-of-the-art (SOTA) performance",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld"
      ]
    },
    "publishedAt": "2025-04-01T11:40:27.000Z",
    "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
    "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00906.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00810",
      "authors": [
        {
          "_id": "67eca0c4cfce948cbbbcff9a",
          "name": "Zhaojian Yu",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9b",
          "name": "Yinghao Wu",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9c",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9e",
          "name": "Xiao-Ping Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:01:50.000Z",
      "submittedOnDailyAt": "2025-04-02T01:59:40.340Z",
      "title": "Z1: Efficient Test-time Scaling with Code",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
      "upvotes": 9,
      "discussionId": "67eca0c6cfce948cbbbd0042",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "test-time computing scaling",
        "thinking tokens",
        "Z1-Code-Reasoning-107K",
        "solution trajectories",
        "Shifted Thinking Window",
        "context-delimiting tags",
        "efficient test-time scaling",
        "R1-Distill-Qwen-7B",
        "GPQA Diamond",
        "efficient reasoning elicitation"
      ]
    },
    "publishedAt": "2025-04-01T10:01:50.000Z",
    "title": "Z1: Efficient Test-time Scaling with Code",
    "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00810.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24377",
      "authors": [
        {
          "_id": "67eca439a62c82ed64354e36",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e37",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e38",
          "name": "Boyang Xue",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e39",
          "name": "Jianhui Pang",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3a",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3b",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3c",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3d",
          "name": "Derek Fai Wong",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3e",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3f",
          "name": "Kam-Fai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-02T01:17:01.812Z",
      "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "67298b338c66e235932ca088",
        "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
        "isPro": false,
        "fullname": "WANG Rui",
        "user": "Ray121381",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
      "upvotes": 9,
      "discussionId": "67eca43aa62c82ed64354e82",
      "githubRepo": "https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers"
    },
    "publishedAt": "2025-03-31T13:58:07.000Z",
    "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67298b338c66e235932ca088",
      "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
      "fullname": "WANG Rui",
      "name": "Ray121381",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01016",
      "authors": [
        {
          "_id": "67ec958ebb1d6dd924f94a31",
          "name": "Tian-Xing Xu",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a32",
          "name": "Xiangjun Gao",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a33",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a34",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a35",
          "name": "Song-Hai Zhang",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a36",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
      ],
      "publishedAt": "2025-04-01T17:58:03.000Z",
      "submittedOnDailyAt": "2025-04-02T00:15:17.585Z",
      "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
      "upvotes": 7,
      "discussionId": "67ec9593bb1d6dd924f94b3e",
      "projectPage": "https://geometrycrafter.github.io/",
      "githubRepo": "https://github.com/TencentARC/GeometryCrafter",
      "ai_keywords": [
        "GeometryCrafter",
        "point map Variational Autoencoder (VAE)",
        "latent space",
        "video latent distributions",
        "point map encoding",
        "point map decoding",
        "video diffusion model",
        "point map sequences",
        "3D accuracy",
        "temporal consistency",
        "generalization capability"
      ]
    },
    "publishedAt": "2025-04-01T13:58:03.000Z",
    "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
    "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00595",
      "authors": [
        {
          "_id": "67ecaf516560da48c5c34106",
          "name": "Weizhi Wang",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34107",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34108",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34109",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c3410a",
          "name": "Xifeng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T09:54:00.000Z",
      "submittedOnDailyAt": "2025-04-02T02:00:49.375Z",
      "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
      "upvotes": 6,
      "discussionId": "67ecaf546560da48c5c341dc",
      "projectPage": "https://victorwz.github.io/Open-Qwen2VL/",
      "githubRepo": "https://github.com/Victorwz/Open-Qwen2VL",
      "ai_keywords": [
        "multimodal LLM",
        "pre-training",
        "high-quality data filtering",
        "multimodal data mixture strategies",
        "sequence packing techniques",
        "training frameworks",
        "Open-Qwen2VL",
        "fully open-source",
        "2B-parameter",
        "image-text pairs",
        "A100-40G GPU hours",
        "low-to-high dynamic image resolution",
        "multimodal sequence packing",
        "data quality",
        "academic level 8xA100-40G GPUs",
        "UCSB",
        "packed multimodal tokens",
        "Qwen2-VL",
        "instruction-tuned",
        "MMBench",
        "SEEDBench",
        "MMstar",
        "MathVista",
        "compute-efficient",
        "FSDP-based training",
        "WebDataset format",
        "pre-training data",
        "training codebase",
        "data filtering techniques",
        "sequence packing scripts",
        "pre-training data in WebDataset format",
        "FSDP-based training codebase",
        "base and instruction"
      ]
    },
    "publishedAt": "2025-04-01T05:54:00.000Z",
    "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
    "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00509",
      "authors": [
        {
          "_id": "67ecae49a34baf018ca3c4cd",
          "user": {
            "_id": "65de7628deee79773f0f46f6",
            "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
            "isPro": false,
            "fullname": "Kai Yan",
            "user": "kaiyan289",
            "type": "user"
          },
          "name": "Kai Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-02T03:26:02.443Z",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4ce",
          "name": "Yufei Xu",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4cf",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d0",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d1",
          "name": "Zheyu Wang",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d2",
          "name": "Xiaowen Guo",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d3",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:57:58.000Z",
      "submittedOnDailyAt": "2025-04-02T01:56:35.333Z",
      "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
      "upvotes": 6,
      "discussionId": "67ecae4aa34baf018ca3c506",
      "projectPage": "https://team.doubao.com/zh/publication/recitation-over-reasoning-how-cutting-edge-language-models-can-fail-on-elementary-school-level-reasoning-problems?view_from=homepage_recommend",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "RoR-Bench",
        "multi-modal benchmark",
        "recitation behavior",
        "elementary school-level arithmetic",
        "reasoning problems",
        "OpenAI-o1",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-04-01T03:57:58.000Z",
    "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
    "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01005",
      "authors": [
        {
          "_id": "67ec95ea3d267d26663ea34b",
          "name": "Nishad Singhi",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34c",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34d",
          "name": "Arian Hosseini",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34e",
          "name": "Aditya Grover",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34f",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea350",
          "name": "Marcus Rohrbach",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea351",
          "name": "Anna Rohrbach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:41:57.000Z",
      "submittedOnDailyAt": "2025-04-02T00:12:49.083Z",
      "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
      "upvotes": 4,
      "discussionId": "67ec95eb3d267d26663ea38b",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Mathematical problem-solving",
        "Self-Consistency (SC)",
        "Reward model (verifier)",
        "Generative Reward Models (GenRM)",
        "Next-token prediction task",
        "Chains-of-thought",
        "Inference budget",
        "Compute-efficient",
        "Inference scaling laws",
        "Compute-optimal inference"
      ]
    },
    "publishedAt": "2025-04-01T13:41:57.000Z",
    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
    "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01005.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00557",
      "authors": [
        {
          "_id": "67eca9a501a4d1c29e4e70f1",
          "name": "Jewon Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f2",
          "name": "Ki-Ung Song",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f3",
          "name": "Seungmin Yang",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f4",
          "name": "Donguk Lim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f5",
          "name": "Jaeyeon Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f6",
          "name": "Wooksu Shin",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f7",
          "name": "Bo-Kyeong Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f8",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f9",
          "name": "Tae-Ho Kim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
      ],
      "publishedAt": "2025-04-01T09:10:32.000Z",
      "submittedOnDailyAt": "2025-04-02T01:41:42.016Z",
      "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
      "submittedOnDailyBy": {
        "_id": "61f44bab7eba274ea80b74ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
        "isPro": false,
        "fullname": "Hyoung-Kyu Song",
        "user": "deepkyu",
        "type": "user"
      },
      "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
      "upvotes": 3,
      "discussionId": "67eca9a601a4d1c29e4e7131",
      "ai_keywords": [
        "visual token reduction",
        "inference costs",
        "image features",
        "large vision-language models (LVLMs)",
        "self-attention-only LVLMs",
        "cross-attention-based models",
        "key-value (KV) cache size",
        "self-attention layers",
        "cross-attention layers",
        "sparse nature",
        "cross-attention maps",
        "redundant visual features",
        "Trimmed Llama",
        "KV cache demands",
        "inference latency",
        "memory usage",
        "benchmark parity"
      ]
    },
    "publishedAt": "2025-04-01T05:10:32.000Z",
    "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
    "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00557.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61f44bab7eba274ea80b74ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
      "fullname": "Hyoung-Kyu Song",
      "name": "deepkyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00294",
      "authors": [
        {
          "_id": "67ecadb199892db0bda56561",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56562",
          "name": "Jingya Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56563",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56564",
          "name": "Shivam Garg",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56565",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56566",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56567",
          "name": "John Langford",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56568",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56569",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656a",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656b",
          "name": "Safoora Yousefi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T23:40:28.000Z",
      "submittedOnDailyAt": "2025-04-02T01:53:41.171Z",
      "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
      "upvotes": 3,
      "discussionId": "67ecadb299892db0bda565aa",
      "ai_keywords": [
        "inference-time scaling",
        "reasoning capabilities",
        "large language models (LLMs)",
        "step-by-step problem solving",
        "generated scratchpads",
        "state-of-the-art models",
        "math and STEM reasoning",
        "calendar planning",
        "NP-hard problems",
        "navigation",
        "spatial reasoning",
        "conventional models",
        "fine-tuned models",
        "model calls",
        "performance bounds",
        "multi-model inference systems",
        "empirical analysis",
        "token usage",
        "accuracy",
        "perfect verifiers",
        "performance gap"
      ]
    },
    "publishedAt": "2025-03-31T19:40:28.000Z",
    "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
    "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23361",
      "authors": [
        {
          "_id": "67eb57a56522661171fb4725",
          "name": "Linxin Song",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4726",
          "name": "Xuwei Ding",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4727",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4728",
          "user": {
            "_id": "62e1b3cb3eb0730f621a83f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
            "isPro": false,
            "fullname": "Taiwei Shi",
            "user": "MaksimSTW",
            "type": "user"
          },
          "name": "Taiwei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:48.349Z",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4729",
          "name": "Ryotaro Shimizu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472a",
          "name": "Rahul Gupta",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472c",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472d",
          "name": "Jieyu Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T08:33:56.000Z",
      "submittedOnDailyAt": "2025-04-02T00:04:15.369Z",
      "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
      "submittedOnDailyBy": {
        "_id": "62e1b3cb3eb0730f621a83f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
        "isPro": false,
        "fullname": "Taiwei Shi",
        "user": "MaksimSTW",
        "type": "user"
      },
      "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
      "upvotes": 3,
      "discussionId": "67eb57a66522661171fb476a",
      "githubRepo": "https://github.com/uscnlp-lime/SEA",
      "ai_keywords": [
        "stochastic error ascent (SEA)",
        "knowledge deficiencies (errors)",
        "closed-weight LLMs",
        "stochastic optimization process",
        "semantic similarity",
        "hierarchical retrieval",
        "document level",
        "paragraph level",
        "relation directed acyclic graph (DAG)",
        "error propagation",
        "systematic failure modes",
        "Automated Capability Discovery",
        "AutoBencher",
        "cost-per-error",
        "human evaluation",
        "ablation analysis",
        "convergence analysis",
        "correlated failure patterns",
        "LLM families",
        "data coverage",
        "targeted fine-tuning"
      ]
    },
    "publishedAt": "2025-03-30T04:33:56.000Z",
    "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
    "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e1b3cb3eb0730f621a83f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
      "fullname": "Taiwei Shi",
      "name": "MaksimSTW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00927",
      "authors": [
        {
          "_id": "67ecc23b6a2bc6abdc2e9183",
          "name": "Olga Golovneva",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9184",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9185",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9186",
          "name": "Sainbayar Sukhbaatar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:59:32.000Z",
      "submittedOnDailyAt": "2025-04-02T03:21:25.809Z",
      "title": "Multi-Token Attention",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
      "upvotes": 2,
      "discussionId": "67ecc23c6a2bc6abdc2e91c2",
      "ai_keywords": [
        "Soft attention",
        "LLMs (Large Language Models)",
        "single token attention",
        "Multi-Token Attention (MTA)",
        "convolution operations",
        "queries",
        "keys",
        "heads",
        "attention weights"
      ]
    },
    "publishedAt": "2025-04-01T11:59:32.000Z",
    "title": "Multi-Token Attention",
    "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00869",
      "authors": [
        {
          "_id": "67ecb60276900f68cd1df503",
          "name": "Xiaoke Huang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df504",
          "name": "Juncheng Wu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df505",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df506",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df507",
          "name": "Yuyin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:57:43.000Z",
      "submittedOnDailyAt": "2025-04-02T02:30:28.553Z",
      "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "63318b2349a9563915469f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
        "isPro": true,
        "fullname": "Xiaoke Huang",
        "user": "xk-huang",
        "type": "user"
      },
      "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
      "upvotes": 2,
      "discussionId": "67ecb60376900f68cd1df550",
      "githubRepo": "https://github.com/UCSC-VLAA/m1",
      "ai_keywords": [
        "test-time scaling",
        "large language models",
        "medical reasoning",
        "knowledge representation",
        "decision-making processes",
        "lightweight fine-tuned models",
        "state-of-the-art performance",
        "reasoning token budget",
        "budget forcing",
        "iterative prompts",
        "medical QA performance",
        "medical knowledge",
        "data scale",
        "data quality",
        "model capacity",
        "medical knowledge grounding",
        "challenging medical benchmarks",
        "reasoning depth"
      ]
    },
    "publishedAt": "2025-04-01T10:57:43.000Z",
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
    "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00869.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63318b2349a9563915469f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
      "fullname": "Xiaoke Huang",
      "name": "xk-huang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23733",
      "authors": [
        {
          "_id": "67ec99788088196efd062021",
          "name": "Yiyang Du",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062022",
          "name": "Xiaochen Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062023",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062024",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062025",
          "name": "Yiru Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062026",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062027",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062028",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062029",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202a",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202b",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202c",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:13:02.000Z",
      "submittedOnDailyAt": "2025-04-02T00:32:49.651Z",
      "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
      "upvotes": 2,
      "discussionId": "67ec99798088196efd062081",
      "ai_keywords": [
        "model merging",
        "Large Language Models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "heterogeneous property",
        "model architecture",
        "parameter space",
        "AdaMMS",
        "mapping function",
        "linear interpolation",
        "hyper-parameter searching",
        "unsupervised hyper-parameter selection",
        "vision-language benchmarks"
      ]
    },
    "publishedAt": "2025-03-31T01:13:02.000Z",
    "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
    "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21860",
      "authors": [
        {
          "_id": "67ec97f65d9c75ff46de2974",
          "name": "Kailin Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2975",
          "name": "Puhao Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2976",
          "name": "Tengyu Liu",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2977",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2978",
          "name": "Siyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:50:30.000Z",
      "submittedOnDailyAt": "2025-04-02T00:21:41.585Z",
      "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
      "upvotes": 2,
      "discussionId": "67ec97f85d9c75ff46de29f0",
      "projectPage": "https://maniptrans.github.io/",
      "ai_keywords": [
        "dexterous robotic manipulation",
        "data-driven embodied AI",
        "trajectory imitator",
        "bimanual skills",
        "interaction constraints",
        "fine-tuning",
        "parameter space",
        "DexManipNet",
        "pen capping",
        "bottle unscrewing",
        "episodes",
        "policy training"
      ]
    },
    "publishedAt": "2025-03-27T13:50:30.000Z",
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
    "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01019",
      "authors": [
        {
          "_id": "67ecd893553eb1c01a311882",
          "name": "Pablo Ruiz-Ponce",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311883",
          "name": "German Barquero",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311884",
          "name": "Cristina Palmero",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311885",
          "name": "Sergio Escalera",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311886",
          "name": "José García-Rodríguez",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
      ],
      "publishedAt": "2025-04-01T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-02T05:06:30.372Z",
      "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "62ace9bc717ee4c12b72e275",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
        "isPro": false,
        "fullname": "Pablo Ruiz-Ponce",
        "user": "pabloruizponce",
        "type": "user"
      },
      "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
      "upvotes": 1,
      "discussionId": "67ecd894553eb1c01a3118df",
      "projectPage": "https://www.pabloruizponce.com/papers/MixerMDM",
      "githubRepo": "https://github.com/pabloruizponce/MixerMDM",
      "ai_keywords": [
        "motion diffusion models",
        "text-conditioned",
        "learnable model composition",
        "dynamic mixing strategy",
        "adversarial fashion",
        "denoising process",
        "fine-grained control",
        "single-person",
        "multi-person",
        "interaction",
        "evaluation technique",
        "alignment between generated motions and conditions"
      ]
    },
    "publishedAt": "2025-04-01T13:59:44.000Z",
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01019.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ace9bc717ee4c12b72e275",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
      "fullname": "Pablo Ruiz-Ponce",
      "name": "pabloruizponce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00698",
      "authors": [
        {
          "_id": "67ecc2eb077a9bc63e7ba1a2",
          "name": "Team Cohere",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a3",
          "name": "Aakanksha",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a4",
          "name": "Arash Ahmadian",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a5",
          "name": "Marwan Ahmed",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a6",
          "name": "Jay Alammar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a7",
          "name": "Yazeed Alnumay",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a8",
          "name": "Sophia Althammer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a9",
          "name": "Arkady Arkhangorodsky",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1aa",
          "name": "Viraat Aryabumi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ab",
          "name": "Dennis Aumiller",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ac",
          "name": "Raphaël Avalos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ad",
          "name": "Zahara Aviv",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ae",
          "name": "Sammie Bae",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1af",
          "name": "Saurabh Baji",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b0",
          "name": "Alexandre Barbet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b1",
          "name": "Max Bartolo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b2",
          "name": "Björn Bebensee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b3",
          "name": "Neeral Beladia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b4",
          "name": "Walter Beller-Morales",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b5",
          "name": "Alexandre Bérard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b6",
          "name": "Andrew Berneshawi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b7",
          "name": "Anna Bialas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b8",
          "name": "Phil Blunsom",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b9",
          "name": "Matt Bobkin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ba",
          "name": "Adi Bongale",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bb",
          "name": "Sam Braun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bc",
          "name": "Maxime Brunet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bd",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1be",
          "name": "David Cairuz",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bf",
          "name": "Jon Ander Campos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c0",
          "name": "Cassie Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c1",
          "name": "Kris Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c2",
          "name": "Roman Castagné",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c3",
          "name": "Julián Cendrero",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c4",
          "name": "Leila Chan Currie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c5",
          "name": "Yash Chandak",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c6",
          "name": "Diane Chang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c7",
          "name": "Giannis Chatziveroglou",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c8",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c9",
          "name": "Claire Cheng",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ca",
          "name": "Alexis Chevalier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cb",
          "name": "Justin T. Chiu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cc",
          "name": "Eugene Cho",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cd",
          "name": "Eugene Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ce",
          "name": "Eujeong Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cf",
          "name": "Tim Chung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d0",
          "name": "Volkan Cirik",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d1",
          "name": "Ana Cismaru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d2",
          "name": "Pierre Clavier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d3",
          "name": "Henry Conklin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d4",
          "name": "Lucas Crawhall-Stein",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d5",
          "name": "Devon Crouse",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d6",
          "name": "Andres Felipe Cruz-Salinas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d7",
          "name": "Ben Cyrus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d8",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d9",
          "name": "Hugo Dalla-Torre",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1da",
          "name": "John Dang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1db",
          "name": "William Darling",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dc",
          "name": "Omar Darwiche Domingues",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dd",
          "name": "Saurabh Dash",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1de",
          "name": "Antoine Debugne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1df",
          "name": "Théo Dehaze",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e0",
          "name": "Shaan Desai",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e1",
          "name": "Joan Devassy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e2",
          "name": "Rishit Dholakia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e3",
          "name": "Kyle Duffy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e4",
          "name": "Ali Edalati",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e5",
          "name": "Ace Eldeib",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e6",
          "name": "Abdullah Elkady",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e7",
          "name": "Sarah Elsharkawy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e8",
          "name": "Irem Ergün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e9",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ea",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1eb",
          "name": "Boyu Fan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ec",
          "name": "Lucas Fayoux",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ed",
          "name": "Yannis Flet-Berliac",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ee",
          "name": "Nick Frosst",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ef",
          "name": "Matthias Gallé",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f0",
          "name": "Wojciech Galuba",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f1",
          "name": "Utsav Garg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f2",
          "name": "Matthieu Geist",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f3",
          "name": "Mohammad Gheshlaghi Azar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f4",
          "name": "Seraphina Goldfarb-Tarrant",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f5",
          "name": "Tomas Goldsack",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f6",
          "name": "Aidan Gomez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f7",
          "name": "Victor Machado Gonzaga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f8",
          "name": "Nithya Govindarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f9",
          "name": "Manoj Govindassamy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fa",
          "name": "Nathan Grinsztajn",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fb",
          "name": "Nikolas Gritsch",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fc",
          "name": "Patrick Gu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fd",
          "name": "Shangmin Guo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fe",
          "name": "Kilian Haefeli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ff",
          "name": "Rod Hajjar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba200",
          "name": "Tim Hawes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba201",
          "name": "Jingyi He",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba202",
          "name": "Sebastian Hofstätter",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba203",
          "name": "Sungjin Hong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba204",
          "name": "Sara Hooker",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba205",
          "name": "Tom Hosking",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba206",
          "name": "Stephanie Howe",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba207",
          "name": "Eric Hu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba208",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba209",
          "name": "Hemant Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20a",
          "name": "Ritika Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20b",
          "name": "Nick Jakobi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20c",
          "name": "Madeline Jenkins",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20d",
          "name": "JJ Jordan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20e",
          "name": "Dhruti Joshi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20f",
          "name": "Jason Jung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba210",
          "name": "Trushant Kalyanpur",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba211",
          "name": "Siddhartha Rao Kamalakara",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba212",
          "name": "Julia Kedrzycki",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba213",
          "name": "Gokce Keskin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba214",
          "name": "Edward Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba215",
          "name": "Joon Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba216",
          "name": "Wei-Yin Ko",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba217",
          "name": "Tom Kocmi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba218",
          "name": "Michael Kozakov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba219",
          "name": "Wojciech Kryściński",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21a",
          "name": "Arnav Kumar Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21b",
          "name": "Komal Kumar Teru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21c",
          "name": "Sander Land",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21d",
          "name": "Michael Lasby",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21e",
          "name": "Olivia Lasche",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21f",
          "name": "Justin Lee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba220",
          "name": "Patrick Lewis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba221",
          "name": "Jeffrey Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba222",
          "name": "Jonathan Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba223",
          "name": "Hangyu Lin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba224",
          "name": "Acyr Locatelli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba225",
          "name": "Kevin Luong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba226",
          "name": "Raymond Ma",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba227",
          "name": "Lukas Mach",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba228",
          "name": "Marina Machado",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba229",
          "name": "Joanne Magbitang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22a",
          "name": "Brenda Malacara Lopez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22b",
          "name": "Aryan Mann",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22c",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22d",
          "name": "Olivia Markham",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22e",
          "name": "Alexandre Matton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22f",
          "name": "Alex McKinney",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba230",
          "name": "Dominic McLoughlin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba231",
          "name": "Jozef Mokry",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba232",
          "name": "Adrien Morisot",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba233",
          "name": "Autumn Moulder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba234",
          "name": "Harry Moynehan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba235",
          "name": "Maximilian Mozes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba236",
          "name": "Vivek Muppalla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba237",
          "name": "Lidiya Murakhovska",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba238",
          "name": "Hemangani Nagarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba239",
          "name": "Alekhya Nandula",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23a",
          "name": "Hisham Nasir",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23b",
          "name": "Shauna Nehra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23c",
          "name": "Josh Netto-Rosen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23d",
          "name": "Daniel Ohashi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23e",
          "name": "James Owers-Bardsley",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23f",
          "name": "Jason Ozuzu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba240",
          "name": "Dennis Padilla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba241",
          "name": "Gloria Park",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba242",
          "name": "Sam Passaglia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba243",
          "name": "Jeremy Pekmez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba244",
          "name": "Laura Penstone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba245",
          "name": "Aleksandra Piktus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba246",
          "name": "Case Ploeg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba247",
          "name": "Andrew Poulton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba248",
          "name": "Youran Qi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba249",
          "name": "Shubha Raghvendra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24a",
          "name": "Miguel Ramos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24b",
          "name": "Ekagra Ranjan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24c",
          "name": "Pierre Richemond",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24d",
          "name": "Cécile Robert-Michon",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24e",
          "name": "Aurélien Rodriguez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24f",
          "name": "Sudip Roy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba250",
          "name": "Laura Ruis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba251",
          "name": "Louise Rust",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba252",
          "name": "Anubhav Sachan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba253",
          "name": "Alejandro Salamanca",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba254",
          "name": "Kailash Karthik Saravanakumar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba255",
          "name": "Isha Satyakam",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba256",
          "name": "Alice Schoenauer Sebag",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba257",
          "name": "Priyanka Sen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba258",
          "name": "Sholeh Sepehri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba259",
          "name": "Preethi Seshadri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25a",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25b",
          "name": "Tom Sherborne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25c",
          "name": "Sylvie Chang Shi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25d",
          "name": "Sanal Shivaprasad",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25e",
          "name": "Vladyslav Shmyhlo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25f",
          "name": "Anirudh Shrinivason",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba260",
          "name": "Inna Shteinbuk",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba261",
          "name": "Amir Shukayev",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba262",
          "name": "Mathieu Simard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba263",
          "name": "Ella Snyder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba264",
          "name": "Ava Spataru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba265",
          "name": "Victoria Spooner",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba266",
          "name": "Trisha Starostina",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba267",
          "name": "Florian Strub",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba268",
          "name": "Yixuan Su",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba269",
          "name": "Jimin Sun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26a",
          "name": "Dwarak Talupuru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26b",
          "name": "Eugene Tarassov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26c",
          "name": "Elena Tommasone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26d",
          "name": "Jennifer Tracey",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26e",
          "name": "Billy Trend",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26f",
          "name": "Evren Tumer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba270",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba271",
          "name": "Bharat Venkitesh",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba272",
          "name": "David Venuto",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba273",
          "name": "Pat Verga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba274",
          "name": "Maxime Voisin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba275",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba276",
          "name": "Donglu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba277",
          "name": "Shijian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba278",
          "name": "Edmond Wen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba279",
          "name": "Naomi White",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27a",
          "name": "Jesse Willman",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27b",
          "name": "Marysia Winkels",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27c",
          "name": "Chen Xia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27d",
          "name": "Jessica Xie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27e",
          "name": "Minjie Xu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27f",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba280",
          "name": "Tan Yi-Chern",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba281",
          "name": "Ivan Zhang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba282",
          "name": "Zhenyu Zhao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba283",
          "name": "Zhoujie Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T12:08:07.000Z",
      "submittedOnDailyAt": "2025-04-02T03:24:20.179Z",
      "title": "Command A: An Enterprise-Ready Large Language Model",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
      "upvotes": 1,
      "discussionId": "67ecc2ec077a9bc63e7ba2bd",
      "ai_keywords": [
        "agent-optimised",
        "multilingual-capable",
        "hybrid architecture",
        "Retrieval Augmented Generation (RAG)",
        "grounding",
        "tool use",
        "decentralised training",
        "self-refinement algorithms",
        "model merging techniques"
      ]
    },
    "publishedAt": "2025-04-01T08:08:07.000Z",
    "title": "Command A: An Enterprise-Ready Large Language Model",
    "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00698.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00072",
      "authors": [
        {
          "_id": "67eccbca1006da75eca94d24",
          "name": "Lucas Ventura",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d25",
          "name": "Antoine Yang",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d26",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d27",
          "name": "Gül Varol",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:41:29.000Z",
      "submittedOnDailyAt": "2025-04-02T04:02:28.798Z",
      "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
      "upvotes": 0,
      "discussionId": "67eccbce1006da75eca94e68",
      "ai_keywords": [
        "large language model (LLM)",
        "context window",
        "speech transcripts",
        "captions",
        "timestamps",
        "frame selection strategy",
        "chapter boundaries",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-31T13:41:29.000Z",
    "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
    "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6564
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24210",
      "authors": [
        {
          "_id": "67ece4b12511c2aab09b84ca",
          "name": "Seungjun Lee",
          "hidden": false
        },
        {
          "_id": "67ece4b12511c2aab09b84cb",
          "name": "Gim Hee Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
      ],
      "publishedAt": "2025-03-31T15:27:07.000Z",
      "submittedOnDailyAt": "2025-04-02T05:49:14.777Z",
      "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "662f5b1e5aeedf55d209741a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
        "isPro": false,
        "fullname": "Seungjun Lee",
        "user": "onandon",
        "type": "user"
      },
      "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
      "upvotes": 0,
      "discussionId": "67ece4b62511c2aab09b8660",
      "projectPage": "https://diet-gs.github.io",
      "githubRepo": "https://github.com/DiET-GS/DiET-GS",
      "ai_keywords": [
        "diffusion prior",
        "event stream",
        "motion deblurring",
        "3DGS (3D Geometry Synthesis)",
        "blur-free event streams",
        "event double integral",
        "edge details",
        "novel view synthesis"
      ]
    },
    "publishedAt": "2025-03-31T11:27:07.000Z",
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
    "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662f5b1e5aeedf55d209741a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
      "fullname": "Seungjun Lee",
      "name": "onandon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]