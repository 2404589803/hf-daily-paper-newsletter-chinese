[
  {
    "paper": {
      "id": "2503.15265",
      "authors": [
        {
          "_id": "67db8c4c9e4f93ee46411c1d",
          "name": "Ruowen Zhao",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1e",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1f",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c20",
          "name": "Guangce Liu",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c21",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c22",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c23",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:39:30.000Z",
      "submittedOnDailyAt": "2025-03-20T03:22:40.364Z",
      "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6522e4fbd89bc7773ddc4b58",
        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
        "isPro": false,
        "fullname": "Ruowen Zhao",
        "user": "zzzrw",
        "type": "user"
      },
      "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
      "upvotes": 20,
      "discussionId": "67db8c519e4f93ee46411d60",
      "projectPage": "https://zhaorw02.github.io/DeepMesh/",
      "githubRepo": "https://github.com/zhaorw02/DeepMesh",
      "ai_keywords": [
        "triangle meshes",
        "auto-regressive methods",
        "discrete vertex tokens",
        "face counts",
        "mesh incompleteness",
        "DeepMesh",
        "tokenization algorithm",
        "data curation",
        "data processing",
        "Reinforcement Learning (RL)",
        "Direct Preference Optimization (DPO)",
        "human evaluation",
        "3D metrics",
        "point clouds",
        "intricate details",
        "precise topology",
        "state-of-the-art methods",
        "precision",
        "quality"
      ]
    },
    "publishedAt": "2025-03-19T10:39:30.000Z",
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
    "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6522e4fbd89bc7773ddc4b58",
      "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
      "fullname": "Ruowen Zhao",
      "name": "zzzrw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15485",
      "authors": [
        {
          "_id": "67db7dd224fe67fe45b21e63",
          "name": "Zineng Tang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e64",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e65",
          "name": "Seun Eisape",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e66",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e67",
          "name": "Roei Herzig",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e68",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e69",
          "name": "Alane Suhr",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6a",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6b",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:58:57.000Z",
      "submittedOnDailyAt": "2025-03-20T01:01:18.127Z",
      "title": "TULIP: Towards Unified Language-Image Pretraining",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
      "upvotes": 16,
      "discussionId": "67db7dd424fe67fe45b21ee1",
      "projectPage": "https://tulip-berkeley.github.io/",
      "ai_keywords": [
        "generative data augmentation",
        "enhanced image-image and text-text contrastive learning",
        "image/text reconstruction regularization",
        "fine-grained visual features",
        "global semantic alignment",
        "zero-shot performance",
        "few-shot classification",
        "vision-language models"
      ]
    },
    "publishedAt": "2025-03-19T13:58:57.000Z",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15475",
      "authors": [
        {
          "_id": "67db729fa720e711cff4d205",
          "name": "Foundation AI Team",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d206",
          "name": "Kiran Bhat",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d207",
          "name": "Nishchaie Khanna",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d208",
          "name": "Karun Channa",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d209",
          "name": "Tinghui Zhou",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20a",
          "name": "Yiheng Zhu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20b",
          "name": "Xiaoxia Sun",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20c",
          "name": "Charles Shang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20d",
          "name": "Anirudh Sudarshan",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20e",
          "name": "Maurice Chu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20f",
          "name": "Daiqing Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d210",
          "name": "Kangle Deng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d211",
          "name": "Jean-Philippe Fauconnier",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d212",
          "name": "Tijmen Verhulsdonck",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d213",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d214",
          "name": "Kayvon Fatahalian",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d215",
          "name": "Alexander Weiss",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d216",
          "name": "Christian Reiser",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d217",
          "name": "Ravi Kiran Chirravuri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d218",
          "name": "Ravali Kandur",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d219",
          "name": "Alejandro Pelaez",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21a",
          "name": "Akash Garg",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21b",
          "name": "Michael Palleschi",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21c",
          "name": "Jessica Wang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21d",
          "name": "Skylar Litz",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21f",
          "name": "Anying Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d220",
          "name": "David Harmon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d221",
          "name": "Derek Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d222",
          "name": "Liangjun Feng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d223",
          "name": "Denis Goupil",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d224",
          "name": "Lukas Kuczynski",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d225",
          "name": "Jihyun Yoon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d226",
          "name": "Naveen Marri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d227",
          "name": "Peiye Zhuang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d228",
          "name": "Yinan Zhang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d229",
          "name": "Brian Yin",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22a",
          "name": "Haomiao Jiang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22b",
          "name": "Marcel van Workum",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22c",
          "name": "Thomas Lane",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22d",
          "name": "Bryce Erickson",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22e",
          "name": "Salil Pathare",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22f",
          "name": "Kyle Price",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d230",
          "name": "Anupam Singh",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d231",
          "name": "David Baszucki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:52:17.000Z",
      "submittedOnDailyAt": "2025-03-20T00:57:52.833Z",
      "title": "Cube: A Roblox View of 3D Intelligence",
      "submittedOnDailyBy": {
        "_id": "62cd5c43299c0c2e0e437842",
        "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
        "isPro": false,
        "fullname": "Jean-Philippe Fauconnier",
        "user": "j4kn",
        "type": "user"
      },
      "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
      "upvotes": 15,
      "discussionId": "67db72a1a720e711cff4d292",
      "githubRepo": "https://github.com/Roblox/cube",
      "ai_keywords": [
        "3D foundation model",
        "3D geometric shapes",
        "3D shape tokenizer",
        "text-to-shape generation",
        "shape-to-text generation",
        "text-to-scene generation",
        "large language models (LLMs)",
        "scene analysis",
        "reasoning",
        "unified foundation model"
      ]
    },
    "publishedAt": "2025-03-19T13:52:17.000Z",
    "title": "Cube: A Roblox View of 3D Intelligence",
    "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd5c43299c0c2e0e437842",
      "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
      "fullname": "Jean-Philippe Fauconnier",
      "name": "j4kn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14868",
      "authors": [
        {
          "_id": "67db9f06842d8b6642a5eeaf",
          "name": "Hoigi Seo",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb0",
          "name": "Wongi Jeong",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb1",
          "name": "Kyungryeol Lee",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb2",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T03:45:37.000Z",
      "submittedOnDailyAt": "2025-03-20T03:25:51.779Z",
      "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
      "upvotes": 15,
      "discussionId": "67db9f11842d8b6642a5f165",
      "projectPage": "https://ignoww.github.io/ZOODiP_project/",
      "githubRepo": "https://github.com/ignoww/ZOODiP",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "quantization techniques",
        "dequantization",
        "gradient-based algorithms",
        "memory-efficient fine-tuning",
        "Textual Inversion",
        "zeroth-order optimization",
        "personalization tokens",
        "gradient estimation",
        "Subspace Gradient",
        "subspace projection",
        "text embedding",
        "Partial Uniform Timestep Sampling",
        "diffusion timesteps",
        "Stable Diffusion",
        "image and text alignment scores"
      ]
    },
    "publishedAt": "2025-03-18T23:45:37.000Z",
    "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
    "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15417",
      "authors": [
        {
          "_id": "67db8e05842d8b6642a135d0",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d1",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d2",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d3",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d4",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d5",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d6",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d7",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T16:59:32.000Z",
      "submittedOnDailyAt": "2025-03-20T02:10:52.068Z",
      "title": "Temporal Regularization Makes Your Video Generator Stronger",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
      "upvotes": 12,
      "discussionId": "67db8e07842d8b6642a1365f",
      "ai_keywords": [
        "temporal augmentation",
        "FluxFlow",
        "temporal perturbations",
        "temporal quality",
        "temporal coherence",
        "UCF-101",
        "VBench",
        "U-Net",
        "DiT",
        "AR-based architectures",
        "spatial fidelity"
      ]
    },
    "publishedAt": "2025-03-19T12:59:32.000Z",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6406
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12532",
      "authors": [
        {
          "_id": "67da1df040371958e1732c83",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c84",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c85",
          "name": "Ziqin Wei",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c86",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c87",
          "name": "Chi-Wing Fu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c88",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:43.000Z",
      "submittedOnDailyAt": "2025-03-20T01:38:29.350Z",
      "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
      "upvotes": 7,
      "discussionId": "67da1df240371958e1732d2f",
      "githubRepo": "https://github.com/FanbinLu/STEVE",
      "ai_keywords": [
        "behavior cloning",
        "trajectory data",
        "suboptimal agents",
        "GPT-4o",
        "step verification pipeline",
        "correctness verification",
        "Kahneman and Tversky Optimization",
        "positive actions",
        "negative actions",
        "vision-language model",
        "computer-use agent",
        "live desktop environment",
        "WinAgentArena"
      ]
    },
    "publishedAt": "2025-03-16T10:53:43.000Z",
    "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
    "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13288",
      "authors": [
        {
          "_id": "67dbc49d85eacb364e913c38",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c39",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3a",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3b",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3d",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3e",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T15:38:33.000Z",
      "submittedOnDailyAt": "2025-03-20T06:08:48.330Z",
      "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
      "upvotes": 5,
      "discussionId": "67dbc49f85eacb364e913d20",
      "githubRepo": "https://github.com/xufangzhi/phi-Decoding",
      "ai_keywords": [
        "inference-time optimization",
        "auto-regressive generation",
        "foresight sampling",
        "$\\phi$-Decoding",
        "joint distribution",
        "in-width and in-depth pruning",
        "LLMs (Large Language Models)"
      ]
    },
    "publishedAt": "2025-03-17T11:38:33.000Z",
    "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
    "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15264",
      "authors": [
        {
          "_id": "67dbbe8fafd5251fc6b55730",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55731",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55732",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55733",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55734",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55735",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55736",
          "name": "Baichuan Zhou",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55737",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55738",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55739",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b5573a",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:37:21.000Z",
      "submittedOnDailyAt": "2025-03-20T05:54:43.430Z",
      "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
      "upvotes": 4,
      "discussionId": "67dbbe92afd5251fc6b55825",
      "projectPage": "https://opendatalab.github.io/LEGION",
      "githubRepo": "https://github.com/opendatalab/LEGION",
      "ai_keywords": [
        "SynthScars",
        "LEGION",
        "multimodal large language model",
        "image forgery analysis framework",
        "artifact detection",
        "segmentation",
        "explanation",
        "mIoU",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-19T10:37:21.000Z",
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12769",
      "authors": [
        {
          "_id": "67d8ded81a1b6ae91f79eb18",
          "name": "Shenghao Fu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb19",
          "name": "Qize Yang",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1a",
          "name": "Yuan-Ming Li",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1b",
          "name": "Yi-Xing Peng",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1c",
          "name": "Kun-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1d",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1e",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1f",
          "name": "Xiaohua Xie",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb20",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T03:05:31.000Z",
      "submittedOnDailyAt": "2025-03-20T00:48:47.112Z",
      "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
      "upvotes": 4,
      "discussionId": "67d8ded91a1b6ae91f79eb5c",
      "ai_keywords": [
        "Large Multi-modal Models (LMMs)",
        "streaming video understanding",
        "Visual Instruction Feedback",
        "visual contents",
        "instructions",
        "gesture recognition",
        "user-agent interactions",
        "subtasks",
        "ViSpeak-Instruct dataset",
        "ViSpeak-Bench",
        "ViSpeak model",
        "GPT-4o-level performance",
        "streaming video understanding benchmarks",
        "finetuning"
      ]
    },
    "publishedAt": "2025-03-16T23:05:31.000Z",
    "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14505",
      "authors": [
        {
          "_id": "67db13f71956dcedf0b4d357",
          "name": "Susung Hong",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d358",
          "name": "Ira Kemelmacher-Shlizerman",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d359",
          "name": "Brian Curless",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d35a",
          "name": "Steven M. Seitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-20T02:39:46.392Z",
      "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
      "submittedOnDailyBy": {
        "_id": "635a6dd21668c4ead3ed19fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
        "isPro": false,
        "fullname": "Susung Hong",
        "user": "susunghong",
        "type": "user"
      },
      "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
      "upvotes": 3,
      "discussionId": "67db13fc1956dcedf0b4d470",
      "ai_keywords": [
        "video diffusion models",
        "multimodal audio-video model",
        "music-video cross-attention",
        "low-rank adapter",
        "dance videos",
        "motion capture data",
        "music-driven video generation",
        "Video-LLMs"
      ]
    },
    "publishedAt": "2025-03-18T13:59:58.000Z",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635a6dd21668c4ead3ed19fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
      "fullname": "Susung Hong",
      "name": "susunghong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11227",
      "authors": [
        {
          "_id": "67da533bb443470b7908a048",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:13.546Z",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a049",
          "name": "Bifan Wei",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04a",
          "name": "Shihao Qi",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04b",
          "name": "haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04d",
          "name": "Qika Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:23:22.000Z",
      "submittedOnDailyAt": "2025-03-20T06:15:24.085Z",
      "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
      "upvotes": 3,
      "discussionId": "67da533db443470b7908a0e6",
      "ai_keywords": [
        "knowledge graph",
        "event knowledge graph",
        "commonsense knowledge graph",
        "natural language processing",
        "unified framework",
        "in-sample data",
        "counter-task data",
        "out-of-distribution data",
        "three-stage curriculum learning fine-tuning framework",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-03-14T05:23:22.000Z",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
    "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13360",
      "authors": [
        {
          "_id": "67d8e21dea26d6d743f2adde",
          "name": "Hai-Long Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2addf",
          "name": "Zhun Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade0",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade1",
          "name": "Han-Jia Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:45:12.000Z",
      "submittedOnDailyAt": "2025-03-20T04:52:23.426Z",
      "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
      "submittedOnDailyBy": {
        "_id": "6623975c728f756224d4b768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
        "isPro": false,
        "fullname": "Allen Sun",
        "user": "Allen8",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
      "upvotes": 1,
      "discussionId": "67d8e21eea26d6d743f2ae50",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "OpenAI o1",
        "Multimodal LLMs (MLLMs)",
        "attention to visual information",
        "text-over-relied outputs",
        "ablate image inputs",
        "long-chain reasoning",
        "MathVista's test-hard subset",
        "Take-along Visual Conditioning (TVC)",
        "critical reasoning stages",
        "dynamic pruning",
        "multimodal reasoning systems"
      ]
    },
    "publishedAt": "2025-03-17T12:45:12.000Z",
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623975c728f756224d4b768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
      "fullname": "Allen Sun",
      "name": "Allen8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15055",
      "authors": [
        {
          "_id": "67db9586a2f164ac51f84c72",
          "user": {
            "_id": "641ee9fe632a1ec42caf1fa6",
            "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
            "isPro": false,
            "fullname": "Arina Razmyslovich",
            "user": "lavriz",
            "type": "user"
          },
          "name": "Arina Razmyslovich",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-20T04:13:32.101Z",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c73",
          "name": "Kseniia Murasheva",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c74",
          "name": "Sofia Sedlova",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c75",
          "name": "Julien Capitaine",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c76",
          "name": "Eugene Dmitriev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T09:46:54.000Z",
      "submittedOnDailyAt": "2025-03-20T02:46:26.189Z",
      "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
      "submittedOnDailyBy": {
        "_id": "641ee9fe632a1ec42caf1fa6",
        "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
        "isPro": false,
        "fullname": "Arina Razmyslovich",
        "user": "lavriz",
        "type": "user"
      },
      "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
      "upvotes": 0,
      "discussionId": "67db958fa2f164ac51f84f51",
      "githubRepo": "https://github.com/1712n/eltex",
      "ai_keywords": [
        "ELTEX",
        "domain-driven framework",
        "high-quality synthetic training data",
        "Large Language Models (LLMs)",
        "cohort indicator extraction",
        "dynamic prompting",
        "critical domain knowledge",
        "blockchain-related cyberattack detection",
        "Gemma-2B",
        "performance competitive",
        "GPT-4",
        "standard classification metrics",
        "uncertainty calibration",
        "computational resources",
        "synthetic dataset",
        "social media texts",
        "domain-driven synthetic data generation",
        "performance gap",
        "resource-efficient models",
        "larger architectures"
      ]
    },
    "publishedAt": "2025-03-19T05:46:54.000Z",
    "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
    "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ee9fe632a1ec42caf1fa6",
      "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
      "fullname": "Arina Razmyslovich",
      "name": "lavriz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]