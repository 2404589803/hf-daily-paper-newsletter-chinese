[
  {
    "paper": {
      "id": "2504.16084",
      "authors": [
        {
          "_id": "6808558a07e80b69b2e351b5",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b6",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b7",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b8",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b9",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351ba",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bb",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bc",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bd",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351be",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-23T01:22:31.055Z",
      "title": "TTRL: Test-Time Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "60bc94cd85a3ab33829b6211",
        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
        "isPro": false,
        "fullname": "Kaiyan Zhang",
        "user": "iseesaw",
        "type": "user"
      },
      "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
      "upvotes": 40,
      "discussionId": "6808558b07e80b69b2e351f3",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "reward estimation",
        "Test-Time Scaling (TTS)",
        "majority voting",
        "Test-Time Reinforcement Learning (TTRL)",
        "self-evolution",
        "pre-trained models",
        "pass@1",
        "Qwen-2.5-Math-7B",
        "AIME 2024",
        "Maj@N metric"
      ]
    },
    "publishedAt": "2025-04-22T13:59:56.000Z",
    "title": "TTRL: Test-Time Reinforcement Learning",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60bc94cd85a3ab33829b6211",
      "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
      "fullname": "Kaiyan Zhang",
      "name": "iseesaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15521",
      "authors": [
        {
          "_id": "6808458f07e80b69b2df2440",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2441",
          "name": "Weixuan Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2442",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2443",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2444",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2445",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2446",
          "name": "Chenyang Lyu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2447",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2448",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2449",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T01:47:37.000Z",
      "submittedOnDailyAt": "2025-04-23T00:13:52.385Z",
      "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
      "submittedOnDailyBy": {
        "_id": "62d4bf8c97ab9eb08762a975",
        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
        "isPro": false,
        "fullname": "Minghao Wu",
        "user": "minghaowu",
        "type": "user"
      },
      "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
      "upvotes": 39,
      "discussionId": "6808459007e80b69b2df249e",
      "ai_keywords": [
        "multilingual large language models (LLMs)",
        "multilingual benchmarks",
        "benchmark performance",
        "human judgments",
        "STEM-related tasks",
        "question answering (e.g., XQuAD)",
        "culturally and linguistically tailored benchmarks",
        "human-aligned benchmarks",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-21T21:47:37.000Z",
    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d4bf8c97ab9eb08762a975",
      "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
      "fullname": "Minghao Wu",
      "name": "minghaowu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15120",
      "authors": [
        {
          "_id": "680733cf7722bb6407ca0787",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T09:37:47.479Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0788",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0789",
          "name": "Mohamed Motaism Hamed",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-23T05:30:42.569Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078b",
          "name": "Omar Hadid",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:17:25.000Z",
      "submittedOnDailyAt": "2025-04-23T03:28:02.778Z",
      "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
      "upvotes": 35,
      "discussionId": "680733d07722bb6407ca07da",
      "githubRepo": "https://github.com/misraj-ai/Kuwain-Arabic-cleaner",
      "ai_keywords": [
        "large language model (LLM)",
        "tiny model",
        "Kuwain",
        "language integration",
        "Arabic language",
        "benchmarks",
        "language model expansion"
      ]
    },
    "publishedAt": "2025-04-21T10:17:25.000Z",
    "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
    "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15466",
      "authors": [
        {
          "_id": "6808480c49c8f78b6a4e492f",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4930",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4931",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4932",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4933",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4934",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4935",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4936",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4937",
          "name": "Alane Suhr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T22:29:02.000Z",
      "submittedOnDailyAt": "2025-04-23T00:30:52.876Z",
      "title": "Learning Adaptive Parallel Reasoning with Language Models",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
      "upvotes": 25,
      "discussionId": "6808480c49c8f78b6a4e4968",
      "githubRepo": "https://github.com/Parallel-Reasoning/APR",
      "ai_keywords": [
        "Adaptive Parallel Reasoning (APR)",
        "serialized chain-of-thought approaches",
        "parallel methods",
        "self-consistency",
        "adaptive multi-threaded inference",
        "spawn()",
        "join()",
        "reinforcement learning strategy",
        "parent inference threads",
        "child inference threads",
        "Countdown reasoning task",
        "context window",
        "scalability",
        "total tokens",
        "reasoning processes",
        "adaptive allocation of computation"
      ]
    },
    "publishedAt": "2025-04-21T18:29:02.000Z",
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16072",
      "authors": [
        {
          "_id": "6808467a867c3ef14f8326ce",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326cf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d0",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d1",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d3",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d4",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d6",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d7",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d8",
          "name": "Yin Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
      ],
      "publishedAt": "2025-04-22T17:51:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:22:38.011Z",
      "title": "Describe Anything: Detailed Localized Image and Video Captioning",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
      "upvotes": 24,
      "discussionId": "6808467e867c3ef14f832831",
      "projectPage": "https://describe-anything.github.io",
      "githubRepo": "https://github.com/NVlabs/describe-anything",
      "ai_keywords": [
        "focal prompt",
        "localized vision backbone",
        "Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP)",
        "segmentation datasets",
        "DLC-Bench",
        "keyword-level",
        "phrase-level",
        "detailed multi-sentence localized image and video captioning"
      ]
    },
    "publishedAt": "2025-04-22T13:51:41.000Z",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15415",
      "authors": [
        {
          "_id": "68084b04ba1dd0e6a077e09f",
          "name": "David Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a1",
          "name": "Jincheng Ren",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a2",
          "name": "Jarvis Guo",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a3",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a4",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a5",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a6",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a7",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a8",
          "name": "Jun Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a9",
          "name": "Xiao Gu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0aa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ab",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ac",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ad",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ae",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0af",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b1",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b2",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T19:53:44.000Z",
      "submittedOnDailyAt": "2025-04-23T00:59:32.168Z",
      "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
      "upvotes": 14,
      "discussionId": "68084b0bba1dd0e6a077e279",
      "githubRepo": "https://github.com/multimodal-art-projection/IV-Bench",
      "ai_keywords": [
        "Image-Grounded Video Perception and Reasoning",
        "IV-Bench",
        "image-text queries",
        "frame number",
        "resolution"
      ]
    },
    "publishedAt": "2025-04-21T15:53:44.000Z",
    "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
    "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14992",
      "authors": [
        {
          "_id": "68074ed102571b837f03463c",
          "user": {
            "_id": "64722a616facfb01d8ae8349",
            "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
            "isPro": false,
            "fullname": "Wu Bohong",
            "user": "bongbohong",
            "type": "user"
          },
          "name": "Bohong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:15.811Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463d",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463e",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463f",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034640",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:18.659Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034641",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034642",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:41:26.000Z",
      "submittedOnDailyAt": "2025-04-23T00:38:26.026Z",
      "title": "Efficient Pretraining Length Scaling",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
      "upvotes": 11,
      "discussionId": "68074ed202571b837f03468b",
      "ai_keywords": [
        "KV cache",
        "original tokens",
        "hidden decoding tokens",
        "long-range dependencies",
        "local dependencies",
        "sliding window attention",
        "chunk-wise sliding window attention"
      ]
    },
    "publishedAt": "2025-04-21T05:41:26.000Z",
    "title": "Efficient Pretraining Length Scaling",
    "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14538",
      "authors": [
        {
          "_id": "680863ed3767f6ed7c969fbf",
          "name": "Yiting Ran",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc1",
          "name": "Tian Qiu",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc2",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc3",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc4",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T08:56:27.000Z",
      "submittedOnDailyAt": "2025-04-23T02:23:09.187Z",
      "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
      "submittedOnDailyBy": {
        "_id": "64c7bf2c4524c2aea7eac0b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
        "isPro": false,
        "fullname": "Xintao Wang",
        "user": "Neph0s",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
      "upvotes": 11,
      "discussionId": "680863ef3767f6ed7c96a026",
      "ai_keywords": [
        "large language models (LLMs)",
        "social simulation",
        "multi-agent systems",
        "agent societies",
        "personas",
        "book-based",
        "comprehensive real-world intricacies",
        "diverse and dynamic characters",
        "fictional worldviews",
        "geographical constraints",
        "story generation",
        "interactive games",
        "creative, high-quality stories",
        "fidelity to the source books"
      ]
    },
    "publishedAt": "2025-04-20T04:56:27.000Z",
    "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
    "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c7bf2c4524c2aea7eac0b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
      "fullname": "Xintao Wang",
      "name": "Neph0s",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13820",
      "authors": [
        {
          "_id": "6805ab2c40034a5a792a26b2",
          "user": {
            "_id": "63f1d16fbe95ed4c9a9418fe",
            "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "yueyang2000",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:59:59.547Z",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b3",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b4",
          "name": "Chenxin Tao",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b5",
          "name": "Pan Liu",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b6",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b7",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:50:43.000Z",
      "submittedOnDailyAt": "2025-04-23T01:09:25.550Z",
      "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
      "submittedOnDailyBy": {
        "_id": "63f1d16fbe95ed4c9a9418fe",
        "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "yueyang2000",
        "type": "user"
      },
      "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
      "upvotes": 11,
      "discussionId": "6805ab2f40034a5a792a27c8",
      "githubRepo": "https://github.com/LeapLabTHU/CheXWorld",
      "ai_keywords": [
        "CheXWorld",
        "self-supervised world model",
        "radiographic images",
        "local anatomical structures",
        "global anatomical layouts",
        "organs",
        "skeletons",
        "domain variations",
        "medical image classification",
        "medical image segmentation",
        "SSL methods",
        "medical foundation models"
      ]
    },
    "publishedAt": "2025-04-18T13:50:43.000Z",
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
    "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f1d16fbe95ed4c9a9418fe",
      "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
      "fullname": "Yang Yue",
      "name": "yueyang2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15681",
      "authors": [
        {
          "_id": "680846defa5a6cc6bd9d2cf3",
          "name": "Vidi Team",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf4",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf5",
          "name": "Chia-Wen Kuo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf6",
          "name": "Dawei Du",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf7",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf8",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf9",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfa",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfb",
          "name": "Lu Guo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfc",
          "name": "Lusha Li",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfd",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfe",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cff",
          "name": "Rachel Deng",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d00",
          "name": "Sijie Zhu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d01",
          "name": "Stuart Siew",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d02",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d03",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d04",
          "name": "Wen Zhong",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d05",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d06",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d07",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d08",
          "name": "Xueqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:04:45.000Z",
      "submittedOnDailyAt": "2025-04-23T00:19:34.185Z",
      "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
      "submittedOnDailyBy": {
        "_id": "65cbdea6d6c974694f09249a",
        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
        "isPro": false,
        "fullname": "Jay",
        "user": "Zilence006",
        "type": "user"
      },
      "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
      "upvotes": 6,
      "discussionId": "680846dffa5a6cc6bd9d2d59",
      "ai_keywords": [
        "Vidi",
        "Large Multimodal Models (LMMs)",
        "temporal retrieval",
        "video editing scenarios",
        "temporal understanding",
        "VUE-TR benchmark",
        "IoU metric"
      ]
    },
    "publishedAt": "2025-04-22T04:04:45.000Z",
    "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbdea6d6c974694f09249a",
      "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
      "fullname": "Jay",
      "name": "Zilence006",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16030",
      "authors": [
        {
          "_id": "68084e2c59762f55a5a8b5f3",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f4",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f5",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f8",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
      ],
      "publishedAt": "2025-04-22T16:52:09.000Z",
      "submittedOnDailyAt": "2025-04-23T00:56:24.970Z",
      "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
      "submittedOnDailyBy": {
        "_id": "642435a1a3adbc7142c3b0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
        "isPro": true,
        "fullname": "Joya Chen",
        "user": "chenjoya",
        "type": "user"
      },
      "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
      "upvotes": 5,
      "discussionId": "68084e2f59762f55a5a8b721",
      "projectPage": "https://showlab.github.io/livecc/",
      "githubRepo": "https://github.com/showlab/livecc",
      "ai_keywords": [
        "Video LLMs",
        "automatic speech recognition (ASR)",
        "streaming training",
        "timestamps",
        "vision-language representation",
        "temporally-aligned",
        "fine-grained vision-language modeling",
        "data production pipeline",
        "YouTube videos",
        "closed captions (CC)",
        "Live-CC-5M",
        "Live-WhisperX-526K",
        "supervised fine-tuning (SFT)",
        "general video QA",
        "real-time video commentary",
        "LiveSports-3K benchmark",
        "LLM-as-a-judge",
        "LiveCC-7B-Base",
        "LiveCC-7B-Instruct",
        "Qwen2.5-VL-72B-Instruct",
        "LLaVA-Video-72B",
        "VideoMME",
        "OVOBench"
      ]
    },
    "publishedAt": "2025-04-22T12:52:09.000Z",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642435a1a3adbc7142c3b0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
      "fullname": "Joya Chen",
      "name": "chenjoya",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16080",
      "authors": [
        {
          "_id": "680845d997f32b8ffc13569c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569d",
          "name": "Liangbing Zhao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569e",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569f",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a0",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a1",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a2",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a3",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
      ],
      "publishedAt": "2025-04-22T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-23T00:16:04.186Z",
      "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
      "upvotes": 4,
      "discussionId": "680845de97f32b8ffc1357c7",
      "ai_keywords": [
        "text-to-image diffusion models",
        "visual quality",
        "training data",
        "model parameters",
        "self-reflection capabilities",
        "diffusion models",
        "inference-time framework",
        "noise-level scaling",
        "latent initialization",
        "prompt-level scaling",
        "semantic guidance",
        "reflection-level scaling",
        "GenRef",
        "multimodal inputs",
        "unified framework",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-04-22T13:58:07.000Z",
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
    "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 605
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16078",
      "authors": [
        {
          "_id": "68087a231e425a6eee93570d",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570e",
          "name": "Jörg Bornschein",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570f",
          "name": "Jordi Grau-Moya",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935710",
          "name": "Markus Wulfmeier",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935711",
          "name": "Razvan Pascanu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:57:14.000Z",
      "submittedOnDailyAt": "2025-04-23T03:57:30.931Z",
      "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
      "upvotes": 4,
      "discussionId": "68087a241e425a6eee93576b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "sub-optimal exploration",
        "knowing-doing gap",
        "decision-making scenarios",
        "greediness",
        "frequency bias",
        "fine-tuning",
        "Reinforcement Learning (RL)",
        "self-generated CoT rationales",
        "multi-armed bandits",
        "contextual bandits",
        "Tic-tac-toe",
        "$\\epsilon$-greedy",
        "self-correction",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-04-22T13:57:14.000Z",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15785",
      "authors": [
        {
          "_id": "6808579f91ba7dbcc19dbd3e",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd3f",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd40",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd41",
          "name": "Guodong Long",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd42",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd43",
          "name": "Jing Jiang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd44",
          "name": "Chengqi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
      ],
      "publishedAt": "2025-04-22T10:58:27.000Z",
      "submittedOnDailyAt": "2025-04-23T01:40:28.955Z",
      "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
      "upvotes": 3,
      "discussionId": "680857a191ba7dbcc19dbda6",
      "githubRepo": "https://github.com/elated-sawyer/WALL-E",
      "ai_keywords": [
        "world models",
        "large language models (LLMs)",
        "symbolic knowledge",
        "knowledge graphs",
        "scene graphs",
        "exploration trajectories",
        "executable codes",
        "policies",
        "RL-free",
        "model-based agent",
        "WALL-E 2.0",
        "model-predictive control (MPC)",
        "neurosymbolic world model",
        "look-ahead optimizer",
        "heuristics",
        "planner",
        "predictions",
        "learning efficiency",
        "open-world challenges",
        "Mars (Minecraft like)",
        "ALFWorld (embodied indoor environments)",
        "success rate",
        "score"
      ]
    },
    "publishedAt": "2025-04-22T06:58:27.000Z",
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11703",
      "authors": [
        {
          "_id": "6800a4f9f16f9f820ed748af",
          "user": {
            "_id": "64f27f74f1b6c235aed4b904",
            "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
            "isPro": false,
            "fullname": "stneng",
            "user": "stneng",
            "type": "user"
          },
          "name": "Tianneng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T10:13:56.123Z",
          "hidden": true
        },
        {
          "_id": "6800a4f9f16f9f820ed748b0",
          "name": "Jingxuan He",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b1",
          "name": "Zhun Wang",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b2",
          "name": "Linyu Wu",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b3",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b4",
          "name": "Wenbo Guo",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b5",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T01:58:40.000Z",
      "submittedOnDailyAt": "2025-04-23T00:27:37.099Z",
      "title": "Progent: Programmable Privilege Control for LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64f27f74f1b6c235aed4b904",
        "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
        "isPro": false,
        "fullname": "stneng",
        "user": "stneng",
        "type": "user"
      },
      "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
      "upvotes": 2,
      "discussionId": "6800a4faf16f9f820ed748ee",
      "ai_keywords": [
        "LLM agents",
        "large language models (LLMs)",
        "principle of least privilege",
        "privilege control mechanism",
        "domain-specific language",
        "privilege control policies",
        "tool calls",
        "agent execution",
        "fine-grained constraints",
        "fallbacks",
        "security",
        "utility",
        "policy writing",
        "automated policy generation",
        "AgentDojo",
        "ASB",
        "AgentPoison"
      ]
    },
    "publishedAt": "2025-04-15T21:58:40.000Z",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f27f74f1b6c235aed4b904",
      "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
      "fullname": "stneng",
      "name": "stneng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16082",
      "authors": [
        {
          "_id": "6808486f043aa415b647ca77",
          "name": "Ziqi Pang",
          "hidden": false
        },
        {
          "_id": "6808486f043aa415b647ca78",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:25:18.841Z",
      "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
      "submittedOnDailyBy": {
        "_id": "642a33ea5673845d9854f458",
        "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
        "isPro": false,
        "fullname": "Ziqi Pang",
        "user": "ziqipang",
        "type": "user"
      },
      "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
      "upvotes": 1,
      "discussionId": "68084870043aa415b647caaf",
      "ai_keywords": [
        "MapReduce",
        "short video clips",
        "sequence-to-sequence vision-language models (VLMs)",
        "sequence parallel perception",
        "context aggregation",
        "context reasoning",
        "key segment selection",
        "key segment retrieval",
        "Captioning",
        "standardizing",
        "repeated characters",
        "shared names",
        "Analysis",
        "relevant information",
        "final answer",
        "LVBench"
      ]
    },
    "publishedAt": "2025-04-22T13:59:41.000Z",
    "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642a33ea5673845d9854f458",
      "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
      "fullname": "Ziqi Pang",
      "name": "ziqipang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15524",
      "authors": [
        {
          "_id": "68084b46fa5a6cc6bd9e6a83",
          "name": "Qiyao Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a84",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a85",
          "name": "Hongbo Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a86",
          "name": "Huaren Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a87",
          "name": "Minghui Zhu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a88",
          "name": "Zhifei Qin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a89",
          "name": "Linwei Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8a",
          "name": "Yilin Yue",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8b",
          "name": "Shiqiang Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8c",
          "name": "Jiayan Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8d",
          "name": "Yihang Wu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8e",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8f",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a90",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a91",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a92",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a93",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a94",
          "name": "Kan Xu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a95",
          "name": "Hongfei Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a96",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a97",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a98",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a99",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T02:00:41.000Z",
      "submittedOnDailyAt": "2025-04-23T05:32:33.756Z",
      "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
      "submittedOnDailyBy": {
        "_id": "64560618bfdf9c63ce2d658a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
        "isPro": false,
        "fullname": "Mathsion Wong",
        "user": "QiYao-Wang",
        "type": "user"
      },
      "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
      "upvotes": 1,
      "discussionId": "68084b4cfa5a6cc6bd9e6c75",
      "projectPage": "https://ipbench.github.io/",
      "githubRepo": "https://github.com/IPBench/IPBench"
    },
    "publishedAt": "2025-04-21T22:00:41.000Z",
    "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
    "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64560618bfdf9c63ce2d658a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
      "fullname": "Mathsion Wong",
      "name": "QiYao-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14977",
      "authors": [
        {
          "_id": "68084dbc2eff5d45775d8f14",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f15",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f16",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f17",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f18",
          "name": "Chao Fan",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f19",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1a",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1b",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:09:21.000Z",
      "submittedOnDailyAt": "2025-04-23T00:49:31.236Z",
      "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
      "submittedOnDailyBy": {
        "_id": "6434caa64b34368fdb07da48",
        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
        "isPro": false,
        "fullname": "Jingkai Zhou",
        "user": "theFoxofSky",
        "type": "user"
      },
      "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
      "upvotes": 1,
      "discussionId": "68084dc02eff5d45775d902c",
      "projectPage": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "githubRepo": "https://github.com/damo-cv/RealisDance",
      "ai_keywords": [
        "RealisDance-DiT",
        "Wan-2.1 video foundation model",
        "Reference Net design",
        "DiT models",
        "low-noise warmup",
        "large batches and small iterations strategies",
        "foundation model architecture",
        "model convergence",
        "priors of the foundation model",
        "TikTok dataset",
        "UBC fashion video dataset"
      ]
    },
    "publishedAt": "2025-04-21T05:09:21.000Z",
    "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
    "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434caa64b34368fdb07da48",
      "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
      "fullname": "Jingkai Zhou",
      "name": "theFoxofSky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]