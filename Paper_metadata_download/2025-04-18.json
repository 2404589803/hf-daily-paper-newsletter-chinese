[
  {
    "paper": {
      "id": "2504.13161",
      "authors": [
        {
          "_id": "6801d661ed5fc062197db592",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db593",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db594",
          "name": "Yonggan Fu",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db595",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db596",
          "name": "Dan Su",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db597",
          "name": "Markus Kliegl",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db598",
          "name": "Zijia Chen",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db599",
          "name": "Peter Belcak",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59a",
          "name": "Yoshi Suhara",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59b",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59c",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59d",
          "name": "Yingyan",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59e",
          "name": "Lin",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db5a0",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:58:13.000Z",
      "submittedOnDailyAt": "2025-04-18T03:05:25.298Z",
      "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/",
      "upvotes": 42,
      "discussionId": "6801d663ed5fc062197db631",
      "ai_keywords": [
        "CLIMB",
        "semantic space",
        "proxy model",
        "predictor",
        "ClimbLab",
        "ClimbMix"
      ]
    },
    "publishedAt": "2025-04-17T13:58:13.000Z",
    "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
    "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13146",
      "authors": [
        {
          "_id": "6801b77dcb758561ae26997e",
          "name": "Yash Savani",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae26997f",
          "name": "Asher Trockman",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269980",
          "name": "Zhili Feng",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269981",
          "name": "Avi Schwarzschild",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269982",
          "name": "Alexander Robey",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269983",
          "name": "Marc Finzi",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269984",
          "name": "J. Zico Kolter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:54:14.000Z",
      "submittedOnDailyAt": "2025-04-18T00:59:33.586Z",
      "title": "Antidistillation Sampling",
      "submittedOnDailyBy": {
        "_id": "6570917c0ea91e592aff0b8c",
        "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
        "isPro": false,
        "fullname": "Avi Schwarzschild",
        "user": "schwarzschild",
        "type": "user"
      },
      "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
      "upvotes": 40,
      "discussionId": "6801b77ecb758561ae269a19",
      "projectPage": "https://antidistillation.com",
      "ai_keywords": [
        "antidistillation sampling",
        "next-token probability distribution",
        "reasoning traces"
      ]
    },
    "publishedAt": "2025-04-17T13:54:14.000Z",
    "title": "Antidistillation Sampling",
    "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13146.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6570917c0ea91e592aff0b8c",
      "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
      "fullname": "Avi Schwarzschild",
      "name": "schwarzschild",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12322",
      "authors": [
        {
          "_id": "6801d3de81552de84a537dd5",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd6",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd7",
          "name": "Zinan Tang",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd8",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd9",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dda",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537ddb",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537ddc",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T06:13:43.000Z",
      "submittedOnDailyAt": "2025-04-18T03:58:23.748Z",
      "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
      "upvotes": 17,
      "discussionId": "6801d3df81552de84a537e20",
      "githubRepo": "https://github.com/GX-XinGao/GRA",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multiple small LLMs involved framework",
        "GRA",
        "Generator",
        "Reviewer",
        "Adjudicator",
        "peer-review-inspired data synthesis pipeline",
        "data-level parity"
      ]
    },
    "publishedAt": "2025-04-11T02:13:43.000Z",
    "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
    "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13169",
      "authors": [
        {
          "_id": "6801bcd484335da5c3e32d0b",
          "name": "Tsung-Han Wu",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0c",
          "name": "Heekyung Lee",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0d",
          "name": "Jiaxin Ge",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0e",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0f",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d10",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:22.000Z",
      "submittedOnDailyAt": "2025-04-18T01:16:30.770Z",
      "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
      "upvotes": 16,
      "discussionId": "6801bcd684335da5c3e32db7",
      "projectPage": "https://reverse-vlm.github.io",
      "githubRepo": "https://github.com/tsunghan-wu/reverse_vlm",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "visual hallucinations",
        "generation adjustment",
        "post-hoc verification",
        "hallucination-aware training",
        "on-the-fly self-verification",
        "hallucination-verification dataset",
        "semi-synthetic samples",
        "inference-time retrospective resampling",
        "CHAIR-MSCOCO",
        "HaloQuest",
        "state-of-the-art hallucination reduction"
      ]
    },
    "publishedAt": "2025-04-17T13:59:22.000Z",
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13169.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12626",
      "authors": [
        {
          "_id": "6801b65181552de84a4b7e29",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "6801b65181552de84a4b7e2a",
          "name": "Maneesh Agrawala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T04:02:31.000Z",
      "submittedOnDailyAt": "2025-04-18T00:47:56.027Z",
      "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.",
      "upvotes": 16,
      "discussionId": "6801b65281552de84a4b7e42",
      "projectPage": "https://lllyasviel.github.io/frame_pack_gitpage/",
      "githubRepo": "https://github.com/lllyasviel/FramePack",
      "ai_keywords": [
        "FramePack",
        "next-frame prediction",
        "transformer context length",
        "video diffusion",
        "computation bottleneck",
        "image diffusion",
        "anti-drifting sampling",
        "inverted temporal order",
        "exposure bias",
        "existing video diffusion models",
        "parameter-efficient fine-tuning",
        "visual quality",
        "diffusion schedulers",
        "extreme flow shift timesteps"
      ]
    },
    "publishedAt": "2025-04-17T00:02:31.000Z",
    "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
    "summary": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12626.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13122",
      "authors": [
        {
          "_id": "6801b62608d748addc187952",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187953",
          "name": "Haodong Chen",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187954",
          "name": "Shengqiong Wu",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187955",
          "name": "Meng Luo",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187956",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187957",
          "name": "Xinya Du",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187958",
          "name": "Hanwang Zhang",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187959",
          "name": "Hao Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:39:41.000Z",
      "submittedOnDailyAt": "2025-04-18T00:47:30.566Z",
      "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
      "submittedOnDailyBy": {
        "_id": "6570450a78d7aca0c361a177",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
        "isPro": false,
        "fullname": "Harold Chen",
        "user": "Harold328",
        "type": "user"
      },
      "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
      "upvotes": 14,
      "discussionId": "6801b62808d748addc1879b2",
      "githubRepo": "https://github.com/HaroldChen19/VistaDPO",
      "ai_keywords": [
        "Large Video Models (LVMs)",
        "Large Language Models (LLMs)",
        "Video Hierarchical Spatial-Temporal Direct Preference Optimization",
        "Instance Level",
        "Temporal Level",
        "Perceptive Level",
        "QA pairs",
        "timestamps",
        "keyframes",
        "bounding boxes",
        "Video Hallucination",
        "Video QA",
        "Captioning"
      ]
    },
    "publishedAt": "2025-04-17T13:39:41.000Z",
    "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
    "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13122.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6570450a78d7aca0c361a177",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
      "fullname": "Harold Chen",
      "name": "Harold328",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12369",
      "authors": [
        {
          "_id": "6801a8453c431c2bbe3b5f94",
          "name": "Zeqi Xiao",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f95",
          "name": "Yushi Lan",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f96",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f97",
          "name": "Wenqi Ouyang",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f98",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f99",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f9a",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
      ],
      "publishedAt": "2025-04-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-04-18T00:24:00.506Z",
      "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
      "submittedOnDailyBy": {
        "_id": "650e37cc11f3210cf7910501",
        "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
        "isPro": false,
        "fullname": "zeqixiao",
        "user": "zeqixiao",
        "type": "user"
      },
      "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.",
      "upvotes": 13,
      "discussionId": "6801a8463c431c2bbe3b5fe4",
      "projectPage": "https://xizaoqu.github.io/worldmem/",
      "githubRepo": "https://github.com/xizaoqu/WorldMem",
      "ai_keywords": [
        "WorldMem",
        "memory bank",
        "memory units",
        "memory frames",
        "states",
        "poses",
        "timestamps",
        "memory attention mechanism",
        "scene generation",
        "long-term consistency",
        "3D spatial consistency",
        "dynamic evolution"
      ]
    },
    "publishedAt": "2025-04-16T13:59:30.000Z",
    "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
    "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e37cc11f3210cf7910501",
      "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
      "fullname": "zeqixiao",
      "name": "zeqixiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05506",
      "authors": [
        {
          "_id": "6801a137a199bc0f78da6930",
          "name": "Ahmed Masry",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6931",
          "name": "Mohammed Saidul Islam",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6932",
          "name": "Mahir Ahmed",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6933",
          "name": "Aayush Bajaj",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6934",
          "name": "Firoz Kabir",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6935",
          "name": "Aaryaman Kartha",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6936",
          "name": "Md Tahmid Rahman Laskar",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6937",
          "name": "Mizanur Rahman",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6938",
          "name": "Shadikur Rahman",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6939",
          "name": "Mehrad Shahmohammadi",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693a",
          "name": "Megh Thakkar",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693b",
          "name": "Md Rizwan Parvez",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693c",
          "name": "Enamul Hoque",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693d",
          "name": "Shafiq Joty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T21:05:06.000Z",
      "submittedOnDailyAt": "2025-04-18T00:21:30.532Z",
      "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
      "submittedOnDailyBy": {
        "_id": "63efd75a5c2ceb16fc6e98fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
        "isPro": true,
        "fullname": "Ahmed Masry",
        "user": "ahmed-masry",
        "type": "user"
      },
      "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
      "upvotes": 8,
      "discussionId": "6801a147a199bc0f78da6d4a",
      "githubRepo": "https://github.com/vis-nlp/ChartQAPro",
      "ai_keywords": [
        "Chart Question Answering (CQA)",
        "visual representations",
        "large vision-language models (LVLMs)",
        "ChartQA",
        "ChartQAPro",
        "infographics",
        "dashboards",
        "multiple-choice",
        "conversational",
        "hypothetical",
        "unanswerable questions",
        "Claude Sonnet 3.5",
        "error analyses",
        "ablation studies",
        "chart reasoning"
      ]
    },
    "publishedAt": "2025-04-07T17:05:06.000Z",
    "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
    "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05506.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63efd75a5c2ceb16fc6e98fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
      "fullname": "Ahmed Masry",
      "name": "ahmed-masry",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 69
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13145",
      "authors": [
        {
          "_id": "6801cbcc382250483109ddd4",
          "name": "Li-Cheng Lan",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd5",
          "name": "Andrew Bai",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd6",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd7",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd8",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd9",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:53:54.000Z",
      "submittedOnDailyAt": "2025-04-18T02:26:03.331Z",
      "title": "Exploring Expert Failures Improves LLM Agent Tuning",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
      "upvotes": 5,
      "discussionId": "6801cbcd382250483109de04",
      "ai_keywords": [
        "Rejection Sampling Fine-Tuning (RFT)",
        "expert-generated successful trajectories",
        "self-generated trajectories",
        "out-of-distribution (OOD)",
        "agent exploration efficiency",
        "acquisition of critical skills",
        "Exploring Expert Failures (EEF)",
        "beneficial actions",
        "harmful actions",
        "agent tuning performance",
        "WebShop",
        "SciWorld"
      ]
    },
    "publishedAt": "2025-04-17T13:53:54.000Z",
    "title": "Exploring Expert Failures Improves LLM Agent Tuning",
    "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13145.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13055",
      "authors": [
        {
          "_id": "6801eb7e3881da18a86691c3",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c4",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c5",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c7",
          "name": "Longxu Dou",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c8",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c9",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691ca",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T16:10:13.000Z",
      "submittedOnDailyAt": "2025-04-18T04:35:54.079Z",
      "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.",
      "upvotes": 4,
      "discussionId": "6801eb7f3881da18a8669226",
      "githubRepo": "https://github.com/John-AI-Lab/NoisyRollout",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "vision-language models (VLMs)",
        "policy exploration",
        "NoisyRollout",
        "visual perception",
        "reasoning process",
        "trajectories",
        "clean and moderately distorted images",
        "targeted diversity",
        "convolutional manner",
        "inductive bias",
        "noise annealing schedule",
        "training samples",
        "out-of-domain benchmarks",
        "reasoning tasks",
        "perception tasks",
        "in-domain performance"
      ]
    },
    "publishedAt": "2025-04-17T12:10:13.000Z",
    "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12364",
      "authors": [
        {
          "_id": "6801d913a0cf74448f93d5c8",
          "name": "Tianhui Song",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5c9",
          "name": "Weixin Feng",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5ca",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cb",
          "name": "Xubin Li",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cc",
          "name": "Tiezheng Ge",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cd",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5ce",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:09:45.000Z",
      "submittedOnDailyAt": "2025-04-18T03:19:21.541Z",
      "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
      "submittedOnDailyBy": {
        "_id": "649e7693a83143427691769c",
        "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
        "isPro": false,
        "fullname": "Tianhui Song",
        "user": "sthuihui",
        "type": "user"
      },
      "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.",
      "upvotes": 4,
      "discussionId": "6801d91ba0cf74448f93d7f5",
      "githubRepo": "https://github.com/MCG-NJU/DMM",
      "ai_keywords": [
        "text-to-image (T2I) generation models",
        "model checkpoints",
        "fine-tuned",
        "parameter redundancy",
        "storage cost",
        "model merging",
        "static linear interpolation",
        "parameter space",
        "style mixing",
        "style-promptable image generation pipeline",
        "style vectors",
        "score distillation",
        "compressed models",
        "versatile T2I model",
        "merging goals",
        "evaluation protocols",
        "teacher models",
        "controllable arbitrary-style generation"
      ]
    },
    "publishedAt": "2025-04-16T11:09:45.000Z",
    "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
    "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649e7693a83143427691769c",
      "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
      "fullname": "Tianhui Song",
      "name": "sthuihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13079",
      "authors": [
        {
          "_id": "6801c2a379ba651f02e807ba",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bb",
          "name": "Archiki Prasad",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bc",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bd",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T16:46:11.000Z",
      "submittedOnDailyAt": "2025-04-18T01:51:26.288Z",
      "title": "Retrieval-Augmented Generation with Conflicting Evidence",
      "submittedOnDailyBy": {
        "_id": "617df9bb402d4d8f8eee3737",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
        "isPro": false,
        "fullname": "Han Wang",
        "user": "HanNight",
        "type": "user"
      },
      "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
      "upvotes": 3,
      "discussionId": "6801c2a479ba651f02e807df",
      "githubRepo": "https://github.com/HanNight/RAMDocs",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "RAMDocs",
        "MADAM-RAG",
        "multi-agent approach",
        "aggregator",
        "disambiguated entities",
        "AmbigDocs",
        "FaithEval",
        "Llama3.3-70B-Instruct",
        "exact match score"
      ]
    },
    "publishedAt": "2025-04-17T12:46:11.000Z",
    "title": "Retrieval-Augmented Generation with Conflicting Evidence",
    "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617df9bb402d4d8f8eee3737",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
      "fullname": "Han Wang",
      "name": "HanNight",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12782",
      "authors": [
        {
          "_id": "6801cd2966aeef19a5cec2a4",
          "name": "Leyang Li",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a5",
          "name": "Shilin Lu",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a6",
          "name": "Yan Ren",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a7",
          "name": "Adams Wai-Kin Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T09:29:30.000Z",
      "submittedOnDailyAt": "2025-04-18T02:25:50.855Z",
      "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
      "submittedOnDailyBy": {
        "_id": "631c4a23aa346997917bcb89",
        "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
        "isPro": false,
        "fullname": "Shilin Lu",
        "user": "Shilin-LU",
        "type": "user"
      },
      "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
      "upvotes": 2,
      "discussionId": "6801cd2b66aeef19a5cec330",
      "ai_keywords": [
        "ANT",
        "deNoising Trajectories",
        "classifier-free guidance",
        "score function field",
        "natural image manifold",
        "augmentation-enhanced weight saliency map",
        "trajectory-aware objective"
      ]
    },
    "publishedAt": "2025-04-17T05:29:30.000Z",
    "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
    "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c4a23aa346997917bcb89",
      "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
      "fullname": "Shilin Lu",
      "name": "Shilin-LU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12157",
      "authors": [
        {
          "_id": "6801d4446d2188af01a9b6b6",
          "name": "Xiaojun Ye",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b8",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b9",
          "name": "Sheng Zhou",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6ba",
          "name": "Liangcheng Li",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6bb",
          "name": "Jiajun Bu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:04:14.000Z",
      "submittedOnDailyAt": "2025-04-18T02:56:22.746Z",
      "title": "FocusedAD: Character-centric Movie Audio Description",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": false,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .",
      "upvotes": 2,
      "discussionId": "6801d4466d2188af01a9b756",
      "ai_keywords": [
        "Character Perception Module (CPM)",
        "Dynamic Prior Module (DPM)",
        "Focused Caption Module (FCM)",
        "soft prompts",
        "zero-shot results",
        "MAD-eval-Named",
        "Cinepile-AD dataset"
      ]
    },
    "publishedAt": "2025-04-16T11:04:14.000Z",
    "title": "FocusedAD: Character-centric Movie Audio Description",
    "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12395",
      "authors": [
        {
          "_id": "6801f92ac9953c32ecda5c12",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c13",
          "name": "Yanbing Zhang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c14",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c15",
          "name": "Yiji Cheng",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c16",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c17",
          "name": "Xu Bai",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c18",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c19",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1a",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1b",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1c",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1d",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
      ],
      "publishedAt": "2025-04-16T18:01:59.000Z",
      "submittedOnDailyAt": "2025-04-18T05:34:45.132Z",
      "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
      "submittedOnDailyBy": {
        "_id": "637745113a63a2983ffbde13",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
        "isPro": false,
        "fullname": "Haofan Wang",
        "user": "wanghaofan",
        "type": "user"
      },
      "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.",
      "upvotes": 1,
      "discussionId": "6801f92ec9953c32ecda5d95",
      "projectPage": "https://instantcharacter.github.io/",
      "githubRepo": "https://github.com/Tencent/InstantCharacter",
      "ai_keywords": [
        "foundation diffusion transformer",
        "InstantCharacter",
        "high-fidelity",
        "scalable adapter",
        "stacked transformer encoders",
        "latent space",
        "textual controllability",
        "open-domain personalization",
        "character appearances",
        "poses",
        "styles",
        "large-scale character dataset",
        "paired (multi-view character)",
        "unpaired (text-image combinations)",
        "identity consistency",
        "textual editability",
        "high-fidelity images",
        "character-driven image generation"
      ]
    },
    "publishedAt": "2025-04-16T14:01:59.000Z",
    "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
    "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637745113a63a2983ffbde13",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
      "fullname": "Haofan Wang",
      "name": "wanghaofan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 75
    },
    "isAuthorParticipating": false
  }
]