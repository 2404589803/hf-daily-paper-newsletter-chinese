[
  {
    "paper": {
      "id": "2512.24615",
      "authors": [
        {
          "_id": "69564d96832867f2535257af",
          "user": {
            "_id": "622b00a776c20fee5d14501b",
            "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg",
            "isPro": false,
            "fullname": "Eason shi",
            "user": "Easonshi",
            "type": "user"
          },
          "name": "Yuchen Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-04T20:09:50.111Z",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b0",
          "name": "Yuzheng Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b1",
          "name": "Siqi Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b2",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b3",
          "name": "Lichao Chen",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b4",
          "user": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "isPro": false,
            "fullname": "Yulei Qin",
            "user": "yolay",
            "type": "user"
          },
          "name": "Yulei Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-04T20:09:48.064Z",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b5",
          "name": "Zhijian Zhou",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b6",
          "name": "Xiang Fei",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b7",
          "name": "Chaofan Qiu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b8",
          "name": "Xiaoyu Tan",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b9",
          "name": "Gang Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257ba",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bb",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bc",
          "name": "Guocan Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bd",
          "name": "Yong Mao",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257be",
          "name": "Yunsheng Wu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bf",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257c0",
          "user": {
            "_id": "647401e50da364bd0d002f2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png",
            "isPro": false,
            "fullname": "XING SUN",
            "user": "tedsun",
            "type": "user"
          },
          "name": "Xing Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-02T15:38:39.390Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T04:17:36.000Z",
      "submittedOnDailyAt": "2026-01-05T00:21:56.456Z",
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "63280915eeee4dd858083092",
        "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
        "isPro": false,
        "fullname": "Ke Li",
        "user": "tristanli",
        "type": "user"
      },
      "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
      "upvotes": 68,
      "discussionId": "69564d96832867f2535257c1",
      "projectPage": "https://tencentcloudadp.github.io/youtu-agent/",
      "githubRepo": "https://github.com/TencentCloudADP/youtu-agent",
      "githubRepoAddedBy": "user",
      "githubStars": 4059,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-30T23:17:36.000Z",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63280915eeee4dd858083092",
      "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
      "fullname": "Ke Li",
      "name": "tristanli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00393",
      "authors": [
        {
          "_id": "695b2297832867f253525d68",
          "name": "Yuxue Yang",
          "hidden": false
        },
        {
          "_id": "695b2297832867f253525d69",
          "name": "Lue Fan",
          "hidden": false
        },
        {
          "_id": "695b2297832867f253525d6a",
          "name": "Ziqi Shi",
          "hidden": false
        },
        {
          "_id": "695b2297832867f253525d6b",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "695b2297832867f253525d6c",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "695b2297832867f253525d6d",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"
      ],
      "publishedAt": "2026-01-01T17:07:30.000Z",
      "submittedOnDailyAt": "2026-01-05T02:49:46.994Z",
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "submittedOnDailyBy": {
        "_id": "66dd71d0140f04eb180d7c2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png",
        "isPro": false,
        "fullname": "Yuxue Yang",
        "user": "Yuppie1204",
        "type": "user"
      },
      "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io",
      "upvotes": 56,
      "discussionId": "695b2297832867f253525d6e",
      "projectPage": "https://neoverse-4d.github.io/",
      "githubRepo": "https://github.com/IamCreateAI/NeoVerse",
      "githubRepoAddedBy": "user",
      "githubStars": 70
    },
    "publishedAt": "2026-01-01T12:07:30.000Z",
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66dd71d0140f04eb180d7c2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png",
      "fullname": "Yuxue Yang",
      "name": "Yuppie1204",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00664",
      "authors": [
        {
          "_id": "695b237a832867f253525d70",
          "name": "Taekyung Ki",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d71",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d72",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d73",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d74",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"
      ],
      "publishedAt": "2026-01-02T11:58:48.000Z",
      "submittedOnDailyAt": "2026-01-05T00:05:44.498Z",
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
      "upvotes": 33,
      "discussionId": "695b237a832867f253525d75",
      "projectPage": "https://taekyungki.github.io/AvatarForcing/",
      "githubRepo": "https://github.com/TaekyungKi/AvatarForcing",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.",
      "ai_keywords": [
        "diffusion forcing",
        "direct preference optimization",
        "synthetic losing samples",
        "causal constraints",
        "multimodal inputs",
        "label-free learning"
      ],
      "githubStars": 4
    },
    "publishedAt": "2026-01-02T06:58:48.000Z",
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24330",
      "authors": [
        {
          "_id": "69560bcd832867f2535256fc",
          "name": "Yong Xien Chng",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256fd",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256fe",
          "name": "Wenwen Tong",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256ff",
          "name": "Xueheng Li",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525700",
          "name": "Jiandong Chen",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525701",
          "name": "Haojia Yu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525702",
          "name": "Jiefan Lu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525703",
          "name": "Hewei Guo",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525704",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525705",
          "name": "Chengjun Xie",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525706",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525707",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525708",
          "name": "Lewei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T16:31:45.000Z",
      "submittedOnDailyAt": "2026-01-05T00:14:32.572Z",
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "647d4f1236e109abce409c3b",
        "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
        "isPro": false,
        "fullname": "Wenwen Tong",
        "user": "tongww",
        "type": "user"
      },
      "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "upvotes": 29,
      "discussionId": "69560bcd832867f253525709",
      "githubRepo": "https://github.com/OpenSenseNova/SenseNova-MARS",
      "githubRepoAddedBy": "user",
      "githubStars": 12,
      "organization": {
        "_id": "64f0405f8a4cf3e5e6b38f9c",
        "name": "sensenova",
        "fullname": "SenseNova",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
      }
    },
    "publishedAt": "2025-12-30T11:31:45.000Z",
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24330.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d4f1236e109abce409c3b",
      "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
      "fullname": "Wenwen Tong",
      "name": "tongww",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64f0405f8a4cf3e5e6b38f9c",
      "name": "sensenova",
      "fullname": "SenseNova",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24271",
      "authors": [
        {
          "_id": "695b1d02832867f253525d50",
          "name": "Zhe Huang",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d51",
          "name": "Hao Wen",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d52",
          "name": "Aiming Hao",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d53",
          "name": "Bingze Song",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d54",
          "name": "Meiqi Wu",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d55",
          "name": "Jiahong Wu",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d56",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d57",
          "name": "Sheng Lu",
          "hidden": false
        },
        {
          "_id": "695b1d02832867f253525d58",
          "name": "Haoqian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T14:53:33.000Z",
      "submittedOnDailyAt": "2026-01-05T01:14:54.357Z",
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.",
      "upvotes": 21,
      "discussionId": "695b1d02832867f253525d59",
      "projectPage": "https://amap-ml.github.io/Taming-Hallucinations/",
      "githubRepo": "https://github.com/AMAP-ML/Taming-Hallucinations",
      "githubRepoAddedBy": "user",
      "githubStars": 25,
      "organization": {
        "_id": "67d11771890254196d3174e5",
        "name": "GD-ML",
        "fullname": "AMAP-ML",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
      }
    },
    "publishedAt": "2025-12-30T09:53:33.000Z",
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67d11771890254196d3174e5",
      "name": "GD-ML",
      "fullname": "AMAP-ML",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00417",
      "authors": [
        {
          "_id": "695b2238832867f253525d62",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d63",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d64",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d65",
          "name": "Quanquan Gu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"
      ],
      "publishedAt": "2026-01-01T18:11:38.000Z",
      "submittedOnDailyAt": "2026-01-05T00:00:42.191Z",
      "title": "Deep Delta Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar β(X). We provide a spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "upvotes": 9,
      "discussionId": "695b2238832867f253525d66",
      "githubRepo": "https://github.com/yifanzhang-pro/deep-delta-learning",
      "githubRepoAddedBy": "user",
      "githubStars": 204
    },
    "publishedAt": "2026-01-01T13:11:38.000Z",
    "title": "Deep Delta Learning",
    "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar β(X). We provide a spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00796",
      "authors": [
        {
          "_id": "695b646f832867f253525e0d",
          "name": "Jiewen Chan",
          "hidden": false
        },
        {
          "_id": "695b646f832867f253525e0e",
          "name": "Zhenjun Zhao",
          "hidden": false
        },
        {
          "_id": "695b646f832867f253525e0f",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/enxEXrkIl4E0MXNfFVAK4.qt"
      ],
      "publishedAt": "2026-01-02T18:59:55.000Z",
      "submittedOnDailyAt": "2026-01-05T04:47:40.380Z",
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/",
      "upvotes": 6,
      "discussionId": "695b646f832867f253525e10",
      "projectPage": "https://jiewenchan.github.io/AdaGaR/"
    },
    "publishedAt": "2026-01-02T13:59:55.000Z",
    "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "summary": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/enxEXrkIl4E0MXNfFVAK4.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24695",
      "authors": [
        {
          "_id": "695b4caa832867f253525dcd",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "695b4caa832867f253525dce",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "695b4caa832867f253525dcf",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "695b4caa832867f253525dd0",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T07:59:43.000Z",
      "submittedOnDailyAt": "2026-01-05T03:02:31.820Z",
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.",
      "upvotes": 4,
      "discussionId": "695b4cab832867f253525dd1"
    },
    "publishedAt": "2025-12-31T02:59:43.000Z",
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "summary": "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24695.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00671",
      "authors": [
        {
          "_id": "695b23dc832867f253525d77",
          "name": "Tianyu Zhao",
          "hidden": false
        },
        {
          "_id": "695b23dc832867f253525d78",
          "name": "Llion Jones",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-02T12:37:53.000Z",
      "submittedOnDailyAt": "2026-01-05T00:07:23.841Z",
      "title": "Fast-weight Product Key Memory",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
      "upvotes": 1,
      "discussionId": "695b23dc832867f253525d79",
      "ai_summary": "FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.",
      "ai_keywords": [
        "Softmax attention",
        "Product Key Memory (PKM)",
        "Fast-weight Product Key Memory (FwPKM)",
        "fast-weight",
        "local chunk-level gradient descent",
        "episodic memory",
        "Needle in a Haystack evaluations",
        "perplexity"
      ],
      "organization": {
        "_id": "65dfe46e4de6f5d5664ef3af",
        "name": "SakanaAI",
        "fullname": "Sakana AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644b983f0fbe4830f192c4f5/7bSM-11NGnrhHd15hN4W_.jpeg"
      }
    },
    "publishedAt": "2026-01-02T07:37:53.000Z",
    "title": "Fast-weight Product Key Memory",
    "summary": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00671.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "organization": {
      "_id": "65dfe46e4de6f5d5664ef3af",
      "name": "SakanaAI",
      "fullname": "Sakana AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644b983f0fbe4830f192c4f5/7bSM-11NGnrhHd15hN4W_.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00575",
      "authors": [
        {
          "_id": "695b64ed832867f253525e16",
          "name": "Ishir Garg",
          "hidden": false
        },
        {
          "_id": "695b64ed832867f253525e17",
          "name": "Neel Kolhe",
          "hidden": false
        },
        {
          "_id": "695b64ed832867f253525e18",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "695b64ed832867f253525e19",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-02T05:26:27.000Z",
      "submittedOnDailyAt": "2026-01-05T04:47:39.325Z",
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/",
      "upvotes": 1,
      "discussionId": "695b64ee832867f253525e1a",
      "ai_summary": "InfoSynth generates novel, diverse coding benchmarks for large language models using information-theoretic metrics and genetic algorithms, enabling scalable and self-verifying evaluation.",
      "ai_keywords": [
        "KL-divergence",
        "entropy",
        "genetic algorithms",
        "self-verifying pipeline"
      ],
      "organization": {
        "_id": "61f20a9ce108f2cba2dc0730",
        "name": "Berkeley",
        "fullname": "UC Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
      }
    },
    "publishedAt": "2026-01-02T00:26:27.000Z",
    "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
    "summary": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00575.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "61f20a9ce108f2cba2dc0730",
      "name": "Berkeley",
      "fullname": "UC Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00204",
      "authors": [
        {
          "_id": "695b4545832867f253525dc5",
          "name": "Xiaokun Sun",
          "hidden": false
        },
        {
          "_id": "695b4545832867f253525dc6",
          "name": "Zeyu Cai",
          "hidden": false
        },
        {
          "_id": "695b4545832867f253525dc7",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "695b4545832867f253525dc8",
          "name": "Ying Tai",
          "hidden": false
        },
        {
          "_id": "695b4545832867f253525dc9",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "695b4545832867f253525dca",
          "name": "Zhenyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-01T04:42:59.000Z",
      "submittedOnDailyAt": "2026-01-05T02:32:57.773Z",
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "submittedOnDailyBy": {
        "_id": "65813fbeabafd960c84fdf2f",
        "avatarUrl": "/avatars/d8f9cb56a2e44c3ca4472099437e0b50.svg",
        "isPro": false,
        "fullname": "Xiaokun Sun",
        "user": "XiaokunSun",
        "type": "user"
      },
      "summary": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.",
      "upvotes": 1,
      "discussionId": "695b4545832867f253525dcb",
      "projectPage": "https://xiaokunsun.github.io/MorphAny3D.github.io",
      "githubRepo": "https://github.com/XiaokunSun/MorphAny3D",
      "githubRepoAddedBy": "user",
      "githubStars": 6
    },
    "publishedAt": "2025-12-31T23:42:59.000Z",
    "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "summary": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65813fbeabafd960c84fdf2f",
      "avatarUrl": "/avatars/d8f9cb56a2e44c3ca4472099437e0b50.svg",
      "fullname": "Xiaokun Sun",
      "name": "XiaokunSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]