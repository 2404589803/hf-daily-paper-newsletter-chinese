[
  {
    "paper": {
      "id": "2503.03601",
      "authors": [
        {
          "_id": "67cbfff12cc05acaab147f07",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f08",
          "name": "Laida Kushnareva",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f09",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0a",
          "name": "Anton Razzhigaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0b",
          "name": "Anastasia Voznyuk",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0c",
          "name": "Irina Piontkovskaya",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0d",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0e",
          "name": "Serguei Barannikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T15:33:52.000Z",
      "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
      "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
      "upvotes": 58,
      "discussionId": "67cbfff22cc05acaab147f4d",
      "ai_keywords": [
        "Sparse Autoencoders",
        "Gemma-2-2b",
        "residual stream",
        "interpretability",
        "domain-specific statistics",
        "model-specific statistics",
        "steering approach",
        "LLM-based interpretation",
        "writing style",
        "information-dense domains",
        "human-like outputs"
      ]
    },
    "publishedAt": "2025-03-05T10:33:52.000Z",
    "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
    "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07605",
      "authors": [
        {
          "_id": "67cfa0c1edb742caa3572982",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572983",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572984",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572985",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572986",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572987",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572988",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572989",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298a",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298b",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:03.000Z",
      "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
      "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
      "upvotes": 32,
      "discussionId": "67cfa0c2edb742caa35729dc",
      "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
      "ai_keywords": [
        "Sparse Expert Activation Pruning (SEAP)",
        "hidden states",
        "activations",
        "task-specific expert activation patterns",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-03-10T13:59:03.000Z",
    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
    "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07365",
      "authors": [
        {
          "_id": "67cf9cd037bc7273882147a3",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a4",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a5",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a6",
          "name": "Zhixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a7",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a8",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a9",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147aa",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ab",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ac",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ad",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ae",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147af",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147b0",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T14:23:12.000Z",
      "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
      "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
      "upvotes": 32,
      "discussionId": "67cf9cd137bc7273882147e2",
      "ai_keywords": [
        "multimodal reasoning",
        "rule-based reinforcement learning (RL)",
        "large-scale rule-based reinforcement learning (RL)",
        "DeepSeek-R1",
        "multimodal space",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "multimodal reasoning capabilities",
        "rule-based RL",
        "supervised fine-tuning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-10T10:23:12.000Z",
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07002",
      "authors": [
        {
          "_id": "67cfa814d212c9c5048845a0",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a1",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a2",
          "name": "Börje F. Karlsson",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a3",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:32:53.000Z",
      "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
      "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
      "upvotes": 28,
      "discussionId": "67cfa818d212c9c504884689",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "vision towers",
        "multi-turn vision question-answering tasks",
        "multi-turn multimodal dialogue dataset (MMDiag)",
        "GPT assistant",
        "multimodal dialogue learning",
        "grounding",
        "reasoning capabilities",
        "Deliberate module",
        "Gaze module",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-03-10T03:32:53.000Z",
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
    "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07314",
      "authors": [
        {
          "_id": "67cfa750c8f2a661dc9798fe",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc9798ff",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc979900",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T13:33:27.000Z",
      "title": "Automated Movie Generation via Multi-Agent CoT Planning",
      "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
      "upvotes": 23,
      "discussionId": "67cfa752c8f2a661dc9799b8",
      "ai_keywords": [
        "MovieAgent",
        "Chain of Thought (CoT)",
        "automated movie/long-video generation",
        "multi-scene, multi-shot long-form videos",
        "coherent narrative",
        "character consistency",
        "synchronized subtitles",
        "stable audio",
        "hierarchical CoT-based reasoning",
        "multiple LLM agents",
        "director",
        "screenwriter",
        "storyboard artist",
        "location manager",
        "script faithfulness",
        "narrative coherence",
        "fully automated movie generation"
      ]
    },
    "publishedAt": "2025-03-10T09:33:27.000Z",
    "title": "Automated Movie Generation via Multi-Agent CoT Planning",
    "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07216",
      "authors": [
        {
          "_id": "67cfa6fcd77496ce0c154bdc",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdd",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bde",
          "name": "Byungjoo Kim",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:55:50.000Z",
      "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
      "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
      "upvotes": 21,
      "discussionId": "67cfa6fdd77496ce0c154c18",
      "ai_keywords": [
        "Federated Learning (FL)",
        "vision-language models (VLMs)",
        "membership inference attacks (MIAs)",
        "FedRand framework",
        "Low-Rank Adaptation (LoRA)",
        "subparameters",
        "non-private client parameters",
        "client parameters",
        "aggregation",
        "robustness"
      ]
    },
    "publishedAt": "2025-03-10T07:55:50.000Z",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07067",
      "authors": [
        {
          "_id": "67cfa99b7c95194db8d75468",
          "name": "Jongwoo Ko",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d75469",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546a",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546b",
          "name": "Tianyu Ding",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546c",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546d",
          "name": "Ilya Zharkov",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:51:32.000Z",
      "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
      "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
      "upvotes": 18,
      "discussionId": "67cfa99c7c95194db8d754bf",
      "githubRepo": "https://github.com/jongwooko/distillm-2",
      "ai_keywords": [
        "contrastive approach",
        "likelihood",
        "DistiLLM-2",
        "instruction-following",
        "code generation",
        "preference alignment",
        "vision-language extensions"
      ]
    },
    "publishedAt": "2025-03-10T04:51:32.000Z",
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06680",
      "authors": [
        {
          "_id": "67cf94d9f2b1fe815db6db40",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db41",
          "user": {
            "_id": "641a9a4b05290a135041a3ed",
            "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
            "isPro": false,
            "fullname": "Pluto",
            "user": "CharonBony",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db42",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db43",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db44",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db45",
          "name": "Guangyue Peng",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db46",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db47",
          "name": "Houfeng Wang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T16:11:57.000Z",
      "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
      "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
      "upvotes": 14,
      "discussionId": "67cf94dbf2b1fe815db6db9e"
    },
    "publishedAt": "2025-03-09T12:11:57.000Z",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
    "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
    "numComments": 5,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07027",
      "authors": [
        {
          "_id": "67cf98fd59dbba733d8c531e",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c531f",
          "name": "Yirui Yuan",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5320",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5321",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5322",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:07:17.000Z",
      "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
      "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
      "upvotes": 12,
      "discussionId": "67cf990359dbba733d8c545d",
      "ai_keywords": [
        "Unet-based diffusion models",
        "ControlNet",
        "IP-Adapter",
        "DiT (Diffusion Transformer)",
        "Condition Injection LoRA Module",
        "Condition Injection",
        "zero-shot multi-condition generalization",
        "Position-Aware Training Paradigm",
        "Position-Aware",
        "Causal Attention Mechanism",
        "KV Cache",
        "conditional generation tasks",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-03-10T04:07:17.000Z",
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
    "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06580",
      "authors": [
        {
          "_id": "67cfa71827c7f0b2db19f7c2",
          "name": "Yuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c3",
          "name": "Yuqi Yang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c4",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c5",
          "name": "Xinyan Wen",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c6",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:19:47.000Z",
      "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
      "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
      "upvotes": 9,
      "discussionId": "67cfa71927c7f0b2db19f817",
      "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
      "ai_keywords": [
        "Large Agent Models (LAMs)",
        "Chain-of-Action (CoA)",
        "AutoCoA framework",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "step-level action triggering",
        "trajectory-level CoA optimization",
        "internal world model"
      ]
    },
    "publishedAt": "2025-03-09T08:19:47.000Z",
    "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
    "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04629",
      "authors": [
        {
          "_id": "67cfbab6607797f40c6d4164",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4165",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4166",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4167",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4168",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4169",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d416a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:15:48.000Z",
      "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
      "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
      "upvotes": 9,
      "discussionId": "67cfbab9607797f40c6d4206",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "SurveyForge",
        "SurveyBench",
        "AutoSurvey"
      ]
    },
    "publishedAt": "2025-03-06T12:15:48.000Z",
    "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
    "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04812",
      "authors": [
        {
          "_id": "67ce5542818e1825dea7440b",
          "user": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "isPro": false,
            "fullname": "Zhibin Lan",
            "user": "zhibinlan",
            "type": "user"
          },
          "name": "Zhibin Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440c",
          "user": {
            "_id": "635239137d071f23d083b056",
            "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
            "isPro": false,
            "fullname": "liqiang niu",
            "user": "lqniu",
            "type": "user"
          },
          "name": "Liqiang Niu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440d",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440f",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T10:21:57.000Z",
      "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
      "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
      "upvotes": 9,
      "discussionId": "67ce5543818e1825dea74480",
      "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
      "ai_keywords": [
        "multimodal embedding models",
        "interleaved image-text retrieval",
        "multimodal RAG",
        "multimodal clustering",
        "LMM-based embedding models",
        "InfoNCE loss",
        "similarity distribution",
        "hard negative pairs",
        "representation learning",
        "LLaVE",
        "MMEB benchmark",
        "meta-tasks",
        "datasets",
        "state-of-the-art (SOTA)",
        "scalability",
        "efficiency",
        "text-video retrieval tasks",
        "zero-shot manner",
        "transfer"
      ]
    },
    "publishedAt": "2025-03-04T05:21:57.000Z",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
    "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07608",
      "authors": [
        {
          "_id": "67cfa5bcb17ca92d24da9033",
          "name": "Bo Jiang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9034",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9035",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9036",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9037",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:42.000Z",
      "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
      "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
      "upvotes": 7,
      "discussionId": "67cfa5bdb17ca92d24da9064",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning",
        "end-to-end models",
        "vision-language models (VLMs)",
        "supervised fine-tuning (SFT)",
        "GRPO-based RL rewards",
        "two-stage planning reasoning training strategy",
        "emergent multimodal planning capabilities"
      ]
    },
    "publishedAt": "2025-03-10T13:59:42.000Z",
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07602",
      "authors": [
        {
          "_id": "67cfb2efb77bc8e7d415f904",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f905",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f906",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f907",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f908",
          "name": "Longxiang Tang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f909",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90a",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90b",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90c",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90e",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:03.000Z",
      "title": "DreamRelation: Relation-Centric Video Customization",
      "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
      "upvotes": 7,
      "discussionId": "67cfb2f1b77bc8e7d415f96b",
      "ai_keywords": [
        "Relational Decoupling Learning",
        "Relational Dynamics Enhancement",
        "relation LoRA triplet",
        "hybrid mask training strategy",
        "attention mechanism",
        "space-time relational contrastive loss",
        "MM-DiT"
      ]
    },
    "publishedAt": "2025-03-10T13:58:03.000Z",
    "title": "DreamRelation: Relation-Centric Video Customization",
    "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07459",
      "authors": [
        {
          "_id": "67cfd1934fed2b7e3e4cbb34",
          "user": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "isPro": false,
            "fullname": "Xiangru Tang",
            "user": "RTT1",
            "type": "user"
          },
          "name": "Xiangru Tang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb35",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb36",
          "name": "Jiwoong Sohn",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb37",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb38",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb39",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3a",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3c",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3d",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3e",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3f",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:38:44.000Z",
      "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
      "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
      "upvotes": 7,
      "discussionId": "67cfd1944fed2b7e3e4cbb81",
      "ai_keywords": [
        "MedAgentsBench",
        "multi-step clinical reasoning",
        "diagnosis formulation",
        "treatment planning",
        "MedAgentsBench",
        "DeepSeek R1",
        "OpenAI o3",
        "advanced search-based agent methods"
      ]
    },
    "publishedAt": "2025-03-10T11:38:44.000Z",
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07507",
      "authors": [
        {
          "_id": "67cfa44c3a9d50150f59ffe1",
          "name": "Jie Hu",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe2",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T16:29:10.000Z",
      "title": "PE3R: Perception-Efficient 3D Reconstruction",
      "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
      "upvotes": 5,
      "discussionId": "67cfa4503a9d50150f5a0137",
      "ai_keywords": [
        "feed-forward architecture",
        "3D semantic field reconstruction",
        "zero-shot generalization",
        "2D-to-3D open-vocabulary segmentation",
        "perception accuracy",
        "reconstruction precision",
        "speedup"
      ]
    },
    "publishedAt": "2025-03-10T12:29:10.000Z",
    "title": "PE3R: Perception-Efficient 3D Reconstruction",
    "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06749",
      "authors": [
        {
          "_id": "67cfb6495944a8e54f24cd9a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9b",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9c",
          "name": "Zijie Zhai",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9d",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9e",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9f",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda1",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T20:06:45.000Z",
      "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
      "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
      "upvotes": 5,
      "discussionId": "67cfb64f5944a8e54f24cf33",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "MLLMs",
        "multimodal reasoning",
        "CoT dataset",
        "modality bridging",
        "data filtering",
        "Vision-R1-cold dataset",
        "Progressive Thinking Suppression Training (PTST)",
        "Group Relative Policy Optimization (GRPO)",
        "hard formatting result reward function",
        "multimodal math dataset",
        "MathVista benchmark",
        "OpenAI O1"
      ]
    },
    "publishedAt": "2025-03-09T16:06:45.000Z",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06520",
      "authors": [
        {
          "_id": "67cf990ca80a73999cc816c3",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c4",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c5",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c6",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c7",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c8",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c9",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T08:48:51.000Z",
      "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
      "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
      "upvotes": 5,
      "discussionId": "67cf990da80a73999cc81723",
      "ai_keywords": [
        "Seg-Zero",
        "decoupled architecture",
        "reasoning model",
        "segmentation model",
        "positional prompts",
        "pixel-level masks",
        "cognitive reinforcement",
        "reward mechanism",
        "reinforcement learning",
        "GRPO",
        "zero-shot generalization",
        "ReasonSeg benchmark",
        "emergent test-time reasoning"
      ]
    },
    "publishedAt": "2025-03-09T04:48:51.000Z",
    "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
    "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06121",
      "authors": [
        {
          "_id": "67cfa436d37b8309603da1ee",
          "user": {
            "_id": "66de61d7174e9c6971dbb253",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
            "isPro": false,
            "fullname": "Alic Li",
            "user": "Alic-Li",
            "type": "user"
          },
          "name": "Li weile",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
          "hidden": false
        },
        {
          "_id": "67cfa436d37b8309603da1ef",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-08T08:31:18.000Z",
      "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
      "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
      "upvotes": 5,
      "discussionId": "67cfa437d37b8309603da253",
      "ai_keywords": [
        "Transformers",
        "LSTMs",
        "GRUs",
        "RWKV-7",
        "meta-learning",
        "state update mechanism",
        "time mix",
        "channel mix",
        "Timer",
        "performance improvement",
        "training time",
        "parameters"
      ]
    },
    "publishedAt": "2025-03-08T03:31:18.000Z",
    "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
    "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07197",
      "authors": [
        {
          "_id": "67cfa76cf36e4221c5009654",
          "name": "Zebin You",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009655",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009656",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009657",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009658",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009659",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:27:12.000Z",
      "title": "Effective and Efficient Masked Image Generation Models",
      "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
      "upvotes": 3,
      "discussionId": "67cfa76df36e4221c5009686",
      "ai_keywords": [
        "masked image generation models",
        "masked diffusion models",
        "training and sampling",
        "Fr\\'echet Inception Distance (FID)",
        "function evaluations (NFEs)",
        "VAR",
        "continuous diffusion models"
      ]
    },
    "publishedAt": "2025-03-10T07:27:12.000Z",
    "title": "Effective and Efficient Masked Image Generation Models",
    "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03499",
      "authors": [
        {
          "_id": "67cb02680a2a716f25805cb4",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb5",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb6",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb7",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb8",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb9",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:44:42.000Z",
      "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
      "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
      "upvotes": 3,
      "discussionId": "67cb02690a2a716f25805cfd",
      "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Prompt Tuning",
        "Prefix-Tuning",
        "State-based methods",
        "State-offset Tuning",
        "timesteps",
        "state-related features",
        "state-at-the-current-step"
      ]
    },
    "publishedAt": "2025-03-05T08:44:42.000Z",
    "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
    "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07603",
      "authors": [
        {
          "_id": "67cfc310f2b1fe815dc24ebf",
          "name": "Sedrick Keh",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec0",
          "name": "Jean Mercat",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec1",
          "name": "Samir Yitzhak Gadre",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec2",
          "name": "Kushal Arora",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec3",
          "name": "Igor Vasiljevic",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec4",
          "name": "Benjamin Burchfiel",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec5",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec6",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec7",
          "name": "Thomas Kollar",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec8",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec9",
          "name": "Achal Dave",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:19.000Z",
      "title": "Should VLMs be Pre-trained with Image Data?",
      "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
      "upvotes": 2,
      "discussionId": "67cfc314f2b1fe815dc24fe3",
      "ai_keywords": [
        "pre-trained LLMs",
        "vision-language tasks",
        "fine-tune",
        "vision tokens"
      ]
    },
    "publishedAt": "2025-03-10T13:58:19.000Z",
    "title": "Should VLMs be Pre-trained with Image Data?",
    "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06885",
      "authors": [
        {
          "_id": "67cfc1c5182d970d40896a5e",
          "name": "Yan Yang",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a5f",
          "user": {
            "_id": "6357362f811ee2fa05070f64",
            "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
            "isPro": false,
            "fullname": "Dongxu Li",
            "user": "dxli1",
            "type": "user"
          },
          "name": "Dongxu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a60",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a61",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a62",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a63",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a64",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T03:29:18.000Z",
      "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
      "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
      "upvotes": 2,
      "discussionId": "67cfc1c7182d970d40896b1d",
      "projectPage": "https://yan98.github.io/ProBench/",
      "githubRepo": "https://github.com/Yan98/ProBench_eval",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "benchmark",
        "user queries",
        "professional expertise",
        "advanced reasoning",
        "high-quality samples",
        "daily productivity demands",
        "fields",
        "sub-fields",
        "visual perception",
        "textual understanding",
        "domain knowledge"
      ]
    },
    "publishedAt": "2025-03-09T23:29:18.000Z",
    "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
    "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02199",
      "authors": [
        {
          "_id": "67c90dad6f3ef3c2c77689b0",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b1",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b2",
          "user": {
            "_id": "67cbb6ea2cc05acaab023f75",
            "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
            "isPro": false,
            "fullname": "Zhirui Chen",
            "user": "ryanchen42",
            "type": "user"
          },
          "name": "Zhirui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b3",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:21:07.000Z",
      "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
      "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
      "upvotes": 2,
      "discussionId": "67c90dae6f3ef3c2c77689ec",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "blind faith in text",
        "modality preferences",
        "textual variations",
        "vision-centric tasks",
        "text bias",
        "instruction prompts",
        "language model size",
        "token order",
        "positional biases",
        "multi-modal data",
        "supervised fine-tuning",
        "text augmentation",
        "balanced training",
        "modality interactions"
      ]
    },
    "publishedAt": "2025-03-03T21:21:07.000Z",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07595",
      "authors": [
        {
          "_id": "67cfa440aff9c98bb3f45a56",
          "user": {
            "_id": "64b3fc1fa24816979609dcb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
            "isPro": false,
            "fullname": "Sinclair Schneider",
            "user": "SinclairSchneider",
            "type": "user"
          },
          "name": "Sinclair Schneider",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a57",
          "name": "Florian Steuber",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a58",
          "name": "Joao A. G. Schneider",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a59",
          "name": "Gabi Dreo Rodosek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:56:25.000Z",
      "title": "Detection Avoidance Techniques for Large Language Models",
      "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
      "upvotes": 1,
      "discussionId": "67cfa441aff9c98bb3f45a95",
      "ai_keywords": [
        "large language models",
        "fake news",
        "classification systems",
        "DetectGPT",
        "evasion techniques",
        "generative models",
        "temperature",
        "shallow learning-detectors",
        "fine-tuning",
        "reinforcement learning",
        "BERT-based-detectors",
        "zero-shot-detectors",
        "rephrasing"
      ]
    },
    "publishedAt": "2025-03-10T13:56:25.000Z",
    "title": "Detection Avoidance Techniques for Large Language Models",
    "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03511",
      "authors": [
        {
          "_id": "67cd7ace999766d8cd73fb18",
          "user": {
            "_id": "6732f2c24c2f18a60e76b915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
            "isPro": false,
            "fullname": "Fan",
            "user": "KianYale",
            "type": "user"
          },
          "name": "Qingyu Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb19",
          "name": "Yinghao Cai",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1a",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1b",
          "name": "Wenzhe He",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1c",
          "name": "Xudong Zheng",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1d",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1e",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1f",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:57:37.000Z",
      "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
      "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
      "upvotes": 0,
      "discussionId": "67cd7ad0999766d8cd73fb77",
      "ai_keywords": [
        "neural surface reconstruction",
        "background priors",
        "material-agnostic grasp detection",
        "transformers",
        "global prior volumes",
        "multi-view features",
        "spatial encoding",
        "narrow and sparse viewing conditions",
        "residual feature enhancement",
        "occupancy-prior volume",
        "transparent objects",
        "specular surfaces"
      ]
    },
    "publishedAt": "2025-03-05T08:57:37.000Z",
    "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
    "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]