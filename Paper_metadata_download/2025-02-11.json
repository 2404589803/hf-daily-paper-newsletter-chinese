[
  {
    "paper": {
      "id": "2502.06781",
      "authors": [
        {
          "_id": "67aacd7e078cdf445284f9f6",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f7",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f8",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f9",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fa",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fb",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fc",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fd",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fe",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9ff",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa00",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa01",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa02",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa03",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa04",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa05",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa06",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:57:29.000Z",
      "title": "Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning",
      "summary": "Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough Outcome REwArd-based reinforcement\nLearning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture researchhttps://github.com/InternLM/OREAL.",
      "upvotes": 24,
      "discussionId": "67aacd7f078cdf445284fa4b"
    },
    "publishedAt": "2025-02-10T23:18:11.727Z",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06703",
      "authors": [
        {
          "_id": "67aabf93c0f8648f68c68ce4",
          "name": "Runze Liu",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce5",
          "name": "Junqi Gao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce6",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce7",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce8",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce9",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68cea",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ceb",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T17:30:23.000Z",
      "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling",
      "summary": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.",
      "upvotes": 10,
      "discussionId": "67aabf94c0f8648f68c68d19"
    },
    "publishedAt": "2025-02-11T00:36:11.270Z",
    "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6015
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05609",
      "authors": [
        {
          "_id": "67aacaaaa03eecbc2d72835f",
          "name": "Sukmin Cho",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728360",
          "name": "Sangjin Choi",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728361",
          "name": "Taeho Hwang",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728362",
          "name": "Jeongyeon Seo",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728363",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728364",
          "name": "Huije Lee",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728365",
          "name": "Hoyun Song",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728366",
          "name": "Jong C. Park",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728367",
          "name": "Youngjin Kwon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T15:32:53.000Z",
      "title": "Lossless Acceleration of Large Language Models with Hierarchical\n  Drafting based on Temporal Locality in Speculative Decoding",
      "summary": "Accelerating inference in Large Language Models (LLMs) is critical for\nreal-time interactions, as they have been widely incorporated into real-world\nservices. Speculative decoding, a fully algorithmic solution, has gained\nattention for improving inference speed by drafting and verifying tokens,\nthereby generating multiple tokens in a single forward pass. However, current\ndrafting strategies usually require significant fine-tuning or have\ninconsistent performance across tasks. To address these challenges, we propose\nHierarchy Drafting (HD), a novel lossless drafting approach that organizes\nvarious token sources into multiple databases in a hierarchical framework based\non temporal locality. In the drafting step, HD sequentially accesses multiple\ndatabases to obtain draft tokens from the highest to the lowest locality,\nensuring consistent acceleration across diverse tasks and minimizing drafting\nlatency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters\ndemonstrate that HD outperforms existing database drafting methods, achieving\nrobust inference speedups across model sizes, tasks, and temperatures.",
      "upvotes": 10,
      "discussionId": "67aacaaca03eecbc2d728394"
    },
    "publishedAt": "2025-02-10T22:58:41.471Z",
    "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec4c04c782d648d28d70fc",
      "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
      "fullname": "Sukmin Cho",
      "name": "zomss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03628",
      "authors": [
        {
          "_id": "67aab82e6024056209d727a8",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727a9",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727aa",
          "name": "Yunhe Gao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ab",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ac",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ad",
          "name": "Yuxiao Chen",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ae",
          "name": "Ting Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727af",
          "name": "Long Zhao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b1",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:34:02.000Z",
      "title": "The Hidden Life of Tokens: Reducing Hallucination of Large\n  Vision-Language Models via Visual Information Steering",
      "summary": "Large Vision-Language Models (LVLMs) can reason effectively over both textual\nand visual inputs, but they tend to hallucinate syntactically coherent yet\nvisually ungrounded contents. In this paper, we investigate the internal\ndynamics of hallucination by examining the tokens logits rankings throughout\nthe generation process, revealing three key patterns in how LVLMs process\ninformation: (1) gradual visual information loss -- visually grounded tokens\ngradually become less favored throughout generation, and (2) early excitation\n-- semantically meaningful tokens achieve peak activation in the layers earlier\nthan the final layer. (3) hidden genuine information -- visually grounded\ntokens though not being eventually decided still retain relatively high\nrankings at inference. Based on these insights, we propose VISTA (Visual\nInformation Steering with Token-logit Augmentation), a training-free\ninference-time intervention framework that reduces hallucination while\npromoting genuine information. VISTA works by combining two complementary\napproaches: reinforcing visual information in activation space and leveraging\nearly layer activations to promote semantically meaningful decoding. Compared\nto existing methods, VISTA requires no external supervision and is applicable\nto various decoding strategies. Extensive experiments show that VISTA on\naverage reduces hallucination by abount 40% on evaluated open-ended generation\ntask, and it consistently outperforms existing methods on four benchmarks\nacross four architectures under three decoding strategies.",
      "upvotes": 7,
      "discussionId": "67aab82f6024056209d727f6"
    },
    "publishedAt": "2025-02-10T21:38:53.032Z",
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05415",
      "authors": [
        {
          "_id": "67aaea0a0acaa007694aed73",
          "name": "Chenkai Xu",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed74",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed75",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed76",
          "name": "Yishun Li",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed77",
          "name": "Tianqi Hou",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed78",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T02:52:25.000Z",
      "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and\n  Generation",
      "summary": "There has been increasing research interest in building unified multimodal\nunderstanding and generation models, among which Show-o stands as a notable\nrepresentative, demonstrating great promise for both text-to-image and\nimage-to-text generation. The inference of Show-o involves progressively\ndenoising image tokens and autoregressively decoding text tokens, and hence,\nunfortunately, suffers from inefficiency issues from both sides. This paper\nintroduces Show-o Turbo to bridge the gap. We first identify a unified\ndenoising perspective for the generation of images and text in Show-o based on\nthe parallel decoding of text tokens. We then propose to extend consistency\ndistillation (CD), a qualified approach for shortening the denoising process of\ndiffusion models, to the multimodal denoising trajectories of Show-o. We\nintroduce a trajectory segmentation strategy and a curriculum learning\nprocedure to improve the training convergence. Empirically, in text-to-image\ngeneration, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps\nwithout using classifier-free guidance (CFG), outperforming that of the\noriginal Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo\nexhibits a 1.5x speedup without significantly sacrificing performance. The code\nis available at https://github.com/zhijie-group/Show-o-Turbo.",
      "upvotes": 6,
      "discussionId": "67aaea100acaa007694aeea5"
    },
    "publishedAt": "2025-02-11T02:09:27.778Z",
    "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06772",
      "authors": [
        {
          "_id": "67aac8adfe33f6d8d695bc40",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc41",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc42",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc43",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:51:47.000Z",
      "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
      "summary": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 6,
      "discussionId": "67aac8affe33f6d8d695bcbd"
    },
    "publishedAt": "2025-02-10T22:49:56.390Z",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06788",
      "authors": [
        {
          "_id": "67aac64de37429ebdbdafc40",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc41",
          "name": "Xiaotong Li",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc42",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc43",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc44",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc45",
          "name": "Ting Pan",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc46",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc47",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc48",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:58.000Z",
      "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
      "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.",
      "upvotes": 4,
      "discussionId": "67aac64ee37429ebdbdafc96"
    },
    "publishedAt": "2025-02-10T22:40:39.442Z",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06635",
      "authors": [
        {
          "_id": "67aac0ba91e6f5eb5476ea76",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea77",
          "name": "Shu Li",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea78",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T16:31:37.000Z",
      "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM",
      "summary": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.",
      "upvotes": 3,
      "discussionId": "67aac0bb91e6f5eb5476eab8"
    },
    "publishedAt": "2025-02-10T22:20:38.168Z",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ab99dcb76bfd863eba64c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
      "fullname": "TY.Zheng",
      "name": "aaabiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06049",
      "authors": [
        {
          "_id": "67aac01bd7b18841e7c266df",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e0",
          "name": "Wenqi Wu",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e1",
          "name": "Filippos Christianos",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e2",
          "name": "Alex J. Chan",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e3",
          "name": "Fraser Greenlee",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e4",
          "name": "George Thomas",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e5",
          "name": "Marvin Purtorab",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e6",
          "name": "Andy Toulis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:11:42.000Z",
      "title": "LM2: Large Memory Models",
      "summary": "This paper introduces the Large Memory Model (LM2), a decoder-only\nTransformer architecture enhanced with an auxiliary memory module that aims to\naddress the limitations of standard Transformers in multi-step reasoning,\nrelational argumentation, and synthesizing information distributed over long\ncontexts. The proposed LM2 incorporates a memory module that acts as a\ncontextual representation repository, interacting with input tokens via cross\nattention and updating through gating mechanisms. To preserve the Transformers\ngeneral-purpose capabilities, LM2 maintains the original information flow while\nintegrating a complementary memory pathway. Experimental results on the\nBABILong benchmark demonstrate that the LM2model outperforms both the\nmemory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3%\non average across tasks. LM2 exhibits exceptional capabilities in multi-hop\ninference, numerical reasoning, and large-context question-answering. On the\nMMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model,\ndemonstrating that its memory module does not degrade performance on general\ntasks. Further, in our analysis, we explore the memory interpretability,\neffectiveness of memory modules, and test-time behavior. Our findings emphasize\nthe importance of explicit memory in enhancing Transformer architectures.",
      "upvotes": 3,
      "discussionId": "67aac01dd7b18841e7c26739"
    },
    "publishedAt": "2025-02-10T22:13:17.117Z",
    "title": "LM2: Large Memory Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6489e10ca13f65198dc6e122",
      "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
      "fullname": "Kang",
      "name": "JaxonK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06155",
      "authors": [
        {
          "_id": "67aab9b4a2bf5e5ea03d4c19",
          "name": "Hangliang Ding",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1a",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1b",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1c",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1d",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1f",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T05:00:56.000Z",
      "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile",
      "summary": "Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
      "upvotes": 3,
      "discussionId": "67aab9bca2bf5e5ea03d4e3c"
    },
    "publishedAt": "2025-02-10T22:09:58.181Z",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 82
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06786",
      "authors": [
        {
          "_id": "67aae91b83b1182df7c0cf54",
          "name": "Pranav Nair",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf55",
          "name": "Puranjay Datta",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf56",
          "name": "Jeff Dean",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf57",
          "name": "Prateek Jain",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf58",
          "name": "Aditya Kusupati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:10.000Z",
      "title": "Matryoshka Quantization",
      "summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.",
      "upvotes": 2,
      "discussionId": "67aae91d83b1182df7c0cff6"
    },
    "publishedAt": "2025-02-11T01:07:50.116Z",
    "title": "Matryoshka Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6015
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06782",
      "authors": [
        {
          "_id": "67aae76c71a9983f50e134ef",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f0",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f1",
          "name": "Yutong Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f2",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f3",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f4",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f5",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f6",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f7",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f8",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f9",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fa",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fb",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fc",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fe",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134ff",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13500",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13501",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:58:11.000Z",
      "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT",
      "summary": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
      "upvotes": 2,
      "discussionId": "67aae76e71a9983f50e1357d"
    },
    "publishedAt": "2025-02-11T01:00:25.383Z",
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6015
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05431",
      "authors": [
        {
          "_id": "67aac392385da1f07cc7fcbd",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbe",
          "name": "Tianqi Chen",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbf",
          "name": "Beidi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T03:41:16.000Z",
      "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
      "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n(APE), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5times\nspeedup by reducing 28times prefilling time for a 128K-length context.",
      "upvotes": 2,
      "discussionId": "67aac393385da1f07cc7fd17"
    },
    "publishedAt": "2025-02-10T22:29:36.102Z",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64f58b970b24e548a85522bc",
      "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
      "fullname": "Xinyu Yang",
      "name": "Hanyuezhuohua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04370",
      "authors": [
        {
          "_id": "67aafd90141fac22732a79b3",
          "name": "Zhenglin Zhou",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b4",
          "name": "Xiaobo Xia",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b5",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b6",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T11:03:08.000Z",
      "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via\n  Direct Preference Optimization",
      "summary": "Text-to-3D generation automates 3D content creation from textual\ndescriptions, which offers transformative potential across various fields.\nHowever, existing methods often struggle to align generated content with human\npreferences, limiting their applicability and flexibility. To address these\nlimitations, in this paper, we propose DreamDPO, an optimization-based\nframework that integrates human preferences into the 3D generation process,\nthrough direct preference optimization. Practically, DreamDPO first constructs\npairwise examples, then compare their alignment with human preferences using\nreward or large multimodal models, and lastly optimizes the 3D representation\nwith a preference-driven loss function. By leveraging pairwise comparison to\nreflect preferences, DreamDPO reduces reliance on precise pointwise quality\nevaluations while enabling fine-grained controllability through\npreference-guided optimization. Experiments demonstrate that DreamDPO achieves\ncompetitive results, and provides higher-quality and more controllable 3D\ncontent compared to existing methods. The code and models will be open-sourced.",
      "upvotes": 1,
      "discussionId": "67aafd94141fac22732a7adc"
    },
    "publishedAt": "2025-02-11T02:46:33.870Z",
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05957",
      "authors": [
        {
          "_id": "67aaecec114e64d6e15e7f41",
          "name": "Jiabin Tang",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f42",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f43",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T16:53:56.000Z",
      "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
      "summary": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce MetaChain-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, MetaChain comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, MetaChain\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate MetaChain's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.",
      "upvotes": 1,
      "discussionId": "67aaecef114e64d6e15e802c"
    },
    "publishedAt": "2025-02-11T01:33:35.134Z",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b751cc5f633a7fa84b325",
      "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
      "fullname": "Tang",
      "name": "Jiabin99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06527",
      "authors": [
        {
          "_id": "67aae4128d478dcb4b39a097",
          "name": "D. She",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a098",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a099",
          "name": "Jingxuan Pang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09a",
          "name": "Jin Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09b",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09c",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09d",
          "name": "Guanghao Zhang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09e",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09f",
          "name": "Qihan Huang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a0",
          "name": "Haobin Tang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a1",
          "name": "Yunlong Yu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a2",
          "name": "Siming Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:50:32.000Z",
      "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers",
      "summary": "Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.",
      "upvotes": 1,
      "discussionId": "67aae4178d478dcb4b39a1e7"
    },
    "publishedAt": "2025-02-11T00:46:11.168Z",
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6015
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06023",
      "authors": [
        {
          "_id": "67aac3a9ef5570c0c9047095",
          "user": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "isPro": false,
            "fullname": "Amir",
            "user": "sahsaeedi",
            "type": "user"
          },
          "name": "Amir Saeidi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047096",
          "name": "Yiran Luo",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047097",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047098",
          "name": "Shamanthak Hegde",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047099",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709a",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709b",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T20:34:43.000Z",
      "title": "Dual Caption Preference Optimization for Diffusion Models",
      "summary": "Recent advancements in human preference optimization, originally developed\nfor Large Language Models (LLMs), have shown significant potential in improving\ntext-to-image diffusion models. These methods aim to learn the distribution of\npreferred samples while distinguishing them from less preferred ones. However,\nexisting preference datasets often exhibit overlap between these distributions,\nleading to a conflict distribution. Additionally, we identified that input\nprompts contain irrelevant information for less preferred images, limiting the\ndenoising network's ability to accurately predict noise in preference\noptimization methods, known as the irrelevant prompt issue. To address these\nchallenges, we propose Dual Caption Preference Optimization (DCPO), a novel\napproach that utilizes two distinct captions to mitigate irrelevant prompts. To\ntackle conflict distribution, we introduce the Pick-Double Caption dataset, a\nmodified version of Pick-a-Pic v2 with separate captions for preferred and less\npreferred images. We further propose three different strategies for generating\ndistinct captions: captioning, perturbation, and hybrid methods. Our\nexperiments show that DCPO significantly improves image quality and relevance\nto prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO,\nand MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval,\nCLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.",
      "upvotes": 1,
      "discussionId": "67aac3b1ef5570c0c9047264"
    },
    "publishedAt": "2025-02-10T22:33:17.468Z",
    "title": "Dual Caption Preference Optimization for Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640f6299ef5c6dcac8b1df52",
      "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
      "fullname": "Amir",
      "name": "sahsaeedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06764",
      "authors": [
        {
          "_id": "67aac6052c02e43558b6b4b0",
          "name": "Kiwhan Song",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b1",
          "name": "Boyuan Chen",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b2",
          "name": "Max Simchowitz",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b3",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b4",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b5",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:44:25.000Z",
      "title": "History-Guided Video Diffusion",
      "summary": "Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Website: https://boyuan.space/history-guidance",
      "upvotes": 0,
      "discussionId": "67aac6072c02e43558b6b543"
    },
    "publishedAt": "2025-02-11T00:55:33.866Z",
    "title": "History-Guided Video Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6015
    },
    "isAuthorParticipating": false
  }
]