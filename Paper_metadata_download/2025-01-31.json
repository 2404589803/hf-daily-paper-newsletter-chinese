[
  {
    "paper": {
      "id": "2501.18492",
      "authors": [
        {
          "_id": "679c4ac5e2c0dbf282597d35",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d36",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d37",
          "name": "Shengfang Zhai",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d38",
          "name": "Jun Xia",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d39",
          "name": "Tianyi Wu",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3a",
          "name": "Zhiwei Xue",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3b",
          "name": "Yulin Chen",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3c",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3d",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3e",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T17:06:06.000Z",
      "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
      "summary": "As LLMs increasingly impact safety-critical applications, ensuring their\nsafety using guardrails remains a key challenge. This paper proposes\nGuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to\nreason. Concretely, we first create the GuardReasonerTrain dataset, which\nconsists of 127K samples with 460K detailed reasoning steps. Then, we introduce\nreasoning SFT to unlock the reasoning capability of guard models. In addition,\nwe present hard sample DPO to further strengthen their reasoning ability. In\nthis manner, GuardReasoner achieves better performance, explainability, and\ngeneralizability. Extensive experiments and analyses on 13 benchmarks of 3\nguardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B\nsurpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on\naverage. We release the training data, code, and models with different scales\n(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
      "upvotes": 27,
      "discussionId": "679c4ac6e2c0dbf282597d80"
    },
    "publishedAt": "2025-01-30T23:01:47.466Z",
    "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/Kza1q-PVKsgu_6SaQ9Oze.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/rqViZgnFQQJcAfgC1a17n.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/5Dk0HJkhOCoSXoWdVUzBo.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/DWg1wTHDx939H4bZPVj1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18585",
      "authors": [
        {
          "_id": "679c5ca666c379e215bc9e74",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e75",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e76",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e77",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e78",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e79",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7a",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7b",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7c",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7d",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7e",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7f",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e80",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e81",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T18:58:18.000Z",
      "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
      "summary": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.",
      "upvotes": 4,
      "discussionId": "679c5ca766c379e215bc9eb1"
    },
    "publishedAt": "2025-01-31T00:16:36.453Z",
    "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18585.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5875
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16411",
      "authors": [
        {
          "_id": "679c4f344061a1ab60ebe6fa",
          "name": "Wei Chow",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fb",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fc",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fd",
          "name": "Daniel Seita",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fe",
          "name": "Vitor Guizilini",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6ff",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T18:59:58.000Z",
      "title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for\n  Physical World Understanding",
      "summary": "Understanding the physical world is a fundamental challenge in embodied AI,\ncritical for enabling agents to perform complex tasks and operate safely in\nreal-world environments. While Vision-Language Models (VLMs) have shown great\npromise in reasoning and task planning for embodied agents, their ability to\ncomprehend physical phenomena remains extremely limited. To close this gap, we\nintroduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'\nphysical world understanding capability across a diverse set of tasks.\nPhysBench contains 10,002 entries of interleaved video-image-text data,\ncategorized into four major domains: physical object properties, physical\nobject relationships, physical scene understanding, and physics-based dynamics,\nfurther divided into 19 subclasses and 8 distinct capability dimensions. Our\nextensive experiments, conducted on 75 representative VLMs, reveal that while\nthese models excel in common-sense reasoning, they struggle with understanding\nthe physical world -- likely due to the absence of physical knowledge in their\ntraining data and the lack of embedded physical priors. To tackle the\nshortfall, we introduce PhysAgent, a novel framework that combines the\ngeneralization strengths of VLMs with the specialized expertise of vision\nmodels, significantly enhancing VLMs' physical understanding across a variety\nof tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results\ndemonstrate that enhancing VLMs' physical world understanding capabilities can\nhelp embodied agents such as MOKA. We believe that PhysBench and PhysAgent\noffer valuable insights and contribute to bridging the gap between VLMs and\nphysical world understanding.",
      "upvotes": 3,
      "discussionId": "679c4f394061a1ab60ebe7f0"
    },
    "publishedAt": "2025-01-30T23:19:24.751Z",
    "title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644b71ddb2e7823a76abcf91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
      "fullname": "zhou wei",
      "name": "WeiChow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18438",
      "authors": [
        {
          "_id": "679c7d0ebd893fb2b7159aa3",
          "user": {
            "_id": "657b3a44de028a439ea2ed9d",
            "avatarUrl": "/avatars/9f05e8eb6809a0ce1b50cd1fc9b5a044.svg",
            "isPro": false,
            "fullname": "Aitor Arrieta",
            "user": "aitorarrieta",
            "type": "user"
          },
          "name": "Aitor Arrieta",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-31T07:34:38.875Z",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa4",
          "name": "Miriam Ugarte",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa5",
          "name": "Pablo Valle",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa6",
          "name": "José Antonio Parejo",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa7",
          "user": {
            "_id": "6790d642a1863df579840ae3",
            "avatarUrl": "/avatars/a10a6f4af327c1bb67513c56d7f84820.svg",
            "isPro": false,
            "fullname": "Sergio Segura",
            "user": "ssegura",
            "type": "user"
          },
          "name": "Sergio Segura",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-31T07:34:38.876Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T15:45:56.000Z",
      "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
      "summary": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this paper we\nconduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generate and execute a total of\n1260 unsafe test inputs on both models. After conducting a semi-automated\nassessment of the outcomes provided by both LLMs, the results indicate that\nDeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our\nevaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts\nwhereas o3-mini only to 1.19%.",
      "upvotes": 1,
      "discussionId": "679c7d0ebd893fb2b7159af5"
    },
    "publishedAt": "2025-01-31T02:35:40.107Z",
    "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65001514f322f9156663f096",
      "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
      "fullname": "Pablo Valle",
      "name": "pablovalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18009",
      "authors": [
        {
          "_id": "679c5b0259e9218a222ab742",
          "user": {
            "_id": "6689f7fb8c440fe1955a51b5",
            "avatarUrl": "/avatars/9b23ee2f05f55615c6174a678436b30d.svg",
            "isPro": false,
            "fullname": "Lan Pan",
            "user": "louanna",
            "type": "user"
          },
          "name": "Lan Pan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-31T06:33:49.785Z",
          "hidden": false
        },
        {
          "_id": "679c5b0259e9218a222ab743",
          "name": "Hanbo Xie",
          "hidden": false
        },
        {
          "_id": "679c5b0259e9218a222ab744",
          "name": "Robert C. Wilson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T21:51:17.000Z",
      "title": "Large Language Models Think Too Fast To Explore Effectively",
      "summary": "Large Language Models have emerged many intellectual capacities. While\nnumerous benchmarks assess their intelligence, limited attention has been given\nto their ability to explore, an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with those\ntraditional LLMs relying primarily on uncertainty driven strategies, unlike\nhumans who balance uncertainty and empowerment. Representational analysis of\nthe models with Sparse Autoencoders revealed that uncertainty and choices are\nrepresented at earlier transformer blocks, while empowerment values are\nprocessed later, causing LLMs to think too fast and make premature decisions,\nhindering effective exploration. These findings shed light on the limitations\nof LLM exploration and suggest directions for improving their adaptability.",
      "upvotes": 1,
      "discussionId": "679c5b0359e9218a222ab76f"
    },
    "publishedAt": "2025-01-31T00:09:40.077Z",
    "title": "Large Language Models Think Too Fast To Explore Effectively",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5875
    },
    "isAuthorParticipating": false
  }
]