[
  {
    "paper": {
      "id": "2510.13554",
      "authors": [
        {
          "_id": "68f04b7636f8b025381e18bf",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c0",
          "name": "Zhichen Dong",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c1",
          "name": "Yuhan Sun",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c2",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c3",
          "name": "Shaopan Xiong",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c4",
          "name": "Yijia Luo",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c5",
          "name": "Jiashun Liu",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c6",
          "user": {
            "_id": "66825d697b0920f40d16c74d",
            "avatarUrl": "/avatars/0048c8865a8de94218cde580fe0f8bc9.svg",
            "isPro": false,
            "fullname": "Han Lu",
            "user": "SJTULH",
            "type": "user"
          },
          "name": "Han Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:32.455Z",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c7",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c8",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18c9",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68f04b7636f8b025381e18ca",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T13:49:51.000Z",
      "submittedOnDailyAt": "2025-10-16T01:50:37.335Z",
      "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6570587b44d0620d221c722b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
        "isPro": false,
        "fullname": "Yang Li (SJTU & SII)",
        "user": "yangcole",
        "type": "user"
      },
      "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
      "upvotes": 37,
      "discussionId": "68f04b7636f8b025381e18cb",
      "ai_summary": "Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.",
      "ai_keywords": [
        "Large language models",
        "Reinforcement learning",
        "attention heads",
        "locally focused",
        "globally focused",
        "Windowed Average Attention Distance",
        "Future Attention Influence",
        "preplan tokens",
        "anchor tokens",
        "targeted credit assignment"
      ],
      "organization": {
        "_id": "64488b334988ee01f2a8d856",
        "name": "alibaba-inc",
        "fullname": "alibaba-inc",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
      }
    },
    "publishedAt": "2025-10-15T09:49:51.000Z",
    "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
    "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6570587b44d0620d221c722b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
      "fullname": "Yang Li (SJTU & SII)",
      "name": "yangcole",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "64488b334988ee01f2a8d856",
      "name": "alibaba-inc",
      "fullname": "alibaba-inc",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13678",
      "authors": [
        {
          "_id": "68f051ed36f8b025381e1902",
          "name": "Xinyang Li",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1903",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1904",
          "user": {
            "_id": "64b4b415a15d33a1bcc6ba6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4b415a15d33a1bcc6ba6a/vqxjy7NFiRJMK9gdtOaz_.jpeg",
            "isPro": false,
            "fullname": "Zixiao Gu",
            "user": "nightkiller",
            "type": "user"
          },
          "name": "Zixiao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:05.791Z",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1905",
          "name": "Shengchuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1906",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1907",
          "name": "Liujuan Cao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
      ],
      "publishedAt": "2025-10-15T15:35:48.000Z",
      "submittedOnDailyAt": "2025-10-16T00:36:24.608Z",
      "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
      "submittedOnDailyBy": {
        "_id": "629631565de6e0eb3292afed",
        "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
        "isPro": false,
        "fullname": "Xinyang Li",
        "user": "imlixinyang",
        "type": "user"
      },
      "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
      "upvotes": 35,
      "discussionId": "68f051ed36f8b025381e1908",
      "projectPage": "https://imlixinyang.github.io/FlashWorld-Project-Page/",
      "githubRepo": "https://github.com/imlixinyang/FlashWorld",
      "ai_summary": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.",
      "ai_keywords": [
        "generative model",
        "3D scenes",
        "single image",
        "text prompt",
        "3D Gaussian representations",
        "MV-oriented paradigm",
        "3D-oriented approach",
        "dual-mode pre-training",
        "cross-mode post-training",
        "video diffusion model",
        "multi-view diffusion model",
        "cross-mode post-training distillation",
        "denoising steps",
        "single-view images",
        "out-of-distribution inputs"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-10-15T11:35:48.000Z",
    "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13678.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629631565de6e0eb3292afed",
      "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
      "fullname": "Xinyang Li",
      "name": "imlixinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13344",
      "authors": [
        {
          "_id": "68f03fba36f8b025381e188c",
          "name": "Zhenyu Liu",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e188d",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e188e",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e188f",
          "name": "Qixun Teng",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1890",
          "name": "Shenyuan Jiang",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1891",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1892",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:35.490Z",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1893",
          "name": "Jinchao Li",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1894",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1895",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1896",
          "name": "Fanbo Meng",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1897",
          "name": "Mingjun Zhao",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1898",
          "name": "Yu Xu",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e1899",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e189a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "68f03fba36f8b025381e189b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T09:30:25.000Z",
      "submittedOnDailyAt": "2025-10-16T01:03:52.180Z",
      "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
      "submittedOnDailyBy": {
        "_id": "64380ae1819f3ab20d17431b",
        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
        "isPro": false,
        "fullname": "ZhenyuLiu",
        "user": "foggyforest",
        "type": "user"
      },
      "summary": "Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.\nArchitecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic\nexpert number allocation, and a hybrid expert design comprising routed experts\nfor domain-specific knowledge, shared experts for domain-agnostic features, and\nnull experts for adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1) Independent Specialist Training\nleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecialized MoE architecture and curated training strategies in advancing the\nfield of universal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html",
      "upvotes": 33,
      "discussionId": "68f03fba36f8b025381e189c",
      "projectPage": "https://mukioxun.github.io/Uni-MoE-site/home.html",
      "githubRepo": "https://github.com/HITsz-TMG/Uni-MoE/tree/master/UniMoE-Audio",
      "ai_summary": "UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.",
      "ai_keywords": [
        "Dynamic-Capacity Mixture-of-Experts",
        "MoE",
        "Top-P routing strategy",
        "hybrid expert design",
        "routed experts",
        "shared experts",
        "null experts",
        "Independent Specialist Training",
        "MoE Integration and Warmup",
        "Synergistic Joint Training",
        "universal audio generation"
      ],
      "githubStars": 774,
      "organization": {
        "_id": "629867d7f2bf8bd3e468706e",
        "name": "HIT-TMG",
        "fullname": "HITsz-Text and Multimodal Generative Intelligence Group(TMG)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png"
      }
    },
    "publishedAt": "2025-10-15T05:30:25.000Z",
    "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
    "summary": "Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.\nArchitecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic\nexpert number allocation, and a hybrid expert design comprising routed experts\nfor domain-specific knowledge, shared experts for domain-agnostic features, and\nnull experts for adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1) Independent Specialist Training\nleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecialized MoE architecture and curated training strategies in advancing the\nfield of universal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64380ae1819f3ab20d17431b",
      "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
      "fullname": "ZhenyuLiu",
      "name": "foggyforest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "629867d7f2bf8bd3e468706e",
      "name": "HIT-TMG",
      "fullname": "HITsz-Text and Multimodal Generative Intelligence Group(TMG)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658735061824-62986540f2bf8bd3e468622a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13795",
      "authors": [
        {
          "_id": "68f05e7636f8b025381e19af",
          "user": {
            "_id": "63b2efb5922f26a27e76381c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
            "isPro": false,
            "fullname": "Yi Zhang",
            "user": "uyzhang",
            "type": "user"
          },
          "name": "Yi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:35:15.644Z",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b0",
          "name": "Bolin Ni",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b1",
          "name": "Xin-Sheng Chen",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b2",
          "name": "Heng-Rui Zhang",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b3",
          "name": "Yongming Rao",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b4",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b5",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b6",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b7",
          "user": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "isPro": false,
            "fullname": "MenghaoGuo",
            "user": "MenghaoGuo",
            "type": "user"
          },
          "name": "Meng-Hao Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:31.512Z",
          "hidden": false
        },
        {
          "_id": "68f05e7636f8b025381e19b8",
          "name": "Shi-Min Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:52:59.000Z",
      "submittedOnDailyAt": "2025-10-16T01:26:28.869Z",
      "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
      "submittedOnDailyBy": {
        "_id": "6571b51fd5c6a6d3b0ba68ad",
        "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
        "isPro": false,
        "fullname": "gmh",
        "user": "menghao22",
        "type": "user"
      },
      "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
      "upvotes": 31,
      "discussionId": "68f05e7736f8b025381e19b9",
      "projectPage": "https://open-bee.github.io/",
      "ai_summary": "A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.",
      "ai_keywords": [
        "multimodal large language models",
        "supervised fine-tuning",
        "Chain-of-Thought",
        "Honey-Data-15M",
        "HoneyPipe",
        "DataStudio",
        "Bee-8B",
        "state-of-the-art"
      ],
      "organization": {
        "_id": "68e8c7928bba74b5f4ab0299",
        "name": "Open-Bee",
        "fullname": "Open-Bee",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b2efb5922f26a27e76381c/PU469_PkFTS-Gs40EDprp.png"
      }
    },
    "publishedAt": "2025-10-15T13:52:59.000Z",
    "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
    "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6571b51fd5c6a6d3b0ba68ad",
      "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
      "fullname": "gmh",
      "name": "menghao22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "68e8c7928bba74b5f4ab0299",
      "name": "Open-Bee",
      "fullname": "Open-Bee",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b2efb5922f26a27e76381c/PU469_PkFTS-Gs40EDprp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13747",
      "authors": [
        {
          "_id": "68f0571d36f8b025381e1957",
          "name": "Wenwen Tong",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1958",
          "user": {
            "_id": "662f58bc5cf9dca4a2b225ca",
            "avatarUrl": "/avatars/4b4f989621ddf2882b3dcdc43de4daf0.svg",
            "isPro": false,
            "fullname": "Hewei Guo",
            "user": "heweiguo",
            "type": "user"
          },
          "name": "Hewei Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:36.074Z",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1959",
          "name": "Dongchuan Ran",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195a",
          "user": {
            "_id": "65531be65d55ccb20e97f34a",
            "avatarUrl": "/avatars/ba919227d8c0b3532b62bb3a2d2d6ff3.svg",
            "isPro": false,
            "fullname": "Chan",
            "user": "Chris2me",
            "type": "user"
          },
          "name": "Jiangnan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T07:25:38.654Z",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195b",
          "name": "Jiefan Lu",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195c",
          "name": "Kaibin Wang",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195d",
          "name": "Keqiang Li",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195e",
          "name": "Xiaoxu Zhu",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e195f",
          "name": "Jiakui Li",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1960",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1961",
          "name": "Xueheng Li",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1962",
          "name": "Lumin Li",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1963",
          "name": "Chenxu Guo",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1964",
          "name": "Jiasheng Zhou",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1965",
          "name": "Jiandong Chen",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1966",
          "name": "Xianye Wu",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1967",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1968",
          "name": "Silei Wu",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1969",
          "name": "Lei Chen",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196a",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196b",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196c",
          "name": "Dinghao Zhou",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196d",
          "name": "Guiping Zhong",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196e",
          "name": "Ken Zheng",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e196f",
          "name": "Shiyin Kang",
          "hidden": false
        },
        {
          "_id": "68f0571d36f8b025381e1970",
          "name": "Lewei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T16:52:48.000Z",
      "submittedOnDailyAt": "2025-10-16T01:06:55.300Z",
      "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
      "submittedOnDailyBy": {
        "_id": "647d4f1236e109abce409c3b",
        "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
        "isPro": false,
        "fullname": "Wenwen Tong",
        "user": "tongww",
        "type": "user"
      },
      "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.",
      "upvotes": 27,
      "discussionId": "68f0571d36f8b025381e1971",
      "ai_summary": "InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.",
      "ai_keywords": [
        "omni-modal large language model",
        "audio-visual multi-turn interaction",
        "vision encoder",
        "audio encoder",
        "large language model",
        "speech decoder",
        "multi-stage training strategy",
        "omni-modal understanding",
        "speech conversation",
        "audio-visual interaction",
        "multi-turn training dataset",
        "multi-modal multi-turn memory benchmark",
        "multi-turn speech interaction benchmark",
        "long-term memory capabilities",
        "Qwen2.5-Omni-7B",
        "InteractiveOmni-4B",
        "InteractiveOmni-8B",
        "image understanding",
        "audio understanding",
        "video understanding",
        "speech generation"
      ]
    },
    "publishedAt": "2025-10-15T12:52:48.000Z",
    "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
    "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d4f1236e109abce409c3b",
      "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
      "fullname": "Wenwen Tong",
      "name": "tongww",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13626",
      "authors": [
        {
          "_id": "68f05beb36f8b025381e1995",
          "name": "Senyu Fei",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e1996",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:33.758Z",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e1997",
          "name": "Junhao Shi",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e1998",
          "name": "Zihao Dai",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e1999",
          "name": "Jikun Cai",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199a",
          "name": "Pengfang Qian",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199b",
          "name": "Li Ji",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199c",
          "name": "Xinzhe He",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199d",
          "name": "Shiduo Zhang",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199e",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e199f",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e19a0",
          "name": "Jingjing Gong",
          "hidden": false
        },
        {
          "_id": "68f05beb36f8b025381e19a1",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:51:36.000Z",
      "submittedOnDailyAt": "2025-10-16T01:20:05.695Z",
      "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
      "upvotes": 22,
      "discussionId": "68f05beb36f8b025381e19a2",
      "projectPage": "https://sylvestf.github.io/LIBERO-plus/",
      "githubRepo": "https://github.com/sylvestf/LIBERO-plus",
      "ai_summary": "State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.",
      "ai_keywords": [
        "Visual-Language-Action models",
        "vulnerability analysis",
        "perturbations",
        "objects layout",
        "camera viewpoints",
        "robot initial states",
        "language instructions",
        "light conditions",
        "background textures",
        "sensor noise",
        "brittleness",
        "reliability"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "fnlp",
        "fullname": "OpenMOSS (SII, Fudan NLP)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
      }
    },
    "publishedAt": "2025-10-15T10:51:36.000Z",
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
    "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13626.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "fnlp",
      "fullname": "OpenMOSS (SII, Fudan NLP)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07944",
      "authors": [
        {
          "_id": "68f05c6336f8b025381e19a4",
          "name": "Tianrui Zhang",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19a5",
          "name": "Yichen Liu",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19a6",
          "name": "Zilin Guo",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19a7",
          "name": "Yuxin Guo",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19a8",
          "name": "Jingcheng Ni",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19a9",
          "name": "Chenjing Ding",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19aa",
          "name": "Dan Xu",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19ab",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68f05c6336f8b025381e19ac",
          "name": "Zehuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T08:41:58.000Z",
      "submittedOnDailyAt": "2025-10-16T01:16:54.120Z",
      "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "6572dcc6bbd6664053b1fa6b",
        "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
        "isPro": false,
        "fullname": "Liu Yichen",
        "user": "lyclyc52",
        "type": "user"
      },
      "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
      "upvotes": 21,
      "discussionId": "68f05c6336f8b025381e19ad",
      "projectPage": "https://sensetime-fvg.github.io/CVD-STORM/",
      "ai_summary": "CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.",
      "ai_keywords": [
        "cross-view video diffusion model",
        "spatial-temporal reconstruction",
        "Variational Autoencoder (VAE)",
        "4D reconstruction",
        "FID",
        "FVD",
        "Gaussian Splatting Decoder"
      ]
    },
    "publishedAt": "2025-10-09T04:41:58.000Z",
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
    "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6572dcc6bbd6664053b1fa6b",
      "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
      "fullname": "Liu Yichen",
      "name": "lyclyc52",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13809",
      "authors": [
        {
          "_id": "68f04e9936f8b025381e18d2",
          "user": {
            "_id": "64775b6ce5ddd7d2a16fadee",
            "avatarUrl": "/avatars/19af37ffd7626ebc58b387a34a8f98d7.svg",
            "isPro": false,
            "fullname": "Sihui Ji",
            "user": "zjuJish",
            "type": "user"
          },
          "name": "Sihui Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:18.148Z",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d3",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d4",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d5",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d6",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:59.000Z",
      "submittedOnDailyAt": "2025-10-16T00:17:16.195Z",
      "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
      "upvotes": 20,
      "discussionId": "68f04e9936f8b025381e18d7",
      "projectPage": "https://sihuiji.github.io/PhysMaster-Page/",
      "ai_summary": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.",
      "ai_keywords": [
        "PhysMaster",
        "PhysEncoder",
        "reinforcement learning",
        "Direct Preference Optimization",
        "physics-aware video generation"
      ]
    },
    "publishedAt": "2025-10-15T13:59:59.000Z",
    "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
    "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13804",
      "authors": [
        {
          "_id": "68f04ef636f8b025381e18d9",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/bgztraPC27L6culMlJw4s.png",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:15.323Z",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18da",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18db",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18dc",
          "name": "Yanbin Cao",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18dd",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18de",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18df",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18e0",
          "name": "Yujiu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:24.000Z",
      "submittedOnDailyAt": "2025-10-16T00:18:57.663Z",
      "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
      "upvotes": 18,
      "discussionId": "68f04ef636f8b025381e18e1",
      "projectPage": "https://omniverifier.github.io/",
      "githubRepo": "https://github.com/Cominclip/OmniVerifier",
      "ai_summary": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.",
      "ai_keywords": [
        "Generative Universal Verifier",
        "ViVerBench",
        "OmniVerifier-7B",
        "OmniVerifier-TTS",
        "multimodal reasoning",
        "vision-language models",
        "visual verification",
        "atomic capabilities",
        "sequential test-time scaling",
        "T2I-ReasonBench",
        "GenEval++"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-15T13:59:24.000Z",
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
    "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04767",
      "authors": [
        {
          "_id": "68f056d836f8b025381e194a",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e194b",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e194c",
          "name": "Seunghyuk Oh",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e194d",
          "user": {
            "_id": "61ad9c3300d01045fca0ad64",
            "avatarUrl": "/avatars/04c53c2d68d80db1053e5ebadbda5592.svg",
            "isPro": false,
            "fullname": "Min Jae Lee",
            "user": "mjbooo",
            "type": "user"
          },
          "name": "Minjae Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:41.418Z",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e194e",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e194f",
          "name": "Shuibai Zhang",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e1950",
          "name": "Coleman Hooper",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e1951",
          "name": "Yuezhou Hu",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e1952",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e1953",
          "name": "Nam Ik Cho",
          "hidden": false
        },
        {
          "_id": "68f056d836f8b025381e1954",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T12:41:31.000Z",
      "submittedOnDailyAt": "2025-10-16T01:00:13.316Z",
      "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "64ae35dc00781825350e880b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
        "isPro": false,
        "fullname": "Seunghyuk Oh",
        "user": "JakeOh",
        "type": "user"
      },
      "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
      "upvotes": 18,
      "discussionId": "68f056d936f8b025381e1955",
      "projectPage": "https://parallelbench.github.io",
      "githubRepo": "https://github.com/furiosa-ai/ParallelBench",
      "ai_summary": "Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.",
      "ai_keywords": [
        "autoregressive LLMs",
        "diffusion LLMs",
        "dLLMs",
        "parallel decoding",
        "conditional independence assumption",
        "information-theoretic analysis",
        "synthetic list operations",
        "ParallelBench",
        "real-world scenarios",
        "speed-quality trade-off"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "6213a1dcb670cb63a38074a1",
        "name": "furiosa-ai",
        "fullname": "FuriosaAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620b6dc29412b0861cb2474a/Bl7ua2mXSFxk9rVo8vMA8.png"
      }
    },
    "publishedAt": "2025-10-06T08:41:31.000Z",
    "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
    "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae35dc00781825350e880b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
      "fullname": "Seunghyuk Oh",
      "name": "JakeOh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "6213a1dcb670cb63a38074a1",
      "name": "furiosa-ai",
      "fullname": "FuriosaAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620b6dc29412b0861cb2474a/Bl7ua2mXSFxk9rVo8vMA8.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13802",
      "authors": [
        {
          "_id": "68f04f2d36f8b025381e18e3",
          "user": {
            "_id": "64b6e2abd3711dd619e476fc",
            "avatarUrl": "/avatars/430f53c44d12c578a5128e42ada62527.svg",
            "isPro": false,
            "fullname": "Xinhang Liu",
            "user": "xinhang0111",
            "type": "user"
          },
          "name": "Xinhang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:12.666Z",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e4",
          "user": {
            "_id": "6688a8f30bf195d6e53ac28d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
            "isPro": true,
            "fullname": "Yuxi Xiao",
            "user": "Yuxihenry",
            "type": "user"
          },
          "name": "Yuxi Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T07:25:42.979Z",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e5",
          "user": {
            "_id": "653862bdbe39573b3b247b44",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653862bdbe39573b3b247b44/oMqOC4USbQlPX5HFLDXt6.jpeg",
            "isPro": false,
            "fullname": "Donny Chen",
            "user": "donydchen",
            "type": "user"
          },
          "name": "Donny Y. Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:10.430Z",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e6",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e7",
          "name": "Yu-Wing Tai",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e8",
          "name": "Chi-Keung Tang",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e9",
          "name": "Bingyi Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:04.000Z",
      "submittedOnDailyAt": "2025-10-16T00:19:47.936Z",
      "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
      "upvotes": 17,
      "discussionId": "68f04f2d36f8b025381e18ea",
      "projectPage": "https://trace-anything.github.io/",
      "ai_summary": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.",
      "ai_keywords": [
        "Trajectory Field",
        "Trace Anything",
        "B-spline",
        "trajectory field estimation",
        "point-tracking benchmarks",
        "goal-conditioned manipulation",
        "motion forecasting",
        "spatio-temporal fusion"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-15T13:59:04.000Z",
    "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13802.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13621",
      "authors": [
        {
          "_id": "68f0515b36f8b025381e18f5",
          "name": "Yuexing Hao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f6",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f7",
          "name": "Haoran Zhang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f8",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f9",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fa",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fb",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fc",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fd",
          "name": "Saleh Kalantari",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fe",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18ff",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:50:45.000Z",
      "submittedOnDailyAt": "2025-10-16T00:29:52.071Z",
      "title": "The Role of Computing Resources in Publishing Foundation Model Research",
      "submittedOnDailyBy": {
        "_id": "639d94ab7145123e0d44e48a",
        "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
        "isPro": false,
        "fullname": "Yue Huang",
        "user": "HowieHwong",
        "type": "user"
      },
      "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
      "upvotes": 10,
      "discussionId": "68f0515b36f8b025381e1900",
      "ai_summary": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.",
      "ai_keywords": [
        "foundation models",
        "computing resources",
        "national funding",
        "citations",
        "research environment",
        "domain",
        "study methodology",
        "shared computing",
        "affordable computing",
        "under-resourced researchers",
        "diversity of ideas",
        "innovation",
        "progress in AI"
      ]
    },
    "publishedAt": "2025-10-15T10:50:45.000Z",
    "title": "The Role of Computing Resources in Publishing Foundation Model Research",
    "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639d94ab7145123e0d44e48a",
      "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
      "fullname": "Yue Huang",
      "name": "HowieHwong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13515",
      "authors": [
        {
          "_id": "68f052c836f8b025381e190a",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190b",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:01.782Z",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190c",
          "name": "Kaichen Zhang",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190d",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190e",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190f",
          "name": "Yueyi Zhang",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1910",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1911",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1912",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T13:07:00.000Z",
      "submittedOnDailyAt": "2025-10-16T00:36:48.179Z",
      "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
      "upvotes": 10,
      "discussionId": "68f052c836f8b025381e1913",
      "projectPage": "https://garygutc.github.io/UniME-v2/",
      "githubRepo": "https://github.com/GaryGuTC/UniME-v2",
      "ai_summary": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.",
      "ai_keywords": [
        "Universal Multimodal Embedding",
        "UniME-V2",
        "MLLMs",
        "in-batch negative mining",
        "global retrieval",
        "hard negative mining",
        "semantic alignment",
        "soft semantic matching scores",
        "soft labels",
        "joint pairwise and listwise optimization",
        "MMEB benchmark",
        "retrieval tasks"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-10-15T09:07:00.000Z",
    "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
    "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13515.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13778",
      "authors": [
        {
          "_id": "68f0545236f8b025381e191a",
          "name": "Xinyi Chen",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191b",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191c",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191d",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191e",
          "name": "Jiaya Jia",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191f",
          "user": {
            "_id": "66608add236f958513d21d2e",
            "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
            "isPro": false,
            "fullname": "Weiyang Jin",
            "user": "Wayne-King",
            "type": "user"
          },
          "name": "Weiyang Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:55.583Z",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1920",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1921",
          "name": "Yao Mu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1922",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1923",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1924",
          "name": "Yang Tian",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1925",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1926",
          "name": "Bolun Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1927",
          "name": "Fangjing Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1928",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1929",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192a",
          "user": {
            "_id": "65537770c9e9343421eb40a1",
            "avatarUrl": "/avatars/682458ecbdd69421c4adb741b31de8d7.svg",
            "isPro": false,
            "fullname": "wang",
            "user": "wz7in",
            "type": "user"
          },
          "name": "Ziqin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:51.687Z",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192b",
          "name": "Xueyuan Wei",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192c",
          "name": "Chao Wu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192d",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192e",
          "name": "Jinhui Ye",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192f",
          "name": "Junqiu Yu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1930",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1931",
          "name": "Jingjing Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1932",
          "name": "Jinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1933",
          "name": "Shi Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1934",
          "name": "Feng Zheng",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1935",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1936",
          "user": {
            "_id": "645b11ed5196a37266166d63",
            "avatarUrl": "/avatars/87ee10835aeda3f358e7d7d16900a324.svg",
            "isPro": false,
            "fullname": "zhuyangkun",
            "user": "tenstep",
            "type": "user"
          },
          "name": "Yangkun Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T07:25:41.023Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:30:05.000Z",
      "submittedOnDailyAt": "2025-10-16T00:42:25.171Z",
      "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
      "upvotes": 9,
      "discussionId": "68f0545236f8b025381e1937",
      "projectPage": "https://internrobotics.github.io/internvla-m1.github.io/",
      "githubRepo": "https://github.com/InternRobotics/InternVLA-M1",
      "ai_summary": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.",
      "ai_keywords": [
        "spatial grounding",
        "vision-language-action training",
        "spatial reasoning",
        "spatial prompting",
        "SimplerEnv",
        "WidowX",
        "LIBERO Franka",
        "pick-and-place",
        "long-horizon reasoning"
      ],
      "githubStars": 135
    },
    "publishedAt": "2025-10-15T13:30:05.000Z",
    "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
    "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13759",
      "authors": [
        {
          "_id": "68f05f6336f8b025381e19c3",
          "user": {
            "_id": "647993d9f966f086918da59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
            "isPro": false,
            "fullname": "kzou",
            "user": "jackyhate",
            "type": "user"
          },
          "name": "Kai Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:35:13.604Z",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c4",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c5",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c6",
          "name": "Shulin Tian",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c7",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c8",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19c9",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19ca",
          "name": "Bin Liu",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19cb",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68f05f6336f8b025381e19cc",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:10:35.000Z",
      "submittedOnDailyAt": "2025-10-16T01:31:54.776Z",
      "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
      "submittedOnDailyBy": {
        "_id": "647993d9f966f086918da59e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
        "isPro": false,
        "fullname": "kzou",
        "user": "jackyhate",
        "type": "user"
      },
      "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
      "upvotes": 9,
      "discussionId": "68f05f6436f8b025381e19cd",
      "projectPage": "https://vchitect.github.io/Uni-MMMU-Project/",
      "githubRepo": "https://github.com/vchitect/Uni-MMMU",
      "ai_summary": "Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.",
      "ai_keywords": [
        "unified multimodal models",
        "visual understanding",
        "visual generation",
        "bidirectional synergy",
        "reasoning-centric domains",
        "conceptual understanding",
        "precise visual synthesis",
        "cognitive scaffold",
        "analytical reasoning",
        "verifiable intermediate reasoning steps",
        "unique ground truths",
        "reproducible scoring protocol",
        "unified models",
        "generation-only models",
        "understanding-only models",
        "cross-modal dependencies"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "6567494b05fb89fb7e0a38da",
        "name": "Vchitect",
        "fullname": "Vchitect",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63201256c6b20f03c829c4b8/-wsKa-mCdj-vuoD5u9N1C.jpeg"
      }
    },
    "publishedAt": "2025-10-15T13:10:35.000Z",
    "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
    "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647993d9f966f086918da59e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
      "fullname": "kzou",
      "name": "jackyhate",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "6567494b05fb89fb7e0a38da",
      "name": "Vchitect",
      "fullname": "Vchitect",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63201256c6b20f03c829c4b8/-wsKa-mCdj-vuoD5u9N1C.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10977",
      "authors": [
        {
          "_id": "68ef0dce486b78128f0e342f",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:41:49.305Z",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3430",
          "name": "Runming Yang",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3431",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3432",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3433",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
      ],
      "publishedAt": "2025-10-13T03:30:01.000Z",
      "submittedOnDailyAt": "2025-10-16T00:46:01.871Z",
      "title": "Revisiting Model Interpolation for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
      "upvotes": 8,
      "discussionId": "68ef0dcf486b78128f0e3434",
      "githubRepo": "https://github.com/wutaiqiang/MI",
      "githubStars": 2,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-12T23:30:01.000Z",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10977.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10921",
      "authors": [
        {
          "_id": "68edbbe5de1fee572713a84e",
          "name": "Chunyu Xie",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a84f",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a850",
          "name": "Fanjing Kong",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a851",
          "name": "Jincheng Li",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a852",
          "name": "Dawei Liang",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a853",
          "name": "Ji Ao",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a854",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "68edbbe5de1fee572713a855",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T02:32:07.000Z",
      "submittedOnDailyAt": "2025-10-16T01:57:29.152Z",
      "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
      "upvotes": 8,
      "discussionId": "68edbbe5de1fee572713a856",
      "projectPage": "https://360cvgroup.github.io/FG-CLIP/",
      "ai_summary": "FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.",
      "ai_keywords": [
        "CLIP",
        "FG-CLIP 2",
        "fine-grained supervision",
        "region-text matching",
        "long-caption modeling",
        "multiple discriminative objectives",
        "Textual Intra-modal Contrastive (TIC) loss",
        "bilingual performance",
        "long-caption retrieval",
        "bounding box classification"
      ]
    },
    "publishedAt": "2025-10-12T22:32:07.000Z",
    "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
    "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13800",
      "authors": [
        {
          "_id": "68f09ba636f8b025381e1a7d",
          "name": "Yiming Chen",
          "hidden": false
        },
        {
          "_id": "68f09ba636f8b025381e1a7e",
          "name": "Zekun Qi",
          "hidden": false
        },
        {
          "_id": "68f09ba636f8b025381e1a7f",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "68f09ba636f8b025381e1a80",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "68f09ba636f8b025381e1a81",
          "name": "Li Zhang",
          "hidden": false
        },
        {
          "_id": "68f09ba636f8b025381e1a82",
          "name": "Peidong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:58:08.000Z",
      "submittedOnDailyAt": "2025-10-16T05:48:15.437Z",
      "title": "Reasoning in Space via Grounding in the World",
      "submittedOnDailyBy": {
        "_id": "63c3e8abc7d7f4c63a515a02",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
        "isPro": false,
        "fullname": "Zekun Qi",
        "user": "qizekun",
        "type": "user"
      },
      "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
      "upvotes": 7,
      "discussionId": "68f09ba636f8b025381e1a83",
      "projectPage": "https://yiming-cc.github.io/gs-reasoner/",
      "githubRepo": "https://github.com/WU-CVGL/GS-Reasoner",
      "ai_summary": "GS-Reasoner, a 3D LLM with a dual-path pooling mechanism, achieves autoregressive grounding and state-of-the-art spatial reasoning without external modules.",
      "ai_keywords": [
        "3D visual grounding",
        "spatial reasoning",
        "Grounded-Spatial Reasoner (GS-Reasoner)",
        "3D LLMs",
        "unified 3D representation",
        "dual-path pooling mechanism",
        "geometric features",
        "semantic cues",
        "positional cues",
        "image patch-based representation",
        "autoregressive grounding",
        "Grounded Chain-of-Thought (GCoT) dataset",
        "3D bounding box annotations",
        "step-by-step reasoning paths"
      ]
    },
    "publishedAt": "2025-10-15T13:58:08.000Z",
    "title": "Reasoning in Space via Grounding in the World",
    "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10274",
      "authors": [
        {
          "_id": "68f0669336f8b025381e19cf",
          "user": {
            "_id": "64c0afc06b2f05ae642e1918",
            "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
            "isPro": false,
            "fullname": "Jinliang Zheng",
            "user": "2toINF",
            "type": "user"
          },
          "name": "Jinliang Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:35:11.003Z",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d0",
          "name": "Jianxiong Li",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d1",
          "name": "Zhihao Wang",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d2",
          "name": "Dongxiu Liu",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d3",
          "name": "Xirui Kang",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d4",
          "name": "Yuchun Feng",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d5",
          "name": "Yinan Zheng",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d6",
          "name": "Jiayin Zou",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d7",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d8",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19d9",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19da",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19db",
          "name": "Jingjing Liu",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19dc",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "68f0669336f8b025381e19dd",
          "name": "Xianyuan Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-11T16:20:17.000Z",
      "submittedOnDailyAt": "2025-10-16T02:08:26.258Z",
      "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
      "submittedOnDailyBy": {
        "_id": "64c0afc06b2f05ae642e1918",
        "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
        "isPro": false,
        "fullname": "Jinliang Zheng",
        "user": "2toINF",
        "type": "user"
      },
      "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
      "upvotes": 5,
      "discussionId": "68f0669336f8b025381e19de",
      "projectPage": "https://thu-air-dream.github.io/X-VLA/",
      "githubRepo": "https://github.com/2toinf/X-VLA.git",
      "ai_summary": "A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.",
      "ai_keywords": [
        "Soft Prompt",
        "prompt learning",
        "cross-embodiment",
        "learnable embeddings",
        "embodiment-specific prompts",
        "flow-matching-based",
        "Transformer encoders",
        "X-VLA",
        "VLA architecture",
        "flexible dexterity",
        "quick adaptation"
      ],
      "githubStars": 28
    },
    "publishedAt": "2025-10-11T12:20:17.000Z",
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
    "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c0afc06b2f05ae642e1918",
      "avatarUrl": "/avatars/70f9a87d123ba65a5f931db028bb095b.svg",
      "fullname": "Jinliang Zheng",
      "name": "2toINF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12560",
      "authors": [
        {
          "_id": "68ef1c02486b78128f0e348c",
          "user": {
            "_id": "655601f1ae085c2ba7a22b95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
            "isPro": false,
            "fullname": "Xiaoji Zheng",
            "user": "Student-Xiaoji",
            "type": "user"
          },
          "name": "Xiaoji Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:08:32.045Z",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e348d",
          "name": "Ziyuan Yang",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e348e",
          "name": "Yanhao Chen",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e348f",
          "name": "Yuhang Peng",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e3490",
          "name": "Yuanrong Tang",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e3491",
          "name": "Gengyuan Liu",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e3492",
          "name": "Bokui Chen",
          "hidden": false
        },
        {
          "_id": "68ef1c02486b78128f0e3493",
          "name": "Jiangtao Gong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655601f1ae085c2ba7a22b95/6S7ap5ao7OKmqif62lOAh.png"
      ],
      "publishedAt": "2025-10-14T14:21:52.000Z",
      "submittedOnDailyAt": "2025-10-16T00:53:23.602Z",
      "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "655601f1ae085c2ba7a22b95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
        "isPro": false,
        "fullname": "Xiaoji Zheng",
        "user": "Student-Xiaoji",
        "type": "user"
      },
      "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
      "upvotes": 4,
      "discussionId": "68ef1c02486b78128f0e3494",
      "projectPage": "https://seu-zxj.github.io/CoIRL-AD/",
      "githubRepo": "https://github.com/SEU-zxj/CoIRL-AD",
      "githubStars": 8,
      "organization": {
        "_id": "628735cbc83a2d6ab8d14a66",
        "name": "Tsinghua",
        "fullname": "Tsinghua University"
      }
    },
    "publishedAt": "2025-10-14T10:21:52.000Z",
    "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
    "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655601f1ae085c2ba7a22b95/6S7ap5ao7OKmqif62lOAh.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12560.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655601f1ae085c2ba7a22b95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4UmxFrc_TEiXcnm3RewZM.jpeg",
      "fullname": "Xiaoji Zheng",
      "name": "Student-Xiaoji",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11958",
      "authors": [
        {
          "_id": "68f0539136f8b025381e1915",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "68f0539136f8b025381e1916",
          "user": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "isPro": false,
            "fullname": "Weizhi Wang",
            "user": "weizhiwang",
            "type": "user"
          },
          "name": "Weizhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:58.035Z",
          "hidden": false
        },
        {
          "_id": "68f0539136f8b025381e1917",
          "name": "Xifeng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T21:42:37.000Z",
      "submittedOnDailyAt": "2025-10-16T00:39:07.024Z",
      "title": "Direct Multi-Token Decoding",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
      "upvotes": 4,
      "discussionId": "68f0539136f8b025381e1918",
      "ai_summary": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.",
      "ai_keywords": [
        "decoder-only transformers",
        "large language models (LLMs)",
        "early layers",
        "middle layers",
        "late layers",
        "hidden states",
        "Direct Multi-Token Decoding (DMTD)",
        "speculative decoding",
        "fine-tuned",
        "Qwen3-4B",
        "scaling analysis"
      ]
    },
    "publishedAt": "2025-10-13T17:42:37.000Z",
    "title": "Direct Multi-Token Decoding",
    "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13602",
      "authors": [
        {
          "_id": "68f05b2436f8b025381e198f",
          "user": {
            "_id": "65394f942ddab2beff1cc967",
            "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
            "isPro": false,
            "fullname": "Shawn Huang",
            "user": "hyx21",
            "type": "user"
          },
          "name": "Yuxiang Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T07:12:24.514Z",
          "hidden": false
        },
        {
          "_id": "68f05b2436f8b025381e1990",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68f05b2436f8b025381e1991",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68f05b2436f8b025381e1992",
          "name": "Zhiyuan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:33:16.000Z",
      "submittedOnDailyAt": "2025-10-16T01:11:14.938Z",
      "title": "NOSA: Native and Offloadable Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "65394f942ddab2beff1cc967",
        "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
        "isPro": false,
        "fullname": "Shawn Huang",
        "user": "hyx21",
        "type": "user"
      },
      "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
      "upvotes": 3,
      "discussionId": "68f05b2436f8b025381e1993",
      "ai_summary": "NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.",
      "ai_keywords": [
        "sparse attention",
        "key-value cache",
        "token selection",
        "query-aware",
        "query-agnostic",
        "NOSA",
        "decoding throughput",
        "pretrain",
        "InfLLM-V2"
      ]
    },
    "publishedAt": "2025-10-15T10:33:16.000Z",
    "title": "NOSA: Native and Offloadable Sparse Attention",
    "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13602.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65394f942ddab2beff1cc967",
      "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
      "fullname": "Shawn Huang",
      "name": "hyx21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13744",
      "authors": [
        {
          "_id": "68f0495d36f8b025381e18b7",
          "name": "Shrey Pandit",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18b8",
          "name": "Austin Xu",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18b9",
          "name": "Xuan-Phi Nguyen",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18ba",
          "name": "Yifei Ming",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18bb",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18bc",
          "name": "Shafiq Joty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T16:50:54.000Z",
      "submittedOnDailyAt": "2025-10-16T00:20:50.470Z",
      "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
      "upvotes": 2,
      "discussionId": "68f0495d36f8b025381e18bd",
      "githubRepo": "https://github.com/SalesforceAIResearch/Hard2Verify",
      "ai_summary": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.",
      "ai_keywords": [
        "Large language model",
        "LLM-based reasoning systems",
        "IMO 2025",
        "mathematical proofs",
        "step-level verification",
        "Hard2Verify",
        "generative critics",
        "process reward models",
        "self-verification",
        "verification-generation dynamics"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-10-15T12:50:54.000Z",
    "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
    "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13744.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13282",
      "authors": [
        {
          "_id": "68f0754736f8b025381e19f9",
          "name": "JiaKui Hu",
          "hidden": false
        },
        {
          "_id": "68f0754736f8b025381e19fa",
          "name": "Zhengjian Yao",
          "hidden": false
        },
        {
          "_id": "68f0754736f8b025381e19fb",
          "name": "Lujia Jin",
          "hidden": false
        },
        {
          "_id": "68f0754736f8b025381e19fc",
          "name": "Yinghao Chen",
          "hidden": false
        },
        {
          "_id": "68f0754736f8b025381e19fd",
          "name": "Yanye Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T08:30:15.000Z",
      "submittedOnDailyAt": "2025-10-16T03:08:58.013Z",
      "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
      "submittedOnDailyBy": {
        "_id": "64ccd5cc4726a3f833831087",
        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
        "isPro": false,
        "fullname": "Hu",
        "user": "Jiakui",
        "type": "user"
      },
      "summary": "This study introduces a Masked Degradation Classification Pre-Training method\n(MaskDCPT), designed to facilitate the classification of degradation types in\ninput images, leading to comprehensive image restoration pre-training. Unlike\nconventional pre-training methods, MaskDCPT uses the degradation type of the\nimage as an extremely weak supervision, while simultaneously leveraging the\nimage reconstruction to enhance performance and robustness. MaskDCPT includes\nan encoder and two decoders: the encoder extracts features from the masked\nlow-quality input image. The classification decoder uses these features to\nidentify the degradation type, whereas the reconstruction decoder aims to\nreconstruct a corresponding high-quality image. This design allows the\npre-training to benefit from both masked image modeling and contrastive\nlearning, resulting in a generalized representation suited for restoration\ntasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained\nencoder can be used to address universal image restoration and achieve\noutstanding performance. Implementing MaskDCPT significantly improves\nperformance for both convolution neural networks (CNNs) and Transformers, with\na minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and\na 34.8% reduction in PIQE compared to baseline in real-world degradation\nscenarios. It also emergences strong generalization to previously unseen\ndegradation types and levels. In addition, we curate and release the UIR-2.5M\ndataset, which includes 2.5 million paired restoration samples across 19\ndegradation types and over 200 degradation levels, incorporating both synthetic\nand real-world data. The dataset, source code, and models are available at\nhttps://github.com/MILab-PKU/MaskDCPT.",
      "upvotes": 2,
      "discussionId": "68f0754736f8b025381e19fe",
      "projectPage": "https://github.com/MILab-PKU/MaskDCPT",
      "githubRepo": "https://github.com/MILab-PKU/MaskDCPT",
      "ai_summary": "A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.",
      "ai_keywords": [
        "Masked Degradation Classification Pre-Training",
        "MaskDCPT",
        "encoder",
        "decoders",
        "masked image modeling",
        "contrastive learning",
        "PSNR",
        "PIQE",
        "UIR-2.5M dataset"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-15T04:30:15.000Z",
    "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
    "summary": "This study introduces a Masked Degradation Classification Pre-Training method\n(MaskDCPT), designed to facilitate the classification of degradation types in\ninput images, leading to comprehensive image restoration pre-training. Unlike\nconventional pre-training methods, MaskDCPT uses the degradation type of the\nimage as an extremely weak supervision, while simultaneously leveraging the\nimage reconstruction to enhance performance and robustness. MaskDCPT includes\nan encoder and two decoders: the encoder extracts features from the masked\nlow-quality input image. The classification decoder uses these features to\nidentify the degradation type, whereas the reconstruction decoder aims to\nreconstruct a corresponding high-quality image. This design allows the\npre-training to benefit from both masked image modeling and contrastive\nlearning, resulting in a generalized representation suited for restoration\ntasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained\nencoder can be used to address universal image restoration and achieve\noutstanding performance. Implementing MaskDCPT significantly improves\nperformance for both convolution neural networks (CNNs) and Transformers, with\na minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and\na 34.8% reduction in PIQE compared to baseline in real-world degradation\nscenarios. It also emergences strong generalization to previously unseen\ndegradation types and levels. In addition, we curate and release the UIR-2.5M\ndataset, which includes 2.5 million paired restoration samples across 19\ndegradation types and over 200 degradation levels, incorporating both synthetic\nand real-world data. The dataset, source code, and models are available at\nhttps://github.com/MILab-PKU/MaskDCPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccd5cc4726a3f833831087",
      "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
      "fullname": "Hu",
      "name": "Jiakui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11062",
      "authors": [
        {
          "_id": "68f089fb36f8b025381e1a3b",
          "name": "Yujie Zhao",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a3c",
          "user": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "isPro": true,
            "fullname": "Lanxiang Hu",
            "user": "Snyhlxde",
            "type": "user"
          },
          "name": "Lanxiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:35:06.135Z",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a3d",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a3e",
          "name": "Minmin Hou",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a3f",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a40",
          "name": "Ke Ding",
          "hidden": false
        },
        {
          "_id": "68f089fb36f8b025381e1a41",
          "name": "Jishen Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T06:55:09.000Z",
      "submittedOnDailyAt": "2025-10-16T04:31:16.464Z",
      "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "6301d6455e305a35cb0846a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
        "isPro": true,
        "fullname": "Lanxiang Hu",
        "user": "Snyhlxde",
        "type": "user"
      },
      "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
      "upvotes": 2,
      "discussionId": "68f089fb36f8b025381e1a42",
      "projectPage": "https://pettingllms-ai.github.io/",
      "githubRepo": "https://github.com/pettingllms-ai/PettingLLMs",
      "ai_summary": "AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.",
      "ai_keywords": [
        "multi-agent systems",
        "reinforcement learning",
        "large language models",
        "role-based orchestration",
        "GRPO-style optimization",
        "on-policy RL",
        "agent-wise grouped RL",
        "turn-wise grouped RL",
        "single-policy",
        "multi-policy",
        "long-horizon planning",
        "reasoning performance",
        "coding tasks",
        "math tasks"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-13T02:55:09.000Z",
    "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
    "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11062.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301d6455e305a35cb0846a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
      "fullname": "Lanxiang Hu",
      "name": "Snyhlxde",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12831",
      "authors": [
        {
          "_id": "68f0566536f8b025381e1939",
          "user": {
            "_id": "60ed74f536ceac2554083559",
            "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
            "isPro": false,
            "fullname": "taicheng guo",
            "user": "taicheng",
            "type": "user"
          },
          "name": "Taicheng Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:38:48.648Z",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193a",
          "name": "Hai Wang",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193b",
          "name": "ChaoChun Liu",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193c",
          "name": "Mohsen Golalikhani",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193d",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193e",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "68f0566536f8b025381e193f",
          "name": "Chandan K. Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T16:12:05.000Z",
      "submittedOnDailyAt": "2025-10-16T00:53:22.241Z",
      "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
      "submittedOnDailyBy": {
        "_id": "60ed74f536ceac2554083559",
        "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
        "isPro": false,
        "fullname": "taicheng guo",
        "user": "taicheng",
        "type": "user"
      },
      "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.",
      "upvotes": 2,
      "discussionId": "68f0566536f8b025381e1940",
      "githubRepo": "https://github.com/taichengguo/MTSQL-R1",
      "ai_summary": "MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.",
      "ai_keywords": [
        "Multi-turn Text-to-SQL",
        "Markov Decision Process (MDP)",
        "dialogue coherence",
        "dialogue memory",
        "iterative propose-execute-verify-refine cycle",
        "conversational semantic parsing"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "5ffdfbadbba2ae614d771970",
        "name": "amazon",
        "fullname": "Amazon",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
      }
    },
    "publishedAt": "2025-10-12T12:12:05.000Z",
    "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
    "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ed74f536ceac2554083559",
      "avatarUrl": "/avatars/28c184ef76f719a720d933d05afb5800.svg",
      "fullname": "taicheng guo",
      "name": "taicheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "organization": {
      "_id": "5ffdfbadbba2ae614d771970",
      "name": "amazon",
      "fullname": "Amazon",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10611",
      "authors": [
        {
          "_id": "68ef4bbb486b78128f0e35e1",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e2",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T15:24:40.752Z",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e3",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e4",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e5",
          "name": "Haochen You",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e6",
          "name": "Lubin Gan",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e7",
          "name": "Yilei Yuan",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e8",
          "name": "Jin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T13:47:42.000Z",
      "submittedOnDailyAt": "2025-10-16T00:46:55.714Z",
      "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
      "upvotes": 2,
      "discussionId": "68ef4bbc486b78128f0e35e9",
      "ai_summary": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.",
      "ai_keywords": [
        "hypergraph-based framework",
        "hyperedges",
        "hypergraph convolutional layers",
        "variational autoencoder",
        "sparsity regularization",
        "GSM8K",
        "token consumption"
      ]
    },
    "publishedAt": "2025-10-12T09:47:42.000Z",
    "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
    "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10611.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 190
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10581",
      "authors": [
        {
          "_id": "68efc102d22df134b7f5ca27",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca28",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T16:55:17.600Z",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca29",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2a",
          "name": "Haochen You",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2b",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2c",
          "name": "Lubin Gan",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2d",
          "name": "Yilei Yuan",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2e",
          "name": "Jin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T12:55:42.000Z",
      "submittedOnDailyAt": "2025-10-16T00:47:00.305Z",
      "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
      "upvotes": 2,
      "discussionId": "68efc102d22df134b7f5ca2f",
      "ai_summary": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.",
      "ai_keywords": [
        "multi-agent systems",
        "Large Language Models",
        "deep search scenarios",
        "temporal attribution",
        "information dependencies",
        "GraphTracer",
        "Information Dependency Graphs",
        "failure attribution",
        "synthetic data generation",
        "Who&When benchmark"
      ]
    },
    "publishedAt": "2025-10-12T08:55:42.000Z",
    "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
    "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 190
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13586",
      "authors": [
        {
          "_id": "68f04f9036f8b025381e18ec",
          "name": "Pasin Buakhaw",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ed",
          "user": {
            "_id": "63a83c5432ed73936eb8363e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
            "isPro": false,
            "fullname": "kun kerdthaisong",
            "user": "augustus2011",
            "type": "user"
          },
          "name": "Kun Kerdthaisong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-16T06:39:08.330Z",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ee",
          "name": "Phuree Phenhiran",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ef",
          "name": "Pitikorn Khlaisamniang",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f0",
          "name": "Supasate Vorathammathorn",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f1",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f2",
          "name": "Nutchanon Yongsatianchot",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:17:23.000Z",
      "submittedOnDailyAt": "2025-10-16T00:21:39.147Z",
      "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
      "submittedOnDailyBy": {
        "_id": "63a83c5432ed73936eb8363e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
        "isPro": false,
        "fullname": "kun kerdthaisong",
        "user": "augustus2011",
        "type": "user"
      },
      "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
      "upvotes": 1,
      "discussionId": "68f04f9036f8b025381e18f3",
      "githubRepo": "https://github.com/Augustus2011/sony-cpdc-2025-starter-kit",
      "ai_summary": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "non-player characters",
        "NPCs",
        "Commonsense Persona-Grounded Dialogue Challenge",
        "CPDC",
        "task-oriented dialogue",
        "context-aware dialogue",
        "lightweight prompting",
        "Deflanderization",
        "fine-tuned models",
        "Qwen3-14B",
        "supervised fine-tuning",
        "SFT",
        "Low-Rank Adaptation",
        "LoRA"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "67aaba5d8d478dcb4b2f4281",
        "name": "Character-lab",
        "fullname": "Character-lab"
      }
    },
    "publishedAt": "2025-10-15T10:17:23.000Z",
    "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
    "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a83c5432ed73936eb8363e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
      "fullname": "kun kerdthaisong",
      "name": "augustus2011",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67aaba5d8d478dcb4b2f4281",
      "name": "Character-lab",
      "fullname": "Character-lab"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11715",
      "authors": [
        {
          "_id": "68f0973e36f8b025381e1a6d",
          "name": "Ayush Shrivastava",
          "hidden": false
        },
        {
          "_id": "68f0973e36f8b025381e1a6e",
          "name": "Sanyam Mehta",
          "hidden": false
        },
        {
          "_id": "68f0973e36f8b025381e1a6f",
          "name": "Daniel Geng",
          "hidden": false
        },
        {
          "_id": "68f0973e36f8b025381e1a70",
          "name": "Andrew Owens",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/r0h0OPhM6EP_K1lCFgbHS.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/WA_D_4fbsbGOHgDY55cH8.mp4"
      ],
      "publishedAt": "2025-10-13T17:59:46.000Z",
      "submittedOnDailyAt": "2025-10-16T05:34:34.768Z",
      "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "66f41c37d8de0f55059d3317",
        "avatarUrl": "/avatars/a730ff35ea1da9a2da79aa56617e82d8.svg",
        "isPro": false,
        "fullname": "Ayush Shrivastava",
        "user": "ayshrv",
        "type": "user"
      },
      "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
      "upvotes": 1,
      "discussionId": "68f0973e36f8b025381e1a71",
      "projectPage": "https://point-prompting.github.io/",
      "ai_summary": "Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.",
      "ai_keywords": [
        "video diffusion models",
        "zero-shot point tracking",
        "image-conditioned video diffusion models",
        "emergent tracks",
        "self-supervised models"
      ]
    },
    "publishedAt": "2025-10-13T13:59:46.000Z",
    "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
    "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/r0h0OPhM6EP_K1lCFgbHS.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66f41c37d8de0f55059d3317/WA_D_4fbsbGOHgDY55cH8.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f41c37d8de0f55059d3317",
      "avatarUrl": "/avatars/a730ff35ea1da9a2da79aa56617e82d8.svg",
      "fullname": "Ayush Shrivastava",
      "name": "ayshrv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]