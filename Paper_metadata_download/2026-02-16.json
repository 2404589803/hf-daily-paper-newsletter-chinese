[
  {
    "paper": {
      "id": "2602.10388",
      "authors": [
        {
          "_id": "698d3bd265c0d15a6d16200e",
          "user": {
            "_id": "6951c555b519522f565dfd0c",
            "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg",
            "isPro": false,
            "fullname": "ZhongzhiLi",
            "user": "Zhongzhi1228",
            "type": "user"
          },
          "name": "Zhongzhi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:05.580Z",
          "hidden": false
        },
        {
          "_id": "698d3bd265c0d15a6d16200f",
          "name": "Xuansheng Wu",
          "hidden": false
        },
        {
          "_id": "698d3bd265c0d15a6d162010",
          "name": "Yijiang Li",
          "hidden": false
        },
        {
          "_id": "698d3bd265c0d15a6d162011",
          "name": "Lijie Hu",
          "hidden": false
        },
        {
          "_id": "698d3bd265c0d15a6d162012",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T00:23:13.000Z",
      "submittedOnDailyAt": "2026-02-16T02:31:34.708Z",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "submittedOnDailyBy": {
        "_id": "6951c555b519522f565dfd0c",
        "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg",
        "isPro": false,
        "fullname": "ZhongzhiLi",
        "user": "Zhongzhi1228",
        "type": "user"
      },
      "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "upvotes": 183,
      "discussionId": "698d3bd265c0d15a6d162013",
      "projectPage": "https://website-sigma-three-35.vercel.app/",
      "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis",
      "githubRepoAddedBy": "user",
      "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
      "ai_keywords": [
        "Feature Activation Coverage",
        "sparse autoencoder",
        "data diversity",
        "downstream performance",
        "instruction following",
        "toxicity detection",
        "reward modeling",
        "behavior steering",
        "cross-model knowledge transfer",
        "data-centric optimization"
      ],
      "githubStars": 28
    },
    "publishedAt": "2026-02-10T19:23:13.000Z",
    "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
    "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6951c555b519522f565dfd0c",
      "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg",
      "fullname": "ZhongzhiLi",
      "name": "Zhongzhi1228",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.11858",
      "authors": [
        {
          "_id": "698ea72fcace060ff123ae5d",
          "name": "Lai Wei",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae5e",
          "name": "Liangbo He",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae5f",
          "name": "Jun Lan",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae60",
          "name": "Lingzhong Dong",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae61",
          "name": "Yutong Cai",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae62",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae63",
          "name": "Huijia Zhu",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae64",
          "name": "Weiqiang Wang",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae65",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae66",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae67",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "698ea72fcace060ff123ae68",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T12:00:35.000Z",
      "submittedOnDailyAt": "2026-02-16T00:45:35.867Z",
      "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
      "upvotes": 49,
      "discussionId": "698ea72fcace060ff123ae69",
      "githubRepo": "https://github.com/inclusionAI/Zooming-without-Zooming",
      "githubRepoAddedBy": "user",
      "ai_summary": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual question answering",
        "fine-grained perception",
        "Thinking-with-Images",
        "region-to-image distillation",
        "micro-cropped regions",
        "teacher-student distillation",
        "ZoomBench",
        "visual reasoning",
        "GUI agents"
      ],
      "githubStars": 43,
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2026-02-12T07:00:35.000Z",
    "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12705",
      "authors": [
        {
          "_id": "6992818050fb2c0be47838b2",
          "name": "Baorong Shi",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b3",
          "name": "Bo Cui",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b4",
          "name": "Boyuan Jiang",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b5",
          "name": "Deli Yu",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b6",
          "name": "Fang Qian",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b7",
          "name": "Haihua Yang",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b8",
          "name": "Huichao Wang",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838b9",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838ba",
          "name": "Jianfei Pan",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838bb",
          "name": "Jieqiong Cao",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838bc",
          "name": "Jinghao Lin",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838bd",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838be",
          "name": "Lin Yang",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838bf",
          "name": "Shengsheng Yao",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c0",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c1",
          "name": "Xiaojun Xiao",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c2",
          "name": "Xiaozhong Ji",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c3",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c4",
          "name": "Yijun He",
          "hidden": false
        },
        {
          "_id": "6992818050fb2c0be47838c5",
          "name": "Zhixiong Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T08:19:38.000Z",
      "submittedOnDailyAt": "2026-02-16T00:05:18.833Z",
      "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
      "submittedOnDailyBy": {
        "_id": "64c636b94c9bebfa6ac80ae4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png",
        "isPro": false,
        "fullname": "kai",
        "user": "KaiWu123",
        "type": "user"
      },
      "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.",
      "upvotes": 39,
      "discussionId": "6992818050fb2c0be47838c6",
      "ai_summary": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "ai_keywords": [
        "vision-language foundation model",
        "entity-aware continual pretraining",
        "heterogeneous medical corpora",
        "long-tail gaps",
        "reinforcement learning",
        "tool-augmented agentic training",
        "multi-step diagnostic reasoning",
        "evidence-grounded reasoning",
        "hallucination reduction",
        "medical instruction adherence"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-02-13T03:19:38.000Z",
    "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
    "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12705.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64c636b94c9bebfa6ac80ae4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png",
      "fullname": "kai",
      "name": "KaiWu123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08683",
      "authors": [
        {
          "_id": "698c7b82eb12ea7453916913",
          "name": "Feilong Tang",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916914",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916915",
          "name": "Yunyao Yan",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916916",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916917",
          "name": "Bin Qin",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916918",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916919",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691a",
          "name": "Yuanhan Zhang",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691b",
          "name": "Chunyuan Li",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691c",
          "name": "Shikun Feng",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691d",
          "name": "Changrui Chen",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691e",
          "name": "Huajie Tan",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea745391691f",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916920",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916921",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916922",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916923",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916924",
          "name": "Zongyuan Ge",
          "hidden": false
        },
        {
          "_id": "698c7b82eb12ea7453916925",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655c70d331c4978366d4b2e6/FmGRuy9Q3t5lOcdOKTvbp.mp4"
      ],
      "publishedAt": "2026-02-09T14:06:17.000Z",
      "submittedOnDailyAt": "2026-02-16T01:10:27.480Z",
      "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
      "submittedOnDailyBy": {
        "_id": "655c70d331c4978366d4b2e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655c70d331c4978366d4b2e6/X-KjTNkxtzeYu9ngBOh_C.jpeg",
        "isPro": false,
        "fullname": "yiyexy",
        "user": "yiyexy",
        "type": "user"
      },
      "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
      "upvotes": 31,
      "discussionId": "698c7b83eb12ea7453916926",
      "projectPage": "https://www.lmms-lab.com/onevision-encoder/index.html",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/OneVision-Encoder",
      "githubRepoAddedBy": "user",
      "ai_summary": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.",
      "ai_keywords": [
        "artificial general intelligence",
        "compression problem",
        "resonance",
        "deep learning",
        "visual signals",
        "discriminative information",
        "pixel grids",
        "compute optimization",
        "video compression",
        "Codec Patchification",
        "3D RoPE",
        "cluster discrimination objective",
        "semantic concepts",
        "object permanence",
        "motion dynamics",
        "LLM",
        "vision backbones",
        "visual tokens",
        "pretraining data",
        "video understanding",
        "sparsity-driven encoding"
      ],
      "githubStars": 219,
      "organization": {
        "_id": "6583eb89bed3689928f5d845",
        "name": "lmms-lab",
        "fullname": "LMMs-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/0sliNO9xGhOjVWw20A1Ge.png"
      }
    },
    "publishedAt": "2026-02-09T09:06:17.000Z",
    "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
    "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655c70d331c4978366d4b2e6/FmGRuy9Q3t5lOcdOKTvbp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08683.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655c70d331c4978366d4b2e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655c70d331c4978366d4b2e6/X-KjTNkxtzeYu9ngBOh_C.jpeg",
      "fullname": "yiyexy",
      "name": "yiyexy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6583eb89bed3689928f5d845",
      "name": "lmms-lab",
      "fullname": "LMMs-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/0sliNO9xGhOjVWw20A1Ge.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12395",
      "authors": [
        {
          "_id": "699295c050fb2c0be478395f",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "699295c050fb2c0be4783960",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "699295c050fb2c0be4783961",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T20:44:27.000Z",
      "submittedOnDailyAt": "2026-02-16T01:33:54.807Z",
      "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
      "submittedOnDailyBy": {
        "_id": "6534a434e778506c5b1e5be8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/n_s7KqjSBOD8E31c1gEtw.jpeg",
        "isPro": true,
        "fullname": "Xirui Li",
        "user": "AIcell",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
      "upvotes": 9,
      "discussionId": "699295c050fb2c0be4783962",
      "projectPage": "https://github.com/tianyi-lab/Frankenstein",
      "githubRepo": "https://github.com/tianyi-lab/Frankenstein",
      "githubRepoAddedBy": "user",
      "githubStars": 1,
      "organization": {
        "_id": "647f5b7daa8c04bbf938c625",
        "name": "umd-zhou-lab",
        "fullname": "Tianyi Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"
      }
    },
    "publishedAt": "2026-02-12T15:44:27.000Z",
    "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
    "summary": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6534a434e778506c5b1e5be8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/n_s7KqjSBOD8E31c1gEtw.jpeg",
      "fullname": "Xirui Li",
      "name": "AIcell",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "647f5b7daa8c04bbf938c625",
      "name": "umd-zhou-lab",
      "fullname": "Tianyi Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12628",
      "authors": [
        {
          "_id": "6992963150fb2c0be4783964",
          "name": "Liangzhi Shi",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be4783965",
          "name": "Shuaihang Chen",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be4783966",
          "name": "Feng Gao",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be4783967",
          "name": "Yinuo Chen",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be4783968",
          "name": "Kang Chen",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be4783969",
          "name": "Tonghe Zhang",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be478396a",
          "name": "Hongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be478396b",
          "name": "Weinan Zhang",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be478396c",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "6992963150fb2c0be478396d",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T05:15:50.000Z",
      "submittedOnDailyAt": "2026-02-16T02:49:48.749Z",
      "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
      "submittedOnDailyBy": {
        "_id": "64ba0f8d842aa47891cb972b",
        "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
        "isPro": false,
        "fullname": "Chao Yu",
        "user": "zoeyuchao",
        "type": "user"
      },
      "summary": "Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and π_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on π_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
      "upvotes": 8,
      "discussionId": "6992963150fb2c0be478396e",
      "ai_summary": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.",
      "ai_keywords": [
        "vision-language-action",
        "sim-real co-training",
        "reinforcement learning",
        "supervised fine-tuning",
        "policy optimization",
        "catastrophic forgetting",
        "real-world data efficiency"
      ],
      "organization": {
        "_id": "689ea978824b212c988bc8f5",
        "name": "RLinf",
        "fullname": "RLinf",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
      }
    },
    "publishedAt": "2026-02-13T00:15:50.000Z",
    "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
    "summary": "Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and π_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on π_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba0f8d842aa47891cb972b",
      "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
      "fullname": "Chao Yu",
      "name": "zoeyuchao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "689ea978824b212c988bc8f5",
      "name": "RLinf",
      "fullname": "RLinf",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11236",
      "authors": [
        {
          "_id": "699290db50fb2c0be478393a",
          "name": "Yandan Yang",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be478393b",
          "name": "Shuang Zeng",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be478393c",
          "name": "Tong Lin",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be478393d",
          "name": "Xinyuan Chang",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be478393e",
          "name": "Dekang Qi",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be478393f",
          "name": "Junjin Xiao",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783940",
          "name": "Haoyun Liu",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783941",
          "name": "Ronghan Chen",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783942",
          "name": "Yuzhi Chen",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783943",
          "name": "Dongjie Huo",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783944",
          "name": "Feng Xiong",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783945",
          "name": "Xing Wei",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783946",
          "name": "Zhiheng Ma",
          "hidden": false
        },
        {
          "_id": "699290db50fb2c0be4783947",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T16:47:01.000Z",
      "submittedOnDailyAt": "2026-02-16T01:18:43.561Z",
      "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
      "submittedOnDailyBy": {
        "_id": "66680c2362af0a041a1daac1",
        "avatarUrl": "/avatars/06e2694a83299734ff0148b2acc04384.svg",
        "isPro": false,
        "fullname": "Shuang",
        "user": "Zengshuang",
        "type": "user"
      },
      "summary": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
      "upvotes": 8,
      "discussionId": "699290db50fb2c0be4783948",
      "projectPage": "https://amap-cvlab.github.io/ABot-Manipulation",
      "githubRepo": "https://github.com/amap-cvlab/ABot-Manipulation",
      "githubRepoAddedBy": "user",
      "ai_summary": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.",
      "ai_keywords": [
        "embodied agents",
        "data curation pipeline",
        "model architecture",
        "training strategies",
        "unified representations",
        "pre-training",
        "knowledge transfer",
        "action manifold hypothesis",
        "action manifold learning",
        "DiT backbone",
        "perception pipeline",
        "VLM semantics",
        "geometric priors",
        "3D modules",
        "VGGT",
        "Qwen-Image-Edit"
      ],
      "githubStars": 151,
      "organization": {
        "_id": "641415d08900ef6afa2fcb73",
        "name": "acvlab",
        "fullname": "Alibaba AMAP CV Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/dfveRtrRy8Xn7QpG684zl.png"
      }
    },
    "publishedAt": "2026-02-11T11:47:01.000Z",
    "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
    "summary": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66680c2362af0a041a1daac1",
      "avatarUrl": "/avatars/06e2694a83299734ff0148b2acc04384.svg",
      "fullname": "Shuang",
      "name": "Zengshuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "641415d08900ef6afa2fcb73",
      "name": "acvlab",
      "fullname": "Alibaba AMAP CV Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/dfveRtrRy8Xn7QpG684zl.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11865",
      "authors": [
        {
          "_id": "698f540a0cf1f9a6bbe7fb64",
          "name": "Nenad Tomašev",
          "hidden": false
        },
        {
          "_id": "698f540a0cf1f9a6bbe7fb65",
          "name": "Matija Franklin",
          "hidden": false
        },
        {
          "_id": "698f540a0cf1f9a6bbe7fb66",
          "name": "Simon Osindero",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T12:11:42.000Z",
      "submittedOnDailyAt": "2026-02-16T00:46:25.547Z",
      "title": "Intelligent AI Delegation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.",
      "upvotes": 6,
      "discussionId": "698f540a0cf1f9a6bbe7fb67",
      "ai_summary": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.",
      "ai_keywords": [
        "task decomposition",
        "delegation",
        "adaptive framework",
        "authority transfer",
        "trust mechanisms",
        "agentic web"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-12T07:11:42.000Z",
    "title": "Intelligent AI Delegation",
    "summary": "AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11865.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.13013",
      "authors": [
        {
          "_id": "69927e3c50fb2c0be478388e",
          "name": "Yunheng Li",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be478388f",
          "name": "Hengrui Zhang",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783890",
          "name": "Meng-Hao Guo",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783891",
          "name": "Wenzhao Gao",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783892",
          "name": "Shaoyong Jia",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783893",
          "name": "Shaohui Jiao",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783894",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "69927e3c50fb2c0be4783895",
          "name": "Ming-Ming Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T15:20:54.000Z",
      "submittedOnDailyAt": "2026-02-16T00:16:15.602Z",
      "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "submittedOnDailyBy": {
        "_id": "67485bfd768f8d6a509d5cd7",
        "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
        "isPro": false,
        "fullname": "Yunheng Li",
        "user": "lyhisme",
        "type": "user"
      },
      "summary": "Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
      "upvotes": 4,
      "discussionId": "69927e3c50fb2c0be4783896",
      "ai_summary": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.",
      "ai_keywords": [
        "audiovisual instruction annotations",
        "supervised fine-tuning",
        "audiovisual captioning",
        "attribute-wise captioning",
        "caption-based QA",
        "caption-based temporal grounding"
      ],
      "organization": {
        "_id": "69899b2e8428a374a7486278",
        "name": "AudioVisual-Caption",
        "fullname": "ASID-Caption",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67485bfd768f8d6a509d5cd7/MBvZEgql0xcWCly-q9RbZ.png"
      }
    },
    "publishedAt": "2026-02-13T10:20:54.000Z",
    "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
    "summary": "Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67485bfd768f8d6a509d5cd7",
      "avatarUrl": "/avatars/ad01e707a2066ef673ac7317ccdbb902.svg",
      "fullname": "Yunheng Li",
      "name": "lyhisme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69899b2e8428a374a7486278",
      "name": "AudioVisual-Caption",
      "fullname": "ASID-Caption",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67485bfd768f8d6a509d5cd7/MBvZEgql0xcWCly-q9RbZ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.04163",
      "authors": [
        {
          "_id": "6992963950fb2c0be4783970",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783971",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783972",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783973",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783974",
          "name": "Qingyao Yang",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783975",
          "name": "He Xiao",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783976",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783977",
          "name": "Taiqiang Wu",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783978",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be4783979",
          "name": "Zhen Peng",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be478397a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be478397b",
          "name": "Long Shi",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be478397c",
          "name": "Hongxia Yang",
          "hidden": false
        },
        {
          "_id": "6992963950fb2c0be478397d",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T02:54:37.000Z",
      "submittedOnDailyAt": "2026-02-16T02:15:14.829Z",
      "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/Ho-uWcQCAEIURM1lhWEWJ.jpeg",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.",
      "upvotes": 4,
      "discussionId": "6992963950fb2c0be478397e",
      "ai_summary": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.",
      "ai_keywords": [
        "post-training quantization",
        "quantization grid",
        "bit-plane decomposition",
        "scalar coefficients",
        "second-order information",
        "quantization error",
        "Hessian-induced geometry"
      ],
      "organization": {
        "_id": "66deb312fd7d68a29348aa8d",
        "name": "TheHKU",
        "fullname": "Hong Kong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
      }
    },
    "publishedAt": "2026-02-03T21:54:37.000Z",
    "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
    "summary": "Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/Ho-uWcQCAEIURM1lhWEWJ.jpeg",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66deb312fd7d68a29348aa8d",
      "name": "TheHKU",
      "fullname": "Hong Kong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12829",
      "authors": [
        {
          "_id": "69928a0150fb2c0be4783904",
          "name": "Lei Lv",
          "hidden": false
        },
        {
          "_id": "69928a0150fb2c0be4783905",
          "name": "Yunfei Li",
          "hidden": false
        },
        {
          "_id": "69928a0150fb2c0be4783906",
          "name": "Yu Luo",
          "hidden": false
        },
        {
          "_id": "69928a0150fb2c0be4783907",
          "name": "Fuchun Sun",
          "hidden": false
        },
        {
          "_id": "69928a0150fb2c0be4783908",
          "name": "Xiao Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T11:32:10.000Z",
      "submittedOnDailyAt": "2026-02-16T00:38:01.355Z",
      "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
      "upvotes": 3,
      "discussionId": "69928a0150fb2c0be4783909",
      "projectPage": "https://pinkmoon-io.github.io/flac.github.io/",
      "ai_summary": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized Schrödinger bridge formulation.",
      "ai_keywords": [
        "diffusion models",
        "flow matching",
        "Maximum Entropy Reinforcement Learning",
        "velocity field",
        "Generalized Schrödinger Bridge",
        "kinetic energy",
        "policy optimization",
        "path-space energy",
        "Lagrangian dual mechanism",
        "off-policy algorithm"
      ]
    },
    "publishedAt": "2026-02-13T06:32:10.000Z",
    "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
    "summary": "Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12617",
      "authors": [
        {
          "_id": "6992825050fb2c0be47838ce",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6992825050fb2c0be47838cf",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "6992825050fb2c0be47838d0",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6992825050fb2c0be47838d1",
          "name": "Dingwen Zhang",
          "hidden": false
        },
        {
          "_id": "6992825050fb2c0be47838d2",
          "name": "MingMing Cheng",
          "hidden": false
        },
        {
          "_id": "6992825050fb2c0be47838d3",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T04:48:05.000Z",
      "submittedOnDailyAt": "2026-02-16T00:07:42.545Z",
      "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
      "submittedOnDailyBy": {
        "_id": "67232832391d8e0c4b2980f4",
        "avatarUrl": "/avatars/9063edc669233954f8b791083ac5be4a.svg",
        "isPro": false,
        "fullname": "Modi Jin",
        "user": "ghost233lism",
        "type": "user"
      },
      "summary": "This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
      "upvotes": 3,
      "discussionId": "6992825050fb2c0be47838d4",
      "projectPage": "https://ghost233lism.github.io/GeoAgent-page/",
      "githubRepo": "https://github.com/HVision-NKU/GeoAgent",
      "githubRepoAddedBy": "user",
      "ai_summary": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.",
      "ai_keywords": [
        "chain-of-thought",
        "geolocation dataset",
        "geo-similarity reward",
        "consistency reward",
        "consistency agent",
        "geographic characteristics",
        "reasoning process"
      ],
      "githubStars": 7
    },
    "publishedAt": "2026-02-12T23:48:05.000Z",
    "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
    "summary": "This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67232832391d8e0c4b2980f4",
      "avatarUrl": "/avatars/9063edc669233954f8b791083ac5be4a.svg",
      "fullname": "Modi Jin",
      "name": "ghost233lism",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12506",
      "authors": [
        {
          "_id": "69928b2550fb2c0be4783930",
          "name": "Rosie Zhao",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783931",
          "name": "Anshul Shah",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783932",
          "name": "Xiaoyu Zhu",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783933",
          "name": "Xinke Deng",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783934",
          "name": "Zhongyu Jiang",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783935",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783936",
          "name": "Joerg Liebelt",
          "hidden": false
        },
        {
          "_id": "69928b2550fb2c0be4783937",
          "name": "Arnab Mondal",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T01:12:00.000Z",
      "submittedOnDailyAt": "2026-02-16T00:42:41.918Z",
      "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
      "upvotes": 3,
      "discussionId": "69928b2550fb2c0be4783938",
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2026-02-12T20:12:00.000Z",
    "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
    "summary": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12506.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12984",
      "authors": [
        {
          "_id": "699288af50fb2c0be47838e7",
          "name": "Yujiong Shen",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838e8",
          "name": "Yajie Yang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838e9",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838ea",
          "name": "Binze Hu",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838eb",
          "name": "Huayu Sha",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838ec",
          "name": "Jiazheng Zhang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838ed",
          "name": "Qiyuan Peng",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838ee",
          "name": "Junlin Shang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838ef",
          "name": "Jixuan Huang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f0",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f1",
          "name": "Jingqi Tong",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f2",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f3",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f4",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f5",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f6",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f7",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f8",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838f9",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "699288af50fb2c0be47838fa",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T14:58:18.000Z",
      "submittedOnDailyAt": "2026-02-16T00:32:14.960Z",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
      "upvotes": 2,
      "discussionId": "699288af50fb2c0be47838fb",
      "githubRepo": "https://github.com/CMarsRover/SciAgentGYM",
      "githubRepoAddedBy": "user",
      "ai_summary": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "ai_keywords": [
        "SciAgentGym",
        "SciAgentBench",
        "SciForge",
        "tool action space",
        "dependency graph",
        "fine-tuning",
        "cross-domain transfer"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-13T09:58:18.000Z",
    "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
    "summary": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.13191",
      "authors": [
        {
          "_id": "69929b0950fb2c0be4783984",
          "name": "Sayan Deb Sarkar",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be4783985",
          "name": "Rémi Pautrat",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be4783986",
          "name": "Ondrej Miksik",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be4783987",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be4783988",
          "name": "Iro Armeni",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be4783989",
          "name": "Mahdi Rad",
          "hidden": false
        },
        {
          "_id": "69929b0950fb2c0be478398a",
          "name": "Mihai Dusmanu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/8pJAAJYDdLS34LfHbZr--.png"
      ],
      "publishedAt": "2026-02-13T18:57:31.000Z",
      "submittedOnDailyAt": "2026-02-16T01:53:27.255Z",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "submittedOnDailyBy": {
        "_id": "650ec19e6620b0c57e2a551b",
        "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
        "isPro": false,
        "fullname": "Sayan Deb Sarkar",
        "user": "sayandsarkar",
        "type": "user"
      },
      "summary": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "upvotes": 1,
      "discussionId": "69929b0950fb2c0be478398b",
      "projectPage": "https://sayands.github.io/cope/",
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-02-13T13:57:31.000Z",
    "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
    "summary": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/8pJAAJYDdLS34LfHbZr--.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650ec19e6620b0c57e2a551b",
      "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
      "fullname": "Sayan Deb Sarkar",
      "name": "sayandsarkar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12684",
      "authors": [
        {
          "_id": "69928a9e50fb2c0be4783917",
          "name": "Rui Cai",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783918",
          "name": "Jun Guo",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783919",
          "name": "Xinze He",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391a",
          "name": "Piaopiao Jin",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391b",
          "name": "Jie Li",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391c",
          "name": "Bingxuan Lin",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391d",
          "name": "Futeng Liu",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478391f",
          "name": "Fei Ma",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783920",
          "name": "Kun Ma",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783921",
          "name": "Feng Qiu",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783922",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783923",
          "name": "Yifei Su",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783924",
          "name": "Qiao Sun",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783925",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783926",
          "name": "Donghao Wang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783927",
          "name": "Yunhong Wang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783928",
          "name": "Rujie Wu",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be4783929",
          "name": "Diyun Xiang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478392a",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478392b",
          "name": "Hangjun Ye",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478392c",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "69928a9e50fb2c0be478392d",
          "name": "Quanyun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T07:30:43.000Z",
      "submittedOnDailyAt": "2026-02-16T00:40:36.822Z",
      "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io",
      "upvotes": 1,
      "discussionId": "69928a9e50fb2c0be478392e",
      "projectPage": "https://xiaomi-robotics-0.github.io/",
      "githubRepo": "https://github.com/XiaomiRobotics/Xiaomi-Robotics-0",
      "githubRepoAddedBy": "user",
      "ai_summary": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "ai_keywords": [
        "vision-language-action",
        "cross-embodiment robot trajectories",
        "pre-trained VLM",
        "catastrophic forgetting",
        "asynchronous execution",
        "inference latency",
        "real-time rollouts",
        "bimanual manipulation",
        "simulation benchmarks",
        "real-robot tasks"
      ],
      "githubStars": 215,
      "organization": {
        "_id": "6821ba7e5a7efab94a235406",
        "name": "xiaomi-research",
        "fullname": "Xiaomi Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/673735e4373ad40af7f81ea1/DR4m0bz2Du1l0Z8Txg351.png"
      }
    },
    "publishedAt": "2026-02-13T02:30:43.000Z",
    "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
    "summary": "In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6821ba7e5a7efab94a235406",
      "name": "xiaomi-research",
      "fullname": "Xiaomi Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/673735e4373ad40af7f81ea1/DR4m0bz2Du1l0Z8Txg351.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12612",
      "authors": [
        {
          "_id": "6992c3b650fb2c0be47839b7",
          "name": "Sein Kim",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839b8",
          "name": "Sangwu Park",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839b9",
          "name": "Hongseok Kang",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839ba",
          "name": "Wonjoong Kim",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839bb",
          "name": "Jimin Seo",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839bc",
          "name": "Yeonjun In",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839bd",
          "name": "Kanghoon Yoon",
          "hidden": false
        },
        {
          "_id": "6992c3b650fb2c0be47839be",
          "name": "Chanyoung Park",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T04:38:32.000Z",
      "submittedOnDailyAt": "2026-02-16T04:50:04.618Z",
      "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
      "submittedOnDailyBy": {
        "_id": "670e3f745e6b07cedfb76001",
        "avatarUrl": "/avatars/b4bbe8a0bffc25cf4a4a5543e10b3655.svg",
        "isPro": false,
        "fullname": "Sein Kim",
        "user": "Sein-Kim",
        "type": "user"
      },
      "summary": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
      "upvotes": 1,
      "discussionId": "6992c3b750fb2c0be47839bf",
      "githubRepo": "https://github.com/Sein-Kim/self_evolverec",
      "githubRepoAddedBy": "user",
      "githubStars": 1
    },
    "publishedAt": "2026-02-12T23:38:32.000Z",
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "summary": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670e3f745e6b07cedfb76001",
      "avatarUrl": "/avatars/b4bbe8a0bffc25cf4a4a5543e10b3655.svg",
      "fullname": "Sein Kim",
      "name": "Sein-Kim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11910",
      "authors": [
        {
          "_id": "699281b350fb2c0be47838c8",
          "name": "Łukasz Staniszewski",
          "hidden": false
        },
        {
          "_id": "699281b350fb2c0be47838c9",
          "name": "Katarzyna Zaleska",
          "hidden": false
        },
        {
          "_id": "699281b350fb2c0be47838ca",
          "name": "Mateusz Modrzejewski",
          "hidden": false
        },
        {
          "_id": "699281b350fb2c0be47838cb",
          "name": "Kamil Deja",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63c7c19721bd95f80ed8ed80/z99Jybc2p6DJ6kvnbbJgQ.png"
      ],
      "publishedAt": "2026-02-12T13:07:14.000Z",
      "submittedOnDailyAt": "2026-02-16T00:41:59.861Z",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "submittedOnDailyBy": {
        "_id": "63c7c19721bd95f80ed8ed80",
        "avatarUrl": "/avatars/0b1c1ace991e0290118d4f99f619d809.svg",
        "isPro": false,
        "fullname": "Lukasz Staniszewski",
        "user": "lukasz-staniszewski",
        "type": "user"
      },
      "summary": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "upvotes": 1,
      "discussionId": "699281b450fb2c0be47838cc",
      "projectPage": "https://audio-steering.github.io",
      "githubRepo": "https://github.com/luk-st/steer-audio",
      "githubRepoAddedBy": "user",
      "ai_summary": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.",
      "ai_keywords": [
        "audio diffusion models",
        "attention layers",
        "activation patching",
        "Contrastive Activation Addition",
        "Sparse Autoencoders",
        "audio synthesis",
        "musical concepts",
        "semantic representation"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-12T08:07:14.000Z",
    "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
    "summary": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63c7c19721bd95f80ed8ed80/z99Jybc2p6DJ6kvnbbJgQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c7c19721bd95f80ed8ed80",
      "avatarUrl": "/avatars/0b1c1ace991e0290118d4f99f619d809.svg",
      "fullname": "Lukasz Staniszewski",
      "name": "lukasz-staniszewski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11769",
      "authors": [
        {
          "_id": "69928a6d50fb2c0be4783911",
          "name": "Zhenghuang Wu",
          "hidden": false
        },
        {
          "_id": "69928a6d50fb2c0be4783912",
          "name": "Kang Chen",
          "hidden": false
        },
        {
          "_id": "69928a6d50fb2c0be4783913",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "69928a6d50fb2c0be4783914",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T09:50:13.000Z",
      "submittedOnDailyAt": "2026-02-16T00:39:54.166Z",
      "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
      "upvotes": 1,
      "discussionId": "69928a6d50fb2c0be4783915",
      "projectPage": "https://aigeeksgroup.github.io/Light4D",
      "githubRepo": "https://github.com/AIGeeksGroup/Light4D",
      "githubRepoAddedBy": "user",
      "ai_summary": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.",
      "ai_keywords": [
        "diffusion-based generative models",
        "4D relighting",
        "training-free framework",
        "disentangled flow guidance",
        "temporal consistency",
        "IC-Light architecture",
        "deterministic regularization",
        "latent space",
        "lighting control",
        "geometric integrity"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-02-12T04:50:13.000Z",
    "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
    "summary": "Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11757",
      "authors": [
        {
          "_id": "69928a0e50fb2c0be478390b",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "69928a0e50fb2c0be478390c",
          "name": "Yunshuang Wang",
          "hidden": false
        },
        {
          "_id": "69928a0e50fb2c0be478390d",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "69928a0e50fb2c0be478390e",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T09:34:28.000Z",
      "submittedOnDailyAt": "2026-02-16T00:38:51.304Z",
      "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
      "upvotes": 1,
      "discussionId": "69928a0e50fb2c0be478390f",
      "projectPage": "https://aigeeksgroup.github.io/Code2Worlds",
      "githubRepo": "https://github.com/AIGeeksGroup/Code2Worlds",
      "githubRepoAddedBy": "user",
      "ai_summary": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.",
      "ai_keywords": [
        "language-to-simulation code generation",
        "dual-stream architecture",
        "retrieval-augmented object generation",
        "hierarchical environmental orchestration",
        "physics-aware closed-loop mechanism",
        "PostProcess Agent",
        "VLM-Motion Critic",
        "self-reflection",
        "simulation code"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-02-12T04:34:28.000Z",
    "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
    "summary": "Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11715",
      "authors": [
        {
          "_id": "698e8f40cace060ff123ac49",
          "user": {
            "_id": "66dbea44946bce6c94afac80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dbea44946bce6c94afac80/MWL4AJEqs8XUVyEAX3QqN.png",
            "isPro": false,
            "fullname": "Haolei Bai",
            "user": "DeadlyKitt3n",
            "type": "user"
          },
          "name": "Haolei Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-13T09:36:44.960Z",
          "hidden": false
        },
        {
          "_id": "698e8f40cace060ff123ac4a",
          "name": "Lingcheng Kong",
          "hidden": false
        },
        {
          "_id": "698e8f40cace060ff123ac4b",
          "name": "Xueyi Chen",
          "hidden": false
        },
        {
          "_id": "698e8f40cace060ff123ac4c",
          "name": "Jianmian Wang",
          "hidden": false
        },
        {
          "_id": "698e8f40cace060ff123ac4d",
          "name": "Zhiqiang Tao",
          "hidden": false
        },
        {
          "_id": "698e8f40cace060ff123ac4e",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T08:45:13.000Z",
      "submittedOnDailyAt": "2026-02-16T03:08:13.602Z",
      "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
      "submittedOnDailyBy": {
        "_id": "66dbea44946bce6c94afac80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dbea44946bce6c94afac80/MWL4AJEqs8XUVyEAX3QqN.png",
        "isPro": false,
        "fullname": "Haolei Bai",
        "user": "DeadlyKitt3n",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.",
      "upvotes": 1,
      "discussionId": "698e8f40cace060ff123ac4f",
      "projectPage": "https://deadlykitten4.github.io/DICE/",
      "githubRepo": "https://github.com/deadlykitten4/DICE",
      "githubRepoAddedBy": "user",
      "ai_summary": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive LLMs",
        "parallel token generation",
        "CUDA kernel generation",
        "supervised fine-tuning",
        "reinforcement learning",
        "bi-phase curated reinforcement learning",
        "kernel generation",
        "KernelBench"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-12T03:45:13.000Z",
    "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
    "summary": "Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66dbea44946bce6c94afac80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dbea44946bce6c94afac80/MWL4AJEqs8XUVyEAX3QqN.png",
      "fullname": "Haolei Bai",
      "name": "DeadlyKitt3n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.04315",
      "authors": [
        {
          "_id": "699289b650fb2c0be47838fd",
          "name": "Guoqing Ma",
          "hidden": false
        },
        {
          "_id": "699289b650fb2c0be47838fe",
          "name": "Siheng Wang",
          "hidden": false
        },
        {
          "_id": "699289b650fb2c0be47838ff",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "699289b650fb2c0be4783900",
          "name": "Shan Yu",
          "hidden": false
        },
        {
          "_id": "699289b650fb2c0be4783901",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T08:30:27.000Z",
      "submittedOnDailyAt": "2026-02-16T00:37:14.424Z",
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
      "upvotes": 1,
      "discussionId": "699289b650fb2c0be4783902",
      "projectPage": "https://aigeeksgroup.github.io/GeneralVLA",
      "githubRepo": "https://github.com/AIGeeksGroup/GeneralVLA",
      "githubRepoAddedBy": "user",
      "ai_summary": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.",
      "ai_keywords": [
        "vision-language-action models",
        "zero-shot capability",
        "affordance segmentation module",
        "3D path prediction",
        "3D-aware control policy",
        "behavior cloning",
        "trajectory planning",
        "hierarchical model"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-02-04T03:30:27.000Z",
    "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
    "summary": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  }
]