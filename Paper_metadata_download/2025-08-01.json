[
  {
    "paper": {
      "id": "2507.23726",
      "authors": [
        {
          "_id": "688c16788c434640078cc348",
          "name": "Luoxin Chen",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc349",
          "name": "Jinming Gu",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34a",
          "name": "Liankai Huang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34b",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34c",
          "name": "Zhicheng Jiang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34d",
          "name": "Allan Jie",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34e",
          "name": "Xiaoran Jin",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc34f",
          "name": "Xing Jin",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc350",
          "name": "Chenggang Li",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc351",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc352",
          "name": "Cheng Ren",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc353",
          "name": "Jiawei Shen",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc354",
          "name": "Wenlei Shi",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc355",
          "name": "Tong Sun",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc356",
          "name": "He Sun",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc357",
          "name": "Jiahui Wang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc358",
          "name": "Siran Wang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc359",
          "name": "Zhihong Wang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35a",
          "name": "Chenrui Wei",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35b",
          "name": "Shufa Wei",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35c",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35d",
          "name": "Yuchen Wu",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35e",
          "name": "Yihang Xia",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc35f",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc360",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc361",
          "name": "Huaiyuan Ying",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc362",
          "name": "Hongyi Yuan",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc363",
          "name": "Zheng Yuan",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc364",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc365",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc366",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc367",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc368",
          "name": "Tianyun Zhao",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc369",
          "name": "Jianqiu Zhao",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc36a",
          "name": "Yichi Zhou",
          "hidden": false
        },
        {
          "_id": "688c16788c434640078cc36b",
          "name": "Thomas Hanwen Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T17:00:30.000Z",
      "submittedOnDailyAt": "2025-08-01T00:07:55.891Z",
      "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
      "submittedOnDailyBy": {
        "_id": "5f694b8b5e78cc6b0ed31bd2",
        "avatarUrl": "/avatars/63f21d30918c34fff9ea592e1039b3f0.svg",
        "isPro": false,
        "fullname": "Zheng Yuan",
        "user": "GanjinZero",
        "type": "user"
      },
      "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\nSeed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine Seed-Geometry, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
      "upvotes": 45,
      "discussionId": "688c16798c434640078cc36c",
      "githubRepo": "https://github.com/ByteDance-Seed/Seed-Prover",
      "ai_summary": "Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.",
      "ai_keywords": [
        "reinforcement learning",
        "long chain-of-thought",
        "theorem proving",
        "formal verification",
        "lemma-style reasoning",
        "Seed-Prover",
        "MiniF2F",
        "PutnamBench",
        "Seed-Geometry",
        "automated mathematical reasoning"
      ],
      "githubStars": 114
    },
    "publishedAt": "2025-07-31T13:00:30.000Z",
    "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
    "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\nSeed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine Seed-Geometry, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f694b8b5e78cc6b0ed31bd2",
      "avatarUrl": "/avatars/63f21d30918c34fff9ea592e1039b3f0.svg",
      "fullname": "Zheng Yuan",
      "name": "GanjinZero",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23779",
      "authors": [
        {
          "_id": "688c55378c434640078cc488",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc489",
          "name": "Ziqiang Xu",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48a",
          "name": "Jialiang Zhu",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48b",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48c",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48d",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48e",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc48f",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc490",
          "name": "Justin Wagle",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc491",
          "name": "Tim Franklin",
          "hidden": false
        },
        {
          "_id": "688c55378c434640078cc492",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62a30bf72dac39c2173c0a8c/Y83hgDElGoQEw90UC2-H6.png"
      ],
      "publishedAt": "2025-07-31T17:59:09.000Z",
      "submittedOnDailyAt": "2025-08-01T04:31:40.973Z",
      "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
      "submittedOnDailyBy": {
        "_id": "62a30bf72dac39c2173c0a8c",
        "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
        "isPro": false,
        "fullname": "Miaosen Zhang",
        "user": "Miaosen",
        "type": "user"
      },
      "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the Phi-Ground model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder 10B parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textbf{43.2} on\nScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\nhttps://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}",
      "upvotes": 17,
      "discussionId": "688c55388c434640078cc493",
      "projectPage": "https://zhangmiaosen2000.github.io/Phi-Ground/",
      "githubRepo": "https://github.com/zhangmiaosen2000/Phi-Ground",
      "ai_summary": "The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.",
      "ai_keywords": [
        "multimodal reasoning models",
        "Computer Use Agents",
        "GUI grounding",
        "end-to-end grounding models",
        "ScreenSpot-pro",
        "UI-Vision",
        "Phi-Ground model family"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-31T13:59:09.000Z",
    "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
    "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the Phi-Ground model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder 10B parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textbf{43.2} on\nScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\nhttps://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62a30bf72dac39c2173c0a8c/Y83hgDElGoQEw90UC2-H6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a30bf72dac39c2173c0a8c",
      "avatarUrl": "/avatars/15fb1ea3dcc7ccd8bc8002ce282e27b3.svg",
      "fullname": "Miaosen Zhang",
      "name": "Miaosen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22968",
      "authors": [
        {
          "_id": "688c13408c434640078cc335",
          "name": "Chengqian Ma",
          "hidden": false
        },
        {
          "_id": "688c13408c434640078cc336",
          "name": "Wei Tao",
          "hidden": false
        },
        {
          "_id": "688c13408c434640078cc337",
          "name": "Yiwen Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T17:56:23.000Z",
      "submittedOnDailyAt": "2025-08-01T00:51:53.259Z",
      "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.",
      "upvotes": 13,
      "discussionId": "688c13418c434640078cc338",
      "projectPage": "https://step-out.github.io/C3-web/",
      "githubRepo": "https://github.com/step-out/C3",
      "ai_summary": "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.",
      "ai_keywords": [
        "Spoken Dialogue Models",
        "SDMs",
        "Large Language Models",
        "LLMs",
        "benchmark dataset",
        "human judgment",
        "ambiguity",
        "polysemy",
        "heterograph",
        "heteronyms",
        "stress patterns",
        "context-dependency",
        "omission",
        "coreference",
        "multi-turn interaction"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-07-30T13:56:23.000Z",
    "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
    "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22968.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22879",
      "authors": [
        {
          "_id": "688aebf98b724c8c7187dd92",
          "name": "Chao Yi",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd93",
          "name": "Dian Chen",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd94",
          "name": "Gaoyang Guo",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd95",
          "name": "Jiakai Tang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd96",
          "name": "Jian Wu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd97",
          "name": "Jing Yu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd98",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd99",
          "name": "Wen Chen",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9a",
          "name": "Wenjun Yang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9b",
          "name": "Yuning Jiang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9c",
          "name": "Zhujin Gao",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9d",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9e",
          "name": "Chi Li",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dd9f",
          "name": "Dimin Wang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda0",
          "name": "Dixuan Wang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda1",
          "name": "Fan Li",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda2",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda3",
          "name": "Haibin Chen",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda4",
          "name": "Haozhuang Liu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda5",
          "name": "Jialin Zhu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda6",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda7",
          "name": "Jiawei Wu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda8",
          "name": "Jin Cui",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187dda9",
          "name": "Ju Huang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddaa",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddab",
          "name": "Kan Liu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddac",
          "name": "Lang Tian",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddad",
          "name": "Liang Rao",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddae",
          "name": "Longbin Li",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddaf",
          "name": "Lulu Zhao",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb0",
          "name": "Mao Zhang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb1",
          "name": "Na He",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb2",
          "name": "Peiyang Wang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb3",
          "name": "Qiqi Huang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb4",
          "name": "Tao Luo",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb6",
          "name": "Xiaoxiao He",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb7",
          "name": "Xin Tong",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb8",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddb9",
          "name": "Xunke Xi",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddba",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddbb",
          "name": "Yaxuan Wu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddbc",
          "name": "Yeqiu Yang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddbd",
          "name": "Yi Hu",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddbe",
          "name": "Yinnan Song",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddbf",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc0",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc1",
          "name": "Yujin Yuan",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc2",
          "name": "Yuliang Yan",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc3",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc4",
          "name": "Zhibo Xiao",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc5",
          "name": "Zhixin Ma",
          "hidden": false
        },
        {
          "_id": "688aebf98b724c8c7187ddc6",
          "name": "Zile Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T17:55:06.000Z",
      "submittedOnDailyAt": "2025-08-01T00:07:39.236Z",
      "title": "RecGPT Technical Report",
      "submittedOnDailyBy": {
        "_id": "65acfb3a14e6582c30b4ce76",
        "avatarUrl": "/avatars/3402ba72fe2436a9c2c2f92e56b15deb.svg",
        "isPro": false,
        "fullname": "TangJiakai",
        "user": "TangJiakai5704",
        "type": "user"
      },
      "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
      "upvotes": 12,
      "discussionId": "688aebf98b724c8c7187ddc7",
      "ai_summary": "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "user intent",
        "user interest mining",
        "item retrieval",
        "explanation generation",
        "reasoning-enhanced pre-alignment",
        "self-training evolution",
        "Human-LLM cooperative judge system"
      ]
    },
    "publishedAt": "2025-07-30T13:55:06.000Z",
    "title": "RecGPT Technical Report",
    "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65acfb3a14e6582c30b4ce76",
      "avatarUrl": "/avatars/3402ba72fe2436a9c2c2f92e56b15deb.svg",
      "fullname": "TangJiakai",
      "name": "TangJiakai5704",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23682",
      "authors": [
        {
          "_id": "688c14038c434640078cc33a",
          "name": "Xiaoyu Chen",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc33b",
          "name": "Hangxing Wei",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc33c",
          "name": "Pushi Zhang",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc33d",
          "name": "Chuheng Zhang",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc33e",
          "name": "Kaixin Wang",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc33f",
          "name": "Yanjiang Guo",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc340",
          "name": "Rushuai Yang",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc341",
          "name": "Yucen Wang",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc342",
          "name": "Xinquan Xiao",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc343",
          "name": "Li Zhao",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc344",
          "name": "Jianyu Chen",
          "hidden": false
        },
        {
          "_id": "688c14038c434640078cc345",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62a4ab8106b784ece3860b88/svZ9laEP2bfiEPcXh7bHy.mp4"
      ],
      "publishedAt": "2025-07-31T15:57:46.000Z",
      "submittedOnDailyAt": "2025-08-01T01:52:58.615Z",
      "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
      "submittedOnDailyBy": {
        "_id": "62a4ab8106b784ece3860b88",
        "avatarUrl": "/avatars/042be18e0586984e73fa7c9c0a124ed9.svg",
        "isPro": false,
        "fullname": "Pushi",
        "user": "zpschang",
        "type": "user"
      },
      "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.",
      "upvotes": 10,
      "discussionId": "688c14038c434640078cc346",
      "projectPage": "https://microsoft.github.io/villa-x/",
      "githubRepo": "https://github.com/microsoft/villa-x/",
      "ai_summary": "The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.",
      "ai_keywords": [
        "Visual-Language-Action (VLA) models",
        "latent actions",
        "ViLLA framework",
        "SIMPLER",
        "LIBERO",
        "gripper manipulation",
        "dexterous hand manipulation"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-31T11:57:46.000Z",
    "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
    "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62a4ab8106b784ece3860b88/svZ9laEP2bfiEPcXh7bHy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23682.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a4ab8106b784ece3860b88",
      "avatarUrl": "/avatars/042be18e0586984e73fa7c9c0a124ed9.svg",
      "fullname": "Pushi",
      "name": "zpschang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23698",
      "authors": [
        {
          "_id": "688c2acb8c434640078cc3c1",
          "name": "Shaofei Cai",
          "hidden": false
        },
        {
          "_id": "688c2acb8c434640078cc3c2",
          "name": "Zhancun Mu",
          "hidden": false
        },
        {
          "_id": "688c2acb8c434640078cc3c3",
          "name": "Haiwen Xia",
          "hidden": false
        },
        {
          "_id": "688c2acb8c434640078cc3c4",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "688c2acb8c434640078cc3c5",
          "name": "Anji Liu",
          "hidden": false
        },
        {
          "_id": "688c2acb8c434640078cc3c6",
          "name": "Yitao Liang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6578459d62d3ac1817ed79fe/4gxspFpC69p3qrB-lwGnT.gif"
      ],
      "publishedAt": "2025-07-31T16:20:02.000Z",
      "submittedOnDailyAt": "2025-08-01T02:56:26.483Z",
      "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents",
      "submittedOnDailyBy": {
        "_id": "6578459d62d3ac1817ed79fe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
        "isPro": false,
        "fullname": "Shaofei Cai",
        "user": "phython96",
        "type": "user"
      },
      "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by 4times and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.",
      "upvotes": 5,
      "discussionId": "688c2acc8c434640078cc3c7",
      "githubRepo": "https://github.com/CraftJarvis/ROCKET-3",
      "ai_summary": "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.",
      "ai_keywords": [
        "Reinforcement Learning",
        "RL",
        "visuomotor agents",
        "zero-shot generalization",
        "spatial reasoning",
        "cross-view goal specification",
        "multi-task RL",
        "automated task synthesis",
        "distributed RL framework"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-31T12:20:02.000Z",
    "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents",
    "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by 4times and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6578459d62d3ac1817ed79fe/4gxspFpC69p3qrB-lwGnT.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23698.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6578459d62d3ac1817ed79fe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
      "fullname": "Shaofei Cai",
      "name": "phython96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21509",
      "authors": [
        {
          "_id": "68898d7531e1218a089281b4",
          "name": "Runjin Chen",
          "hidden": false
        },
        {
          "_id": "68898d7531e1218a089281b5",
          "name": "Andy Arditi",
          "hidden": false
        },
        {
          "_id": "68898d7531e1218a089281b6",
          "name": "Henry Sleight",
          "hidden": false
        },
        {
          "_id": "68898d7531e1218a089281b7",
          "name": "Owain Evans",
          "hidden": false
        },
        {
          "_id": "68898d7531e1218a089281b8",
          "name": "Jack Lindsey",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T05:20:14.000Z",
      "submittedOnDailyAt": "2025-08-01T04:27:53.696Z",
      "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "6504cfc81dab48600892b2db",
        "avatarUrl": "/avatars/90f372c251d02733c0c5f10e3c0e0026.svg",
        "isPro": false,
        "fullname": "Runjin Chen",
        "user": "Runjin",
        "type": "user"
      },
      "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.",
      "upvotes": 4,
      "discussionId": "68898d7631e1218a089281b9",
      "githubRepo": "https://github.com/safety-research/persona_vectors/tree/main",
      "ai_summary": "Persona vectors in large language models can monitor and control personality changes during training and deployment, enabling the identification and mitigation of undesirable traits.",
      "ai_keywords": [
        "activation space",
        "persona vectors",
        "evil",
        "sycophancy",
        "hallucination",
        "personality shifts",
        "finetuning",
        "post-hoc intervention",
        "preventative steering method",
        "training data",
        "natural-language description"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-29T01:20:14.000Z",
    "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models",
    "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6504cfc81dab48600892b2db",
      "avatarUrl": "/avatars/90f372c251d02733c0c5f10e3c0e0026.svg",
      "fullname": "Runjin Chen",
      "name": "Runjin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23632",
      "authors": [
        {
          "_id": "688c37828c434640078cc3e2",
          "name": "Gabriel Mongaras",
          "hidden": false
        },
        {
          "_id": "688c37828c434640078cc3e3",
          "name": "Eric C. Larson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T15:10:03.000Z",
      "submittedOnDailyAt": "2025-08-01T02:12:49.709Z",
      "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective",
      "submittedOnDailyBy": {
        "_id": "62a1280b88bfb47fc40fe75b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/tDqTDW-XZh3n1zk3nKFxR.png",
        "isPro": false,
        "fullname": "Gabriel Mongaras",
        "user": "gmongaras",
        "type": "user"
      },
      "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.",
      "upvotes": 2,
      "discussionId": "688c37828c434640078cc3e4",
      "githubRepo": "https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective",
      "ai_summary": "Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.",
      "ai_keywords": [
        "softmax attention",
        "transformer architectures",
        "linear attention",
        "quadratic memory requirement",
        "computational complexity",
        "sequence length",
        "recurrent form",
        "recurrent neural networks (RNNs)"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-31T11:10:03.000Z",
    "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective",
    "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1280b88bfb47fc40fe75b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/tDqTDW-XZh3n1zk3nKFxR.png",
      "fullname": "Gabriel Mongaras",
      "name": "gmongaras",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23436",
      "authors": [
        {
          "_id": "688c61208c434640078cc4a7",
          "name": "Abdellah Zakaria Sellam",
          "hidden": false
        },
        {
          "_id": "688c61208c434640078cc4a8",
          "name": "Salah Eddine Bekhouche",
          "hidden": false
        },
        {
          "_id": "688c61208c434640078cc4a9",
          "name": "Cosimo Distante",
          "hidden": false
        },
        {
          "_id": "688c61208c434640078cc4aa",
          "name": "Abdelmalik Taleb-Ahmed",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/w3dfsmyhAiMHLCVPYa_SG.png"
      ],
      "publishedAt": "2025-07-31T11:16:00.000Z",
      "submittedOnDailyAt": "2025-08-01T05:11:17.433Z",
      "title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification",
      "submittedOnDailyBy": {
        "_id": "63e7493fdb40d9e67feaccdd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
        "isPro": false,
        "fullname": "Salah Eddine Bekhouche",
        "user": "Bekhouche",
        "type": "user"
      },
      "summary": "Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.",
      "upvotes": 1,
      "discussionId": "688c61208c434640078cc4ab",
      "ai_summary": "Enhancing dual-teacher self-supervised frameworks with Kolmogorov-Arnold Networks improves art style classification by better modeling nonlinear feature correlations and disentangling complex style manifolds.",
      "ai_keywords": [
        "dual-teacher",
        "self-supervised frameworks",
        "Kolmogorov-Arnold Networks",
        "KANs",
        "MLP projection",
        "prediction heads",
        "spline-based activations",
        "nonlinear feature correlations",
        "style manifolds",
        "linear probe accuracy"
      ]
    },
    "publishedAt": "2025-07-31T07:16:00.000Z",
    "title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification",
    "summary": "Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/w3dfsmyhAiMHLCVPYa_SG.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23436.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e7493fdb40d9e67feaccdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
      "fullname": "Salah Eddine Bekhouche",
      "name": "Bekhouche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23404",
      "authors": [
        {
          "_id": "688c5ffe8c434640078cc4a0",
          "name": "Salah Eddine Bekhouche",
          "hidden": false
        },
        {
          "_id": "688c5ffe8c434640078cc4a1",
          "name": "Azeddine Benlamoudi",
          "hidden": false
        },
        {
          "_id": "688c5ffe8c434640078cc4a2",
          "name": "Yazid Bounab",
          "hidden": false
        },
        {
          "_id": "688c5ffe8c434640078cc4a3",
          "name": "Fadi Dornaika",
          "hidden": false
        },
        {
          "_id": "688c5ffe8c434640078cc4a4",
          "name": "Abdenour Hadid",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/VszhggUGEZKhBhR5aEJtB.png"
      ],
      "publishedAt": "2025-07-31T10:18:28.000Z",
      "submittedOnDailyAt": "2025-08-01T05:06:34.307Z",
      "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring",
      "submittedOnDailyBy": {
        "_id": "63e7493fdb40d9e67feaccdd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
        "isPro": false,
        "fullname": "Salah Eddine Bekhouche",
        "user": "Bekhouche",
        "type": "user"
      },
      "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat https://github.com/Bekhouche/APR{GitHub}.",
      "upvotes": 1,
      "discussionId": "688c5ffe8c434640078cc4a5",
      "githubRepo": "https://github.com/Bekhouche/APR",
      "ai_summary": "An enhanced Dense Passage Retrieval framework for Arabic uses a novel Attentive Relevance Scoring mechanism to improve retrieval performance and ranking accuracy.",
      "ai_keywords": [
        "Dense Passage Retrieval",
        "Attentive Relevance Scoring",
        "semantic relevance",
        "pre-trained Arabic language models"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-31T06:18:28.000Z",
    "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring",
    "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat https://github.com/Bekhouche/APR{GitHub}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/VszhggUGEZKhBhR5aEJtB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e7493fdb40d9e67feaccdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
      "fullname": "Salah Eddine Bekhouche",
      "name": "Bekhouche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21584",
      "authors": [
        {
          "_id": "688c690b8c434640078cc4ad",
          "name": "Kejia Zhang",
          "hidden": false
        },
        {
          "_id": "688c690b8c434640078cc4ae",
          "name": "Keda Tao",
          "hidden": false
        },
        {
          "_id": "688c690b8c434640078cc4af",
          "name": "Zhiming Luo",
          "hidden": false
        },
        {
          "_id": "688c690b8c434640078cc4b0",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "688c690b8c434640078cc4b1",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "688c690b8c434640078cc4b2",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T08:39:19.000Z",
      "submittedOnDailyAt": "2025-08-01T05:44:51.754Z",
      "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs",
      "submittedOnDailyBy": {
        "_id": "655469586bc4180700cf7a34",
        "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
        "isPro": false,
        "fullname": "Kejia Zhang",
        "user": "KejiaRobust",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.",
      "upvotes": 1,
      "discussionId": "688c690b8c434640078cc4b3",
      "projectPage": "https://kejiazhang-robust.github.io/tars_web/",
      "githubRepo": "https://github.com/KejiaZhang-Robust/TARS",
      "ai_summary": "TARS, a token-adaptive preference strategy, improves multimodal large language models by reducing hallucinations through min-max optimization under semantic constraints.",
      "ai_keywords": [
        "multimodal large language models",
        "vision-language reasoning",
        "direct preference optimization",
        "hallucinations",
        "token-adaptive preference strategy",
        "min-max optimization",
        "distributional shifts",
        "semantic constraints",
        "preference loss",
        "causal grounding",
        "overfitting",
        "hallucination benchmarks",
        "cognition value",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-07-29T04:39:19.000Z",
    "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs",
    "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655469586bc4180700cf7a34",
      "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
      "fullname": "Kejia Zhang",
      "name": "KejiaRobust",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20519",
      "authors": [
        {
          "_id": "688c441f8c434640078cc40d",
          "name": "Risa Shinoda",
          "hidden": false
        },
        {
          "_id": "688c441f8c434640078cc40e",
          "name": "Nakamasa Inoue",
          "hidden": false
        },
        {
          "_id": "688c441f8c434640078cc40f",
          "name": "Hirokatsu Kataoka",
          "hidden": false
        },
        {
          "_id": "688c441f8c434640078cc410",
          "name": "Masaki Onishi",
          "hidden": false
        },
        {
          "_id": "688c441f8c434640078cc411",
          "name": "Yoshitaka Ushiku",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/651a7684a1a5e5d617e28f84/fNTuPJz2ztWbkxQRXe0O0.png"
      ],
      "publishedAt": "2025-07-28T04:58:29.000Z",
      "submittedOnDailyAt": "2025-08-01T03:07:15.443Z",
      "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
      "submittedOnDailyBy": {
        "_id": "651a7684a1a5e5d617e28f84",
        "avatarUrl": "/avatars/f484ae7c8d980818cd2ba3ffa682b781.svg",
        "isPro": true,
        "fullname": "Risa Shinoda",
        "user": "risashinoda",
        "type": "user"
      },
      "summary": "Precise automated understanding of agricultural tasks such as disease\nidentification is essential for sustainable crop production. Recent advances in\nvision-language models (VLMs) are expected to further expand the range of\nagricultural tasks by facilitating human-model interaction through easy,\ntext-based communication. Here, we introduce AgroBench (Agronomist AI\nBenchmark), a benchmark for evaluating VLM models across seven agricultural\ntopics, covering key areas in agricultural engineering and relevant to\nreal-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is\nannotated by expert agronomists. Our AgroBench covers a state-of-the-art range\nof categories, including 203 crop categories and 682 disease categories, to\nthoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal\nthat VLMs have room for improvement in fine-grained identification tasks.\nNotably, in weed identification, most open-source VLMs perform close to random.\nWith our wide range of topics and expert-annotated categories, we analyze the\ntypes of errors made by VLMs and suggest potential pathways for future VLM\ndevelopment. Our dataset and code are available at\nhttps://dahlian00.github.io/AgroBenchPage/ .",
      "upvotes": 1,
      "discussionId": "688c441f8c434640078cc412",
      "projectPage": "https://dahlian00.github.io/AgroBenchPage/",
      "githubRepo": "https://github.com/dahlian00/AgroBench/tree/main",
      "ai_summary": "AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "AgroBench",
        "agricultural engineering",
        "crop categories",
        "disease categories",
        "fine-grained identification",
        "weed identification"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-28T00:58:29.000Z",
    "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
    "summary": "Precise automated understanding of agricultural tasks such as disease\nidentification is essential for sustainable crop production. Recent advances in\nvision-language models (VLMs) are expected to further expand the range of\nagricultural tasks by facilitating human-model interaction through easy,\ntext-based communication. Here, we introduce AgroBench (Agronomist AI\nBenchmark), a benchmark for evaluating VLM models across seven agricultural\ntopics, covering key areas in agricultural engineering and relevant to\nreal-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is\nannotated by expert agronomists. Our AgroBench covers a state-of-the-art range\nof categories, including 203 crop categories and 682 disease categories, to\nthoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal\nthat VLMs have room for improvement in fine-grained identification tasks.\nNotably, in weed identification, most open-source VLMs perform close to random.\nWith our wide range of topics and expert-annotated categories, we analyze the\ntypes of errors made by VLMs and suggest potential pathways for future VLM\ndevelopment. Our dataset and code are available at\nhttps://dahlian00.github.io/AgroBenchPage/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/651a7684a1a5e5d617e28f84/fNTuPJz2ztWbkxQRXe0O0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20519.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651a7684a1a5e5d617e28f84",
      "avatarUrl": "/avatars/f484ae7c8d980818cd2ba3ffa682b781.svg",
      "fullname": "Risa Shinoda",
      "name": "risashinoda",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.14793",
      "authors": [
        {
          "_id": "688c32248c434640078cc3d8",
          "name": "T. Anderson Keller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T02:52:21.000Z",
      "submittedOnDailyAt": "2025-08-01T03:29:17.163Z",
      "title": "Flow Equivariant Recurrent Neural Networks",
      "submittedOnDailyBy": {
        "_id": "661e07e02a8496916011c08a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
        "isPro": false,
        "fullname": "Md Ashiqur Rahman",
        "user": "ashiq24",
        "type": "user"
      },
      "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.",
      "upvotes": 1,
      "discussionId": "688c32248c434640078cc3d9",
      "ai_summary": "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.",
      "ai_keywords": [
        "equivariant",
        "symmetries",
        "sequence models",
        "recurrent neural networks",
        "RNNs",
        "flow equivariance",
        "one-parameter Lie subgroups",
        "visual motion",
        "training speed",
        "length generalization",
        "velocity generalization",
        "next step prediction",
        "sequence classification"
      ]
    },
    "publishedAt": "2025-07-19T22:52:21.000Z",
    "title": "Flow Equivariant Recurrent Neural Networks",
    "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14793.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661e07e02a8496916011c08a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
      "fullname": "Md Ashiqur Rahman",
      "name": "ashiq24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23374",
      "authors": [
        {
          "_id": "688c51de8c434640078cc467",
          "name": "Shuangkang Fang",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc468",
          "name": "I-Chao Shen",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc469",
          "name": "Takeo Igarashi",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc46a",
          "name": "Yufeng Wang",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc46b",
          "name": "ZeSheng Wang",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc46c",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc46d",
          "name": "Wenrui Ding",
          "hidden": false
        },
        {
          "_id": "688c51de8c434640078cc46e",
          "name": "Shuchang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T09:43:31.000Z",
      "submittedOnDailyAt": "2025-08-01T04:09:49.515Z",
      "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "66ee3663518ccb4e62be37c4",
        "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
        "isPro": false,
        "fullname": "Shuangkang Fang",
        "user": "fsk515",
        "type": "user"
      },
      "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.",
      "upvotes": 0,
      "discussionId": "688c51de8c434640078cc46f",
      "ai_summary": "NeRF-GS combines Neural Radiance Fields and 3D Gaussian Splatting to enhance 3D scene representation and performance through joint optimization and shared spatial information.",
      "ai_keywords": [
        "Neural Radiance Fields",
        "3D Gaussian Splatting",
        "spatial representation",
        "Gaussian initialization",
        "spatial awareness",
        "inter-Gaussian correlations",
        "residual vectors",
        "implicit features",
        "Gaussian positions",
        "hybrid approaches"
      ]
    },
    "publishedAt": "2025-07-31T05:43:31.000Z",
    "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee3663518ccb4e62be37c4",
      "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
      "fullname": "Shuangkang Fang",
      "name": "fsk515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]