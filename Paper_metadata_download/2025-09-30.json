[
  {
    "paper": {
      "id": "2509.24006",
      "authors": [
        {
          "_id": "68db424ed2bf1f4b15ec730a",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:32:43.935Z",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec730b",
          "name": "Haoxu Wang",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec730c",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec730d",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec730e",
          "name": "Kaiwen Zheng",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec730f",
          "name": "Haocheng Xi",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7310",
          "name": "Ziteng Wang",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7311",
          "name": "Hongzhou Zhu",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7312",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7313",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7314",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7315",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "68db424ed2bf1f4b15ec7316",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T17:58:59.000Z",
      "submittedOnDailyAt": "2025-09-30T01:13:12.259Z",
      "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
      "upvotes": 78,
      "discussionId": "68db424fd2bf1f4b15ec7317",
      "ai_summary": "SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.",
      "ai_keywords": [
        "Diffusion Transformer",
        "attention latency",
        "sequence length",
        "quadratic complexity",
        "sparse acceleration",
        "low-rank acceleration",
        "SLA",
        "critical weights",
        "marginal weights",
        "negligible weights",
        "GPU kernel",
        "fine-tuning",
        "attention computation",
        "generation quality",
        "end-to-end speedup"
      ]
    },
    "publishedAt": "2025-09-28T13:58:59.000Z",
    "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
    "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24006.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "submitterOrganization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23102",
      "authors": [
        {
          "_id": "68db42f2d2bf1f4b15ec7323",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7324",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7325",
          "user": {
            "_id": "65b8909c89eb3dfbe8d26780",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
            "isPro": false,
            "fullname": "Weihao XUAN",
            "user": "weihao1115",
            "type": "user"
          },
          "name": "Weihao Xuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:32:41.015Z",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7326",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7327",
          "name": "Yijia Xiao",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7328",
          "name": "Guancheng Wan",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec7329",
          "name": "Xiaomin Li",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec732a",
          "name": "Bing Hu",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec732b",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec732c",
          "name": "Jure Leskovec",
          "hidden": false
        },
        {
          "_id": "68db42f2d2bf1f4b15ec732d",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T04:18:33.000Z",
      "submittedOnDailyAt": "2025-09-30T01:12:14.832Z",
      "title": "Multiplayer Nash Preference Optimization",
      "submittedOnDailyBy": {
        "_id": "675e0d5cdd3e9eeed6954f5a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
        "isPro": false,
        "fullname": "Fang Wu",
        "user": "fangwu97",
        "type": "user"
      },
      "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\nn-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.",
      "upvotes": 46,
      "discussionId": "68db42f3d2bf1f4b15ec732e",
      "ai_summary": "Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.",
      "ai_keywords": [
        "Reinforcement learning from human feedback",
        "RLHF",
        "large language models",
        "LLMs",
        "Bradley-Terry assumption",
        "Nash learning from human feedback",
        "NLHF",
        "INPO",
        "ONPO",
        "EGPO",
        "Multiplayer Nash Preference Optimization",
        "MNPO",
        "Nash equilibria",
        "duality gap",
        "instruction-following benchmarks",
        "heterogeneous annotator conditions",
        "mixed-policy evaluation"
      ]
    },
    "publishedAt": "2025-09-27T00:18:33.000Z",
    "title": "Multiplayer Nash Preference Optimization",
    "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\nn-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675e0d5cdd3e9eeed6954f5a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
      "fullname": "Fang Wu",
      "name": "fangwu97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "submitterOrganization": {
      "_id": "6112d84f8c2e1f4060908c9e",
      "name": "stanfordnlp",
      "fullname": "Stanford NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24897",
      "authors": [
        {
          "_id": "68db4c16d2bf1f4b15ec744e",
          "user": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "isPro": false,
            "fullname": "Yang Shi",
            "user": "DogNeverSleep",
            "type": "user"
          },
          "name": "Yang Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:34.746Z",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec744f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7450",
          "name": "Yue Ding",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7451",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7452",
          "name": "Xuanyu Zhu",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7453",
          "name": "Sheng Zhou",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7454",
          "name": "Wenting Liu",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7455",
          "name": "Haochen Tian",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7456",
          "name": "Rundong Wang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7457",
          "name": "Huanqian Wang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7458",
          "name": "Zuyan Liu",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7459",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745a",
          "name": "Ruizhe Chen",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745b",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745c",
          "name": "Zhuoran Zhang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745d",
          "name": "Xinlong Chen",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745e",
          "name": "Chengzhuo Tong",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec745f",
          "user": {
            "_id": "661e62c6bac5d981f886f77b",
            "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg",
            "isPro": false,
            "fullname": "Bozhou Li",
            "user": "zooblastlbz",
            "type": "user"
          },
          "name": "Bozhou Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:22.595Z",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7460",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7461",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7462",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7463",
          "name": "Wenjing Yang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7464",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7465",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7466",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "68db4c16d2bf1f4b15ec7467",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T15:07:28.000Z",
      "submittedOnDailyAt": "2025-09-30T01:50:34.924Z",
      "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.",
      "upvotes": 36,
      "discussionId": "68db4c17d2bf1f4b15ec7468",
      "githubRepo": "https://github.com/FrankYang-17/RealUnify",
      "ai_summary": "RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.",
      "ai_keywords": [
        "multimodal models",
        "RealUnify",
        "bidirectional capability synergy",
        "understanding enhances generation",
        "generation enhances understanding",
        "dual-evaluation protocol",
        "unified models",
        "specialized baselines"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-29T11:07:28.000Z",
    "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark",
    "summary": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24897.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25190",
      "authors": [
        {
          "_id": "68db41e1d2bf1f4b15ec72f5",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "68db41e1d2bf1f4b15ec72f6",
          "name": "Yushan Zhang",
          "hidden": false
        },
        {
          "_id": "68db41e1d2bf1f4b15ec72f7",
          "user": {
            "_id": "64b4a717aa03b6520839e9b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
            "isPro": false,
            "fullname": "Haiwen Diao",
            "user": "Paranioar",
            "type": "user"
          },
          "name": "Haiwen Diao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:32:48.312Z",
          "hidden": false
        },
        {
          "_id": "68db41e1d2bf1f4b15ec72f8",
          "user": {
            "_id": "62d3f7d84b0933c48f3cdd9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/rSoSyH0Td9fD9HVlyK7bh.jpeg",
            "isPro": true,
            "fullname": "Bo Li",
            "user": "luodian",
            "type": "user"
          },
          "name": "Bo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:32:57.152Z",
          "hidden": false
        },
        {
          "_id": "68db41e1d2bf1f4b15ec72f9",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68db41e1d2bf1f4b15ec72fa",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-09-30T01:06:08.539Z",
      "title": "Visual Jigsaw Post-Training Improves MLLMs",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Reinforcement learning based post-training has recently emerged as a powerful\nparadigm for enhancing the alignment and reasoning capabilities of multimodal\nlarge language models (MLLMs). While vision-centric post-training is crucial\nfor enhancing MLLMs' intrinsic understanding of visual signals, current\npost-training paradigms are predominantly text-centric, where dense visual\ninputs are only leveraged to extract sparse cues for text-based reasoning.\nThere exist a few approaches in this direction, however, they often still rely\non text as an intermediate mediator or introduce additional visual generative\ndesigns. In this work, we introduce Visual Jigsaw, a generic self-supervised\npost-training framework designed to strengthen visual understanding in MLLMs.\nVisual Jigsaw is formulated as a general ordering task: visual inputs are\npartitioned, shuffled, and the model must reconstruct the visual information by\nproducing the correct permutation in natural language. This naturally aligns\nwith reinforcement learning from verifiable rewards (RLVR), requires no\nadditional visual generative components, and derives its supervisory signal\nautomatically without any annotations. We instantiate Visual Jigsaw across\nthree visual modalities, including images, videos, and 3D data. Extensive\nexperiments demonstrate substantial improvements in fine-grained perception,\ntemporal reasoning, and 3D spatial understanding. Our findings highlight the\npotential of self-supervised vision-centric tasks in post-training MLLMs and\naim to inspire further research on vision-centric pretext designs. Project\nPage: https://penghao-wu.github.io/visual_jigsaw/",
      "upvotes": 33,
      "discussionId": "68db41e1d2bf1f4b15ec72fb",
      "projectPage": "https://penghao-wu.github.io/visual_jigsaw/",
      "githubRepo": "https://github.com/penghao-wu/visual_jigsaw",
      "ai_summary": "Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.",
      "ai_keywords": [
        "reinforcement learning",
        "post-training",
        "multimodal large language models",
        "visual understanding",
        "self-supervised",
        "permutation task",
        "reinforcement learning from verifiable rewards",
        "fine-grained perception",
        "temporal reasoning",
        "3D spatial understanding"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-09-29T13:59:57.000Z",
    "title": "Visual Jigsaw Post-Training Improves MLLMs",
    "summary": "Reinforcement learning based post-training has recently emerged as a powerful\nparadigm for enhancing the alignment and reasoning capabilities of multimodal\nlarge language models (MLLMs). While vision-centric post-training is crucial\nfor enhancing MLLMs' intrinsic understanding of visual signals, current\npost-training paradigms are predominantly text-centric, where dense visual\ninputs are only leveraged to extract sparse cues for text-based reasoning.\nThere exist a few approaches in this direction, however, they often still rely\non text as an intermediate mediator or introduce additional visual generative\ndesigns. In this work, we introduce Visual Jigsaw, a generic self-supervised\npost-training framework designed to strengthen visual understanding in MLLMs.\nVisual Jigsaw is formulated as a general ordering task: visual inputs are\npartitioned, shuffled, and the model must reconstruct the visual information by\nproducing the correct permutation in natural language. This naturally aligns\nwith reinforcement learning from verifiable rewards (RLVR), requires no\nadditional visual generative components, and derives its supervisory signal\nautomatically without any annotations. We instantiate Visual Jigsaw across\nthree visual modalities, including images, videos, and 3D data. Extensive\nexperiments demonstrate substantial improvements in fine-grained perception,\ntemporal reasoning, and 3D spatial understanding. Our findings highlight the\npotential of self-supervised vision-centric tasks in post-training MLLMs and\naim to inspire further research on vision-centric pretext designs. Project\nPage: https://penghao-wu.github.io/visual_jigsaw/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25190.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24900",
      "authors": [
        {
          "_id": "68db4e25d2bf1f4b15ec747c",
          "name": "Zhihong Chen",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec747d",
          "name": "Xuehai Bai",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec747e",
          "user": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "isPro": false,
            "fullname": "Yang Shi",
            "user": "DogNeverSleep",
            "type": "user"
          },
          "name": "Yang Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:34:34.223Z",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec747f",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7480",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7481",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7482",
          "name": "Xiaoyan Sun",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7483",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7484",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7485",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7486",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68db4e25d2bf1f4b15ec7487",
          "name": "Yi-Fan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T15:11:09.000Z",
      "submittedOnDailyAt": "2025-09-30T01:57:57.108Z",
      "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
      "upvotes": 33,
      "discussionId": "68db4e25d2bf1f4b15ec7488",
      "ai_summary": "OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.",
      "ai_keywords": [
        "unified multimodal models",
        "image generation",
        "image editing",
        "training data",
        "hierarchical task taxonomy",
        "automated data generation",
        "text rendering",
        "style control",
        "scientific imagery",
        "complex instruction editing",
        "instruction-image pairs",
        "controlled diversity",
        "fine-tuning",
        "UniWorld-V1",
        "ImgEdit-Bench",
        "Harmon",
        "GenEval"
      ]
    },
    "publishedAt": "2025-09-29T11:11:09.000Z",
    "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing",
    "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23426",
      "authors": [
        {
          "_id": "68db40a4d2bf1f4b15ec72e8",
          "name": "Shanghua Gao",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72e9",
          "name": "Richard Zhu",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72ea",
          "name": "Pengwei Sui",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72eb",
          "user": {
            "_id": "5f2c36551ebc8c6ede2f0e53",
            "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
            "isPro": false,
            "fullname": "Tony Kong",
            "user": "TonyK",
            "type": "user"
          },
          "name": "Zhenglun Kong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:34:01.295Z",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72ec",
          "name": "Sufian Aldogom",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72ed",
          "name": "Yepeng Huang",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72ee",
          "name": "Ayush Noori",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72ef",
          "user": {
            "_id": "686445c64b2dd0745a049817",
            "avatarUrl": "/avatars/34c4b2ef075f80baf9376cbcfec841d6.svg",
            "isPro": false,
            "fullname": "Reza Shamji",
            "user": "rezashamji",
            "type": "user"
          },
          "name": "Reza Shamji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:33:00.082Z",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72f0",
          "name": "Krishna Parvataneni",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72f1",
          "name": "Theodoros Tsiligkaridis",
          "hidden": false
        },
        {
          "_id": "68db40a4d2bf1f4b15ec72f2",
          "name": "Marinka Zitnik",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6350fc5ba8822aadf571304f/DrcSsDtZXCxIzFp92C15P.mp4"
      ],
      "publishedAt": "2025-09-27T17:38:53.000Z",
      "submittedOnDailyAt": "2025-09-30T01:11:08.584Z",
      "title": "Democratizing AI scientists using ToolUniverse",
      "submittedOnDailyBy": {
        "_id": "6350fc5ba8822aadf571304f",
        "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
        "isPro": false,
        "fullname": "gasvn",
        "user": "shgao",
        "type": "user"
      },
      "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.",
      "upvotes": 29,
      "discussionId": "68db40a4d2bf1f4b15ec72f3",
      "projectPage": "https://aiscientist.tools/",
      "githubRepo": "https://github.com/mims-harvard/ToolUniverse",
      "ai_summary": "ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.",
      "ai_keywords": [
        "AI scientists",
        "ToolUniverse",
        "machine learning models",
        "datasets",
        "APIs",
        "scientific packages",
        "data analysis",
        "knowledge retrieval",
        "experimental design",
        "tool interfaces",
        "natural language descriptions",
        "tool specifications",
        "agentic workflows",
        "hypercholesterolemia",
        "drug analogs"
      ],
      "githubStars": 233
    },
    "publishedAt": "2025-09-27T13:38:53.000Z",
    "title": "Democratizing AI scientists using ToolUniverse",
    "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6350fc5ba8822aadf571304f/DrcSsDtZXCxIzFp92C15P.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23426.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6350fc5ba8822aadf571304f",
      "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
      "fullname": "gasvn",
      "name": "shgao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "submitterOrganization": {
      "_id": "63663038361a96184dbad334",
      "name": "Harvard",
      "fullname": "Harvard University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1667641389305-6366110f575c93cedafde54e.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25160",
      "authors": [
        {
          "_id": "68db4fc5d2bf1f4b15ec74a4",
          "name": "Fan Yuan",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74a5",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:51.986Z",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74a6",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74a7",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74a8",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74a9",
          "name": "Jinyan Chen",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74aa",
          "name": "Yanwei Lou",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74ab",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74ac",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74ad",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74ae",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "68db4fc5d2bf1f4b15ec74af",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:57:05.000Z",
      "submittedOnDailyAt": "2025-09-30T02:04:37.669Z",
      "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Vision language models (VLMs) achieve unified modeling of images and text,\nenabling them to accomplish complex real-world tasks through perception,\nplanning, and reasoning. Among these tasks, reasoning is particularly\nrepresentative, with mathematical reasoning serving as a prominent example. It\nhighlights the high-level capability of VLMs to comprehend mathematical\ninformation in images and to perform sophisticated reasoning. Recently,\nnumerous visual mathematical reasoning benchmarks have been proposed, but they\nare often restricted to geometry, lack coverage of math word problems, and\nrarely assess reasoning across multiple images. To address these gaps, we\nintroduce GSM8K-V, a purely visual multi-image mathematical reasoning\nbenchmark. GSM8K-V is built by systematically mapping each sample from the\nwidely used text-based GSM8K into visual form. Through a carefully designed\nautomated image-generation pipeline combined with meticulous human annotation,\nwe curate 1,319 high-quality samples. We evaluate a wide range of open-source\nand closed-source models on GSM8K-V. Results show that although existing VLMs\nhave nearly saturated performance on text-based GSM8K, there remains\nsubstantial room for improvement on GSM8K-V. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on\nGSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the\nlimitations of current models as well as potential directions for improvement.\nGSM8K-V offers a new perspective on visual mathematical reasoning and\nestablishes a benchmark to guide the development of more robust and\ngeneralizable VLMs.",
      "upvotes": 22,
      "discussionId": "68db4fc5d2bf1f4b15ec74b0",
      "projectPage": "https://zju-real.github.io/GSM8K-V/",
      "githubRepo": "https://github.com/ZJU-REAL/GSM8K-V",
      "ai_summary": "GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.",
      "ai_keywords": [
        "vision language models",
        "VLMs",
        "mathematical reasoning",
        "visual mathematical reasoning",
        "GSM8K-V",
        "image-generation pipeline",
        "Gemini-2.5-Pro"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-29T13:57:05.000Z",
    "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts",
    "summary": "Vision language models (VLMs) achieve unified modeling of images and text,\nenabling them to accomplish complex real-world tasks through perception,\nplanning, and reasoning. Among these tasks, reasoning is particularly\nrepresentative, with mathematical reasoning serving as a prominent example. It\nhighlights the high-level capability of VLMs to comprehend mathematical\ninformation in images and to perform sophisticated reasoning. Recently,\nnumerous visual mathematical reasoning benchmarks have been proposed, but they\nare often restricted to geometry, lack coverage of math word problems, and\nrarely assess reasoning across multiple images. To address these gaps, we\nintroduce GSM8K-V, a purely visual multi-image mathematical reasoning\nbenchmark. GSM8K-V is built by systematically mapping each sample from the\nwidely used text-based GSM8K into visual form. Through a carefully designed\nautomated image-generation pipeline combined with meticulous human annotation,\nwe curate 1,319 high-quality samples. We evaluate a wide range of open-source\nand closed-source models on GSM8K-V. Results show that although existing VLMs\nhave nearly saturated performance on text-based GSM8K, there remains\nsubstantial room for improvement on GSM8K-V. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on\nGSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the\nlimitations of current models as well as potential directions for improvement.\nGSM8K-V offers a new perspective on visual mathematical reasoning and\nestablishes a benchmark to guide the development of more robust and\ngeneralizable VLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 114
    },
    "submitterOrganization": {
      "_id": "682cd1054ef56a9cb302716c",
      "name": "ZJU-REAL",
      "fullname": "REAL Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64098738342c26884c792c93/0cK2dzrgQem8r2utlMm98.webp"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24695",
      "authors": [
        {
          "_id": "68db4b9ad2bf1f4b15ec7425",
          "user": {
            "_id": "645b5b09bc7518912e1f9733",
            "avatarUrl": "/avatars/4d35f728b41f93881a9b67c337f4d1df.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "Lawrence-cj",
            "type": "user"
          },
          "name": "Junsong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:43.494Z",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7426",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7427",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7428",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7429",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742a",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742b",
          "name": "Xianbang Wang",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742c",
          "name": "Yicheng Pan",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742d",
          "name": "Daquan Zhou",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742e",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec742f",
          "name": "Haozhe Liu",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7430",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7431",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7432",
          "user": {
            "_id": "63129589bbaa385279d1826e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
            "isPro": true,
            "fullname": "Muyang Li",
            "user": "Lmxyy",
            "type": "user"
          },
          "name": "Muyang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:40.674Z",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7433",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7434",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7435",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7436",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7437",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68db4b9ad2bf1f4b15ec7438",
          "name": "Enze Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T12:28:09.000Z",
      "submittedOnDailyAt": "2025-09-30T01:49:24.742Z",
      "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
      "submittedOnDailyBy": {
        "_id": "64638bd36c27a7e33b26654b",
        "avatarUrl": "/avatars/2ef5aeb94ef7016082975b4cc201873e.svg",
        "isPro": false,
        "fullname": "Yuyang",
        "user": "Yuyang-z",
        "type": "user"
      },
      "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
      "upvotes": 22,
      "discussionId": "68db4b9bd2bf1f4b15ec7439",
      "projectPage": "https://nvlabs.github.io/Sana/Video",
      "ai_summary": "SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.",
      "ai_keywords": [
        "diffusion model",
        "Linear DiT",
        "linear attention",
        "constant-memory KV cache",
        "block-wise autoregressive",
        "text-video alignment",
        "MovieGen",
        "NVFP4 precision"
      ]
    },
    "publishedAt": "2025-09-29T08:28:09.000Z",
    "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
    "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24695.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64638bd36c27a7e33b26654b",
      "avatarUrl": "/avatars/2ef5aeb94ef7016082975b4cc201873e.svg",
      "fullname": "Yuyang",
      "name": "Yuyang-z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "submitterOrganization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25175",
      "authors": [
        {
          "_id": "68db4b13d2bf1f4b15ec741b",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:48.835Z",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec741c",
          "user": {
            "_id": "68db69d52d9cd53de0af2129",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/BGCasdq8kKQWaPHzkt6NS.webp",
            "isPro": false,
            "fullname": "Xinyu Mei",
            "user": "xinyumei",
            "type": "user"
          },
          "name": "Xinyu Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:46.012Z",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec741d",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:51.653Z",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec741e",
          "name": "Rui Zhou",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec741f",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec7420",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec7421",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "68db4b13d2bf1f4b15ec7422",
          "name": "Yongliang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:07.000Z",
      "submittedOnDailyAt": "2025-09-30T01:50:03.295Z",
      "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering",
      "submittedOnDailyBy": {
        "_id": "6692aff88db712bad780f02a",
        "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
        "isPro": false,
        "fullname": "xhl",
        "user": "zjuxhl",
        "type": "user"
      },
      "summary": "Large language model (LLM) steering has emerged as a promising paradigm for\ncontrolling model behavior at inference time through targeted manipulation of\nhidden states, offering a lightweight alternative to expensive retraining.\nHowever, existing steering frameworks suffer from critical limitations:\ncomputational inefficiency, limited extensibility, and restricted functionality\nthat hinder both research progress and practical deployment. We present\nEasySteer, a unified framework for high-performance, extensible LLM steering\nbuilt on vLLM. Our system features modular architecture with pluggable\ninterfaces for both analysis-based and learning-based methods, fine-grained\nparameter control, pre-computed steering vectors for eight application domains,\nand an interactive demonstration system. Through deep integration with vLLM's\noptimized inference engine, EasySteer achieves 5.5-11.4times speedup over\nexisting frameworks. Extensive experiments demonstrate its effectiveness in\noverthinking mitigation, hallucination reduction, and other key applications.\nEasySteer transforms steering from research technique to production-ready\ncapability, establishing critical infrastructure for deployable, controllable\nlanguage models.",
      "upvotes": 21,
      "discussionId": "68db4b13d2bf1f4b15ec7423",
      "githubRepo": "https://github.com/ZJU-REAL/EasySteer",
      "ai_summary": "EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.",
      "ai_keywords": [
        "large language model steering",
        "hidden states",
        "vLLM",
        "modular architecture",
        "pluggable interfaces",
        "analysis-based methods",
        "learning-based methods",
        "fine-grained parameter control",
        "steering vectors",
        "overthinking mitigation",
        "hallucination reduction",
        "deployable",
        "controllable language models"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-09-29T13:59:07.000Z",
    "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering",
    "summary": "Large language model (LLM) steering has emerged as a promising paradigm for\ncontrolling model behavior at inference time through targeted manipulation of\nhidden states, offering a lightweight alternative to expensive retraining.\nHowever, existing steering frameworks suffer from critical limitations:\ncomputational inefficiency, limited extensibility, and restricted functionality\nthat hinder both research progress and practical deployment. We present\nEasySteer, a unified framework for high-performance, extensible LLM steering\nbuilt on vLLM. Our system features modular architecture with pluggable\ninterfaces for both analysis-based and learning-based methods, fine-grained\nparameter control, pre-computed steering vectors for eight application domains,\nand an interactive demonstration system. Through deep integration with vLLM's\noptimized inference engine, EasySteer achieves 5.5-11.4times speedup over\nexisting frameworks. Extensive experiments demonstrate its effectiveness in\noverthinking mitigation, hallucination reduction, and other key applications.\nEasySteer transforms steering from research technique to production-ready\ncapability, establishing critical infrastructure for deployable, controllable\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6692aff88db712bad780f02a",
      "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
      "fullname": "xhl",
      "name": "zjuxhl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23909",
      "authors": [
        {
          "_id": "68db3dd7d2bf1f4b15ec72c4",
          "name": "Xin Luo",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72c5",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72c6",
          "name": "Chenyuan Wu",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72c7",
          "name": "Shitao Xiao",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72c8",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72c9",
          "name": "Defu Lian",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72ca",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72cb",
          "name": "Dong Liu",
          "hidden": false
        },
        {
          "_id": "68db3dd7d2bf1f4b15ec72cc",
          "name": "Zheng liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T14:28:24.000Z",
      "submittedOnDailyAt": "2025-09-30T00:49:08.471Z",
      "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
      "submittedOnDailyBy": {
        "_id": "641bd1737c21ab946bf69aff",
        "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
        "isPro": false,
        "fullname": "xin luo",
        "user": "sienna223",
        "type": "user"
      },
      "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
      "upvotes": 20,
      "discussionId": "68db3dd8d2bf1f4b15ec72cd",
      "githubRepo": "https://github.com/VectorSpaceLab/EditScore",
      "ai_summary": "A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.",
      "ai_keywords": [
        "Reinforcement Learning",
        "reward model",
        "EditReward-Bench",
        "EditScore",
        "OmniGen2",
        "policy optimization"
      ],
      "githubStars": 31
    },
    "publishedAt": "2025-09-28T10:28:24.000Z",
    "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
    "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641bd1737c21ab946bf69aff",
      "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
      "fullname": "xin luo",
      "name": "sienna223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "submitterOrganization": {
      "_id": "61be9739d2f9358e24ca0a4f",
      "name": "BAAI",
      "fullname": "Beijing Academy of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25106",
      "authors": [
        {
          "_id": "68db7847d2bf1f4b15ec7720",
          "name": "Yuan Liang",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7721",
          "name": "Jiaxian Li",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7722",
          "user": {
            "_id": "68528ed95eb3deddd065648a",
            "avatarUrl": "/avatars/e55960126fa7a2ceb43dd0f2bc8190cb.svg",
            "isPro": false,
            "fullname": "wang",
            "user": "yuqing666",
            "type": "user"
          },
          "name": "Yuqing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:30.169Z",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7723",
          "name": "Piaohong Wang",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7724",
          "name": "Motong Tian",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7725",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7726",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7727",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7728",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec7729",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec772a",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec772b",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec772c",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:36:05.963Z",
          "hidden": false
        },
        {
          "_id": "68db7847d2bf1f4b15ec772d",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:39:17.000Z",
      "submittedOnDailyAt": "2025-09-30T05:00:28.938Z",
      "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
      "upvotes": 16,
      "discussionId": "68db7847d2bf1f4b15ec772e",
      "ai_summary": "A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.",
      "ai_keywords": [
        "Deep Research Agents",
        "Personalized Deep Research Bench",
        "PQR Evaluation Framework",
        "Personalization Alignment",
        "Content Quality",
        "Factual Reliability"
      ]
    },
    "publishedAt": "2025-09-29T13:39:17.000Z",
    "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
    "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "submitterOrganization": {
      "_id": "67177eecd0fad5b4ccc09461",
      "name": "OPPOer",
      "fullname": "OPPO",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24014",
      "authors": [
        {
          "_id": "68db3a3dd2bf1f4b15ec72ae",
          "user": {
            "_id": "668e740f1173ab43d9d9ed5e",
            "avatarUrl": "/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg",
            "isPro": false,
            "fullname": "Zeqing Wang",
            "user": "INV-WZQ",
            "type": "user"
          },
          "name": "Zeqing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:34:37.327Z",
          "hidden": false
        },
        {
          "_id": "68db3a3dd2bf1f4b15ec72af",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "68db3a3dd2bf1f4b15ec72b0",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "68db3a3dd2bf1f4b15ec72b1",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:20.257Z",
          "hidden": false
        },
        {
          "_id": "68db3a3dd2bf1f4b15ec72b2",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/P1TvbBP2RcB6n7w3yvi5s.mp4"
      ],
      "publishedAt": "2025-09-28T18:10:10.000Z",
      "submittedOnDailyAt": "2025-09-30T00:49:13.077Z",
      "title": "SparseD: Sparse Attention for Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to 1.50times speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.",
      "upvotes": 16,
      "discussionId": "68db3a3dd2bf1f4b15ec72b3",
      "githubRepo": "https://github.com/INV-WZQ/SparseD",
      "ai_summary": "SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.",
      "ai_keywords": [
        "diffusion language models",
        "autoregressive models",
        "attention",
        "quadratic complexity",
        "context length",
        "query-key pairs",
        "sparse attention",
        "denoising steps",
        "SparseD",
        "head-specific structures",
        "generation quality",
        "FlashAttention"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-09-28T14:10:10.000Z",
    "title": "SparseD: Sparse Attention for Diffusion Language Models",
    "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to 1.50times speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/P1TvbBP2RcB6n7w3yvi5s.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24014.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24007",
      "authors": [
        {
          "_id": "68db4940d2bf1f4b15ec73f9",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73fa",
          "user": {
            "_id": "6571382c7644d1128561cebe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
            "isPro": false,
            "fullname": "Cao Yue",
            "user": "yuecao0119",
            "type": "user"
          },
          "name": "Yue Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:54.229Z",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73fb",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73fc",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73fd",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73fe",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec73ff",
          "name": "Xiaobo Liang",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7400",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7401",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7402",
          "name": "Changyao Tian",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7403",
          "name": "Yanting Zhang",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7404",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7405",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7406",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7407",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68db4940d2bf1f4b15ec7408",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T17:59:15.000Z",
      "submittedOnDailyAt": "2025-09-30T01:39:11.662Z",
      "title": "Sequential Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "6571382c7644d1128561cebe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
        "isPro": false,
        "fullname": "Cao Yue",
        "user": "yuecao0119",
        "type": "user"
      },
      "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
      "upvotes": 15,
      "discussionId": "68db4941d2bf1f4b15ec7409",
      "projectPage": "https://internvl.github.io/blog/2025-09-29-SDLM/",
      "githubRepo": "https://github.com/OpenGVLab/SDLM",
      "ai_summary": "Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.",
      "ai_keywords": [
        "diffusion language models",
        "fixed-length decoding",
        "key-value caches",
        "block diffusion",
        "next sequence prediction",
        "next-token prediction",
        "sequential diffusion language model",
        "autoregressive language models",
        "diffusion inference",
        "mask blocks",
        "model confidence",
        "robustness",
        "throughput",
        "Qwen-2.5"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-09-28T13:59:15.000Z",
    "title": "Sequential Diffusion Language Models",
    "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6571382c7644d1128561cebe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
      "fullname": "Cao Yue",
      "name": "yuecao0119",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "submitterOrganization": {
      "_id": "64006c57a3b8fe3ac0e9af7c",
      "name": "OpenGVLab",
      "fullname": "OpenGVLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25123",
      "authors": [
        {
          "_id": "68db6278d2bf1f4b15ec75e3",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e4",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e5",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e6",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e7",
          "name": "Hanbin Wang",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e8",
          "name": "Ziming You",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75e9",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75ea",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75eb",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "68db6278d2bf1f4b15ec75ec",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:44:27.000Z",
      "submittedOnDailyAt": "2025-09-30T03:34:35.092Z",
      "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
      "submittedOnDailyBy": {
        "_id": "648312243b7fe59c876c0dca",
        "avatarUrl": "/avatars/c26ad76cd213529e4670bb599b8199bb.svg",
        "isPro": false,
        "fullname": "weize",
        "user": "weizechen",
        "type": "user"
      },
      "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
      "upvotes": 13,
      "discussionId": "68db6278d2bf1f4b15ec75ed",
      "ai_summary": "Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "compositional skills",
        "reasoning behaviors",
        "next-token training"
      ]
    },
    "publishedAt": "2025-09-29T13:44:27.000Z",
    "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
    "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648312243b7fe59c876c0dca",
      "avatarUrl": "/avatars/c26ad76cd213529e4670bb599b8199bb.svg",
      "fullname": "weize",
      "name": "weizechen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24473",
      "authors": [
        {
          "_id": "68db44add2bf1f4b15ec7378",
          "name": "Shijie Lian",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec7379",
          "user": {
            "_id": "67b55e66d454cc4d10d21cfd",
            "avatarUrl": "/avatars/3b18014fa7e603a5940175896f89372a.svg",
            "isPro": false,
            "fullname": "Changti Wu",
            "user": "MaplesWCT",
            "type": "user"
          },
          "name": "Changti Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:31.203Z",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec737a",
          "name": "Laurence Tianruo Yang",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec737b",
          "name": "Hang Yuan",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec737c",
          "name": "Bin Yu",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec737d",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68db44add2bf1f4b15ec737e",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T08:49:21.000Z",
      "submittedOnDailyAt": "2025-09-30T01:27:36.076Z",
      "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks",
      "submittedOnDailyBy": {
        "_id": "65ec01fd770aa0e25d9374dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
        "isPro": false,
        "fullname": "Shijie Lian",
        "user": "LiamLian0727",
        "type": "user"
      },
      "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
      "upvotes": 13,
      "discussionId": "68db44add2bf1f4b15ec737f",
      "projectPage": "https://zgca-ai4edu.github.io/Euclids_Gift/",
      "githubRepo": "https://github.com/LiamLian0727/Euclids_Gift",
      "ai_summary": "Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.",
      "ai_keywords": [
        "Euclidean geometry",
        "Group Relative Policy Optimization (GRPO)",
        "Qwen2.5VL",
        "RoboBrain2.0",
        "zero-shot gains",
        "spatial reasoning benchmarks",
        "Super-CLEVR",
        "Omni3DBench",
        "VSI-Bench",
        "MindCube",
        "vision-language models"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-09-29T04:49:21.000Z",
    "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks",
    "summary": "Spatial intelligence spans a rich suite of abilities, including visualising\nand transforming shapes, mentally rotating objects, judging relational\npositions and containment, and estimating numerosity. However, it still remains\na critical unresolved challenge for Multimodal Large Language Models (MLLMs).To\nfill this gap, we propose to treat Euclidean geometry problem-solving as a\nsurrogate task. Specifically, we meticulously constructed a curated multimodal\ndataset, called Euclid30K, comprising approximately 30K plane and solid\ngeometry problems. To enable the model to acquire and apply Euclidean\nprinciples from these geometry problems, we employed Group Relative Policy\nOptimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,\ninspiring the models to identify shapes, count, and relate entities, and\nperform multi-step deductive reasoning using Euclidean principles. Our\nexperiments demonstrate that the resulting models achieve substantial zero-shot\ngains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,\nVSI-Bench, and MindCube) without any task-specific adaptations. Notably, after\ntraining on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models\nrose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,\nRoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous\nstate-of-the-art model, Spatial-MLLM.To our knowledge, this is the first\nsystematic study showing that geometry-centric fine-tuning can confer\nvision-language models with broadly transferable spatial skills. Code and\nEuclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24473.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65ec01fd770aa0e25d9374dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
      "fullname": "Shijie Lian",
      "name": "LiamLian0727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "68896d3a716ee5bfb1428441",
      "name": "ZGCA",
      "fullname": "Zhongguancun Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23808",
      "authors": [
        {
          "_id": "68db671fd2bf1f4b15ec762e",
          "user": {
            "_id": "666060eefff0e904a291c9ca",
            "avatarUrl": "/avatars/56c43cc0ee3376a3e4d2d78fc641804e.svg",
            "isPro": false,
            "fullname": "Fanding Huang",
            "user": "Niugan",
            "type": "user"
          },
          "name": "Fanding Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:22.925Z",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec762f",
          "user": {
            "_id": "68db7b19a72e7113caffb1cb",
            "avatarUrl": "/avatars/355871f30bdc97141004d15b2dc2378c.svg",
            "isPro": false,
            "fullname": "Guanbo huang",
            "user": "Gambel",
            "type": "user"
          },
          "name": "Guanbo Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:27.555Z",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7630",
          "user": {
            "_id": "670bd8e972c44e17cd6900bd",
            "avatarUrl": "/avatars/898137782df8762e3a6ca4451dd871ea.svg",
            "isPro": false,
            "fullname": "Fan Xiao",
            "user": "AnikiFan",
            "type": "user"
          },
          "name": "Xiao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:26.824Z",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7631",
          "name": "Yi He",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7632",
          "user": {
            "_id": "6560763e152b659e623865ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
            "isPro": false,
            "fullname": "Xiao Liang",
            "user": "MasterVito",
            "type": "user"
          },
          "name": "Xiao Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:17.152Z",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7633",
          "user": {
            "_id": "676fcb88f40e1460661f0d9b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U-npvTTY6ub1bjIHbjVjm.png",
            "isPro": false,
            "fullname": "Xiao Chen",
            "user": "Cevaaa",
            "type": "user"
          },
          "name": "Xiao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:14.332Z",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7634",
          "name": "Qinting Jiang",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7635",
          "name": "Faisal Nadeem Khan",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7636",
          "name": "Jingyan Jiang",
          "hidden": false
        },
        {
          "_id": "68db671fd2bf1f4b15ec7637",
          "name": "Zhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T11:14:58.000Z",
      "submittedOnDailyAt": "2025-09-30T03:45:57.562Z",
      "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
      "submittedOnDailyBy": {
        "_id": "6560763e152b659e623865ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
        "isPro": false,
        "fullname": "Xiao Liang",
        "user": "MasterVito",
        "type": "user"
      },
      "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
      "upvotes": 12,
      "discussionId": "68db6720d2bf1f4b15ec7638",
      "projectPage": "https://hf618.github.io/VERL.github.io/",
      "githubRepo": "https://github.com/hf618/VERL",
      "ai_summary": "Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning for Verifiable Rewards",
        "exploration-exploitation trade-off",
        "token-level metrics",
        "semantically rich hidden-state space",
        "Effective Rank",
        "Effective Rank Velocity",
        "Effective Rank Acceleration",
        "Velocity-Exploiting Rank-Learning",
        "RL advantage function",
        "predictive meta-controller",
        "dual-channel incentive structure",
        "Gaokao 2024 dataset"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-28T07:14:58.000Z",
    "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
    "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6560763e152b659e623865ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
      "fullname": "Xiao Liang",
      "name": "MasterVito",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.22799",
      "authors": [
        {
          "_id": "68db55b9d2bf1f4b15ec7504",
          "name": "Xuan He",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7505",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7506",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7507",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7508",
          "name": "Zhengxuan Jiang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7509",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750a",
          "name": "Wentao Ma",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750b",
          "name": "Junru Lin",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750c",
          "name": "Chun Ye",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750d",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750e",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec750f",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7510",
          "name": "Quy Duc Do",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7511",
          "name": "Zhuofeng Li",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7512",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7513",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7514",
          "name": "Guo Cheng",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7515",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7516",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7517",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7518",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec7519",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec751a",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68db55b9d2bf1f4b15ec751b",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T18:09:03.000Z",
      "submittedOnDailyAt": "2025-09-30T02:30:24.889Z",
      "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
      "upvotes": 12,
      "discussionId": "68db55b9d2bf1f4b15ec751c",
      "ai_summary": "VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.",
      "ai_keywords": [
        "text-to-video generation",
        "visual quality",
        "semantic alignment",
        "physical consistency",
        "VideoScore2",
        "VideoFeedback2",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Relative Policy Optimization (GRPO)",
        "VideoScore-Bench-v2",
        "VideoGenReward-Bench",
        "VideoPhy2",
        "Best-of-N sampling"
      ]
    },
    "publishedAt": "2025-09-26T14:09:03.000Z",
    "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
    "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22799.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 44
    },
    "submitterOrganization": {
      "_id": "6313a90017838d05194fd282",
      "name": "TIGER-Lab",
      "fullname": "TIGER-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24663",
      "authors": [
        {
          "_id": "68db57fdd2bf1f4b15ec7593",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7594",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7595",
          "name": "Zhou Su",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7596",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7597",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7598",
          "name": "Yanghao Li",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec7599",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759a",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759b",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759c",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759d",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759e",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68db57fdd2bf1f4b15ec759f",
          "name": "Zhiyuan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T12:08:33.000Z",
      "submittedOnDailyAt": "2025-09-30T02:42:18.336Z",
      "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
      "submittedOnDailyBy": {
        "_id": "608f6d72283d0a8d7be9d1f9",
        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
        "isPro": false,
        "fullname": "Chaojun XIAO",
        "user": "xcjthu",
        "type": "user"
      },
      "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional pretrain-on-short, finetune-on-long\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4times faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.",
      "upvotes": 11,
      "discussionId": "68db57fdd2bf1f4b15ec75a0",
      "ai_summary": "A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.",
      "ai_keywords": [
        "self-attention mechanism",
        "Transformer architecture",
        "trainable sparse attention",
        "dense-sparse switchable attention",
        "InfLLM-V2",
        "parameter-free architecture modification",
        "dense attention",
        "sparse attention",
        "long-context understanding",
        "chain-of-thought reasoning",
        "MiniCPM4.1"
      ]
    },
    "publishedAt": "2025-09-29T08:08:33.000Z",
    "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
    "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional pretrain-on-short, finetune-on-long\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4times faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608f6d72283d0a8d7be9d1f9",
      "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
      "fullname": "Chaojun XIAO",
      "name": "xcjthu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22824",
      "authors": [
        {
          "_id": "68db550cd2bf1f4b15ec74fe",
          "name": "Chi Ruan",
          "hidden": false
        },
        {
          "_id": "68db550cd2bf1f4b15ec74ff",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "68db550cd2bf1f4b15ec7500",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "68db550cd2bf1f4b15ec7501",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T18:30:49.000Z",
      "submittedOnDailyAt": "2025-09-30T02:27:39.733Z",
      "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduce Critique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat Critique-Coder consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our Critique-Coder-8B can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.",
      "upvotes": 11,
      "discussionId": "68db550cd2bf1f4b15ec7502",
      "projectPage": "https://tiger-ai-lab.github.io/Critique-Coder/",
      "ai_summary": "Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Critique-Fine-Tuning (CFT)",
        "Critique-Guided-Distillation (CGD)",
        "Critique Reinforcement Learning (CRL)",
        "Critique-Coder",
        "LiveCodeBench",
        "DeepCoder-14B",
        "GPT-o1",
        "BBEH dataset"
      ]
    },
    "publishedAt": "2025-09-26T14:30:49.000Z",
    "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
    "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduce Critique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat Critique-Coder consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our Critique-Coder-8B can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22824.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 44
    },
    "submitterOrganization": {
      "_id": "6313a90017838d05194fd282",
      "name": "TIGER-Lab",
      "fullname": "TIGER-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22572",
      "authors": [
        {
          "_id": "68db4efed2bf1f4b15ec7497",
          "user": {
            "_id": "673d4716cc1ef74a349cd2ad",
            "avatarUrl": "/avatars/a88f1d461c199a2caa1d5e13b70921fe.svg",
            "isPro": false,
            "fullname": "Yixuan Han",
            "user": "yixuan7878",
            "type": "user"
          },
          "name": "Yixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:25.187Z",
          "hidden": false
        },
        {
          "_id": "68db4efed2bf1f4b15ec7498",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "68db4efed2bf1f4b15ec7499",
          "name": "Ruijie Quan",
          "hidden": false
        },
        {
          "_id": "68db4efed2bf1f4b15ec749a",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T16:49:10.000Z",
      "submittedOnDailyAt": "2025-09-30T02:02:06.748Z",
      "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
      "submittedOnDailyBy": {
        "_id": "65eaa1e2b11eeb516a973508",
        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
        "isPro": false,
        "fullname": "Dewei Zhou",
        "user": "limuloo1999",
        "type": "user"
      },
      "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
      "upvotes": 11,
      "discussionId": "68db4efed2bf1f4b15ec749b",
      "ai_summary": "Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "Dynamic Experts Search (DES)",
        "Dynamic MoE",
        "Expert Configuration Inheritance",
        "reasoning trajectories",
        "verifiers",
        "reasoning benchmarks",
        "math",
        "code",
        "knowledge"
      ]
    },
    "publishedAt": "2025-09-26T12:49:10.000Z",
    "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
    "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language\nmodels (LLMs) by allocating additional computation during inference. However,\nexisting approaches primarily rely on output-level sampling while overlooking\nthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we\nobserve that varying the number of activated experts yields complementary\nsolution sets with stable accuracy, revealing a new and underexplored source of\ndiversity. Motivated by this observation, we propose Dynamic Experts Search\n(DES), a TTS strategy that elevates expert activation into a controllable\ndimension of the search space. DES integrates two key components: (1) Dynamic\nMoE, which enables direct control of expert counts during inference to generate\ndiverse reasoning trajectories without additional cost; and (2) Expert\nConfiguration Inheritance, which preserves consistent expert counts within a\nreasoning path while varying them across runs, thereby balancing stability and\ndiversity throughout the search. Extensive experiments across MoE\narchitectures, verifiers and reasoning benchmarks (i.e., math, code and\nknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing\naccuracy and stability without additional cost. These results highlight DES as\na practical and scalable form of architecture-aware TTS, illustrating how\nstructural flexibility in modern LLMs can advance reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22572.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eaa1e2b11eeb516a973508",
      "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
      "fullname": "Dewei Zhou",
      "name": "limuloo1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25161",
      "authors": [
        {
          "_id": "68db4f7bd2bf1f4b15ec749d",
          "name": "Kunhao Liu",
          "hidden": false
        },
        {
          "_id": "68db4f7bd2bf1f4b15ec749e",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:54.201Z",
          "hidden": false
        },
        {
          "_id": "68db4f7bd2bf1f4b15ec749f",
          "name": "Jiale Xu",
          "hidden": false
        },
        {
          "_id": "68db4f7bd2bf1f4b15ec74a0",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "68db4f7bd2bf1f4b15ec74a1",
          "name": "Shijian Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:57:14.000Z",
      "submittedOnDailyAt": "2025-09-30T02:03:32.019Z",
      "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Streaming video generation, as one fundamental component in interactive world\nmodels and neural game engines, aims to generate high-quality, low-latency, and\ntemporally coherent long video streams. However, most existing work suffers\nfrom severe error accumulation that often significantly degrades the generated\nstream videos over long horizons. We design Rolling Forcing, a novel video\ngeneration technique that enables streaming long videos with minimal error\naccumulation. Rolling Forcing comes with three novel designs. First, instead of\niteratively sampling individual frames, which accelerates error propagation, we\ndesign a joint denoising scheme that simultaneously denoises multiple frames\nwith progressively increasing noise levels. This design relaxes the strict\ncausality across adjacent frames, effectively suppressing error growth. Second,\nwe introduce the attention sink mechanism into the long-horizon stream video\ngeneration task, which allows the model to keep key value states of initial\nframes as a global context anchor and thereby enhances long-term global\nconsistency. Third, we design an efficient training algorithm that enables\nfew-step distillation over largely extended denoising windows. This algorithm\noperates on non-overlapping windows and mitigates exposure bias conditioned on\nself-generated histories. Extensive experiments show that Rolling Forcing\nenables real-time streaming generation of multi-minute videos on a single GPU,\nwith substantially reduced error accumulation.",
      "upvotes": 10,
      "discussionId": "68db4f7cd2bf1f4b15ec74a2",
      "projectPage": "https://kunhao-liu.github.io/Rolling_Forcing_Webpage/",
      "githubRepo": "https://github.com/TencentARC/RollingForcing",
      "ai_summary": "Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.",
      "ai_keywords": [
        "joint denoising",
        "attention sink mechanism",
        "error accumulation",
        "long-horizon stream video generation",
        "global context anchor",
        "long-term global consistency",
        "efficient training algorithm",
        "few-step distillation",
        "denoising windows",
        "exposure bias"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-09-29T13:57:14.000Z",
    "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
    "summary": "Streaming video generation, as one fundamental component in interactive world\nmodels and neural game engines, aims to generate high-quality, low-latency, and\ntemporally coherent long video streams. However, most existing work suffers\nfrom severe error accumulation that often significantly degrades the generated\nstream videos over long horizons. We design Rolling Forcing, a novel video\ngeneration technique that enables streaming long videos with minimal error\naccumulation. Rolling Forcing comes with three novel designs. First, instead of\niteratively sampling individual frames, which accelerates error propagation, we\ndesign a joint denoising scheme that simultaneously denoises multiple frames\nwith progressively increasing noise levels. This design relaxes the strict\ncausality across adjacent frames, effectively suppressing error growth. Second,\nwe introduce the attention sink mechanism into the long-horizon stream video\ngeneration task, which allows the model to keep key value states of initial\nframes as a global context anchor and thereby enhances long-term global\nconsistency. Third, we design an efficient training algorithm that enables\nfew-step distillation over largely extended denoising windows. This algorithm\noperates on non-overlapping windows and mitigates exposure bias conditioned on\nself-generated histories. Extensive experiments show that Rolling Forcing\nenables real-time streaming generation of multi-minute videos on a single GPU,\nwith substantially reduced error accumulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25161.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 114
    },
    "submitterOrganization": {
      "_id": "60e3f7f641ca131919975fe5",
      "name": "TencentARC",
      "fullname": "ARC Lab, Tencent PCG",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23285",
      "authors": [
        {
          "_id": "68db3809d2bf1f4b15ec728b",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "68db3809d2bf1f4b15ec728c",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "68db3809d2bf1f4b15ec728d",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T12:53:37.000Z",
      "submittedOnDailyAt": "2025-09-30T00:35:38.566Z",
      "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6621ec2524eb2673fe0790fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
        "isPro": false,
        "fullname": "Ania Forge",
        "user": "zhangboguodong",
        "type": "user"
      },
      "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to\nimprove their internal reasoning ability by integrating external tools.\nHowever, models employing TIR often display suboptimal behaviors, such as\ninsufficient or excessive tool usage and overthinking after tool calls. The\nchallenge of incentivizing LLMs to perform TIR efficiently and accurately,\nwhile stabilizing the reasoning process, remains an open question. In this\npaper, we start by exploring the impact of tool calls on model reasoning from\nthe perspective of information entropy. Our findings indicate that tool call\nresults lead to a distinct change in the information entropy of subsequent\nreasoning, with the overall entropy of the reasoning chain varying based on the\nnumber of tool calls. Building on these insights, we propose Tool-Light, a\nframework designed to encourage LLMs to perform TIR efficiently and accurately.\nOur framework includes dataset construction and multi-stage fine-tuning. For\ndataset construction, we employ continuous self-evolved sampling using the\nfine-tuned model, integrating both vanilla sampling and entropy-guided\nsampling. Besides, we establish strict criteria for selecting positive-negative\npairs during sampling. The training process involves a two-stage approach,\ncomprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference\nOptimization (DPO). Experimental results on 10 datasets demonstrate the\neffectiveness of Tool-Light, significantly improving the model's efficiency in\nexecuting TIR tasks.",
      "upvotes": 10,
      "discussionId": "68db3809d2bf1f4b15ec728e",
      "githubRepo": "https://github.com/asilverlight/Tool-Light",
      "ai_summary": "Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.",
      "ai_keywords": [
        "Tool-Integrated Reasoning",
        "large language models",
        "information entropy",
        "Tool-Light",
        "dataset construction",
        "continuous self-evolved sampling",
        "entropy-guided sampling",
        "positive-negative pairs",
        "Supervised Fine-Tuning",
        "Self-Evolved Direct Preference Optimization"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-27T08:53:37.000Z",
    "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
    "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to\nimprove their internal reasoning ability by integrating external tools.\nHowever, models employing TIR often display suboptimal behaviors, such as\ninsufficient or excessive tool usage and overthinking after tool calls. The\nchallenge of incentivizing LLMs to perform TIR efficiently and accurately,\nwhile stabilizing the reasoning process, remains an open question. In this\npaper, we start by exploring the impact of tool calls on model reasoning from\nthe perspective of information entropy. Our findings indicate that tool call\nresults lead to a distinct change in the information entropy of subsequent\nreasoning, with the overall entropy of the reasoning chain varying based on the\nnumber of tool calls. Building on these insights, we propose Tool-Light, a\nframework designed to encourage LLMs to perform TIR efficiently and accurately.\nOur framework includes dataset construction and multi-stage fine-tuning. For\ndataset construction, we employ continuous self-evolved sampling using the\nfine-tuned model, integrating both vanilla sampling and entropy-guided\nsampling. Besides, we establish strict criteria for selecting positive-negative\npairs during sampling. The training process involves a two-stage approach,\ncomprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference\nOptimization (DPO). Experimental results on 10 datasets demonstrate the\neffectiveness of Tool-Light, significantly improving the model's efficiency in\nexecuting TIR tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6621ec2524eb2673fe0790fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg",
      "fullname": "Ania Forge",
      "name": "zhangboguodong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "submitterOrganization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25191",
      "authors": [
        {
          "_id": "68db55d7d2bf1f4b15ec751e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68db55d7d2bf1f4b15ec751f",
          "name": "Chuanchen Luo",
          "hidden": false
        },
        {
          "_id": "68db55d7d2bf1f4b15ec7520",
          "name": "Zimo Tang",
          "hidden": false
        },
        {
          "_id": "68db55d7d2bf1f4b15ec7521",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "68db55d7d2bf1f4b15ec7522",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:59.000Z",
      "submittedOnDailyAt": "2025-09-30T03:17:54.551Z",
      "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
      "submittedOnDailyBy": {
        "_id": "66ef6fd0ea7d19a2399d6b1f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rJIMhaB4NB8bvBtmsw1vA.png",
        "isPro": false,
        "fullname": "Yang Liu",
        "user": "TeslaYang123",
        "type": "user"
      },
      "summary": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel\nView Synthesis (NVS). Despite significant progress in Novel View Synthesis\npowered by NeRF and 3DGS, current approaches remain reliant on accurate 3D\nattributes (e.g., camera poses and point clouds) acquired from\nStructure-from-Motion (SfM), which is often slow and fragile in low-texture or\nlow-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over\nthe traditional pipeline and great potential for online NVS. But most of the\nvalidation and conclusions are confined to sparse-view settings. Our study\nreveals that naively scaling 3DFMs to dense views encounters two fundamental\nbarriers: dramatically increasing VRAM burden and imperfect outputs that\ndegrade initialization-sensitive 3D training. To address these barriers, we\nintroduce VGGT-X, incorporating a memory-efficient VGGT implementation that\nscales to 1,000+ images, an adaptive global alignment for VGGT output\nenhancement, and robust 3DGS training practices. Extensive experiments show\nthat these measures substantially close the fidelity gap with\nCOLMAP-initialized pipelines, achieving state-of-the-art results in dense\nCOLMAP-free NVS and pose estimation. Additionally, we analyze the causes of\nremaining gaps with COLMAP-initialized rendering, providing insights for the\nfuture development of 3D foundation models and dense NVS. Our project page is\navailable at https://dekuliutesla.github.io/vggt-x.github.io/",
      "upvotes": 9,
      "discussionId": "68db55d7d2bf1f4b15ec7523",
      "projectPage": "https://dekuliutesla.github.io/vggt-x.github.io/",
      "githubRepo": "https://github.com/Linketic/VGGT-X",
      "ai_summary": "VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.",
      "ai_keywords": [
        "3D Foundation Models",
        "Novel View Synthesis",
        "NeRF",
        "3DGS",
        "Structure-from-Motion",
        "VGGT-X",
        "memory-efficient",
        "adaptive global alignment",
        "robust 3DGS training",
        "COLMAP-free NVS",
        "pose estimation"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-09-29T13:59:59.000Z",
    "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
    "summary": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel\nView Synthesis (NVS). Despite significant progress in Novel View Synthesis\npowered by NeRF and 3DGS, current approaches remain reliant on accurate 3D\nattributes (e.g., camera poses and point clouds) acquired from\nStructure-from-Motion (SfM), which is often slow and fragile in low-texture or\nlow-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over\nthe traditional pipeline and great potential for online NVS. But most of the\nvalidation and conclusions are confined to sparse-view settings. Our study\nreveals that naively scaling 3DFMs to dense views encounters two fundamental\nbarriers: dramatically increasing VRAM burden and imperfect outputs that\ndegrade initialization-sensitive 3D training. To address these barriers, we\nintroduce VGGT-X, incorporating a memory-efficient VGGT implementation that\nscales to 1,000+ images, an adaptive global alignment for VGGT output\nenhancement, and robust 3DGS training practices. Extensive experiments show\nthat these measures substantially close the fidelity gap with\nCOLMAP-initialized pipelines, achieving state-of-the-art results in dense\nCOLMAP-free NVS and pose estimation. Additionally, we analyze the causes of\nremaining gaps with COLMAP-initialized rendering, providing insights for the\nfuture development of 3D foundation models and dense NVS. Our project page is\navailable at https://dekuliutesla.github.io/vggt-x.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef6fd0ea7d19a2399d6b1f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rJIMhaB4NB8bvBtmsw1vA.png",
      "fullname": "Yang Liu",
      "name": "TeslaYang123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25176",
      "authors": [
        {
          "_id": "68db421ad2bf1f4b15ec72fd",
          "name": "Haoming Wen",
          "hidden": false
        },
        {
          "_id": "68db421ad2bf1f4b15ec72fe",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "68db421ad2bf1f4b15ec72ff",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68db421ad2bf1f4b15ec7300",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:08.000Z",
      "submittedOnDailyAt": "2025-09-30T01:35:51.163Z",
      "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.",
      "upvotes": 9,
      "discussionId": "68db421ad2bf1f4b15ec7301",
      "projectPage": "https://huggingface.co/collections/THU-KEG/siri-68d65a4ecf9f20dac7322dfe",
      "ai_summary": "SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.",
      "ai_keywords": [
        "Scaling Iterative Reinforcement Learning",
        "Interleaved Compression",
        "Large Reasoning Models",
        "RL",
        "rollout length",
        "reasoning density",
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "AIME24",
        "Pareto frontier",
        "performance-efficiency trade-off"
      ]
    },
    "publishedAt": "2025-09-29T13:59:08.000Z",
    "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression",
    "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25176.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "submitterOrganization": {
      "_id": "62ad27f19096e7f9ecb1853a",
      "name": "zai-org",
      "fullname": "Z.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24981",
      "authors": [
        {
          "_id": "68db71fbd2bf1f4b15ec76e3",
          "name": "Haoran He",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e4",
          "name": "Yuxiao Ye",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e5",
          "name": "Qingpeng Cai",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e6",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e7",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e8",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68db71fbd2bf1f4b15ec76e9",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T16:09:07.000Z",
      "submittedOnDailyAt": "2025-09-30T04:31:29.787Z",
      "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
      "submittedOnDailyBy": {
        "_id": "6672937ceac0fb1b9e516595",
        "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
        "isPro": false,
        "fullname": "haoran he",
        "user": "haoranhe",
        "type": "user"
      },
      "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both quality (+8.2 on pass@1,\n+16.8 on pass@256) and diversity (+17.6\\%), despite\nits radical simplification compared to strong, complicated existing methods.",
      "upvotes": 9,
      "discussionId": "68db71fcd2bf1f4b15ec76ea",
      "githubRepo": "https://github.com/tinnerhrhe/ROVER",
      "ai_summary": "ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.",
      "ai_keywords": [
        "RL with Verifiable Rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "policy optimization",
        "PPO",
        "GRPO",
        "generalized policy iteration",
        "Markov Decision Process",
        "deterministic state transitions",
        "tree-structured dynamics",
        "binary terminal rewards",
        "Q-function",
        "Random Policy Valuation for Diverse Reasoning",
        "ROVER",
        "softmax",
        "exploration",
        "pass@1",
        "pass@256",
        "diversity"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-09-29T12:09:07.000Z",
    "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
    "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both quality (+8.2 on pass@1,\n+16.8 on pass@256) and diversity (+17.6\\%), despite\nits radical simplification compared to strong, complicated existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6672937ceac0fb1b9e516595",
      "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
      "fullname": "haoran he",
      "name": "haoranhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23951",
      "authors": [
        {
          "_id": "68db4706d2bf1f4b15ec73a0",
          "name": "Siyu Cao",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a1",
          "name": "Hangting Chen",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a2",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a3",
          "name": "Yiji Cheng",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a4",
          "name": "Yutao Cui",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a5",
          "name": "Xinchi Deng",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a6",
          "name": "Ying Dong",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a7",
          "name": "Kipper Gong",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a8",
          "name": "Tianpeng Gu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73a9",
          "name": "Xiusen Gu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73aa",
          "name": "Tiankai Hang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ab",
          "name": "Duojun Huang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ac",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ad",
          "name": "Zhengkai Jiang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ae",
          "name": "Weijie Kong",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73af",
          "name": "Changlin Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b0",
          "name": "Donghao Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b1",
          "name": "Junzhe Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b2",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b3",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b4",
          "name": "Zhenxi Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b5",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b6",
          "name": "Jiaxin Lin",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b7",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b8",
          "name": "Lucaz Liu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73b9",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ba",
          "name": "Songtao Liu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73bb",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73bc",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73bd",
          "name": "Yanxin Long",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73be",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73bf",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c0",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c1",
          "name": "Yuanbo Peng",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c2",
          "name": "Xiangwei Shen",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c3",
          "name": "Yixuan Shi",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c4",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c5",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c6",
          "name": "Qi Tian",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c7",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c8",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73c9",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ca",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73cb",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73cc",
          "name": "Lucas Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73cd",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73ce",
          "name": "Weiyan Wang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73cf",
          "name": "Hao Wen",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d0",
          "name": "Bing Wu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d1",
          "name": "Jianbing Wu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d2",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d3",
          "name": "Senhao Xie",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d4",
          "name": "Fang Yang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d5",
          "name": "Miles Yang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d6",
          "name": "Xiaofeng Yang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d7",
          "name": "Xuan Yang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d8",
          "name": "Zhantao Yang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73d9",
          "name": "Jingmiao Yu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73da",
          "name": "Zheng Yuan",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73db",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73dc",
          "name": "Jian-Wei Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73dd",
          "name": "Peizhen Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73de",
          "name": "Shi-Xue Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73df",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e0",
          "name": "Weigang Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e1",
          "name": "Yepeng Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e2",
          "name": "Yingfang Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e3",
          "name": "Zihao Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e4",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e5",
          "name": "Penghao Zhao",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e6",
          "name": "Zhiyuan Zhao",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e7",
          "name": "Xuefei Zhe",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e8",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "68db4706d2bf1f4b15ec73e9",
          "name": "Zhao Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T16:14:10.000Z",
      "submittedOnDailyAt": "2025-09-30T02:20:41.661Z",
      "title": "HunyuanImage 3.0 Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "upvotes": 8,
      "discussionId": "68db4706d2bf1f4b15ec73ea",
      "githubRepo": "https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
      "ai_summary": "HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.",
      "ai_keywords": [
        "multimodal model",
        "autoregressive framework",
        "Chain-of-Thoughts schema",
        "progressive model pre-training",
        "aggressive model post-training",
        "Mixture-of-Experts (MoE)",
        "text-image alignment",
        "visual quality"
      ],
      "githubStars": 844
    },
    "publishedAt": "2025-09-28T12:14:10.000Z",
    "title": "HunyuanImage 3.0 Technical Report",
    "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 114
    },
    "submitterOrganization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23196",
      "authors": [
        {
          "_id": "68db481cd2bf1f4b15ec73ec",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73ed",
          "name": "Weida Liang",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73ee",
          "name": "Zihang Fu",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73ef",
          "name": "Nie Zheng",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f0",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f1",
          "name": "Yao Tong",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f2",
          "user": {
            "_id": "647db42711084fb58318b053",
            "avatarUrl": "/avatars/901a4db8e99f6e0c7e7e596c251b315c.svg",
            "isPro": true,
            "fullname": "Tongyao",
            "user": "tyzhu",
            "type": "user"
          },
          "name": "Tongyao Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:29:58.090Z",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f3",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f4",
          "name": "Chuang Li",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f5",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "68db481cd2bf1f4b15ec73f6",
          "name": "Kenji Kawaguchi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T08:59:31.000Z",
      "submittedOnDailyAt": "2025-09-30T01:33:39.149Z",
      "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs",
      "submittedOnDailyBy": {
        "_id": "6496b06a4a9a7e1fe4253ae2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/144NlRW_ETmmOgSYUs_SM.png",
        "isPro": false,
        "fullname": "Haonan Wang",
        "user": "haonan3",
        "type": "user"
      },
      "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based\nreinforcement learning, often perform worse with few-shot CoT than with direct\nanswering. We revisit this paradox using high-quality reasoning traces from\nDeepSeek-R1 as demonstrations and find that adding more exemplars consistently\ndegrades accuracy, even when demonstrations are optimal. A detailed analysis\nreveals two mechanisms behind this decline: (i) semantic misguidance, where\nhigh textual similarity leads the model to treat the target as the same as the\nexemplar and to copy intermediate steps verbatim; and (ii) strategy transfer\nfailure, where the model struggles to extract useful reasoning strategies and\napply them to target questions. Guided by these, we introduce Insight-to-Solve\n(I2S), a sequential test-time procedure that turns demonstrations into\nexplicit, reusable insights and derives a target-specific reasoning trace;\noptionally, the reasoning is self-refined for coherence and correctness (I2S+).\nExtensive experiments on diverse benchmarks show that I2S and I2S+ consistently\noutperform both direct answering and test-time scaling baselines across open-\nand closed-source models. Even for GPT models, our method helps: on AIME'25,\nGPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on\nGPQA, indicating that in-context demonstrations can be harnessed effectively\nvia insight-refine-solve framework.",
      "upvotes": 8,
      "discussionId": "68db481cd2bf1f4b15ec73f7",
      "ai_summary": "Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.",
      "ai_keywords": [
        "reasoning LLMs",
        "verifier-based reinforcement learning",
        "few-shot CoT",
        "DeepSeek-R1",
        "semantic misguidance",
        "strategy transfer failure",
        "Insight-to-Solve",
        "I2S",
        "I2S+",
        "AIME'25",
        "GPT-4.1",
        "o1-mini",
        "GPQA"
      ]
    },
    "publishedAt": "2025-09-27T04:59:31.000Z",
    "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs",
    "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based\nreinforcement learning, often perform worse with few-shot CoT than with direct\nanswering. We revisit this paradox using high-quality reasoning traces from\nDeepSeek-R1 as demonstrations and find that adding more exemplars consistently\ndegrades accuracy, even when demonstrations are optimal. A detailed analysis\nreveals two mechanisms behind this decline: (i) semantic misguidance, where\nhigh textual similarity leads the model to treat the target as the same as the\nexemplar and to copy intermediate steps verbatim; and (ii) strategy transfer\nfailure, where the model struggles to extract useful reasoning strategies and\napply them to target questions. Guided by these, we introduce Insight-to-Solve\n(I2S), a sequential test-time procedure that turns demonstrations into\nexplicit, reusable insights and derives a target-specific reasoning trace;\noptionally, the reasoning is self-refined for coherence and correctness (I2S+).\nExtensive experiments on diverse benchmarks show that I2S and I2S+ consistently\noutperform both direct answering and test-time scaling baselines across open-\nand closed-source models. Even for GPT models, our method helps: on AIME'25,\nGPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on\nGPQA, indicating that in-context demonstrations can be harnessed effectively\nvia insight-refine-solve framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23196.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6496b06a4a9a7e1fe4253ae2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/144NlRW_ETmmOgSYUs_SM.png",
      "fullname": "Haonan Wang",
      "name": "haonan3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25077",
      "authors": [
        {
          "_id": "68db59b1d2bf1f4b15ec75a2",
          "user": {
            "_id": "64898937431b7a5e075d2b7b",
            "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
            "isPro": false,
            "fullname": "L",
            "user": "Dingning",
            "type": "user"
          },
          "name": "Dingning Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:38.759Z",
          "hidden": false
        },
        {
          "_id": "68db59b1d2bf1f4b15ec75a3",
          "user": {
            "_id": "652ce0d4c543a08aa92e010f",
            "avatarUrl": "/avatars/7978304e3fe99b0d4d0712441c6a24f3.svg",
            "isPro": false,
            "fullname": "Haoyu Guo",
            "user": "ghy0324",
            "type": "user"
          },
          "name": "Haoyu Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:36.341Z",
          "hidden": false
        },
        {
          "_id": "68db59b1d2bf1f4b15ec75a4",
          "name": "Jingyi Zhou",
          "hidden": false
        },
        {
          "_id": "68db59b1d2bf1f4b15ec75a5",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:19:45.000Z",
      "submittedOnDailyAt": "2025-09-30T02:48:50.666Z",
      "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
      "submittedOnDailyBy": {
        "_id": "64898937431b7a5e075d2b7b",
        "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
        "isPro": false,
        "fullname": "L",
        "user": "Dingning",
        "type": "user"
      },
      "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
      "upvotes": 7,
      "discussionId": "68db59b1d2bf1f4b15ec75a6",
      "projectPage": "https://dingning-liu.github.io/bridge.github.io/",
      "githubRepo": "https://github.com/lnbxldn/BRIDGE",
      "ai_summary": "BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.",
      "ai_keywords": [
        "monocular depth estimation",
        "RL-optimized",
        "depth-to-image",
        "ground truth depth",
        "hybrid supervision",
        "teacher pseudo-labels",
        "complex scene detail capture"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-09-29T13:19:45.000Z",
    "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
    "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64898937431b7a5e075d2b7b",
      "avatarUrl": "/avatars/26d919d4ce925909de660303786e98c3.svg",
      "fullname": "L",
      "name": "Dingning",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "submitterOrganization": {
      "_id": "6747ee5decec679eafb90450",
      "name": "ShanghaiAiLab",
      "fullname": "shanghai ailab "
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25084",
      "authors": [
        {
          "_id": "68db796dd2bf1f4b15ec773d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec773e",
          "name": "Yanqiu Zhao",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec773f",
          "name": "Zhisong Qiu",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7740",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7741",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7742",
          "name": "Zhao Bin",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7743",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": true,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:02.808Z",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7744",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7745",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7746",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68db796dd2bf1f4b15ec7747",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:23:08.000Z",
      "submittedOnDailyAt": "2025-09-30T05:02:40.602Z",
      "title": "Scaling Generalist Data-Analytic Agents",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
      "upvotes": 6,
      "discussionId": "68db796dd2bf1f4b15ec7748",
      "ai_summary": "DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.",
      "ai_keywords": [
        "fine-grained task taxonomy",
        "recursive easy-to-hard task composition",
        "knowledge-augmented trajectory sampling",
        "model-based filtering",
        "rule-based filtering",
        "dynamically adjustable training objective",
        "SFT",
        "RL losses",
        "memory-frugal",
        "code-based multi-turn rollout",
        "DataMind-12K",
        "DataMind-14B",
        "DataMind-7B"
      ]
    },
    "publishedAt": "2025-09-29T13:23:08.000Z",
    "title": "Scaling Generalist Data-Analytic Agents",
    "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "submitterOrganization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24193",
      "authors": [
        {
          "_id": "68db4081d2bf1f4b15ec72dc",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72dd",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72de",
          "name": "Zihan Dong",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72df",
          "name": "Jonathan Wang",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e0",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e1",
          "name": "Joyce C. Ho",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e2",
          "name": "Linjun Zhang",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e3",
          "name": "Haoyu Wang",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e4",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "68db4081d2bf1f4b15ec72e5",
          "name": "Carl Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T02:14:30.000Z",
      "submittedOnDailyAt": "2025-09-30T01:00:55.755Z",
      "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play",
      "submittedOnDailyBy": {
        "_id": "6471bddd609ae9f56368f132",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6471bddd609ae9f56368f132/G91Q4iCGN2dy3oMaz-LrO.jpeg",
        "isPro": true,
        "fullname": "Yuchen Zhuang",
        "user": "yczhuang",
        "type": "user"
      },
      "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.",
      "upvotes": 6,
      "discussionId": "68db4082d2bf1f4b15ec72e6",
      "ai_summary": "AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.",
      "ai_keywords": [
        "large language model",
        "decomposer",
        "solver",
        "supervised fine-tuning",
        "reinforcement fine-tuning",
        "exact match improvement",
        "document-level finance reasoning",
        "parameter efficiency"
      ]
    },
    "publishedAt": "2025-09-28T22:14:30.000Z",
    "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play",
    "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6471bddd609ae9f56368f132/G91Q4iCGN2dy3oMaz-LrO.jpeg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23866",
      "authors": [
        {
          "_id": "68db503dd2bf1f4b15ec74b2",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b3",
          "name": "Zechen Hu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b4",
          "name": "Zirui Shang",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b5",
          "name": "Jingrong Wu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b6",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b7",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b8",
          "name": "Zhi Gao",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74b9",
          "name": "Chenrui Shi",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74ba",
          "name": "Bofei Zhang",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74bb",
          "name": "Zihao Zhang",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74bc",
          "name": "Xiaochuan Shi",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74bd",
          "name": "Zedong YU",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74be",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74bf",
          "name": "Xinxiao Wu",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74c0",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74c1",
          "name": "Liuyu Xiang",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74c2",
          "name": "Zhaofeng He",
          "hidden": false
        },
        {
          "_id": "68db503dd2bf1f4b15ec74c3",
          "name": "Qing Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T13:19:20.000Z",
      "submittedOnDailyAt": "2025-09-30T02:10:09.841Z",
      "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
      "submittedOnDailyBy": {
        "_id": "6455f8c2c8f569b995d603c9",
        "avatarUrl": "/avatars/3cb2cad3ab123887c270234bb6a4ca43.svg",
        "isPro": false,
        "fullname": "Qing Li",
        "user": "li-qing",
        "type": "user"
      },
      "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
      "upvotes": 6,
      "discussionId": "68db503dd2bf1f4b15ec74c4",
      "projectPage": "https://computer-use-agents.github.io/dart-gui/",
      "githubRepo": "https://github.com/computer-use-agents/dart-gui",
      "ai_summary": "DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "GUI agents",
        "DART",
        "Decoupled Agentic RL Training",
        "environment cluster",
        "rollout service",
        "data manager",
        "trainer",
        "non-blocking communication",
        "asynchronous training",
        "rollout-wise trajectory sampling",
        "per-worker model synchronization",
        "adaptive data curation",
        "successful trajectories",
        "rollout numbers",
        "trajectory lengths",
        "high-entropy steps",
        "truncated importance sampling",
        "OSWorld benchmark",
        "DART-GUI-7B"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-28T09:19:20.000Z",
    "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
    "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6455f8c2c8f569b995d603c9",
      "avatarUrl": "/avatars/3cb2cad3ab123887c270234bb6a4ca43.svg",
      "fullname": "Qing Li",
      "name": "li-qing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "63a95ac93453852ef5399a77",
      "name": "bigai",
      "fullname": "Beijing Institute for General Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24786",
      "authors": [
        {
          "_id": "68db43edd2bf1f4b15ec734e",
          "user": {
            "_id": "67067633351e0c16a5c27497",
            "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
            "isPro": false,
            "fullname": "Shenghao Fu",
            "user": "fushh7",
            "type": "user"
          },
          "name": "Shenghao Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:44.180Z",
          "hidden": false
        },
        {
          "_id": "68db43edd2bf1f4b15ec734f",
          "name": "Qize Yang",
          "hidden": false
        },
        {
          "_id": "68db43edd2bf1f4b15ec7350",
          "user": {
            "_id": "644fe6a9e1d7a97f3b66e906",
            "avatarUrl": "/avatars/ad1a45f0b1c8a4d03ba87f2a3ce5a8f8.svg",
            "isPro": false,
            "fullname": "Yuanming-Li",
            "user": "Lymann",
            "type": "user"
          },
          "name": "Yuan-Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:39.978Z",
          "hidden": false
        },
        {
          "_id": "68db43edd2bf1f4b15ec7351",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "68db43edd2bf1f4b15ec7352",
          "name": "Xiaohua Xie",
          "hidden": false
        },
        {
          "_id": "68db43edd2bf1f4b15ec7353",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T13:43:55.000Z",
      "submittedOnDailyAt": "2025-09-30T01:18:04.944Z",
      "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning",
      "submittedOnDailyBy": {
        "_id": "67067633351e0c16a5c27497",
        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
        "isPro": false,
        "fullname": "Shenghao Fu",
        "user": "fushh7",
        "type": "user"
      },
      "summary": "Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.",
      "upvotes": 5,
      "discussionId": "68db43edd2bf1f4b15ec7354",
      "ai_summary": "LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.",
      "ai_keywords": [
        "Large Video-Language Models",
        "LVLMs",
        "uniform frame sampling",
        "adaptive zoom",
        "densely sampled frames",
        "large frame resolution",
        "multi-step reasoning",
        "decoupled reinforcement finetuning",
        "slow-fast adaptive frame sampling"
      ]
    },
    "publishedAt": "2025-09-29T09:43:55.000Z",
    "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning",
    "summary": "Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67067633351e0c16a5c27497",
      "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
      "fullname": "Shenghao Fu",
      "name": "fushh7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "submitterOrganization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24335",
      "authors": [
        {
          "_id": "68db4439d2bf1f4b15ec7374",
          "user": {
            "_id": "6348de0c62c668c7b48d83c9",
            "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
            "isPro": false,
            "fullname": "Guolin Ke",
            "user": "guolinke",
            "type": "user"
          },
          "name": "Guolin Ke",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:33.722Z",
          "hidden": false
        },
        {
          "_id": "68db4439d2bf1f4b15ec7375",
          "name": "Hui Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T06:34:24.000Z",
      "submittedOnDailyAt": "2025-09-30T01:28:30.854Z",
      "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6348de0c62c668c7b48d83c9",
        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
        "isPro": false,
        "fullname": "Guolin Ke",
        "user": "guolinke",
        "type": "user"
      },
      "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
      "upvotes": 5,
      "discussionId": "68db4439d2bf1f4b15ec7376",
      "githubRepo": "https://github.com/guolinke/SphereAR",
      "ai_summary": "SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.",
      "ai_keywords": [
        "autoregressive models",
        "continuous-token AR",
        "latent diffusion",
        "masked-generation models",
        "VAE latents",
        "classifier-free guidance",
        "variance collapse",
        "hypersphere",
        "hyperspherical VAEs",
        "FID",
        "SphereAR-H",
        "SphereAR-L",
        "SphereAR-B",
        "MAR-H",
        "VAR-d30"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-29T02:34:24.000Z",
    "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation",
    "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6348de0c62c668c7b48d83c9",
      "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
      "fullname": "Guolin Ke",
      "name": "guolinke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23924",
      "authors": [
        {
          "_id": "68db800bd2bf1f4b15ec77d0",
          "user": {
            "_id": "64f73a44102fbfb26410962e",
            "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
            "isPro": false,
            "fullname": "jingyi Yang",
            "user": "JY-Young",
            "type": "user"
          },
          "name": "Jingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:40.232Z",
          "hidden": false
        },
        {
          "_id": "68db800bd2bf1f4b15ec77d1",
          "name": "Guanxu Chen",
          "hidden": false
        },
        {
          "_id": "68db800bd2bf1f4b15ec77d2",
          "name": "Xuhao Hu",
          "hidden": false
        },
        {
          "_id": "68db800bd2bf1f4b15ec77d3",
          "name": "Jing Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T15:01:15.000Z",
      "submittedOnDailyAt": "2025-09-30T05:38:56.124Z",
      "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
      "submittedOnDailyBy": {
        "_id": "64f73a44102fbfb26410962e",
        "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
        "isPro": false,
        "fullname": "jingyi Yang",
        "user": "JY-Young",
        "type": "user"
      },
      "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
      "upvotes": 5,
      "discussionId": "68db800bd2bf1f4b15ec77d4",
      "githubRepo": "https://github.com/yjyddq/EOSER-ASS-RL",
      "ai_summary": "Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.",
      "ai_keywords": [
        "Masked diffusion language models",
        "autoregressive models",
        "parallel decoding",
        "flexible generation orders",
        "block-wise decoding",
        "semi-AR decoding",
        "full diffusion-style decoding",
        "reinforcement learning",
        "non-causal decoding",
        "rollout trajectory",
        "optimization trajectory",
        "EOS Early Rejection",
        "Ascending Step-Size",
        "Consistency Trajectory Group Relative Policy Optimization",
        "LLaDA-8B-Instruct"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-09-28T11:01:15.000Z",
    "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
    "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f73a44102fbfb26410962e",
      "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
      "fullname": "jingyi Yang",
      "name": "JY-Young",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "643cb0625fcffe09fb6ca688",
      "name": "Fudan-University",
      "fullname": "Fudan University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.21953",
      "authors": [
        {
          "_id": "68db399cd2bf1f4b15ec7297",
          "user": {
            "_id": "676a2ca72d7050defde9b25d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
            "isPro": false,
            "fullname": "Suger",
            "user": "SugerWu",
            "type": "user"
          },
          "name": "Tao Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:34:46.687Z",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec7298",
          "name": "Yibo Jiang",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec7299",
          "name": "Yehao Lu",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec729a",
          "name": "Zhizhong Wang",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec729b",
          "name": "Zeyi Huang",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec729c",
          "name": "Zequn Qin",
          "hidden": false
        },
        {
          "_id": "68db399cd2bf1f4b15ec729d",
          "name": "Xi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T06:41:43.000Z",
      "submittedOnDailyAt": "2025-09-30T00:30:46.930Z",
      "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "676a2ca72d7050defde9b25d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
        "isPro": false,
        "fullname": "Suger",
        "user": "SugerWu",
        "type": "user"
      },
      "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a\nsingle image while preserving subject fidelity, ensuring prompt consistency,\nand aligning with human aesthetic preferences. However, existing methods,\nparticularly those built on the In-Context-Learning paradigm, are limited by\ntheir reliance on simple reconstruction-based objectives, leading to both\nsevere attribute leakage that compromises subject fidelity and failing to align\nwith nuanced human preferences. To address this, we propose MultiCrafter, a\nframework that ensures high-fidelity, preference-aligned generation. First, we\nfind that the root cause of attribute leakage is a significant entanglement of\nattention between different subjects during the generation process. Therefore,\nwe introduce explicit positional supervision to explicitly separate attention\nregions for each subject, effectively mitigating attribute leakage. To enable\nthe model to accurately plan the attention region of different subjects in\ndiverse scenarios, we employ a Mixture-of-Experts architecture to enhance the\nmodel's capacity, allowing different experts to focus on different scenarios.\nFinally, we design a novel online reinforcement learning framework to align the\nmodel with human preferences, featuring a scoring mechanism to accurately\nassess multi-subject fidelity and a more stable training strategy tailored for\nthe MoE architecture. Experiments validate that our framework significantly\nimproves subject fidelity while aligning with human preferences better.",
      "upvotes": 5,
      "discussionId": "68db399dd2bf1f4b15ec729e",
      "projectPage": "https://wutao-cs.github.io/MultiCrafter/",
      "ai_summary": "MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.",
      "ai_keywords": [
        "multi-subject image generation",
        "attribute leakage",
        "explicit positional supervision",
        "Mixture-of-Experts architecture",
        "online reinforcement learning",
        "scoring mechanism"
      ]
    },
    "publishedAt": "2025-09-26T02:41:43.000Z",
    "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
    "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a\nsingle image while preserving subject fidelity, ensuring prompt consistency,\nand aligning with human aesthetic preferences. However, existing methods,\nparticularly those built on the In-Context-Learning paradigm, are limited by\ntheir reliance on simple reconstruction-based objectives, leading to both\nsevere attribute leakage that compromises subject fidelity and failing to align\nwith nuanced human preferences. To address this, we propose MultiCrafter, a\nframework that ensures high-fidelity, preference-aligned generation. First, we\nfind that the root cause of attribute leakage is a significant entanglement of\nattention between different subjects during the generation process. Therefore,\nwe introduce explicit positional supervision to explicitly separate attention\nregions for each subject, effectively mitigating attribute leakage. To enable\nthe model to accurately plan the attention region of different subjects in\ndiverse scenarios, we employ a Mixture-of-Experts architecture to enhance the\nmodel's capacity, allowing different experts to focus on different scenarios.\nFinally, we design a novel online reinforcement learning framework to align the\nmodel with human preferences, featuring a scoring mechanism to accurately\nassess multi-subject fidelity and a more stable training strategy tailored for\nthe MoE architecture. Experiments validate that our framework significantly\nimproves subject fidelity while aligning with human preferences better.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.21953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "676a2ca72d7050defde9b25d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qNWwTpxhHfbRSHdtdDhQl.png",
      "fullname": "Suger",
      "name": "SugerWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25185",
      "authors": [
        {
          "_id": "68db4dacd2bf1f4b15ec7471",
          "name": "Shuoshuo Zhang",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7472",
          "user": {
            "_id": "64d31c55d8b712baf198602f",
            "avatarUrl": "/avatars/fad7972744f879116a8dc8b406f8b91c.svg",
            "isPro": false,
            "fullname": "Zijian Li",
            "user": "zli999",
            "type": "user"
          },
          "name": "Zijian Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:27:54.665Z",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7473",
          "name": "Yizhen Zhang",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7474",
          "name": "Jingjing Fu",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7475",
          "name": "Lei Song",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7476",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7477",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7478",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "68db4dacd2bf1f4b15ec7479",
          "name": "Rui Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:59:49.000Z",
      "submittedOnDailyAt": "2025-09-30T02:05:17.140Z",
      "title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on\n  Structured Images",
      "submittedOnDailyBy": {
        "_id": "64d31c55d8b712baf198602f",
        "avatarUrl": "/avatars/fad7972744f879116a8dc8b406f8b91c.svg",
        "isPro": false,
        "fullname": "Zijian Li",
        "user": "zli999",
        "type": "user"
      },
      "summary": "Structured images (e.g., charts and geometric diagrams) remain challenging\nfor multimodal large language models (MLLMs), as perceptual slips can cascade\ninto erroneous conclusions. Intermediate visual cues can steer reasoning;\nhowever, existing cue-based methods are constrained with low-fidelity image\nprocessing and linear, rigid reasoning patterns, limiting their effectiveness\non complex structured-image tasks. In this paper, we propose PixelCraft, a\nnovel multi-agent system for high-fidelity image processing and flexible visual\nreasoning on structured images. The system comprises a dispatcher, a planner, a\nreasoner, critics, and a set of visual tool agents. To achieve high-fidelity\nprocessing, we construct a high-quality corpus and fine-tune an MLLM into a\ngrounding model, whose pixel-level localizations are integrated with\ntraditional computer vision (CV) algorithms in tool agents. Building on this\nfoundation, PixelCraft facilitates flexible visual reasoning through a dynamic\nthree-stage workflow of tool selection, agent discussion, and self-criticism.\nMoreover, unlike prior linear reasoning patterns that simply append historical\nimages, PixelCraft maintains an image memory to allow the planner to adaptively\nrevisit earlier visual steps, explore alternative reasoning branches, and\ndynamically adjust the reasoning trajectory during discussion. Extensive\nexperiments on challenging chart and geometry benchmarks demonstrate that\nPixelCraft significantly improves visual reasoning performance for advanced\nMLLMs, setting a new standard for structured image reasoning. Our code will be\navailable at https://github.com/microsoft/PixelCraft.",
      "upvotes": 4,
      "discussionId": "68db4dacd2bf1f4b15ec747a",
      "githubRepo": "https://github.com/microsoft/PixelCraft",
      "ai_summary": "PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.",
      "ai_keywords": [
        "multimodal large language models",
        "perceptual slips",
        "intermediate visual cues",
        "multi-agent system",
        "high-fidelity image processing",
        "flexible visual reasoning",
        "dispatcher",
        "planner",
        "reasoner",
        "critics",
        "visual tool agents",
        "grounding model",
        "pixel-level localizations",
        "traditional computer vision",
        "dynamic three-stage workflow",
        "tool selection",
        "agent discussion",
        "self-criticism",
        "image memory",
        "visual reasoning performance",
        "structured image reasoning"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-29T13:59:49.000Z",
    "title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on\n  Structured Images",
    "summary": "Structured images (e.g., charts and geometric diagrams) remain challenging\nfor multimodal large language models (MLLMs), as perceptual slips can cascade\ninto erroneous conclusions. Intermediate visual cues can steer reasoning;\nhowever, existing cue-based methods are constrained with low-fidelity image\nprocessing and linear, rigid reasoning patterns, limiting their effectiveness\non complex structured-image tasks. In this paper, we propose PixelCraft, a\nnovel multi-agent system for high-fidelity image processing and flexible visual\nreasoning on structured images. The system comprises a dispatcher, a planner, a\nreasoner, critics, and a set of visual tool agents. To achieve high-fidelity\nprocessing, we construct a high-quality corpus and fine-tune an MLLM into a\ngrounding model, whose pixel-level localizations are integrated with\ntraditional computer vision (CV) algorithms in tool agents. Building on this\nfoundation, PixelCraft facilitates flexible visual reasoning through a dynamic\nthree-stage workflow of tool selection, agent discussion, and self-criticism.\nMoreover, unlike prior linear reasoning patterns that simply append historical\nimages, PixelCraft maintains an image memory to allow the planner to adaptively\nrevisit earlier visual steps, explore alternative reasoning branches, and\ndynamically adjust the reasoning trajectory during discussion. Extensive\nexperiments on challenging chart and geometry benchmarks demonstrate that\nPixelCraft significantly improves visual reasoning performance for advanced\nMLLMs, setting a new standard for structured image reasoning. Our code will be\navailable at https://github.com/microsoft/PixelCraft.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d31c55d8b712baf198602f",
      "avatarUrl": "/avatars/fad7972744f879116a8dc8b406f8b91c.svg",
      "fullname": "Zijian Li",
      "name": "zli999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "submitterOrganization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25131",
      "authors": [
        {
          "_id": "68db442ad2bf1f4b15ec7368",
          "user": {
            "_id": "6423e35b30b0e4ab36dd1b16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
            "isPro": false,
            "fullname": "Wang Chengyao",
            "user": "wcy1122",
            "type": "user"
          },
          "name": "Chengyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:36.950Z",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec7369",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736a",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736b",
          "name": "Senqiao Yang",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736c",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736d",
          "name": "Haokun Gui",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736e",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec736f",
          "name": "Jingyao Li",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec7370",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "68db442ad2bf1f4b15ec7371",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:48:28.000Z",
      "submittedOnDailyAt": "2025-09-30T01:18:28.196Z",
      "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
      "submittedOnDailyBy": {
        "_id": "6423e35b30b0e4ab36dd1b16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
        "isPro": false,
        "fullname": "Wang Chengyao",
        "user": "wcy1122",
        "type": "user"
      },
      "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
      "upvotes": 4,
      "discussionId": "68db442bd2bf1f4b15ec7372",
      "githubRepo": "https://github.com/dvlab-research/MGM-Omni",
      "ai_summary": "MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.",
      "ai_keywords": [
        "Omni LLM",
        "brain-mouth design",
        "dual-track architecture",
        "token-based architecture",
        "multimodal reasoning",
        "real-time speech generation",
        "unified training strategy",
        "dual audio encoder",
        "chunk-based parallel decoding",
        "text-speech token-rate gap",
        "streaming zero-shot voice cloning",
        "timbre identity",
        "natural speech",
        "context-aware speech",
        "long-form audio",
        "omnimodal understanding",
        "end-to-end paradigm",
        "controllable speech generation"
      ],
      "githubStars": 119
    },
    "publishedAt": "2025-09-29T13:48:28.000Z",
    "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
    "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6423e35b30b0e4ab36dd1b16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423e35b30b0e4ab36dd1b16/pea6LVDS9PQAxvQt9GUiZ.jpeg",
      "fullname": "Wang Chengyao",
      "name": "wcy1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "submitterOrganization": {
      "_id": "6390c6fdd00f25601f445cd4",
      "name": "CUHK-CSE",
      "fullname": "The Chinese University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23371",
      "authors": [
        {
          "_id": "68db3818d2bf1f4b15ec7290",
          "user": {
            "_id": "653f1d243bd61358055ad51d",
            "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
            "isPro": false,
            "fullname": "junmingyang",
            "user": "jmyang",
            "type": "user"
          },
          "name": "Junming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:34:49.878Z",
          "hidden": false
        },
        {
          "_id": "68db3818d2bf1f4b15ec7291",
          "name": "Ning Xu",
          "hidden": false
        },
        {
          "_id": "68db3818d2bf1f4b15ec7292",
          "name": "Biao Liu",
          "hidden": false
        },
        {
          "_id": "68db3818d2bf1f4b15ec7293",
          "name": "Shiqi Qiao",
          "hidden": false
        },
        {
          "_id": "68db3818d2bf1f4b15ec7294",
          "name": "Xin Geng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T15:38:24.000Z",
      "submittedOnDailyAt": "2025-09-30T00:28:47.911Z",
      "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
      "submittedOnDailyBy": {
        "_id": "653f1d243bd61358055ad51d",
        "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
        "isPro": false,
        "fullname": "junmingyang",
        "user": "jmyang",
        "type": "user"
      },
      "summary": "Preference optimization is crucial for aligning large language models (LLMs)\nwith human values and intentions. A significant challenge in this process is\nthe distribution mismatch between pre-collected offline preference data and the\nevolving model policy. Existing methods attempt to reduce this gap using static\nheuristics or decoupled online sampling strategies, but they often fail to\nadapt to the model's dynamic learning state. To bridge this gap, we propose\nMeta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework\nthat dynamically couples data generation with model training. MetaAPO employs a\nlightweight meta-learner, as an \"alignment gap estimator\", to evaluate the\npotential benefits of on-policy sampling in relation to offline data. This\nguides targeted online generation and assigns sample-wise meta-weights to the\noptimization objective, dynamically balancing the quality and distribution of\nonline and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench\ndemonstrate that MetaAPO consistently outperforms existing preference\noptimization approaches across various settings, while reducing 42% in online\nannotation costs.",
      "upvotes": 4,
      "discussionId": "68db3819d2bf1f4b15ec7295",
      "ai_summary": "Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.",
      "ai_keywords": [
        "Meta-Weighted Adaptive Preference Optimization",
        "MetaAPO",
        "meta-learner",
        "alignment gap estimator",
        "on-policy sampling",
        "AlpacaEval 2",
        "Arena-Hard",
        "MT-Bench"
      ]
    },
    "publishedAt": "2025-09-27T11:38:24.000Z",
    "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
    "summary": "Preference optimization is crucial for aligning large language models (LLMs)\nwith human values and intentions. A significant challenge in this process is\nthe distribution mismatch between pre-collected offline preference data and the\nevolving model policy. Existing methods attempt to reduce this gap using static\nheuristics or decoupled online sampling strategies, but they often fail to\nadapt to the model's dynamic learning state. To bridge this gap, we propose\nMeta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework\nthat dynamically couples data generation with model training. MetaAPO employs a\nlightweight meta-learner, as an \"alignment gap estimator\", to evaluate the\npotential benefits of on-policy sampling in relation to offline data. This\nguides targeted online generation and assigns sample-wise meta-weights to the\noptimization objective, dynamically balancing the quality and distribution of\nonline and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench\ndemonstrate that MetaAPO consistently outperforms existing preference\noptimization approaches across various settings, while reducing 42% in online\nannotation costs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23371.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653f1d243bd61358055ad51d",
      "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
      "fullname": "junmingyang",
      "name": "jmyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.23338",
      "authors": [
        {
          "_id": "68db7c7dd2bf1f4b15ec778b",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec778c",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec778d",
          "name": "Haoyu Wang",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec778e",
          "name": "Yuxing Han",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec778f",
          "name": "Xufei Wu",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec7790",
          "name": "Fan Wu",
          "hidden": false
        },
        {
          "_id": "68db7c7dd2bf1f4b15ec7791",
          "name": "Xuanhe Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T14:41:13.000Z",
      "submittedOnDailyAt": "2025-09-30T05:20:05.195Z",
      "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation",
      "submittedOnDailyBy": {
        "_id": "68216c63856b96f869d1d116",
        "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg",
        "isPro": false,
        "fullname": "Wei Zhou",
        "user": "weizhoudb",
        "type": "user"
      },
      "summary": "Large language models (LLMS) have shown increasing effectiveness in\nText-to-SQL tasks. However, another closely related problem, Cross-System SQL\nTranslation (a.k.a., SQL-to-SQL), which adapts a query written for one database\nsystem (e.g., MySQL) into its equivalent one for another system (e.g.,\nClickHouse), is of great practical importance but remains underexplored.\nExisting SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which\n(1) focus on a limited set of database systems (often just SQLite) and (2)\ncannot capture many system-specific SQL dialects (e.g., customized functions,\ndata types, and syntax rules). Thus, in this paper, we introduce PARROT, a\nPractical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT\ncomprises 598 translation pairs from 38 open-source benchmarks and real-world\nbusiness services, specifically prepared to challenge system-specific SQL\nunderstanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We\nalso provide multiple benchmark variants, including PARROT-Diverse with 28,003\ntranslations (for extensive syntax testing) and PARROT-Simple with 5,306\nrepresentative samples (for focused stress testing), covering 22\nproduction-grade database systems. To promote future research, we release a\npublic leaderboard and source code at: https://code4db.github.io/parrot-bench/.",
      "upvotes": 3,
      "discussionId": "68db7c7ed2bf1f4b15ec7792",
      "projectPage": "https://code4db.github.io/parrot-bench/",
      "githubRepo": "https://github.com/weAIDB/PARROT",
      "ai_summary": "PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.",
      "ai_keywords": [
        "Large language models",
        "Text-to-SQL",
        "Cross-System SQL Translation",
        "SQL-to-SQL",
        "database systems",
        "SQL dialects",
        "PARROT",
        "benchmark",
        "system-specific SQL",
        "public leaderboard"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-27T10:41:13.000Z",
    "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation",
    "summary": "Large language models (LLMS) have shown increasing effectiveness in\nText-to-SQL tasks. However, another closely related problem, Cross-System SQL\nTranslation (a.k.a., SQL-to-SQL), which adapts a query written for one database\nsystem (e.g., MySQL) into its equivalent one for another system (e.g.,\nClickHouse), is of great practical importance but remains underexplored.\nExisting SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which\n(1) focus on a limited set of database systems (often just SQLite) and (2)\ncannot capture many system-specific SQL dialects (e.g., customized functions,\ndata types, and syntax rules). Thus, in this paper, we introduce PARROT, a\nPractical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT\ncomprises 598 translation pairs from 38 open-source benchmarks and real-world\nbusiness services, specifically prepared to challenge system-specific SQL\nunderstanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We\nalso provide multiple benchmark variants, including PARROT-Diverse with 28,003\ntranslations (for extensive syntax testing) and PARROT-Simple with 5,306\nrepresentative samples (for focused stress testing), covering 22\nproduction-grade database systems. To promote future research, we release a\npublic leaderboard and source code at: https://code4db.github.io/parrot-bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68216c63856b96f869d1d116",
      "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg",
      "fullname": "Wei Zhou",
      "name": "weizhoudb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "submitterOrganization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22570",
      "authors": [
        {
          "_id": "68da9e53d2bf1f4b15ec7151",
          "user": {
            "_id": "6388a7e98a5dbe2f3dc61faa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
            "isPro": false,
            "fullname": "Qi Mao",
            "user": "HelenMao",
            "type": "user"
          },
          "name": "Qi Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:35:51.051Z",
          "hidden": false
        },
        {
          "_id": "68da9e53d2bf1f4b15ec7152",
          "name": "Tinghan Yang",
          "hidden": false
        },
        {
          "_id": "68da9e53d2bf1f4b15ec7153",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "68da9e53d2bf1f4b15ec7154",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68da9e53d2bf1f4b15ec7155",
          "name": "Libiao Jin",
          "hidden": false
        },
        {
          "_id": "68da9e53d2bf1f4b15ec7156",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T16:46:12.000Z",
      "submittedOnDailyAt": "2025-09-30T01:48:30.980Z",
      "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI\n  Collaboration",
      "submittedOnDailyBy": {
        "_id": "6388a7e98a5dbe2f3dc61faa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
        "isPro": false,
        "fullname": "Qi Mao",
        "user": "HelenMao",
        "type": "user"
      },
      "summary": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI\nagents is transforming human-AI collaboration into bidirectional, multimodal\ninteraction. However, existing codecs remain optimized for unimodal, one-way\ncommunication, resulting in repeated degradation under conventional\ncompress-transmit-reconstruct pipelines. To address this limitation, we propose\nUniMIC, a Unified token-based Multimodal Interactive Coding framework that\nbridges edge devices and cloud AI agents. Instead of transmitting raw pixels or\nplain text, UniMIC employs compact tokenized representations as the\ncommunication medium, enabling efficient low-bitrate transmission while\nmaintaining compatibility with LMMs. To further enhance compression,\nlightweight Transformer-based entropy models with scenario-specific\ndesigns-generic, masked, and text-conditioned-effectively minimize inter-token\nredundancy. Extensive experiments on text-to-image generation, text-guided\ninpainting, outpainting, and visual question answering show that UniMIC\nachieves substantial bitrate savings and remains robust even at ultra-low\nbitrates (<0.05bpp), without compromising downstream task performance. These\nresults establish UniMIC as a practical and forward-looking paradigm for\nnext-generation multimodal interactive communication.",
      "upvotes": 3,
      "discussionId": "68da9e53d2bf1f4b15ec7157",
      "ai_summary": "UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.",
      "ai_keywords": [
        "Large Multimodal Models",
        "UniMIC",
        "token-based",
        "multimodal interactive coding",
        "edge devices",
        "cloud AI agents",
        "compact tokenized representations",
        "lightweight Transformer-based entropy models",
        "text-to-image generation",
        "text-guided inpainting",
        "outpainting",
        "visual question answering",
        "bitrate savings",
        "ultra-low bitrates"
      ]
    },
    "publishedAt": "2025-09-26T12:46:12.000Z",
    "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI\n  Collaboration",
    "summary": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI\nagents is transforming human-AI collaboration into bidirectional, multimodal\ninteraction. However, existing codecs remain optimized for unimodal, one-way\ncommunication, resulting in repeated degradation under conventional\ncompress-transmit-reconstruct pipelines. To address this limitation, we propose\nUniMIC, a Unified token-based Multimodal Interactive Coding framework that\nbridges edge devices and cloud AI agents. Instead of transmitting raw pixels or\nplain text, UniMIC employs compact tokenized representations as the\ncommunication medium, enabling efficient low-bitrate transmission while\nmaintaining compatibility with LMMs. To further enhance compression,\nlightweight Transformer-based entropy models with scenario-specific\ndesigns-generic, masked, and text-conditioned-effectively minimize inter-token\nredundancy. Extensive experiments on text-to-image generation, text-guided\ninpainting, outpainting, and visual question answering show that UniMIC\nachieves substantial bitrate savings and remains robust even at ultra-low\nbitrates (<0.05bpp), without compromising downstream task performance. These\nresults establish UniMIC as a practical and forward-looking paradigm for\nnext-generation multimodal interactive communication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22570.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6388a7e98a5dbe2f3dc61faa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
      "fullname": "Qi Mao",
      "name": "HelenMao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "submitterOrganization": {
      "_id": "67dab498ed21a53369f5de73",
      "name": "CUC-MIPG",
      "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.25149",
      "authors": [
        {
          "_id": "68db57aed2bf1f4b15ec7538",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7539",
          "name": "Felix Abecassis",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753a",
          "name": "Anjulie Agrusa",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753b",
          "name": "Dong Ahn",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753c",
          "name": "Jonah Alben",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753d",
          "name": "Stefania Alborghetti",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753e",
          "name": "Michael Andersch",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec753f",
          "name": "Sivakumar Arayandi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7540",
          "name": "Alexis Bjorlin",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7541",
          "name": "Aaron Blakeman",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7542",
          "name": "Evan Briones",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7543",
          "name": "Ian Buck",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7544",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7545",
          "name": "Jinhang Choi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7546",
          "name": "Mike Chrzanowski",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7547",
          "name": "Eric Chung",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7548",
          "name": "Victor Cui",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7549",
          "name": "Steve Dai",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754a",
          "name": "Bita Darvish Rouhani",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754b",
          "name": "Carlo del Mundo",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754c",
          "name": "Deena Donia",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754d",
          "name": "Burc Eryilmaz",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754e",
          "name": "Henry Estela",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec754f",
          "name": "Abhinav Goel",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7550",
          "name": "Oleg Goncharov",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7551",
          "name": "Yugi Guvvala",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7552",
          "name": "Robert Hesse",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7553",
          "name": "Russell Hewett",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7554",
          "name": "Herbert Hum",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7555",
          "name": "Ujval Kapasi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7556",
          "name": "Brucek Khailany",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7557",
          "name": "Mikail Khona",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7558",
          "name": "Nick Knight",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7559",
          "name": "Alex Kondratenko",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755a",
          "name": "Ronny Krashinsky",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755b",
          "name": "Ben Lanir",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755c",
          "name": "Simon Layton",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755d",
          "name": "Michael Lightstone",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755e",
          "name": "Daniel Lo",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec755f",
          "name": "Paulius Micikevicius",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7560",
          "name": "Asit Mishra",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7561",
          "name": "Tim Moon",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7562",
          "name": "Deepak Narayanan",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7563",
          "name": "Chao Ni",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7564",
          "name": "Abhijit Paithankar",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7565",
          "name": "Satish Pasumarthi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7566",
          "name": "Ankit Patel",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7567",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7568",
          "name": "Ashwin Poojary",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7569",
          "name": "Gargi Prasad",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756a",
          "name": "Sweta Priyadarshi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756b",
          "name": "Yigong Qin",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756c",
          "name": "Xiaowei Ren",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756d",
          "name": "Oleg Rybakov",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756e",
          "name": "Charbel Sakr",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec756f",
          "name": "Sanjeev Satheesh",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7570",
          "name": "Stas Sergienko",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7571",
          "name": "Pasha Shamis",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7572",
          "name": "Kirthi Shankar",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7573",
          "name": "Nishant Sharma",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7574",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7575",
          "name": "Michael Siu",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7576",
          "name": "Misha Smelyanskiy",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7577",
          "name": "Darko Stosic",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7578",
          "name": "Dusan Stosic",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7579",
          "name": "Bor-Yiing Su",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757a",
          "name": "Frank Sun",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757b",
          "name": "Nima Tajbakhsh",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757c",
          "name": "Shelby Thomas",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757d",
          "name": "Przemek Tredak",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757e",
          "name": "Evgeny Tsykunov",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec757f",
          "name": "Gandhi Vaithilingam",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7580",
          "name": "Aditya Vavre",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7581",
          "name": "Rangharajan Venkatesan",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7582",
          "name": "Roger Waleffe",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7583",
          "name": "Qiyu Wan",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7584",
          "name": "Hexin Wang",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7585",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7586",
          "name": "Lizzie Wei",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7587",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7588",
          "name": "Evan Wu",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7589",
          "name": "Keith Wyss",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758a",
          "name": "Ning Xu",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758b",
          "name": "Jinze Xue",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758c",
          "name": "Charlene Yang",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758d",
          "name": "Yujia Zhai",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758e",
          "name": "Ruoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec758f",
          "name": "Jingyang Zhu",
          "hidden": false
        },
        {
          "_id": "68db57aed2bf1f4b15ec7590",
          "name": "Zhongbo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:53:17.000Z",
      "submittedOnDailyAt": "2025-09-30T02:38:26.887Z",
      "title": "Pretraining Large Language Models with NVFP4",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) today are powerful problem solvers across many\ndomains, and they continue to get stronger as they scale in model size,\ntraining set size, and training set quality, as shown by extensive research and\nexperimentation across the industry. Training a frontier model today requires\non the order of tens to hundreds of yottaflops, which is a massive investment\nof time, compute, and energy. Improving pretraining efficiency is therefore\nessential to enable the next generation of even more capable LLMs. While 8-bit\nfloating point (FP8) training is now widely adopted, transitioning to even\nnarrower precision, such as 4-bit floating point (FP4), could unlock additional\nimprovements in computational speed and resource utilization. However,\nquantization at this level poses challenges to training stability, convergence,\nand implementation, notably for large-scale models trained on long token\nhorizons.\n  In this study, we introduce a novel approach for stable and accurate training\nof large language models (LLMs) using the NVFP4 format. Our method integrates\nRandom Hadamard transforms (RHT) to bound block-level outliers, employs a\ntwo-dimensional quantization scheme for consistent representations across both\nthe forward and backward passes, utilizes stochastic rounding for unbiased\ngradient estimation, and incorporates selective high-precision layers. We\nvalidate our approach by training a 12-billion-parameter model on 10 trillion\ntokens -- the longest publicly documented training run in 4-bit precision to\ndate. Our results show that the model trained with our NVFP4-based pretraining\ntechnique achieves training loss and downstream task accuracies comparable to\nan FP8 baseline. These findings highlight that NVFP4, when combined with our\ntraining approach, represents a major step forward in narrow-precision LLM\ntraining algorithms.",
      "upvotes": 2,
      "discussionId": "68db57afd2bf1f4b15ec7591",
      "ai_summary": "A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "8-bit floating point (FP8)",
        "4-bit floating point (FP4)",
        "quantization",
        "training stability",
        "convergence",
        "implementation",
        "Random Hadamard transforms (RHT)",
        "two-dimensional quantization",
        "stochastic rounding",
        "selective high-precision layers",
        "NVFP4",
        "training loss",
        "downstream task accuracies"
      ]
    },
    "publishedAt": "2025-09-29T13:53:17.000Z",
    "title": "Pretraining Large Language Models with NVFP4",
    "summary": "Large Language Models (LLMs) today are powerful problem solvers across many\ndomains, and they continue to get stronger as they scale in model size,\ntraining set size, and training set quality, as shown by extensive research and\nexperimentation across the industry. Training a frontier model today requires\non the order of tens to hundreds of yottaflops, which is a massive investment\nof time, compute, and energy. Improving pretraining efficiency is therefore\nessential to enable the next generation of even more capable LLMs. While 8-bit\nfloating point (FP8) training is now widely adopted, transitioning to even\nnarrower precision, such as 4-bit floating point (FP4), could unlock additional\nimprovements in computational speed and resource utilization. However,\nquantization at this level poses challenges to training stability, convergence,\nand implementation, notably for large-scale models trained on long token\nhorizons.\n  In this study, we introduce a novel approach for stable and accurate training\nof large language models (LLMs) using the NVFP4 format. Our method integrates\nRandom Hadamard transforms (RHT) to bound block-level outliers, employs a\ntwo-dimensional quantization scheme for consistent representations across both\nthe forward and backward passes, utilizes stochastic rounding for unbiased\ngradient estimation, and incorporates selective high-precision layers. We\nvalidate our approach by training a 12-billion-parameter model on 10 trillion\ntokens -- the longest publicly documented training run in 4-bit precision to\ndate. Our results show that the model trained with our NVFP4-based pretraining\ntechnique achieves training loss and downstream task accuracies comparable to\nan FP8 baseline. These findings highlight that NVFP4, when combined with our\ntraining approach, represents a major step forward in narrow-precision LLM\ntraining algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 114
    },
    "submitterOrganization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25052",
      "authors": [
        {
          "_id": "68db56bdd2bf1f4b15ec752d",
          "name": "Sai Wang",
          "hidden": false
        },
        {
          "_id": "68db56bdd2bf1f4b15ec752e",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "68db56bdd2bf1f4b15ec752f",
          "user": {
            "_id": "672db76fa34d64e774fc42c9",
            "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
            "isPro": false,
            "fullname": "Zhongwen Xu",
            "user": "zhongwenxu",
            "type": "user"
          },
          "name": "Zhongwen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:41.196Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T17:02:31.000Z",
      "submittedOnDailyAt": "2025-09-30T02:34:51.572Z",
      "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
      "submittedOnDailyBy": {
        "_id": "672db76fa34d64e774fc42c9",
        "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
        "isPro": false,
        "fullname": "Zhongwen Xu",
        "user": "zhongwenxu",
        "type": "user"
      },
      "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
      "upvotes": 2,
      "discussionId": "68db56bed2bf1f4b15ec7530",
      "projectPage": "https://zhongwenxu.notion.site/Cogito-Ergo-Ludo-An-Agent-that-Learns-to-Play-by-Reasoning-and-Planning-27a1c4e140e3808f8470fb059bb44f0e",
      "ai_summary": "CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.",
      "ai_keywords": [
        "deep reinforcement learning",
        "Large Language Model",
        "Cogito",
        "ergo ludo",
        "CEL",
        "tabula rasa",
        "interaction and reflection",
        "Rule Induction",
        "Strategy and Playbook Summarization",
        "Minesweeper",
        "Frozen Lake",
        "Sokoban",
        "explicit reasoning",
        "transparent model"
      ]
    },
    "publishedAt": "2025-09-29T13:02:31.000Z",
    "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
    "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25052.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672db76fa34d64e774fc42c9",
      "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
      "fullname": "Zhongwen Xu",
      "name": "zhongwenxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "submitterOrganization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24910",
      "authors": [
        {
          "_id": "68db4317d2bf1f4b15ec7330",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "SongzeLi",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:32:37.366Z",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7331",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7332",
          "name": "Gengze Zhou",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7333",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7334",
          "name": "Xiangyu Zeng",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7335",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7336",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7337",
          "name": "Qi Wu",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7338",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "68db4317d2bf1f4b15ec7339",
          "name": "Yi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T15:15:54.000Z",
      "submittedOnDailyAt": "2025-09-30T01:26:14.055Z",
      "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving\n  Demonstrations at Scale",
      "submittedOnDailyBy": {
        "_id": "64acbbd51aee69ece03c6c0c",
        "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
        "isPro": false,
        "fullname": "Songze Li",
        "user": "SongzeLi",
        "type": "user"
      },
      "summary": "Goal-oriented language-guided navigation requires robust exploration\ncapabilities for agents to navigate to specified goals in unknown environments\nwithout step-by-step instructions. Existing methods tend to exclusively utilize\nshortest-path trajectories, lacking effective exploration priors for training\nnavigation agents. To address the above challenges, we present SID, a\ngoal-oriented language-guided navigation learning approach with Self-Improving\nDemonstrations. Specifically, SID learns an initial agent on the shortest-path\ndata sampled from environments and then leverages this agent to generate novel\nexploration trajectories. The novel rollouts provide demonstrations with\nstronger exploration strategies to train a better agent, which in turn produces\nhigher-quality agent demonstrations for the next round of training. We show\nthat this iterative self-improving pipeline readily scales to new environments,\nand the resulting demonstrations can be transferred across a variety of\nlanguage-guided navigation tasks, elevating the performance ceiling in diverse\ngoal-oriented navigation tasks. Extensive experiments demonstrate that SID\nsignificantly boosts the exploration capabilities and generalization of\nnavigation agents. The resulting agent achieves new state-of-the-art\nperformance on goal-oriented language-guided navigation tasks, including\nREVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation\nsplits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
      "upvotes": 2,
      "discussionId": "68db4317d2bf1f4b15ec733a",
      "githubRepo": "https://github.com/OpenGVLab/SID-VLN",
      "ai_summary": "SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.",
      "ai_keywords": [
        "goal-oriented language-guided navigation",
        "Self-Improving Demonstrations",
        "shortest-path trajectories",
        "exploration strategies",
        "iterative self-improving pipeline",
        "generalization",
        "REVERIE",
        "SOON"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-29T11:15:54.000Z",
    "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving\n  Demonstrations at Scale",
    "summary": "Goal-oriented language-guided navigation requires robust exploration\ncapabilities for agents to navigate to specified goals in unknown environments\nwithout step-by-step instructions. Existing methods tend to exclusively utilize\nshortest-path trajectories, lacking effective exploration priors for training\nnavigation agents. To address the above challenges, we present SID, a\ngoal-oriented language-guided navigation learning approach with Self-Improving\nDemonstrations. Specifically, SID learns an initial agent on the shortest-path\ndata sampled from environments and then leverages this agent to generate novel\nexploration trajectories. The novel rollouts provide demonstrations with\nstronger exploration strategies to train a better agent, which in turn produces\nhigher-quality agent demonstrations for the next round of training. We show\nthat this iterative self-improving pipeline readily scales to new environments,\nand the resulting demonstrations can be transferred across a variety of\nlanguage-guided navigation tasks, elevating the performance ceiling in diverse\ngoal-oriented navigation tasks. Extensive experiments demonstrate that SID\nsignificantly boosts the exploration capabilities and generalization of\nnavigation agents. The resulting agent achieves new state-of-the-art\nperformance on goal-oriented language-guided navigation tasks, including\nREVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation\nsplits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64acbbd51aee69ece03c6c0c",
      "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
      "fullname": "Songze Li",
      "name": "SongzeLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "submitterOrganization": {
      "_id": "64006c57a3b8fe3ac0e9af7c",
      "name": "OpenGVLab",
      "fullname": "OpenGVLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24269",
      "authors": [
        {
          "_id": "68db45a6d2bf1f4b15ec7381",
          "name": "Zihao Zhu",
          "hidden": false
        },
        {
          "_id": "68db45a6d2bf1f4b15ec7382",
          "name": "Xinyu Wu",
          "hidden": false
        },
        {
          "_id": "68db45a6d2bf1f4b15ec7383",
          "name": "Gehan Hu",
          "hidden": false
        },
        {
          "_id": "68db45a6d2bf1f4b15ec7384",
          "name": "Siwei Lyu",
          "hidden": false
        },
        {
          "_id": "68db45a6d2bf1f4b15ec7385",
          "name": "Ke Xu",
          "hidden": false
        },
        {
          "_id": "68db45a6d2bf1f4b15ec7386",
          "name": "Baoyuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T04:27:23.000Z",
      "submittedOnDailyAt": "2025-09-30T01:46:37.766Z",
      "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety\n  Alignment of Large Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "64030d9956038547951c7d55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64030d9956038547951c7d55/gRCzhy1yd7cOV6GN-KGHJ.png",
        "isPro": false,
        "fullname": "Zihao Zhu",
        "user": "ZihaoZhu",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the snowball effect, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models.",
      "upvotes": 2,
      "discussionId": "68db45a6d2bf1f4b15ec7387",
      "ai_summary": "AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.",
      "ai_keywords": [
        "Chain-of-Thought",
        "CoT reasoning",
        "snowball effect",
        "adversarial CoT tuning",
        "Temptation-Correction",
        "Hesitation-Correction",
        "jailbreak attacks",
        "CoT hijacking",
        "over-refusal",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-09-29T00:27:23.000Z",
    "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety\n  Alignment of Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the snowball effect, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24269.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64030d9956038547951c7d55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64030d9956038547951c7d55/gRCzhy1yd7cOV6GN-KGHJ.png",
      "fullname": "Zihao Zhu",
      "name": "ZihaoZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "submitterOrganization": {
      "_id": "674550398eebd2eaf7ebe4f8",
      "name": "CUHK-SZ",
      "fullname": "The Chinese University of Hongkong,Shenzhen"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23143",
      "authors": [
        {
          "_id": "68db2802d2bf1f4b15ec7233",
          "user": {
            "_id": "67c604ff3f8c7b027d479671",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c604ff3f8c7b027d479671/IhfN3eWRQ8iNfcsDKA1Ep.png",
            "isPro": false,
            "fullname": "Charles Wang",
            "user": "charleslwang",
            "type": "user"
          },
          "name": "Charles L. Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:35:31.804Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T06:06:36.000Z",
      "submittedOnDailyAt": "2025-09-30T02:02:26.925Z",
      "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "67c604ff3f8c7b027d479671",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c604ff3f8c7b027d479671/IhfN3eWRQ8iNfcsDKA1Ep.png",
        "isPro": false,
        "fullname": "Charles Wang",
        "user": "charleslwang",
        "type": "user"
      },
      "summary": "This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument (G approx\n1, phi approx 0). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.",
      "upvotes": 2,
      "discussionId": "68db2802d2bf1f4b15ec7234",
      "ai_summary": "MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.",
      "ai_keywords": [
        "MathBode",
        "dynamic diagnostic",
        "mathematical reasoning",
        "large language models",
        "parametric problem",
        "sinusoidal drive",
        "first-harmonic responses",
        "gain",
        "phase",
        "Bode-style fingerprints",
        "closed-form families",
        "linear solve",
        "ratio/saturation",
        "compound interest",
        "2x2 linear systems",
        "similar triangles",
        "low-pass behavior",
        "phase lag",
        "symbolic baseline",
        "reasoning fidelity",
        "consistency"
      ]
    },
    "publishedAt": "2025-09-27T02:06:36.000Z",
    "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning",
    "summary": "This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument (G approx\n1, phi approx 0). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c604ff3f8c7b027d479671",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c604ff3f8c7b027d479671/IhfN3eWRQ8iNfcsDKA1Ep.png",
      "fullname": "Charles Wang",
      "name": "charleslwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "submitterOrganization": {
      "_id": "68cb9cc79ed520a4b79ec89c",
      "name": "cognitive-metrology-lab",
      "fullname": "Cognitive Metrology Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c604ff3f8c7b027d479671/lZlWRjr4XzvzxrJ-65hsN.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.22830",
      "authors": [
        {
          "_id": "68db4615d2bf1f4b15ec7391",
          "user": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "isPro": false,
            "fullname": "HWANCHANG",
            "user": "HwanChang0106",
            "type": "user"
          },
          "name": "Hwan Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:30:28.646Z",
          "hidden": false
        },
        {
          "_id": "68db4615d2bf1f4b15ec7392",
          "name": "Yonghyun Jun",
          "hidden": false
        },
        {
          "_id": "68db4615d2bf1f4b15ec7393",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647c4a2692182942d7c2e698/Zf8T_OO2ZkAOPiQe8wuMW.png"
      ],
      "publishedAt": "2025-09-26T18:38:07.000Z",
      "submittedOnDailyAt": "2025-09-30T01:26:24.607Z",
      "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents",
      "submittedOnDailyBy": {
        "_id": "647c4a2692182942d7c2e698",
        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
        "isPro": false,
        "fullname": "HWANCHANG",
        "user": "HwanChang0106",
        "type": "user"
      },
      "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.",
      "upvotes": 2,
      "discussionId": "68db4615d2bf1f4b15ec7394",
      "ai_summary": "ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.",
      "ai_keywords": [
        "large language model",
        "LLM",
        "indirect prompt injection",
        "chat templates",
        "persuasion-driven",
        "multi-turn dialogues",
        "AgentDojo",
        "InjecAgent",
        "prompt-based defenses"
      ]
    },
    "publishedAt": "2025-09-26T14:38:07.000Z",
    "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents",
    "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647c4a2692182942d7c2e698/Zf8T_OO2ZkAOPiQe8wuMW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c4a2692182942d7c2e698",
      "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
      "fullname": "HWANCHANG",
      "name": "HwanChang0106",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "668d03f09761c585a203385f",
      "name": "Chung-AngUniversity",
      "fullname": "Chung-Ang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/668d037bb09a05f3d78d5b4c/eMdFRsUfQGWwCoPeQ3mkH.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.22518",
      "authors": [
        {
          "_id": "68d9f3650177a6054b013b1b",
          "user": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "isPro": false,
            "fullname": "Bo Li",
            "user": "liboaccn",
            "type": "user"
          },
          "name": "Bo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-29T05:58:06.996Z",
          "hidden": true
        },
        {
          "_id": "68d9f3650177a6054b013b1c",
          "name": "Guanzhi Deng",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b1d",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b1e",
          "name": "Junrong Yue",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b1f",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b20",
          "name": "Qinghua Zhao",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b21",
          "name": "Linqi Song",
          "hidden": false
        },
        {
          "_id": "68d9f3650177a6054b013b22",
          "name": "Lijie Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T16:02:27.000Z",
      "submittedOnDailyAt": "2025-09-30T02:16:36.753Z",
      "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model",
      "submittedOnDailyBy": {
        "_id": "6582c482f3006507ea10302a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
        "isPro": false,
        "fullname": "Bo Li",
        "user": "liboaccn",
        "type": "user"
      },
      "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.",
      "upvotes": 2,
      "discussionId": "68d9f3650177a6054b013b23",
      "ai_summary": "The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.",
      "ai_keywords": [
        "Large Language Models",
        "Reasoning Manifold",
        "internal representations",
        "k-nearest neighbors",
        "geometric deviation",
        "reasoning failures",
        "REMA framework",
        "multimodal models",
        "low-dimensional nature",
        "separability"
      ]
    },
    "publishedAt": "2025-09-26T12:02:27.000Z",
    "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model",
    "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6582c482f3006507ea10302a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
      "fullname": "Bo Li",
      "name": "liboaccn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24908",
      "authors": [
        {
          "_id": "68db79b0d2bf1f4b15ec774a",
          "name": "Andrs Fernndez Garca",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec774b",
          "user": {
            "_id": "5ef3829e518622264685b0cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1593016943046-noauth.jpeg",
            "isPro": false,
            "fullname": "Javier de la Rosa",
            "user": "versae",
            "type": "user"
          },
          "name": "Javier de la Rosa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T07:12:32.334Z",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec774c",
          "name": "Julio Gonzalo",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec774d",
          "name": "Roser Morante",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec774e",
          "name": "Enrique Amig",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec774f",
          "name": "Alejandro Benito-Santos",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7750",
          "name": "Jorge Carrillo-de-Albornoz",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7751",
          "name": "Vctor Fresno",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7752",
          "name": "Adrian Ghajari",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7753",
          "name": "Guillermo Marco",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7754",
          "name": "Laura Plaza",
          "hidden": false
        },
        {
          "_id": "68db79b0d2bf1f4b15ec7755",
          "name": "Eva Snchez Salido",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T15:15:17.000Z",
      "submittedOnDailyAt": "2025-09-30T05:06:21.699Z",
      "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
      "submittedOnDailyBy": {
        "_id": "5ef3829e518622264685b0cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1593016943046-noauth.jpeg",
        "isPro": false,
        "fullname": "Javier de la Rosa",
        "user": "versae",
        "type": "user"
      },
      "summary": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%).",
      "upvotes": 1,
      "discussionId": "68db79b1d2bf1f4b15ec7756",
      "ai_summary": "BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.",
      "ai_keywords": [
        "BOE-XSUM",
        "Boletn Oficial del Estado",
        "large language models",
        "LLMs",
        "fine-tuned models",
        "zero-shot setting",
        "BERTIN GPT-J 6B",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-09-29T11:15:17.000Z",
    "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
    "summary": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ef3829e518622264685b0cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1593016943046-noauth.jpeg",
      "fullname": "Javier de la Rosa",
      "name": "versae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "submitterOrganization": {
      "_id": "60f066a3ea00391b63806e56",
      "name": "bertin-project",
      "fullname": "BERTIN Project",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1627122830113-5ef3829e518622264685b0cd.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.24709",
      "authors": [
        {
          "_id": "68db518fd2bf1f4b15ec74c9",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74ca",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74cb",
          "name": "Yufan Shen",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74cc",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74cd",
          "name": "Tianyuan Huang",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74ce",
          "name": "Xinyu Fang",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74cf",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d0",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d1",
          "name": "Cheng Yang",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d2",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d3",
          "name": "Jianbiao Mei",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d4",
          "name": "Rong Wu",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d5",
          "name": "Licheng Wen",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d6",
          "name": "Xuemeng Yang",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d7",
          "name": "Song Mao",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d8",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74d9",
          "name": "Zhi Yu",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74da",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74db",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68db518fd2bf1f4b15ec74dc",
          "name": "Botian Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T12:38:06.000Z",
      "submittedOnDailyAt": "2025-09-30T02:12:18.116Z",
      "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user\n  interaction video?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The webpage-to-code task requires models to understand visual representations\nof webpages and generate corresponding code. However, existing benchmarks\nprimarily focus on static screenshot-to-code tasks, thereby overlooking the\ndynamic interactions fundamental to real-world web applications. To address\nthis limitation, this paper introduces IWR-Bench, a novel benchmark for\nevaluating the capabilities of Large Vision-Language Models (LVLMs) in\ninteractive webpage reconstruction from video. IWR-Bench comprises 113\nmeticulously curated tasks from 100 real-world websites, with 1,001 actions and\nfeaturing diverse interaction complexities (e.g., web games), visual styles,\nand domains. Aligning with standard web development practices, each task\nincludes not only user interaction videos but also all crawled static assets\n(e.g., images, videos). This benchmark evaluates models on two fundamental\nchallenges: comprehensive multi-modal reasoning to infer interaction logic from\nvideo and assets, and advanced code generation to translate this logic into\nfunctional code. An agent-as-a-judge framework with a comprehensive metric\nsystem automatically assesses the functional correctness and visual fidelity of\ngenerated webpages. Extensive experiments on 28 LVLMs reveal a significant\nchallenge: the best model achieves an overall score of only 36.35%, as\nfunctional correctness (24.39% IFS) lags significantly behind visual fidelity\n(64.25% VFS). These results highlight critical limitations in current models'\nability to reason about temporal dynamics and synthesize event-driven logic,\nestablishing IWR-Bench as a challenging frontier for vision-language research.\nThe benchmark and evaluation code will be made publicly available. Code is\navailable at https://github.com/L-O-I/IWR-Bench.",
      "upvotes": 1,
      "discussionId": "68db5190d2bf1f4b15ec74dd",
      "githubRepo": "https://github.com/L-O-I/IWR-Bench",
      "ai_summary": "IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "IWR-Bench",
        "multi-modal reasoning",
        "code generation",
        "interactive webpage reconstruction",
        "video",
        "static assets",
        "functional correctness",
        "visual fidelity",
        "agent-as-a-judge framework"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-29T08:38:06.000Z",
    "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user\n  interaction video?",
    "summary": "The webpage-to-code task requires models to understand visual representations\nof webpages and generate corresponding code. However, existing benchmarks\nprimarily focus on static screenshot-to-code tasks, thereby overlooking the\ndynamic interactions fundamental to real-world web applications. To address\nthis limitation, this paper introduces IWR-Bench, a novel benchmark for\nevaluating the capabilities of Large Vision-Language Models (LVLMs) in\ninteractive webpage reconstruction from video. IWR-Bench comprises 113\nmeticulously curated tasks from 100 real-world websites, with 1,001 actions and\nfeaturing diverse interaction complexities (e.g., web games), visual styles,\nand domains. Aligning with standard web development practices, each task\nincludes not only user interaction videos but also all crawled static assets\n(e.g., images, videos). This benchmark evaluates models on two fundamental\nchallenges: comprehensive multi-modal reasoning to infer interaction logic from\nvideo and assets, and advanced code generation to translate this logic into\nfunctional code. An agent-as-a-judge framework with a comprehensive metric\nsystem automatically assesses the functional correctness and visual fidelity of\ngenerated webpages. Extensive experiments on 28 LVLMs reveal a significant\nchallenge: the best model achieves an overall score of only 36.35%, as\nfunctional correctness (24.39% IFS) lags significantly behind visual fidelity\n(64.25% VFS). These results highlight critical limitations in current models'\nability to reason about temporal dynamics and synthesize event-driven logic,\nestablishing IWR-Bench as a challenging frontier for vision-language research.\nThe benchmark and evaluation code will be made publicly available. Code is\navailable at https://github.com/L-O-I/IWR-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24709.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 114
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23115",
      "authors": [
        {
          "_id": "68db5413d2bf1f4b15ec74ef",
          "name": "Haoyu He",
          "hidden": false
        },
        {
          "_id": "68db5413d2bf1f4b15ec74f0",
          "user": {
            "_id": "63ef0af2bfe4ead22ca8f69a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676610243576-noauth.jpeg",
            "isPro": false,
            "fullname": "Haozheng Luo",
            "user": "robinzixuan",
            "type": "user"
          },
          "name": "Haozheng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-30T06:00:46.112Z",
          "hidden": false
        },
        {
          "_id": "68db5413d2bf1f4b15ec74f1",
          "name": "Yan Chen",
          "hidden": false
        },
        {
          "_id": "68db5413d2bf1f4b15ec74f2",
          "name": "Qi R. Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T04:55:56.000Z",
      "submittedOnDailyAt": "2025-09-30T02:24:17.837Z",
      "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human\n  Mobility",
      "submittedOnDailyBy": {
        "_id": "63ef0af2bfe4ead22ca8f69a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676610243576-noauth.jpeg",
        "isPro": false,
        "fullname": "Haozheng Luo",
        "user": "robinzixuan",
        "type": "user"
      },
      "summary": "Predicting human mobility is inherently challenging due to complex long-range\ndependencies and multi-scale periodic behaviors. To address this, we introduce\nRHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),\na unified framework that leverages large language models (LLMs) as\ngeneral-purpose spatio-temporal predictors and trajectory reasoners.\nMethodologically, RHYTHM employs temporal tokenization to partition each\ntrajectory into daily segments and encode them as discrete tokens with\nhierarchical attention that captures both daily and weekly dependencies,\nthereby significantly reducing the sequence length while preserving cyclical\ninformation. Additionally, we enrich token representations by adding\npre-computed prompt embeddings for trajectory segments and prediction targets\nvia a frozen LLM, and feeding these combined embeddings back into the LLM\nbackbone to capture complex interdependencies. Computationally, RHYTHM freezes\nthe pretrained LLM's backbone to reduce attention complexity and memory cost.\nWe evaluate our model against state-of-the-art methods using three real-world\ndatasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a\n5.0% increase on weekends, and a 24.6% reduction in training time. Code is\npublicly available at https://github.com/he-h/rhythm.",
      "upvotes": 1,
      "discussionId": "68db5413d2bf1f4b15ec74f3",
      "githubRepo": "https://github.com/he-h/rhythm/tree/main",
      "ai_summary": "RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.",
      "ai_keywords": [
        "temporal tokenization",
        "hierarchical attention",
        "pre-computed prompt embeddings",
        "frozen LLM",
        "spatio-temporal predictors",
        "trajectory reasoners"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-09-27T00:55:56.000Z",
    "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human\n  Mobility",
    "summary": "Predicting human mobility is inherently challenging due to complex long-range\ndependencies and multi-scale periodic behaviors. To address this, we introduce\nRHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),\na unified framework that leverages large language models (LLMs) as\ngeneral-purpose spatio-temporal predictors and trajectory reasoners.\nMethodologically, RHYTHM employs temporal tokenization to partition each\ntrajectory into daily segments and encode them as discrete tokens with\nhierarchical attention that captures both daily and weekly dependencies,\nthereby significantly reducing the sequence length while preserving cyclical\ninformation. Additionally, we enrich token representations by adding\npre-computed prompt embeddings for trajectory segments and prediction targets\nvia a frozen LLM, and feeding these combined embeddings back into the LLM\nbackbone to capture complex interdependencies. Computationally, RHYTHM freezes\nthe pretrained LLM's backbone to reduce attention complexity and memory cost.\nWe evaluate our model against state-of-the-art methods using three real-world\ndatasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a\n5.0% increase on weekends, and a 24.6% reduction in training time. Code is\npublicly available at https://github.com/he-h/rhythm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ef0af2bfe4ead22ca8f69a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676610243576-noauth.jpeg",
      "fullname": "Haozheng Luo",
      "name": "robinzixuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "680e1986c980bb2d024cdfaa",
      "name": "northwestern-university",
      "fullname": "Northwestern University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680e15aacadcd7ce86cac9cf/0qOIptd897seA531DA6k2.png"
    },
    "isAuthorParticipating": true
  }
]