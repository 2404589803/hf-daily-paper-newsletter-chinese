[
  {
    "paper": {
      "id": "2507.15758",
      "authors": [
        {
          "_id": "687ef94833947f780d9b4ac7",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4ac8",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:07.023Z",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4ac9",
          "name": "Shangke Lyu",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4aca",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4acb",
          "name": "Yiwen Qiu",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4acc",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:05.190Z",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4acd",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4ace",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4acf",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "687ef94833947f780d9b4ad0",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T16:14:41.000Z",
      "submittedOnDailyAt": "2025-07-25T02:07:25.946Z",
      "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
      "upvotes": 15,
      "discussionId": "687ef94833947f780d9b4ad1",
      "githubRepo": "https://github.com/ZJU-REAL/HBPO",
      "githubStars": 10
    },
    "publishedAt": "2025-07-21T12:14:41.000Z",
    "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
    "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14958",
      "authors": [
        {
          "_id": "688332d5846d2e78b7548b6f",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b70",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b71",
          "name": "Rongman Xu",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b72",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b73",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b74",
          "name": "Haoran Luo",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b75",
          "name": "Xiaobao Wu",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b76",
          "name": "Luu Anh Tuan",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b77",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b78",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "688332d5846d2e78b7548b79",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T13:36:19.000Z",
      "submittedOnDailyAt": "2025-07-25T06:03:00.614Z",
      "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.",
      "upvotes": 13,
      "discussionId": "688332d6846d2e78b7548b7a",
      "githubRepo": "https://github.com/yayayacc/MUR",
      "ai_summary": "Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.",
      "ai_keywords": [
        "Large Language Models",
        "Test-Time Scaling",
        "Momentum Uncertainty-guided Reasoning",
        "reasoning budgets",
        "stepwise uncertainty",
        "gamma-control",
        "MATH-500",
        "AIME24",
        "AIME25",
        "GPQA-diamond",
        "Qwen3"
      ]
    },
    "publishedAt": "2025-07-20T09:36:19.000Z",
    "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18537",
      "authors": [
        {
          "_id": "6882ea76846d2e78b7548a40",
          "name": "Zhekai Chen",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a41",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a42",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a43",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a44",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a45",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "6882ea76846d2e78b7548a46",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T16:04:55.000Z",
      "submittedOnDailyAt": "2025-07-25T01:24:35.060Z",
      "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation",
      "submittedOnDailyBy": {
        "_id": "62d812e143df7719860d05d1",
        "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
        "isPro": false,
        "fullname": "zhekai chen",
        "user": "Azily",
        "type": "user"
      },
      "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.",
      "upvotes": 8,
      "discussionId": "6882ea76846d2e78b7548a47",
      "githubRepo": "https://github.com/ali-vilab/TTS-VAR",
      "ai_summary": "TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.",
      "ai_keywords": [
        "test-time scaling",
        "visual auto-regressive models",
        "path searching problem",
        "adaptive descending batch size schedule",
        "causal generation process",
        "clustering-based diversity search",
        "resampling-based potential selection",
        "potential scores",
        "reward functions",
        "multi-scale generation history",
        "Infinity model",
        "GenEval score"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-07-24T12:04:55.000Z",
    "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation",
    "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18537.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d812e143df7719860d05d1",
      "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
      "fullname": "zhekai chen",
      "name": "Azily",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18634",
      "authors": [
        {
          "_id": "6882f472846d2e78b7548a58",
          "name": "Junfei Xiao",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a59",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5a",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5b",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5c",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5d",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5e",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a5f",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a60",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "6882f472846d2e78b7548a61",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-25T01:35:44.053Z",
      "title": "Captain Cinema: Towards Short Movie Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
      "upvotes": 7,
      "discussionId": "6882f473846d2e78b7548a62",
      "projectPage": "https://thecinema.ai/",
      "ai_summary": "Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.",
      "ai_keywords": [
        "top-down keyframe planning",
        "bottom-up video synthesis",
        "Multimodal Diffusion Transformers",
        "MM-DiT",
        "long-context learning",
        "interleaved training strategy",
        "cinematic dataset",
        "visually coherent",
        "narrative consistent"
      ]
    },
    "publishedAt": "2025-07-24T13:59:56.000Z",
    "title": "Captain Cinema: Towards Short Movie Generation",
    "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18634.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16535",
      "authors": [
        {
          "_id": "688237c76a54dd1e77daa95c",
          "user": {
            "_id": "64de108815b7ebabdfa928d2",
            "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
            "isPro": false,
            "fullname": "Liu",
            "user": "ShuYaoLiu",
            "type": "user"
          },
          "name": "Shang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-24T14:11:16.075Z",
          "hidden": false
        },
        {
          "_id": "688237c76a54dd1e77daa95d",
          "name": "Chenjie Cao",
          "hidden": false
        },
        {
          "_id": "688237c76a54dd1e77daa95e",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "688237c76a54dd1e77daa95f",
          "name": "Wen Qian",
          "hidden": false
        },
        {
          "_id": "688237c76a54dd1e77daa960",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "688237c76a54dd1e77daa961",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T12:46:48.000Z",
      "submittedOnDailyAt": "2025-07-25T00:36:12.372Z",
      "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
      "submittedOnDailyBy": {
        "_id": "64de108815b7ebabdfa928d2",
        "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
        "isPro": false,
        "fullname": "Liu",
        "user": "ShuYaoLiu",
        "type": "user"
      },
      "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
      "upvotes": 4,
      "discussionId": "688237c76a54dd1e77daa962"
    },
    "publishedAt": "2025-07-22T08:46:48.000Z",
    "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
    "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16535.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de108815b7ebabdfa928d2",
      "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
      "fullname": "Liu",
      "name": "ShuYaoLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15844",
      "authors": [
        {
          "_id": "687ef9a833947f780d9b4ad3",
          "name": "Shangke Lyu",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad4",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad5",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:03.191Z",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad6",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad7",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad8",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:48:59.928Z",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ad9",
          "name": "Peisheng Jiang",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4ada",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4adb",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "687ef9a833947f780d9b4adc",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T17:52:34.000Z",
      "submittedOnDailyAt": "2025-07-25T02:09:28.395Z",
      "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
      "upvotes": 4,
      "discussionId": "687ef9a933947f780d9b4add",
      "githubRepo": "https://github.com/zju-real/hbpo",
      "githubStars": 10
    },
    "publishedAt": "2025-07-21T13:52:34.000Z",
    "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
    "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14988",
      "authors": [
        {
          "_id": "6882ea91846d2e78b7548a49",
          "name": "Yinghao Aaron Li",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4a",
          "name": "Xilin Jiang",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4b",
          "name": "Fei Tao",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4c",
          "name": "Cheng Niu",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4d",
          "name": "Kaifeng Xu",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4e",
          "name": "Juntong Song",
          "hidden": false
        },
        {
          "_id": "6882ea91846d2e78b7548a4f",
          "name": "Nima Mesgarani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T14:48:48.000Z",
      "submittedOnDailyAt": "2025-07-25T01:01:10.523Z",
      "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis",
      "submittedOnDailyBy": {
        "_id": "6531a65daed617662c7f1007",
        "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
        "isPro": false,
        "fullname": "Xilin Jiang",
        "user": "xi-j",
        "type": "user"
      },
      "summary": "Diffusion-based text-to-speech (TTS) systems have made remarkable progress in\nzero-shot speech synthesis, yet optimizing all components for perceptual\nmetrics remains challenging. Prior work with DMOSpeech demonstrated direct\nmetric optimization for speech generation components, but duration prediction\nremained unoptimized. This paper presents DMOSpeech 2, which extends metric\noptimization to the duration predictor through a reinforcement learning\napproach. The proposed system implements a novel duration policy framework\nusing group relative preference optimization (GRPO) with speaker similarity and\nword error rate as reward signals. By optimizing this previously unoptimized\ncomponent, DMOSpeech 2 creates a more complete metric-optimized synthesis\npipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid\napproach leveraging a teacher model for initial denoising steps before\ntransitioning to the student model, significantly improving output diversity\nwhile maintaining efficiency. Comprehensive evaluations demonstrate superior\nperformance across all metrics compared to previous systems, while reducing\nsampling steps by half without quality degradation. These advances represent a\nsignificant step toward speech synthesis systems with metric optimization\nacross multiple components. The audio samples, code and pre-trained models are\navailable at https://dmospeech2.github.io/.",
      "upvotes": 2,
      "discussionId": "6882ea92846d2e78b7548a50",
      "ai_summary": "DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.",
      "ai_keywords": [
        "diffusion-based text-to-speech",
        "DMOSpeech",
        "metric optimization",
        "duration prediction",
        "reinforcement learning",
        "group relative preference optimization",
        "GRPO",
        "speaker similarity",
        "word error rate",
        "teacher-guided sampling",
        "speech synthesis",
        "output diversity"
      ]
    },
    "publishedAt": "2025-07-20T10:48:48.000Z",
    "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis",
    "summary": "Diffusion-based text-to-speech (TTS) systems have made remarkable progress in\nzero-shot speech synthesis, yet optimizing all components for perceptual\nmetrics remains challenging. Prior work with DMOSpeech demonstrated direct\nmetric optimization for speech generation components, but duration prediction\nremained unoptimized. This paper presents DMOSpeech 2, which extends metric\noptimization to the duration predictor through a reinforcement learning\napproach. The proposed system implements a novel duration policy framework\nusing group relative preference optimization (GRPO) with speaker similarity and\nword error rate as reward signals. By optimizing this previously unoptimized\ncomponent, DMOSpeech 2 creates a more complete metric-optimized synthesis\npipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid\napproach leveraging a teacher model for initial denoising steps before\ntransitioning to the student model, significantly improving output diversity\nwhile maintaining efficiency. Comprehensive evaluations demonstrate superior\nperformance across all metrics compared to previous systems, while reducing\nsampling steps by half without quality degradation. These advances represent a\nsignificant step toward speech synthesis systems with metric optimization\nacross multiple components. The audio samples, code and pre-trained models are\navailable at https://dmospeech2.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14988.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6531a65daed617662c7f1007",
      "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
      "fullname": "Xilin Jiang",
      "name": "xi-j",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18192",
      "authors": [
        {
          "_id": "6882db3f846d2e78b7548a37",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a38",
          "name": "Guo-Hua Wang",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a39",
          "name": "Xiaohao Chen",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a3a",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a3b",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a3c",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6882db3f846d2e78b7548a3d",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T08:45:40.000Z",
      "submittedOnDailyAt": "2025-07-25T02:04:07.946Z",
      "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text\nEmbeddings Fusion), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6times faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.",
      "upvotes": 1,
      "discussionId": "6882db40846d2e78b7548a3e",
      "githubRepo": "https://github.com/AIDC-AI/TeEFusion",
      "ai_summary": "TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.",
      "ai_keywords": [
        "sampling strategies",
        "classifier-free guidance",
        "TeEFusion",
        "text embeddings",
        "distillation method",
        "linear operations",
        "student model",
        "teacher model",
        "inference speeds",
        "image quality",
        "SD3"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-24T04:45:40.000Z",
    "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
    "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text\nEmbeddings Fusion), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6times faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18103",
      "authors": [
        {
          "_id": "68832a97846d2e78b7548ad6",
          "name": "Riley Carlson",
          "hidden": false
        },
        {
          "_id": "68832a97846d2e78b7548ad7",
          "name": "John Bauer",
          "hidden": false
        },
        {
          "_id": "68832a97846d2e78b7548ad8",
          "name": "Christopher D. Manning",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T05:29:18.000Z",
      "submittedOnDailyAt": "2025-07-25T05:26:55.424Z",
      "title": "A New Pair of GloVes",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.",
      "upvotes": 1,
      "discussionId": "68832a97846d2e78b7548ad9",
      "ai_summary": "New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.",
      "ai_keywords": [
        "GloVe",
        "Global Vectors for Word Representation",
        "word embeddings",
        "Wikipedia",
        "Gigaword",
        "Dolma",
        "vocabulary comparison",
        "analogy",
        "similarity",
        "Named Entity Recognition",
        "NER"
      ]
    },
    "publishedAt": "2025-07-24T01:29:18.000Z",
    "title": "A New Pair of GloVes",
    "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2980
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18546",
      "authors": [
        {
          "_id": "688335fe846d2e78b7548ba7",
          "name": "Urchade Zaratiana",
          "hidden": false
        },
        {
          "_id": "688335fe846d2e78b7548ba8",
          "name": "Gil Pasternak",
          "hidden": false
        },
        {
          "_id": "688335fe846d2e78b7548ba9",
          "name": "Oliver Boyd",
          "hidden": false
        },
        {
          "_id": "688335fe846d2e78b7548baa",
          "name": "George Hurn-Maloney",
          "hidden": false
        },
        {
          "_id": "688335fe846d2e78b7548bab",
          "name": "Ash Lewis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/LkSJWb7jx2Js7OVwSPV9g.png"
      ],
      "publishedAt": "2025-07-24T16:11:14.000Z",
      "submittedOnDailyAt": "2025-07-25T06:20:20.273Z",
      "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
      "upvotes": 0,
      "discussionId": "688335fe846d2e78b7548bac",
      "ai_summary": "GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.",
      "ai_keywords": [
        "transformer encoder",
        "named entity recognition",
        "text classification",
        "hierarchical structured data extraction",
        "multi-task composition",
        "schema-based interface"
      ]
    },
    "publishedAt": "2025-07-24T12:11:14.000Z",
    "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
    "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/LkSJWb7jx2Js7OVwSPV9g.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18546.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2980
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15595",
      "authors": [
        {
          "_id": "68833580846d2e78b7548b98",
          "name": "Salah Eddine Bekhouche",
          "hidden": false
        },
        {
          "_id": "68833580846d2e78b7548b99",
          "name": "Gaby Maroun",
          "hidden": false
        },
        {
          "_id": "68833580846d2e78b7548b9a",
          "name": "Fadi Dornaika",
          "hidden": false
        },
        {
          "_id": "68833580846d2e78b7548b9b",
          "name": "Abdenour Hadid",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/cDOcgi2KvA5oqyayMqsZh.png"
      ],
      "publishedAt": "2025-07-21T13:18:05.000Z",
      "submittedOnDailyAt": "2025-07-25T06:17:55.434Z",
      "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging",
      "submittedOnDailyBy": {
        "_id": "63e7493fdb40d9e67feaccdd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
        "isPro": false,
        "fullname": "Salah Eddine Bekhouche",
        "user": "Bekhouche",
        "type": "user"
      },
      "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\nhttps://github.com/Bekhouche/SegDT{GitHub}.",
      "upvotes": 0,
      "discussionId": "68833580846d2e78b7548b9c",
      "githubRepo": "https://github.com/Bekhouche/SegDT/",
      "ai_summary": "SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.",
      "ai_keywords": [
        "diffusion transformer",
        "Rectified Flow",
        "medical image segmentation",
        "skin lesions",
        "skin cancer",
        "benchmarking datasets",
        "inference speeds"
      ]
    },
    "publishedAt": "2025-07-21T09:18:05.000Z",
    "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging",
    "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\nhttps://github.com/Bekhouche/SegDT{GitHub}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/cDOcgi2KvA5oqyayMqsZh.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e7493fdb40d9e67feaccdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
      "fullname": "Salah Eddine Bekhouche",
      "name": "Bekhouche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]