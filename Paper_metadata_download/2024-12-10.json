[
    {
        "paper": {
            "id": "2412.06531",
            "authors": [
                {
                    "_id": "67583ace95bb6a9c895df272",
                    "user": {
                        "_id": "6668687caee0993c95b0eb81",
                        "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
                        "isPro": false,
                        "fullname": "Egor Cherepanov",
                        "user": "avanturist",
                        "type": "user"
                    },
                    "name": "Egor Cherepanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T13:06:34.114Z",
                    "hidden": false
                },
                {
                    "_id": "67583ace95bb6a9c895df273",
                    "user": {
                        "_id": "64705ef84be5cf1f3348e283",
                        "avatarUrl": "/avatars/915875e7c4118098ab460831d5e8ef0e.svg",
                        "isPro": false,
                        "fullname": "Nikita",
                        "user": "tttonyalpha",
                        "type": "user"
                    },
                    "name": "Nikita Kachaev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T15:15:21.788Z",
                    "hidden": false
                },
                {
                    "_id": "67583ace95bb6a9c895df274",
                    "user": {
                        "_id": "6267cdd3f91d1c1633c08bbf",
                        "avatarUrl": "/avatars/51d7961098f52f39ee406009a12982b8.svg",
                        "isPro": false,
                        "fullname": "Artem Zholus",
                        "user": "artemZholus",
                        "type": "user"
                    },
                    "name": "Artem Zholus",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:07:54.862Z",
                    "hidden": false
                },
                {
                    "_id": "67583ace95bb6a9c895df275",
                    "user": {
                        "_id": "64198f70ed725fef6442b37e",
                        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
                        "isPro": false,
                        "fullname": "Alexey Kovalev",
                        "user": "AlexeyKov",
                        "type": "user"
                    },
                    "name": "Alexey K. Kovalev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:07:49.350Z",
                    "hidden": false
                },
                {
                    "_id": "67583ace95bb6a9c895df276",
                    "user": {
                        "_id": "6633243f5ddb7702ad3ec216",
                        "avatarUrl": "/avatars/0abaf60f1a52e148c3f6fce57eb31eb4.svg",
                        "isPro": false,
                        "fullname": "Aleksandr Panov",
                        "user": "grafft",
                        "type": "user"
                    },
                    "name": "Aleksandr I. Panov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:07:02.724Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T14:34:31.000Z",
            "title": "Unraveling the Complexity of Memory in RL Agents: an Approach for\n  Classification and Evaluation",
            "summary": "The incorporation of memory into agents is essential for numerous tasks\nwithin the domain of Reinforcement Learning (RL). In particular, memory is\nparamount for tasks that require the utilization of past information,\nadaptation to novel environments, and improved sample efficiency. However, the\nterm ``memory'' encompasses a wide range of concepts, which, coupled with the\nlack of a unified methodology for validating an agent's memory, leads to\nerroneous judgments about agents' memory capabilities and prevents objective\ncomparison with other memory-enhanced agents. This paper aims to streamline the\nconcept of memory in RL by providing practical precise definitions of agent\nmemory types, such as long-term versus short-term memory and declarative versus\nprocedural memory, inspired by cognitive science. Using these definitions, we\ncategorize different classes of agent memory, propose a robust experimental\nmethodology for evaluating the memory capabilities of RL agents, and\nstandardize evaluations. Furthermore, we empirically demonstrate the importance\nof adhering to the proposed methodology when evaluating different types of\nagent memory by conducting experiments with different RL agents and what its\nviolation leads to.",
            "upvotes": 48,
            "discussionId": "67583ad095bb6a9c895df2c9"
        },
        "publishedAt": "2024-12-10T07:59:16.604Z",
        "title": "Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06531.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6668687caee0993c95b0eb81",
            "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
            "fullname": "Egor Cherepanov",
            "name": "avanturist",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2412.06559",
            "authors": [
                {
                    "_id": "6757e135887c48188a1f5015",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:17.928Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f5016",
                    "user": {
                        "_id": "652f13fa6504631c10f308ed",
                        "avatarUrl": "/avatars/440de01158c1d9152afbf33617fe27d1.svg",
                        "isPro": false,
                        "fullname": "zhenrui",
                        "user": "zhenrui",
                        "type": "user"
                    },
                    "name": "Zhenru Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:13:54.636Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f5017",
                    "name": "Beichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f5018",
                    "user": {
                        "_id": "649a52e5de0fb7f3f499e583",
                        "avatarUrl": "/avatars/25f6106fa168ae57ad5cd8ef55c70d31.svg",
                        "isPro": false,
                        "fullname": "Runji Lin",
                        "user": "RunjiLin",
                        "type": "user"
                    },
                    "name": "Runji Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:16.002Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f5019",
                    "name": "Keming Lu",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f501a",
                    "user": {
                        "_id": "6438b43ab2ea24b52ebac2b9",
                        "avatarUrl": "/avatars/84133cd719a4b1e2f5c1a74178425f86.svg",
                        "isPro": false,
                        "fullname": "Bowen Yu",
                        "user": "bwy",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:12:57.169Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f501b",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:12:41.742Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f501c",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:12:36.306Z",
                    "hidden": false
                },
                {
                    "_id": "6757e135887c48188a1f501d",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:12:26.676Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T15:11:40.000Z",
            "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
            "summary": "As language models regularly make mistakes when solving math problems,\nautomated identification of errors in the reasoning process becomes\nincreasingly significant for their scalable oversight. In this paper, we\nintroduce ProcessBench for measuring the ability to identify erroneous steps in\nmathematical reasoning. It consists of 3,400 test cases, primarily focused on\ncompetition- and Olympiad-level math problems. Each test case contains a\nstep-by-step solution with error location annotated by human experts. Models\nare required to identify the earliest step that contains an error, or conclude\nthat all steps are correct. We conduct extensive evaluation on ProcessBench,\ninvolving two types of models: process reward models (PRMs) and critic models,\nwhere for the latter we prompt general language models to critique each\nsolution step by step. We draw two main observations: (1) Existing PRMs\ntypically fail to generalize to more challenging math problems beyond GSM8K and\nMATH. They underperform both critic models (i.e., prompted general language\nmodels) and our own trained PRM that is straightforwardly fine-tuned on the\nPRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has\ndemonstrated the critique capability competitive with the proprietary model\nGPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We\nhope ProcessBench can foster future research in reasoning process assessment,\npaving the way toward scalable oversight of language models.",
            "upvotes": 33,
            "discussionId": "6757e136887c48188a1f503f"
        },
        "publishedAt": "2024-12-10T01:38:20.551Z",
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06559.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "fullname": "Chujie Zheng",
            "name": "chujiezheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 22
        }
    },
    {
        "paper": {
            "id": "2412.06769",
            "authors": [
                {
                    "_id": "6757e0f3411d2c9cb4906fef",
                    "user": {
                        "_id": "660ee5df35d092e3fc2a3685",
                        "avatarUrl": "/avatars/a7e0472fb7ea49973f74e3eea13dc964.svg",
                        "isPro": false,
                        "fullname": "Shibo Hao",
                        "user": "Shibo-UCSD",
                        "type": "user"
                    },
                    "name": "Shibo Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:15:26.794Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff0",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:15:21.216Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff1",
                    "name": "DiJia Su",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff2",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff3",
                    "user": {
                        "_id": "665bfa1b0d71762b8613282d",
                        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
                        "isPro": false,
                        "fullname": "Zhiting Hu",
                        "user": "zhitinghu",
                        "type": "user"
                    },
                    "name": "Zhiting Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:15:00.114Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff4",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:14:54.266Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0f3411d2c9cb4906ff5",
                    "user": {
                        "_id": "6344cf73ee1504dbcd5bdfe7",
                        "avatarUrl": "/avatars/6dd2bf1f9c5679e5c8c85d62c9836aac.svg",
                        "isPro": false,
                        "fullname": "Yuandong Tian",
                        "user": "tydsh",
                        "type": "user"
                    },
                    "name": "Yuandong Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:14:48.475Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T18:55:56.000Z",
            "title": "Training Large Language Models to Reason in a Continuous Latent Space",
            "summary": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
            "upvotes": 15,
            "discussionId": "6757e0f7411d2c9cb49070fe"
        },
        "publishedAt": "2024-12-10T01:34:48.167Z",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06769.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "660ee5df35d092e3fc2a3685",
            "avatarUrl": "/avatars/a7e0472fb7ea49973f74e3eea13dc964.svg",
            "fullname": "Shibo Hao",
            "name": "Shibo-UCSD",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.05939",
            "authors": [
                {
                    "_id": "6757a3cef4cf69dc8e379cae",
                    "user": {
                        "_id": "616fb788e2ad27af26561b1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Xu",
                        "user": "LooperXX",
                        "type": "user"
                    },
                    "name": "Xiao Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:23.468Z",
                    "hidden": false
                },
                {
                    "_id": "6757a3cef4cf69dc8e379caf",
                    "name": "Tianhao Niu",
                    "hidden": false
                },
                {
                    "_id": "6757a3cef4cf69dc8e379cb0",
                    "user": {
                        "_id": "647730d403fe88eff54d9b24",
                        "avatarUrl": "/avatars/66a41a42a4f9adaca7477b73ab1a0c0e.svg",
                        "isPro": false,
                        "fullname": "Sigrid Xie",
                        "user": "yuxixie",
                        "type": "user"
                    },
                    "name": "Yuxi Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:11.040Z",
                    "hidden": false
                },
                {
                    "_id": "6757a3cef4cf69dc8e379cb1",
                    "user": {
                        "_id": "63e8f4c0ccae1fe5c61b636f",
                        "avatarUrl": "/avatars/013e124e37b6194d5564a1db9aad2b20.svg",
                        "isPro": false,
                        "fullname": "qin",
                        "user": "liboqin",
                        "type": "user"
                    },
                    "name": "Libo Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:17.307Z",
                    "hidden": false
                },
                {
                    "_id": "6757a3cef4cf69dc8e379cb2",
                    "user": {
                        "_id": "6325418ae5d49a0d40f66176",
                        "avatarUrl": "/avatars/59a0e29ce09e9113edc53cebb728b099.svg",
                        "isPro": false,
                        "fullname": "Wanxiang Che",
                        "user": "wanxiangche",
                        "type": "user"
                    },
                    "name": "Wanxiang Che",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:23.487Z",
                    "hidden": false
                },
                {
                    "_id": "6757a3cef4cf69dc8e379cb3",
                    "user": {
                        "_id": "628f137053028c04a3b750e4",
                        "avatarUrl": "/avatars/85d174cc041ab91e724bb9d9d4e46c88.svg",
                        "isPro": false,
                        "fullname": "Min-Yen Kan",
                        "user": "knmnyn",
                        "type": "user"
                    },
                    "name": "Min-Yen Kan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:28.903Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-08T13:45:44.000Z",
            "title": "Exploring Multi-Grained Concept Annotations for Multimodal Large\n  Language Models",
            "summary": "Multimodal Large Language Models (MLLMs) excel in vision--language tasks by\npre-training solely on coarse-grained concept annotations (e.g., image\ncaptions). We hypothesize that integrating fine-grained concept annotations\n(e.g., object labels and object regions) will further improve performance, as\nboth data granularities complement each other in terms of breadth and depth in\nconcept representation. We introduce a new dataset featuring Multimodal\nMulti-Grained Concept annotations (MMGiC) for MLLMs. In constructing MMGiC, we\nexplore the impact of different data recipes on multimodal comprehension and\ngeneration. Our analyses reveal that multi-grained concept annotations\nintegrate and complement each other, under our structured template and a\ngeneral MLLM framework. We clearly explore and demonstrate the potential of\nMMGiC to help MLLMs better locate and learn concepts, aligning vision and\nlanguage at multiple granularities. We further validate our hypothesis by\ninvestigating the fair comparison and effective collaboration between MMGiC and\nimage--caption data on 12 multimodal comprehension and generation benchmarks,\ne.g., their appropriate combination achieve 3.95% and 2.34% absolute\nimprovements over image--caption data alone on POPE and SEED-Bench. Code, data\nand models will be available at https://github.com/LooperXX/MMGiC.",
            "upvotes": 8,
            "discussionId": "6757a3cff4cf69dc8e379d3b"
        },
        "publishedAt": "2024-12-10T04:20:10.922Z",
        "title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05939.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "616fb788e2ad27af26561b1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
            "fullname": "Xiao Xu",
            "name": "LooperXX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        }
    },
    {
        "paper": {
            "id": "2412.04432",
            "authors": [
                {
                    "_id": "675260813c0a3886978b2e48",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:42.443Z",
                    "hidden": false
                },
                {
                    "_id": "675260813c0a3886978b2e49",
                    "user": {
                        "_id": "630b094f8b327c7b8b94d24c",
                        "avatarUrl": "/avatars/f2e1e9b1c9af9856298f792a7872e224.svg",
                        "isPro": false,
                        "fullname": "Yizhuo Li",
                        "user": "liyz",
                        "type": "user"
                    },
                    "name": "Yizhuo Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:48.137Z",
                    "hidden": false
                },
                {
                    "_id": "675260813c0a3886978b2e4a",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:16:54.425Z",
                    "hidden": false
                },
                {
                    "_id": "675260813c0a3886978b2e4b",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:17:01.613Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T18:53:04.000Z",
            "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
            "summary": "In recent years, there has been a significant surge of interest in unifying\nimage comprehension and generation within Large Language Models (LLMs). This\ngrowing interest has prompted us to explore extending this unification to\nvideos. The core challenge lies in developing a versatile video tokenizer that\ncaptures both the spatial characteristics and temporal dynamics of videos to\nobtain representations for LLMs, and the representations can be further decoded\ninto realistic video clips to enable video generation. In this work, we\nintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the\ndiffusion process for self-supervised video representation learning. We posit\nthat if a video diffusion model can effectively de-noise video clips by taking\nthe features of a video tokenizer as the condition, then the tokenizer has\nsuccessfully captured robust spatial and temporal information. Additionally,\nthe video diffusion model inherently functions as a de-tokenizer, decoding\nvideos from their representations. Building upon the Divot tokenizer, we\npresent Divot-Vicuna through video-to-text autoregression and text-to-video\ngeneration by modeling the distributions of continuous-valued Divot features\nwith a Gaussian Mixture Model. Experimental results demonstrate that our\ndiffusion-based video tokenizer, when integrated with a pre-trained LLM,\nachieves competitive performance across various video comprehension and\ngeneration benchmarks. The instruction tuned Divot-Vicuna also excels in video\nstorytelling, generating interleaved narratives and corresponding videos.",
            "upvotes": 7,
            "discussionId": "675260823c0a3886978b2edb"
        },
        "publishedAt": "2024-12-10T06:42:48.585Z",
        "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04432.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "fullname": "Yuying Ge",
            "name": "tttoaster",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.06699",
            "authors": [
                {
                    "_id": "6757e5546adb3cd340be4820",
                    "user": {
                        "_id": "642e9e28ccdcf5da7f978c3e",
                        "avatarUrl": "/avatars/fb30b4ea96d5a9ad743d24dae090b7a0.svg",
                        "isPro": false,
                        "fullname": "m",
                        "user": "bruiiii",
                        "type": "user"
                    },
                    "name": "Baorui Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:10.775Z",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4821",
                    "user": {
                        "_id": "6551e04e286b72eb7c8b1d4f",
                        "avatarUrl": "/avatars/e36cbaf02f8b56823d81c747df60abf1.svg",
                        "isPro": false,
                        "fullname": "Huachen Gao",
                        "user": "Infinite888",
                        "type": "user"
                    },
                    "name": "Huachen Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:03.987Z",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4822",
                    "user": {
                        "_id": "64dd9404f2c8f66d52604fb3",
                        "avatarUrl": "/avatars/17d6702a2f2e390925383eadda6642ca.svg",
                        "isPro": false,
                        "fullname": "Haoge Deng",
                        "user": "Bitterdhg",
                        "type": "user"
                    },
                    "name": "Haoge Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:11:22.594Z",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4823",
                    "name": "Zhengxiong Luo",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4824",
                    "name": "Tiejun Huang",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4825",
                    "name": "Lulu Tang",
                    "hidden": false
                },
                {
                    "_id": "6757e5546adb3cd340be4826",
                    "name": "Xinlong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T17:44:56.000Z",
            "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at\n  Scale",
            "summary": "Recent 3D generation models typically rely on limited-scale 3D `gold-labels'\nor 2D diffusion priors for 3D content creation. However, their performance is\nupper-bounded by constrained 3D priors due to the lack of scalable learning\nparadigms. In this work, we present See3D, a visual-conditional multi-view\ndiffusion model trained on large-scale Internet videos for open-world 3D\ncreation. The model aims to Get 3D knowledge by solely Seeing the visual\ncontents from the vast and rapidly growing video data -- You See it, You Got\nit. To achieve this, we first scale up the training data using a proposed data\ncuration pipeline that automatically filters out multi-view inconsistencies and\ninsufficient observations from source videos. This results in a high-quality,\nrichly diverse, large-scale dataset of multi-view images, termed WebVi3D,\ncontaining 320M frames from 16M video clips. Nevertheless, learning generic 3D\npriors from videos without explicit 3D geometry or camera pose annotations is\nnontrivial, and annotating poses for web-scale videos is prohibitively\nexpensive. To eliminate the need for pose conditions, we introduce an\ninnovative visual-condition - a purely 2D-inductive visual signal generated by\nadding time-dependent noise to the masked video data. Finally, we introduce a\nnovel visual-conditional 3D generation framework by integrating See3D into a\nwarping-based pipeline for high-fidelity 3D generation. Our numerical and\nvisual comparisons on single and sparse reconstruction benchmarks show that\nSee3D, trained on cost-effective and scalable video data, achieves notable\nzero-shot and open-world generation capabilities, markedly outperforming models\ntrained on costly and constrained 3D datasets. Please refer to our project page\nat: https://vision.baai.ac.cn/see3d",
            "upvotes": 7,
            "discussionId": "6757e5596adb3cd340be49aa"
        },
        "publishedAt": "2024-12-10T03:28:19.173Z",
        "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06699.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ca558304c979828311c5a5",
            "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
            "fullname": "Xinlong Wang",
            "name": "xinlongwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        }
    },
    {
        "paper": {
            "id": "2412.06781",
            "authors": [
                {
                    "_id": "6758588398dbc91f46c532b5",
                    "user": {
                        "_id": "630652803aed65d34e98eee3",
                        "avatarUrl": "/avatars/97b838cfb2a67d74f9d7e4defbc69994.svg",
                        "isPro": false,
                        "fullname": "Nicolas Dufour",
                        "user": "nicolas-dufour",
                        "type": "user"
                    },
                    "name": "Nicolas Dufour",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T16:12:23.515Z",
                    "hidden": false
                },
                {
                    "_id": "6758588398dbc91f46c532b6",
                    "user": {
                        "_id": "6385b65fde54faafdea46f0b",
                        "avatarUrl": "/avatars/aba9aefa1dfa3d61ee3487b375a48ed5.svg",
                        "isPro": false,
                        "fullname": "David",
                        "user": "davidpicard",
                        "type": "user"
                    },
                    "name": "David Picard",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:17:26.420Z",
                    "hidden": false
                },
                {
                    "_id": "6758588398dbc91f46c532b7",
                    "name": "Vicky Kalogeiton",
                    "hidden": false
                },
                {
                    "_id": "6758588398dbc91f46c532b8",
                    "name": "Loic Landrieu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T18:59:04.000Z",
            "title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual\n  Geolocation",
            "summary": "Global visual geolocation predicts where an image was captured on Earth.\nSince images vary in how precisely they can be localized, this task inherently\ninvolves a significant degree of ambiguity. However, existing approaches are\ndeterministic and overlook this aspect. In this paper, we aim to close the gap\nbetween traditional geolocalization and modern generative methods. We propose\nthe first generative geolocation approach based on diffusion and Riemannian\nflow matching, where the denoising process operates directly on the Earth's\nsurface. Our model achieves state-of-the-art performance on three visual\ngeolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition,\nwe introduce the task of probabilistic visual geolocation, where the model\npredicts a probability distribution over all possible locations instead of a\nsingle point. We introduce new metrics and baselines for this task,\ndemonstrating the advantages of our diffusion-based approach. Codes and models\nwill be made available.",
            "upvotes": 6,
            "discussionId": "6758588498dbc91f46c53323"
        },
        "publishedAt": "2024-12-10T10:05:19.517Z",
        "title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06781.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630652803aed65d34e98eee3",
            "avatarUrl": "/avatars/97b838cfb2a67d74f9d7e4defbc69994.svg",
            "fullname": "Nicolas Dufour",
            "name": "nicolas-dufour",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.03123",
            "authors": [
                {
                    "_id": "6756459a8e874bcf29364ca1",
                    "user": {
                        "_id": "675644c7d83c390221b45a61",
                        "avatarUrl": "/avatars/ee2d659662e39a9adf3ea859fbb8d051.svg",
                        "isPro": false,
                        "fullname": "Xiaojun Xu",
                        "user": "xiaojunxu",
                        "type": "user"
                    },
                    "name": "Xiaojun Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-09T11:06:12.238Z",
                    "hidden": false
                },
                {
                    "_id": "6756459a8e874bcf29364ca2",
                    "user": {
                        "_id": "62b35eb9003cd12329d748a4",
                        "avatarUrl": "/avatars/871cedeb9aa62c5ac57d54287b12afa7.svg",
                        "isPro": false,
                        "fullname": "Jinghan Jia",
                        "user": "flyingbugs",
                        "type": "user"
                    },
                    "name": "Jinghan Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:10:20.552Z",
                    "hidden": false
                },
                {
                    "_id": "6756459a8e874bcf29364ca3",
                    "name": "Yuanshun Yao",
                    "hidden": false
                },
                {
                    "_id": "6756459a8e874bcf29364ca4",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6756459a8e874bcf29364ca5",
                    "user": {
                        "_id": "61342a4b488458a484dee6c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1630808595161-noauth.png",
                        "isPro": false,
                        "fullname": "Hang Li",
                        "user": "hanglics",
                        "type": "user"
                    },
                    "name": "Hang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:10:59.902Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-04T08:43:12.000Z",
            "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
            "summary": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing\nwith LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave\ndifferently so that their paraphrasing difference reflected in the text\nsemantics can be identified by a trained decoder. To embed our multi-bit\nwatermark, we use two paraphrasers alternatively to encode the pre-defined\nbinary code at the sentence level. Then we use a text classifier as the decoder\nto decode each bit of the watermark. Through extensive experiments, we show\nthat our watermarks can achieve over 99.99\\% detection AUC with small (1.1B)\ntext paraphrasers while keeping the semantic information of the original\nsentence. More importantly, our pipeline is robust under word substitution and\nsentence paraphrasing perturbations and generalizes well to\nout-of-distributional data. We also show the stealthiness of our watermark with\nLLM-based evaluation. We open-source the code:\nhttps://github.com/xiaojunxu/multi-bit-text-watermark.",
            "upvotes": 5,
            "discussionId": "6756459b8e874bcf29364cd9"
        },
        "publishedAt": "2024-12-10T02:20:35.237Z",
        "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03123.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "675644c7d83c390221b45a61",
            "avatarUrl": "/avatars/ee2d659662e39a9adf3ea859fbb8d051.svg",
            "fullname": "Xiaojun Xu",
            "name": "xiaojunxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.05600",
            "authors": [
                {
                    "_id": "6757e3ad6b8a608297ec0356",
                    "user": {
                        "_id": "6304c06eeb6d777a838eab63",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678741407493-6304c06eeb6d777a838eab63.png",
                        "isPro": false,
                        "fullname": "Mikolaj Czerkawski",
                        "user": "mikonvergence",
                        "type": "user"
                    },
                    "name": "Mikolaj Czerkawski",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:13.544Z",
                    "hidden": false
                },
                {
                    "_id": "6757e3ad6b8a608297ec0357",
                    "user": {
                        "_id": "66aa04a381dd5b0fe363b4da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aa04a381dd5b0fe363b4da/2bJIilWC1Oqm323qSrMO2.png",
                        "isPro": false,
                        "fullname": "Marcin Kluczek",
                        "user": "mkluczek",
                        "type": "user"
                    },
                    "name": "Marcin Kluczek",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-12-10T06:46:11.399Z",
                    "hidden": false
                },
                {
                    "_id": "6757e3ad6b8a608297ec0358",
                    "name": "JÄ™drzej S. Bojanowski",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-07T09:49:47.000Z",
            "title": "Global and Dense Embeddings of Earth: Major TOM Floating in the Latent\n  Space",
            "summary": "With the ever-increasing volumes of the Earth observation data present in the\narchives of large programmes such as Copernicus, there is a growing need for\nefficient vector representations of the underlying raw data. The approach of\nextracting feature representations from pretrained deep neural networks is a\npowerful approach that can provide semantic abstractions of the input data.\nHowever, the way this is done for imagery archives containing geospatial data\nhas not yet been defined. In this work, an extension is proposed to an existing\ncommunity project, Major TOM, focused on the provision and standardization of\nopen and free AI-ready datasets for Earth observation. Furthermore, four global\nand dense embedding datasets are released openly and for free along with the\npublication of this manuscript, resulting in the most comprehensive global open\ndataset of geospatial visual embeddings in terms of covered Earth's surface.",
            "upvotes": 4,
            "discussionId": "6757e3b36b8a608297ec04e6"
        },
        "publishedAt": "2024-12-10T08:29:16.483Z",
        "title": "Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05600.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6304c06eeb6d777a838eab63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678741407493-6304c06eeb6d777a838eab63.png",
            "fullname": "Mikolaj Czerkawski",
            "name": "mikonvergence",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 26
        }
    },
    {
        "paper": {
            "id": "2412.06782",
            "authors": [
                {
                    "_id": "6757e0d412d7d9e5dcffaf95",
                    "user": {
                        "_id": "64cc96cb275c76304617ba4e",
                        "avatarUrl": "/avatars/abf0c7561515670e53954b9602c93722.svg",
                        "isPro": false,
                        "fullname": "zhefei gong",
                        "user": "gonzalezZzZ",
                        "type": "user"
                    },
                    "name": "Zhefei Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:08:37.754Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf96",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf97",
                    "name": "Shangke Lyu",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf98",
                    "user": {
                        "_id": "65fd82762bf2cd20ddaa193f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                        "isPro": false,
                        "fullname": "Siteng Huang",
                        "user": "huangsiteng",
                        "type": "user"
                    },
                    "name": "Siteng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:09:32.865Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf99",
                    "name": "Mingyang Sun",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf9a",
                    "name": "Wei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf9b",
                    "user": {
                        "_id": "6462eab02538819c72a0b2cc",
                        "avatarUrl": "/avatars/b4ece21ce35da7506db9d8bef4abcf68.svg",
                        "isPro": false,
                        "fullname": "Zhaoxin Fan",
                        "user": "Jason007x",
                        "type": "user"
                    },
                    "name": "Zhaoxin Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T13:10:01.836Z",
                    "hidden": false
                },
                {
                    "_id": "6757e0d412d7d9e5dcffaf9c",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-09T18:59:18.000Z",
            "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive\n  Prediction",
            "summary": "In robotic visuomotor policy learning, diffusion-based models have achieved\nsignificant success in improving the accuracy of action trajectory generation\ncompared to traditional autoregressive models. However, they suffer from\ninefficiency due to multiple denoising steps and limited flexibility from\ncomplex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive\nPolicy (CARP), a novel paradigm for visuomotor policy learning that redefines\nthe autoregressive action generation process as a coarse-to-fine, next-scale\napproach. CARP decouples action generation into two stages: first, an action\nautoencoder learns multi-scale representations of the entire action sequence;\nthen, a GPT-style transformer refines the sequence prediction through a\ncoarse-to-fine autoregressive process. This straightforward and intuitive\napproach produces highly accurate and smooth actions, matching or even\nsurpassing the performance of diffusion-based policies while maintaining\nefficiency on par with autoregressive policies. We conduct extensive\nevaluations across diverse settings, including single-task and multi-task\nscenarios on state-based and image-based simulation benchmarks, as well as\nreal-world tasks. CARP achieves competitive success rates, with up to a 10%\nimprovement, and delivers 10x faster inference compared to state-of-the-art\npolicies, establishing a high-performance, efficient, and flexible paradigm for\naction generation in robotic tasks.",
            "upvotes": 3,
            "discussionId": "6757e0d712d7d9e5dcffb077"
        },
        "publishedAt": "2024-12-10T01:34:12.842Z",
        "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06782.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.04144",
            "authors": [
                {
                    "_id": "6757f0c88cf3fb3bbeba6b2d",
                    "user": {
                        "_id": "5f350fe67e5835433862161b",
                        "avatarUrl": "/avatars/8aecfdea5202968b1099412800a152e0.svg",
                        "isPro": false,
                        "fullname": "Muhammad Khalifa",
                        "user": "mkhalifa",
                        "type": "user"
                    },
                    "name": "Muhammad Khalifa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T10:25:02.075Z",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b2e",
                    "name": "Yi-Chern Tan",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b2f",
                    "name": "Arash Ahmadian",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b30",
                    "name": "Tom Hosking",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b31",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b32",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b33",
                    "name": "Ahmet ÃœstÃ¼n",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b34",
                    "name": "Tom Sherborne",
                    "hidden": false
                },
                {
                    "_id": "6757f0c88cf3fb3bbeba6b35",
                    "name": "Matthias GallÃ©",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-05T13:12:51.000Z",
            "title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale\n  Mitigates Performance Tradeoffs",
            "summary": "Model merging has shown great promise at combining expert models, but the\nbenefit of merging is unclear when merging ``generalist'' models trained on\nmany tasks. We explore merging in the context of large (sim100B) models, by\nrecycling checkpoints that exhibit tradeoffs among different tasks.\nSuch checkpoints are often created in the process of developing a frontier\nmodel, and many suboptimal ones are usually discarded. Given a pool of model\ncheckpoints obtained from different training runs (e.g., different stages,\nobjectives, hyperparameters, and data mixtures), which naturally show tradeoffs\nacross different language capabilities (e.g., instruction following vs. code\ngeneration), we investigate whether merging can recycle such suboptimal models\ninto a Pareto-optimal one. Our optimization algorithm tunes the weight of each\ncheckpoint in a linear combination, resulting in a Pareto-optimal models that\noutperforms both individual models and merge-based baselines. Further analysis\nshows that good merges tend to include almost all checkpoints with with\nnon-zero weights, indicating that even seemingly bad initial checkpoints can\ncontribute to good final merges.",
            "upvotes": 1,
            "discussionId": "6757f0c98cf3fb3bbeba6b6b"
        },
        "publishedAt": "2024-12-10T11:49:39.263Z",
        "title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04144.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f350fe67e5835433862161b",
            "avatarUrl": "/avatars/8aecfdea5202968b1099412800a152e0.svg",
            "fullname": "Muhammad Khalifa",
            "name": "mkhalifa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.05355",
            "authors": [
                {
                    "_id": "675853a66bb8fc7121d27886",
                    "user": {
                        "_id": "63597e84d72fc0539e72b507",
                        "avatarUrl": "/avatars/f568302b70064220e3c824577e5bece4.svg",
                        "isPro": false,
                        "fullname": "Hidir Yesiltepe",
                        "user": "Hidir",
                        "type": "user"
                    },
                    "name": "Hidir Yesiltepe",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-10T15:15:16.912Z",
                    "hidden": false
                },
                {
                    "_id": "675853a66bb8fc7121d27887",
                    "user": {
                        "_id": "64f8b03f83807928d25e766f",
                        "avatarUrl": "/avatars/68fd4ee967a1673a1d78a7581be8b3da.svg",
                        "isPro": false,
                        "fullname": "Tuna Han Salih Meral",
                        "user": "tmeral",
                        "type": "user"
                    },
                    "name": "Tuna Han Salih Meral",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T15:18:51.292Z",
                    "hidden": false
                },
                {
                    "_id": "675853a66bb8fc7121d27888",
                    "user": {
                        "_id": "65302b5769e5dbd6a2d4b4ee",
                        "avatarUrl": "/avatars/cac79a5fb051a539ffaa15596974673b.svg",
                        "isPro": false,
                        "fullname": "Connor Dunlop",
                        "user": "cdunlop",
                        "type": "user"
                    },
                    "name": "Connor Dunlop",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-10T16:21:14.722Z",
                    "hidden": false
                },
                {
                    "_id": "675853a66bb8fc7121d27889",
                    "name": "Pinar Yanardag",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-06T18:59:17.000Z",
            "title": "MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with\n  Mixture of Score Guidance",
            "summary": "In this work, we propose the first motion transfer approach in diffusion\ntransformer through Mixture of Score Guidance (MSG), a theoretically-grounded\nframework for motion transfer in diffusion models. Our key theoretical\ncontribution lies in reformulating conditional score to decompose motion score\nand content score in diffusion models. By formulating motion transfer as a\nmixture of potential energies, MSG naturally preserves scene composition and\nenables creative scene transformations while maintaining the integrity of\ntransferred motion patterns. This novel sampling operates directly on\npre-trained video diffusion models without additional training or fine-tuning.\nThrough extensive experiments, MSG demonstrates successful handling of diverse\nscenarios including single object, multiple objects, and cross-object motion\ntransfer as well as complex camera motion transfer. Additionally, we introduce\nMotionBench, the first motion transfer dataset consisting of 200 source videos\nand 1000 transferred motions, covering single/multi-object transfers, and\ncomplex camera motions.",
            "upvotes": 0,
            "discussionId": "675853a86bb8fc7121d2790b"
        },
        "publishedAt": "2024-12-10T09:48:26.424Z",
        "title": "MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63597e84d72fc0539e72b507/ZR00a6Y7ygfym5h82MpIq.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05355.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63597e84d72fc0539e72b507",
            "avatarUrl": "/avatars/f568302b70064220e3c824577e5bece4.svg",
            "fullname": "Hidir Yesiltepe",
            "name": "Hidir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]