[
  {
    "paper": {
      "id": "2508.05748",
      "authors": [
        {
          "_id": "689c0152fab6fdd2e52ac85d",
          "user": {
            "_id": "682b22ebac526172e1b4ed1b",
            "avatarUrl": "/avatars/a9e486bf72d27013e6c1903b64a7754c.svg",
            "isPro": false,
            "fullname": "Geng Xinyu",
            "user": "Ornamentt",
            "type": "user"
          },
          "name": "Xinyu Geng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:48.684Z",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac85e",
          "user": {
            "_id": "643e9ee6f6bb3c31a26e7bc4",
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
            "isPro": false,
            "fullname": "Peng Xia",
            "user": "richardxp888",
            "type": "user"
          },
          "name": "Peng Xia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:38.450Z",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac85f",
          "user": {
            "_id": "646abd0f8dfd6ff79b6cfbb9",
            "avatarUrl": "/avatars/3de3b7cbeda95c2b4f460f87f8e9a1f7.svg",
            "isPro": false,
            "fullname": "ZhenZhang",
            "user": "zhzhen23",
            "type": "user"
          },
          "name": "Zhen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:20:47.118Z",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac860",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac861",
          "name": "Qiuchen Wang",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac862",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac863",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac864",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:43.643Z",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac865",
          "user": {
            "_id": "66e4019518a1920fb7ca19d7",
            "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
            "isPro": false,
            "fullname": "Yida Zhao",
            "user": "zhaoyd",
            "type": "user"
          },
          "name": "Yida Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:34.963Z",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac866",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac867",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac868",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac869",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "689c0152fab6fdd2e52ac86a",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png",
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png"
      ],
      "publishedAt": "2025-08-07T18:03:50.000Z",
      "submittedOnDailyAt": "2025-08-13T02:05:16.133Z",
      "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
      "upvotes": 60,
      "discussionId": "689c0152fab6fdd2e52ac86b",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent//",
      "ai_summary": "WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.",
      "ai_keywords": [
        "multimodal",
        "visual-language reasoning",
        "high-quality synthetic multimodal trajectories",
        "reinforcement learning",
        "BrowseComp-VL",
        "VQA benchmarks"
      ],
      "githubStars": 5965
    },
    "publishedAt": "2025-08-07T14:03:50.000Z",
    "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
    "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05748.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.08086",
      "authors": [
        {
          "_id": "689ac0a0fab6fdd2e52ac4e0",
          "name": "Zhongqi Yang",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e1",
          "name": "Wenhang Ge",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e2",
          "name": "Yuqi Li",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e3",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e4",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e5",
          "user": {
            "_id": "67a9664af5f1253c64259c50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Jk9W1C8704pqxNOsqZ0d9.png",
            "isPro": false,
            "fullname": "an",
            "user": "dearamy",
            "type": "user"
          },
          "name": "Mengyin An",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:19:49.471Z",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e6",
          "user": {
            "_id": "67a9b36a2fbf63093c19d3de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a9b36a2fbf63093c19d3de/LbB2XH0ezcJ_tdu4hU0Of.png",
            "isPro": false,
            "fullname": "ËÄÅk",
            "user": "kangfei",
            "type": "user"
          },
          "name": "Fei Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:19:46.490Z",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e7",
          "name": "Hua Xue",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e8",
          "name": "Baixin Xu",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4e9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4ea",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4eb",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4ec",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4ed",
          "name": "Hao-Xiang Guo",
          "hidden": false
        },
        {
          "_id": "689ac0a0fab6fdd2e52ac4ee",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64758ee7d815855e4efa206b/gD6oavgX7cDu24qQLbXir.png"
      ],
      "publishedAt": "2025-08-11T15:29:57.000Z",
      "submittedOnDailyAt": "2025-08-13T00:45:23.058Z",
      "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
      "submittedOnDailyBy": {
        "_id": "64758ee7d815855e4efa206b",
        "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
        "isPro": false,
        "fullname": "wenhang ge",
        "user": "spongy",
        "type": "user"
      },
      "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
      "upvotes": 36,
      "discussionId": "689ac0a0fab6fdd2e52ac4ef",
      "projectPage": "https://matrix-3d.github.io/",
      "githubRepo": "https://github.com/SkyworkAI/Matrix-3D/tree/main",
      "ai_summary": "Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.",
      "ai_keywords": [
        "panoramic representation",
        "wide-coverage",
        "omnidirectional",
        "explorable 3D world generation",
        "conditional video generation",
        "panoramic 3D reconstruction",
        "trajectory-guided",
        "panoramic video diffusion model",
        "scene mesh renders",
        "feed-forward large panorama reconstruction model",
        "optimization-based pipeline",
        "Matrix-Pano dataset",
        "panoramic video generation",
        "3D world generation"
      ],
      "githubStars": 178
    },
    "publishedAt": "2025-08-11T11:29:57.000Z",
    "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
    "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64758ee7d815855e4efa206b/gD6oavgX7cDu24qQLbXir.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64758ee7d815855e4efa206b",
      "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
      "fullname": "wenhang ge",
      "name": "spongy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07976",
      "authors": [
        {
          "_id": "689bf8c2fab6fdd2e52ac825",
          "name": "Jiaxuan Gao",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac826",
          "name": "Wei Fu",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac827",
          "name": "Minyang Xie",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac828",
          "name": "Shusheng Xu",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac829",
          "name": "Chuyi He",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac82a",
          "name": "Zhiyu Mei",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac82b",
          "name": "Banghua Zhu",
          "hidden": false
        },
        {
          "_id": "689bf8c2fab6fdd2e52ac82c",
          "name": "Yi Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T13:36:57.000Z",
      "submittedOnDailyAt": "2025-08-13T01:34:49.634Z",
      "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
      "submittedOnDailyBy": {
        "_id": "63159678915d0b80682fe9f9",
        "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
        "isPro": false,
        "fullname": "Shusheng Xu",
        "user": "xssstory",
        "type": "user"
      },
      "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
      "upvotes": 22,
      "discussionId": "689bf8c2fab6fdd2e52ac82d",
      "githubRepo": "https://github.com/inclusionAI/ASearcher",
      "ai_summary": "ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.",
      "ai_keywords": [
        "LLM-based agents",
        "search tools",
        "Search Intelligence",
        "open-source agents",
        "online RL methods",
        "fully asynchronous RL training",
        "prompt-based LLM agent",
        "QA dataset",
        "xBench",
        "GAIA",
        "Avg@4",
        "tool calls",
        "output tokens",
        "ASearcher-Web-QwQ"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-08-11T09:36:57.000Z",
    "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
    "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63159678915d0b80682fe9f9",
      "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
      "fullname": "Shusheng Xu",
      "name": "xssstory",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09138",
      "authors": [
        {
          "_id": "689befdbfab6fdd2e52ac80c",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac80d",
          "name": "Bozhen Fang",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac80e",
          "name": "Chenchen Jing",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac80f",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:13:02.765Z",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac810",
          "name": "Yangyi Shen",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac811",
          "user": {
            "_id": "64981bea09cea550852652af",
            "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg",
            "isPro": false,
            "fullname": "Qiuyu Wang",
            "user": "qiuyuu",
            "type": "user"
          },
          "name": "Qiuyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:13:07.409Z",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac812",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac813",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "689befdbfab6fdd2e52ac814",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f089456309c84d5f47f951/znHPdJqj7uNrvTS4acpqM.png"
      ],
      "publishedAt": "2025-08-12T17:59:57.000Z",
      "submittedOnDailyAt": "2025-08-13T00:24:54.833Z",
      "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "63f089456309c84d5f47f951",
        "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
        "isPro": false,
        "fullname": "Wen Wang",
        "user": "wwen1997",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
      "upvotes": 17,
      "discussionId": "689befdbfab6fdd2e52ac815",
      "projectPage": "https://aim-uofa.github.io/dLLM-MidTruth/",
      "githubRepo": "https://github.com/aim-uofa/dLLM-MidTruth",
      "ai_summary": "Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.",
      "ai_keywords": [
        "diffusion large language models",
        "denoising",
        "temporal oscillation",
        "Temporal Self-Consistency Voting",
        "Temporal Consistency Reinforcement",
        "Temporal Semantic Entropy",
        "semantic stability",
        "Countdown dataset",
        "GSM8K",
        "MATH500",
        "SVAMP"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-08-12T13:59:57.000Z",
    "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
    "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f089456309c84d5f47f951/znHPdJqj7uNrvTS4acpqM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f089456309c84d5f47f951",
      "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
      "fullname": "Wen Wang",
      "name": "wwen1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07409",
      "authors": [
        {
          "_id": "689c1928fab6fdd2e52ac8a0",
          "user": {
            "_id": "663b5f08389689b517109b77",
            "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
            "isPro": false,
            "fullname": "Gaojunyao",
            "user": "Gaojunyao",
            "type": "user"
          },
          "name": "Junyao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:20.003Z",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a1",
          "user": {
            "_id": "65464049e70ffa3c07f22e92",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EAYwKThe-BGeza-CmzIdx.jpeg",
            "isPro": false,
            "fullname": " Li Jiaxing",
            "user": "LiJiaxing",
            "type": "user"
          },
          "name": "Jiaxing Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:23.461Z",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a2",
          "name": "Wenran Liu",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a3",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a4",
          "name": "Fei Shen",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a5",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a6",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "689c1928fab6fdd2e52ac8a7",
          "name": "Cairong Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/663b5f08389689b517109b77/unLyeVoJYZogIaDHKufWQ.gif"
      ],
      "publishedAt": "2025-08-10T16:15:04.000Z",
      "submittedOnDailyAt": "2025-08-13T03:19:56.895Z",
      "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
      "submittedOnDailyBy": {
        "_id": "663b5f08389689b517109b77",
        "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
        "isPro": false,
        "fullname": "Gaojunyao",
        "user": "Gaojunyao",
        "type": "user"
      },
      "summary": "In this paper, we propose CharacterShot, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.",
      "upvotes": 17,
      "discussionId": "689c1928fab6fdd2e52ac8a8",
      "githubRepo": "https://github.com/Jeoyal/CharacterShot",
      "ai_summary": "CharacterShot is a 4D character animation framework that uses a DiT-based model and dual-attention module to generate consistent 3D animations from a single image and 2D pose sequence.",
      "ai_keywords": [
        "DiT-based image-to-video model",
        "dual-attention module",
        "camera prior",
        "neighbor-constrained 4D gaussian splatting",
        "Character4D dataset",
        "CharacterBench benchmark"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-08-10T12:15:04.000Z",
    "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
    "summary": "In this paper, we propose CharacterShot, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/663b5f08389689b517109b77/unLyeVoJYZogIaDHKufWQ.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07409.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "663b5f08389689b517109b77",
      "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
      "fullname": "Gaojunyao",
      "name": "Gaojunyao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.08088",
      "authors": [
        {
          "_id": "689ac80efab6fdd2e52ac510",
          "user": {
            "_id": "62e52483a944e2a56cd2c6ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
            "isPro": false,
            "fullname": "Jiejun Tan",
            "user": "zstanjj",
            "type": "user"
          },
          "name": "Jiejun Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:19:26.764Z",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac511",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac512",
          "name": "Yan Yu",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac513",
          "name": "Jiehan Cheng",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac514",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac515",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "689ac80efab6fdd2e52ac516",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T15:31:47.000Z",
      "submittedOnDailyAt": "2025-08-13T01:08:11.771Z",
      "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
      "submittedOnDailyBy": {
        "_id": "62e52483a944e2a56cd2c6ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
        "isPro": false,
        "fullname": "Jiejun Tan",
        "user": "zstanjj",
        "type": "user"
      },
      "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
      "upvotes": 14,
      "discussionId": "689ac80efab6fdd2e52ac517",
      "githubRepo": "https://github.com/plageon/HierSearch",
      "ai_summary": "HierSearch, a hierarchical agentic deep search framework using hierarchical RL, improves performance in multi-source retrieval tasks by coordinating local and Web search agents and refining knowledge.",
      "ai_keywords": [
        "reinforcement learning",
        "hierarchical RL",
        "local deep search agent",
        "Web deep search agent",
        "planner agent",
        "knowledge refiner",
        "hallucinations",
        "irrelevant evidence",
        "multi-source retrieval-augmented generation",
        "benchmarks"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-08-11T11:31:47.000Z",
    "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
    "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08088.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62e52483a944e2a56cd2c6ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
      "fullname": "Jiejun Tan",
      "name": "zstanjj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.05615",
      "authors": [
        {
          "_id": "6895643848b0ae5ca2710d5a",
          "user": {
            "_id": "67ef9bdeeaf849151c740adf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-n7Ivd3RJnKZvFjF-z0ji.jpeg",
            "isPro": false,
            "fullname": "Yong Du",
            "user": "DIONG1024",
            "type": "user"
          },
          "name": "Yong Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:27:47.987Z",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d5b",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:49:57.884Z",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d5c",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d5d",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d5e",
          "name": "Chang Zong",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d5f",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d60",
          "name": "Shengpei Jiang",
          "hidden": false
        },
        {
          "_id": "6895643848b0ae5ca2710d61",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:50:01.160Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T17:54:27.000Z",
      "submittedOnDailyAt": "2025-08-13T00:38:26.010Z",
      "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
      "upvotes": 9,
      "discussionId": "6895643948b0ae5ca2710d62",
      "projectPage": "https://zju-real.github.io/gui-rcpo/",
      "githubRepo": "https://github.com/zju-real/gui-rcpo",
      "ai_summary": "GUI-RC and GUI-RCPO enhance GUI grounding accuracy by leveraging spatial consistency and reinforcement learning without additional training data.",
      "ai_keywords": [
        "GUI grounding",
        "spatial overlap patterns",
        "spatial voting grids",
        "consensus regions",
        "test-time scaling",
        "test-time reinforcement learning",
        "ScreenSpot benchmarks",
        "Qwen2.5-VL-3B-Instruct",
        "ScreenSpot-v2"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-08-07T13:54:27.000Z",
    "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
    "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.05399",
      "authors": [
        {
          "_id": "689bea5afab6fdd2e52ac7d1",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d2",
          "name": "Byeongkeun Ahn",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d3",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d4",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d5",
          "user": {
            "_id": "64ae35dc00781825350e880b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
            "isPro": false,
            "fullname": "Seunghyuk Oh",
            "user": "JakeOh",
            "type": "user"
          },
          "name": "Seunghyuk Oh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:13:14.929Z",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d6",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "689bea5afab6fdd2e52ac7d7",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T13:51:17.000Z",
      "submittedOnDailyAt": "2025-08-13T00:40:20.574Z",
      "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "63e1d5247fbb6ae4d4f4cc8e",
        "avatarUrl": "/avatars/8a8f700adf9e8000641c2c2f6bd56080.svg",
        "isPro": false,
        "fullname": "Wonjun Kang",
        "user": "wjkang",
        "type": "user"
      },
      "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
      "upvotes": 8,
      "discussionId": "689bea5bfab6fdd2e52ac7d8",
      "githubRepo": "https://github.com/furiosa-ai/uncage",
      "ai_summary": "UNCAGE, a training-free method using contrastive attention guidance, enhances compositional fidelity in text-to-image generation by prioritizing the unmasking of object-representing tokens.",
      "ai_keywords": [
        "Diffusion Models",
        "Autoregressive Models",
        "Masked Generative Transformers",
        "bidirectional attention",
        "parallel decoding",
        "compositional T2I generation",
        "attention maps",
        "text-image alignment",
        "UNCAGE"
      ]
    },
    "publishedAt": "2025-08-07T09:51:17.000Z",
    "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
    "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e1d5247fbb6ae4d4f4cc8e",
      "avatarUrl": "/avatars/8a8f700adf9e8000641c2c2f6bd56080.svg",
      "fullname": "Wonjun Kang",
      "name": "wjkang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08665",
      "authors": [
        {
          "_id": "689c18cffab6fdd2e52ac89b",
          "user": {
            "_id": "683d78e0ed8d78bf6d3e653f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
            "isPro": false,
            "fullname": "Ritvik Rastogi",
            "user": "RitvikPW",
            "type": "user"
          },
          "name": "Ritvik Rastogi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:28.385Z",
          "hidden": false
        },
        {
          "_id": "689c18cffab6fdd2e52ac89c",
          "name": "Sachin Dharashivkar",
          "hidden": false
        },
        {
          "_id": "689c18cffab6fdd2e52ac89d",
          "name": "Sandeep Varma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T06:20:07.000Z",
      "submittedOnDailyAt": "2025-08-13T06:05:27.587Z",
      "title": "Aryabhata: An exam-focused language model for JEE Math",
      "submittedOnDailyBy": {
        "_id": "683d78e0ed8d78bf6d3e653f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
        "isPro": false,
        "fullname": "Ritvik Rastogi",
        "user": "RitvikPW",
        "type": "user"
      },
      "summary": "We present Aryabhata 1.0, a compact 7B parameter math reasoning\nmodel optimized for the Indian academic exam, the Joint Entrance Examination\n(JEE). Despite rapid progress in large language models (LLMs), current models\noften remain unsuitable for educational use. Aryabhata 1.0 is built by merging\nstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)\nwith curriculum learning on verified chain-of-thought (CoT) traces curated\nthrough best-of-n rejection sampling. To further boost performance, we apply\nreinforcement learning with verifiable rewards (RLVR) using A2C objective with\ngroup-relative advantage estimation alongwith novel exploration strategies such\nas Adaptive Group Resizing and Temperature Scaling.\nEvaluated on both in-distribution (JEE Main 2025) and out-of-distribution\n(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and\nefficiency, while offering pedagogically useful step-by-step reasoning. We\nrelease Aryabhata as a foundation model to advance exam-centric, open-source\nsmall language models. This marks our first open release for community feedback\n(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0\non Hugging Face}); PW is actively training future models to further improve\nlearning outcomes for students.",
      "upvotes": 7,
      "discussionId": "689c18d0fab6fdd2e52ac89e",
      "ai_summary": "Aryabhata 1.0, a compact math reasoning model, outperforms existing models on educational exams and benchmarks by using supervised fine-tuning, reinforcement learning with verifiable rewards, and novel exploration strategies.",
      "ai_keywords": [
        "open-weight reasoning models",
        "supervised fine-tuning",
        "curriculum learning",
        "chain-of-thought traces",
        "reinforcement learning with verifiable rewards",
        "A2C objective",
        "group-relative advantage estimation",
        "Adaptive Group Resizing",
        "Temperature Scaling",
        "in-distribution",
        "out-of-distribution",
        "pedagogically useful step-by-step reasoning"
      ]
    },
    "publishedAt": "2025-08-12T02:20:07.000Z",
    "title": "Aryabhata: An exam-focused language model for JEE Math",
    "summary": "We present Aryabhata 1.0, a compact 7B parameter math reasoning\nmodel optimized for the Indian academic exam, the Joint Entrance Examination\n(JEE). Despite rapid progress in large language models (LLMs), current models\noften remain unsuitable for educational use. Aryabhata 1.0 is built by merging\nstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)\nwith curriculum learning on verified chain-of-thought (CoT) traces curated\nthrough best-of-n rejection sampling. To further boost performance, we apply\nreinforcement learning with verifiable rewards (RLVR) using A2C objective with\ngroup-relative advantage estimation alongwith novel exploration strategies such\nas Adaptive Group Resizing and Temperature Scaling.\nEvaluated on both in-distribution (JEE Main 2025) and out-of-distribution\n(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and\nefficiency, while offering pedagogically useful step-by-step reasoning. We\nrelease Aryabhata as a foundation model to advance exam-centric, open-source\nsmall language models. This marks our first open release for community feedback\n(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0\non Hugging Face}); PW is actively training future models to further improve\nlearning outcomes for students.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08665.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683d78e0ed8d78bf6d3e653f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
      "fullname": "Ritvik Rastogi",
      "name": "RitvikPW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.09062",
      "authors": [
        {
          "_id": "689bf9adfab6fdd2e52ac82f",
          "user": {
            "_id": "679da97f2000f0eecaaa8e9c",
            "avatarUrl": "/avatars/646128f0629143f05d6496cc93719184.svg",
            "isPro": false,
            "fullname": "Xiang Zhang",
            "user": "zx1239856",
            "type": "user"
          },
          "name": "Xiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:54.982Z",
          "hidden": false
        },
        {
          "_id": "689bf9adfab6fdd2e52ac830",
          "name": "Yawar Siddiqui",
          "hidden": false
        },
        {
          "_id": "689bf9adfab6fdd2e52ac831",
          "name": "Armen Avetisyan",
          "hidden": false
        },
        {
          "_id": "689bf9adfab6fdd2e52ac832",
          "name": "Chris Xie",
          "hidden": false
        },
        {
          "_id": "689bf9adfab6fdd2e52ac833",
          "name": "Jakob Engel",
          "hidden": false
        },
        {
          "_id": "689bf9adfab6fdd2e52ac834",
          "name": "Henry Howard-Jenkins",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65d4991111930466954340ae/6d5aOWmFbEs3X6Sg-T5iq.qt"
      ],
      "publishedAt": "2025-08-12T16:25:46.000Z",
      "submittedOnDailyAt": "2025-08-13T03:34:12.555Z",
      "title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
      "submittedOnDailyBy": {
        "_id": "65d4991111930466954340ae",
        "avatarUrl": "/avatars/ffc07be80323aa4b7dadf8849b70ffde.svg",
        "isPro": false,
        "fullname": "Henry Howard-Jenkins",
        "user": "henryhj",
        "type": "user"
      },
      "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.",
      "upvotes": 5,
      "discussionId": "689bf9aefab6fdd2e52ac835",
      "projectPage": "https://vertexregen.github.io/",
      "ai_summary": "VertexRegen generates meshes with continuous detail by reversing edge collapse through a generative model, offering anytime generation and flexibility in detail levels.",
      "ai_keywords": [
        "mesh generation",
        "autoregressive methods",
        "progressive meshes",
        "edge collapse",
        "vertex split",
        "generative model",
        "anytime generation"
      ]
    },
    "publishedAt": "2025-08-12T12:25:46.000Z",
    "title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
    "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65d4991111930466954340ae/6d5aOWmFbEs3X6Sg-T5iq.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09062.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d4991111930466954340ae",
      "avatarUrl": "/avatars/ffc07be80323aa4b7dadf8849b70ffde.svg",
      "fullname": "Henry Howard-Jenkins",
      "name": "henryhj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08940",
      "authors": [
        {
          "_id": "689c3f0cfab6fdd2e52ac93a",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "689c3f0cfab6fdd2e52ac93b",
          "name": "Kumail Alhamoud",
          "hidden": false
        },
        {
          "_id": "689c3f0cfab6fdd2e52ac93c",
          "name": "Abed Hammoud",
          "hidden": false
        },
        {
          "_id": "689c3f0cfab6fdd2e52ac93d",
          "name": "Elie Bou-Zeid",
          "hidden": false
        },
        {
          "_id": "689c3f0cfab6fdd2e52ac93e",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        },
        {
          "_id": "689c3f0cfab6fdd2e52ac93f",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/4b_TnX67UO9R8vBKO9NkT.png"
      ],
      "publishedAt": "2025-08-12T13:48:03.000Z",
      "submittedOnDailyAt": "2025-08-13T06:01:08.584Z",
      "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": true,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
      "upvotes": 4,
      "discussionId": "689c3f0cfab6fdd2e52ac940",
      "ai_summary": "A curriculum learning strategy using Group Relative Policy Optimization (GRPO) enhances the reasoning abilities of large language models by progressively tightening token budgets, improving accuracy and token efficiency.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "curriculum learning",
        "token budgets",
        "verifier feedback",
        "structural tags",
        "GSM8K",
        "MATH500",
        "SVAMP",
        "College Math",
        "GSM+"
      ]
    },
    "publishedAt": "2025-08-12T09:48:03.000Z",
    "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
    "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/4b_TnX67UO9R8vBKO9NkT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08940.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08244",
      "authors": [
        {
          "_id": "689bfef7fab6fdd2e52ac837",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac838",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac839",
          "name": "Jiajun Li",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac83a",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac83b",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac83c",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "689bfef7fab6fdd2e52ac83d",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T17:56:59.000Z",
      "submittedOnDailyAt": "2025-08-13T03:01:55.702Z",
      "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
      "submittedOnDailyBy": {
        "_id": "66a49f9b2b460286b05646b8",
        "avatarUrl": "/avatars/5123cf7c3d00b5bc47a86011afd6abe8.svg",
        "isPro": false,
        "fullname": "jwhe",
        "user": "jwhejwhe",
        "type": "user"
      },
      "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
      "upvotes": 4,
      "discussionId": "689bfef7fab6fdd2e52ac83e",
      "ai_summary": "Cut2Next, a framework using a Diffusion Transformer with in-context tuning and hierarchical prompting, generates high-quality, cinematically coherent shots that adhere to professional editing patterns.",
      "ai_keywords": [
        "Diffusion Transformer",
        "DiT",
        "in-context tuning",
        "Hierarchical Multi-Prompting",
        "Relational Prompts",
        "Individual Prompts",
        "Context-Aware Condition Injection",
        "CACI",
        "Hierarchical Attention Mask",
        "HAM",
        "RawCuts",
        "CuratedCuts",
        "CutBench"
      ]
    },
    "publishedAt": "2025-08-11T13:56:59.000Z",
    "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
    "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08244.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a49f9b2b460286b05646b8",
      "avatarUrl": "/avatars/5123cf7c3d00b5bc47a86011afd6abe8.svg",
      "fullname": "jwhe",
      "name": "jwhejwhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09101",
      "authors": [
        {
          "_id": "689c00c7fab6fdd2e52ac84b",
          "name": "Jason Chou",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac84c",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac84d",
          "name": "Yuchi Deng",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac84e",
          "name": "Zhiying Zeng",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac84f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac850",
          "name": "Haotian Zhu",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac851",
          "name": "Jianwei Cai",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac852",
          "name": "Yue Mao",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac853",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac854",
          "name": "Lingyun Tan",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac855",
          "name": "Ziyan Xu",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac856",
          "name": "Bohui Zhai",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac857",
          "name": "Hengyi Liu",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac858",
          "name": "Speed Zhu",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac859",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "689c00c7fab6fdd2e52ac85a",
          "name": "Fengzong Lian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T17:29:20.000Z",
      "submittedOnDailyAt": "2025-08-13T01:38:17.109Z",
      "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "xxzcc",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
      "upvotes": 3,
      "discussionId": "689c00c7fab6fdd2e52ac85b",
      "ai_summary": "AutoCodeGen creates a large-scale, multilingual code generation benchmark, AutoCodeBench, to evaluate LLMs on diverse and complex tasks without manual annotations.",
      "ai_keywords": [
        "Large Language Models",
        "code generation",
        "benchmarks",
        "manual annotations",
        "multilingual",
        "test cases",
        "multilingual sandbox",
        "reverse-order problem generation",
        "filtering steps",
        "AutoCodeBench",
        "few-shot code generation",
        "AutoCodeBench-Lite",
        "AutoCodeBench-Complete"
      ]
    },
    "publishedAt": "2025-08-12T13:29:20.000Z",
    "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09101.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "xxzcc",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09123",
      "authors": [
        {
          "_id": "689bed2efab6fdd2e52ac7da",
          "user": {
            "_id": "67b327cdd4665a0448eef7d5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b327cdd4665a0448eef7d5/_B5Z9MCa_qiFrDj1axKlz.png",
            "isPro": true,
            "fullname": "Xinyuan Wang",
            "user": "xywang626",
            "type": "user"
          },
          "name": "Xinyuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:13:11.475Z",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7db",
          "name": "Bowen Wang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7dc",
          "name": "Dunjie Lu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7dd",
          "name": "Junlin Yang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7de",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7df",
          "name": "Junli Wang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e0",
          "name": "Jiaqi Deng",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e1",
          "name": "Xiaole Guo",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e2",
          "name": "Yiheng Xu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e3",
          "name": "Chen Henry Wu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e4",
          "name": "Zhennan Shen",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e5",
          "name": "Zhuokai Li",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e6",
          "name": "Ryan Li",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e7",
          "name": "Xiaochuan Li",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e8",
          "name": "Junda Chen",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7e9",
          "name": "Boyuan Zheng",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ea",
          "name": "Peihang Li",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7eb",
          "name": "Fangyu Lei",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ec",
          "name": "Ruisheng Cao",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ed",
          "name": "Yeqiao Fu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ee",
          "name": "Dongchan Shin",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ef",
          "name": "Martin Shin",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f0",
          "name": "Jiarui Hu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f1",
          "name": "Yuyan Wang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f2",
          "name": "Jixuan Chen",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f3",
          "name": "Yuxiao Ye",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f4",
          "name": "Danyang Zhang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f5",
          "name": "Dikang Du",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f6",
          "name": "Hao Hu",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f7",
          "name": "Huarong Chen",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f8",
          "name": "Zaida Zhou",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7f9",
          "name": "Yipu Wang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7fa",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7fb",
          "name": "Diyi Yang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7fc",
          "name": "Victor Zhong",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7fd",
          "name": "Flood Sung",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7fe",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac7ff",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "689bed2efab6fdd2e52ac800",
          "name": "Tao Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T17:52:32.000Z",
      "submittedOnDailyAt": "2025-08-13T05:53:45.913Z",
      "title": "OpenCUA: Open Foundations for Computer-Use Agents",
      "submittedOnDailyBy": {
        "_id": "669ca7e678115e16bdfc9bfc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669ca7e678115e16bdfc9bfc/pku8NvQKqfNQACRqm1YrW.jpeg",
        "isPro": true,
        "fullname": "Lu Dunjie",
        "user": "ludunjie",
        "type": "user"
      },
      "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
      "upvotes": 2,
      "discussionId": "689bed2efab6fdd2e52ac801",
      "ai_summary": "OpenCUA is an open-source framework for vision-language models as computer-use agents, featuring an annotation infrastructure, a large-scale dataset, and a scalable pipeline that achieves state-of-the-art performance.",
      "ai_keywords": [
        "vision-language models",
        "computer-use agents",
        "annotation infrastructure",
        "AgentNet",
        "state-action pairs",
        "Chain-of-Thought reasoning",
        "OSWorld-Verified",
        "OpenAI CUA",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-08-12T13:52:32.000Z",
    "title": "OpenCUA: Open Foundations for Computer-Use Agents",
    "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669ca7e678115e16bdfc9bfc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669ca7e678115e16bdfc9bfc/pku8NvQKqfNQACRqm1YrW.jpeg",
      "fullname": "Lu Dunjie",
      "name": "ludunjie",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08791",
      "authors": [
        {
          "_id": "689c08b8fab6fdd2e52ac871",
          "user": {
            "_id": "66384be673c2c55f2ded89fa",
            "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
            "isPro": false,
            "fullname": "Junjie Ye",
            "user": "Junjie-Ye",
            "type": "user"
          },
          "name": "Junjie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:12:31.844Z",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac872",
          "name": "Changhao Jiang",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac873",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac874",
          "name": "Yufei Xu",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac875",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac876",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac877",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac878",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac879",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "689c08b8fab6fdd2e52ac87a",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T09:45:19.000Z",
      "submittedOnDailyAt": "2025-08-13T02:35:32.989Z",
      "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
      "submittedOnDailyBy": {
        "_id": "66384be673c2c55f2ded89fa",
        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
        "isPro": false,
        "fullname": "Junjie Ye",
        "user": "Junjie-Ye",
        "type": "user"
      },
      "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
      "upvotes": 2,
      "discussionId": "689c08b8fab6fdd2e52ac87b",
      "githubRepo": "https://github.com/bytedance/FTRL",
      "ai_summary": "An automated pipeline for constructing training environments and a verifiable reward mechanism enhance large language models' tool-use performance without compromising general capabilities.",
      "ai_keywords": [
        "reinforcement learning",
        "scenario decomposition",
        "document generation",
        "function integration",
        "complexity scaling",
        "localized deployment",
        "verifiable reward mechanism",
        "trajectory data",
        "standard RL algorithms",
        "context understanding",
        "reasoning",
        "MLP parameters"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-12T05:45:19.000Z",
    "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
    "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66384be673c2c55f2ded89fa",
      "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
      "fullname": "Junjie Ye",
      "name": "Junjie-Ye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.09050",
      "authors": [
        {
          "_id": "689c2f33fab6fdd2e52ac8dd",
          "name": "Germ√°n D√≠az Agreda",
          "hidden": false
        },
        {
          "_id": "689c2f33fab6fdd2e52ac8de",
          "name": "Carlos Andres Duran Paredes",
          "hidden": false
        },
        {
          "_id": "689c2f33fab6fdd2e52ac8df",
          "name": "Mateo Buenaventura Samboni",
          "hidden": false
        },
        {
          "_id": "689c2f33fab6fdd2e52ac8e0",
          "name": "Jhon Alejandro Andrade",
          "hidden": false
        },
        {
          "_id": "689c2f33fab6fdd2e52ac8e1",
          "user": {
            "_id": "628ddf04986ae70e823298f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
            "isPro": false,
            "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
            "user": "sebasmos",
            "type": "user"
          },
          "name": "Sebasti√°n Andr√©s Cajas Ordo√±ez",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:10:31.803Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T16:10:05.000Z",
      "submittedOnDailyAt": "2025-08-13T04:54:47.461Z",
      "title": "Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware",
      "submittedOnDailyBy": {
        "_id": "628ddf04986ae70e823298f7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
        "isPro": false,
        "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
        "user": "sebasmos",
        "type": "user"
      },
      "summary": "Implementing quantum game theory on real hardware is challenging due to\nnoise, decoherence, and limited qubit connectivity, yet such demonstrations are\nessential to validate theoretical predictions. We present one of the first full\nexperimental realizations of the Battle of the Sexes game under the\nEisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke\nsuperconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi))\nwere evaluated across 31 entanglement values gamma in [0, pi] using 2048\nshots per configuration, enabling a direct comparison between analytical\npredictions and hardware execution. To mitigate noise and variability, we\nintroduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit\npairs and optimizes routing based on real-time topology and calibration data.\nThe analytical model forecasts up to 108% payoff improvement over the\nclassical equilibrium, and despite hardware-induced deviations, experimental\nresults with GCM preserve the expected payoff trends within 3.5%-12%\nrelative error. These findings show that quantum advantages in strategic\ncoordination can persist under realistic NISQ conditions, providing a pathway\ntoward practical applications of quantum game theory in multi-agent, economic,\nand distributed decision-making systems.",
      "upvotes": 1,
      "discussionId": "689c2f34fab6fdd2e52ac8e2",
      "ai_summary": "Quantum game theory demonstrated on IBM Quantum hardware using the Eisert-Wilkens-Lewenstein framework shows persistent quantum advantages in strategic coordination despite noise and decoherence.",
      "ai_keywords": [
        "quantum game theory",
        "Eisert-Wilkens-Lewenstein framework",
        "IBM Quantum",
        "superconducting processor",
        "quantum strategies",
        "entanglement",
        "Guided Circuit Mapping",
        "NISQ conditions",
        "multi-agent systems",
        "economic systems",
        "distributed decision-making systems"
      ]
    },
    "publishedAt": "2025-08-12T12:10:05.000Z",
    "title": "Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware",
    "summary": "Implementing quantum game theory on real hardware is challenging due to\nnoise, decoherence, and limited qubit connectivity, yet such demonstrations are\nessential to validate theoretical predictions. We present one of the first full\nexperimental realizations of the Battle of the Sexes game under the\nEisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke\nsuperconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi))\nwere evaluated across 31 entanglement values gamma in [0, pi] using 2048\nshots per configuration, enabling a direct comparison between analytical\npredictions and hardware execution. To mitigate noise and variability, we\nintroduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit\npairs and optimizes routing based on real-time topology and calibration data.\nThe analytical model forecasts up to 108% payoff improvement over the\nclassical equilibrium, and despite hardware-induced deviations, experimental\nresults with GCM preserve the expected payoff trends within 3.5%-12%\nrelative error. These findings show that quantum advantages in strategic\ncoordination can persist under realistic NISQ conditions, providing a pathway\ntoward practical applications of quantum game theory in multi-agent, economic,\nand distributed decision-making systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628ddf04986ae70e823298f7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
      "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
      "name": "sebasmos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.04195",
      "authors": [
        {
          "_id": "689c33fbfab6fdd2e52ac8ec",
          "name": "Huan Liao",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8ed",
          "name": "Qinke Ni",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8ee",
          "name": "Yuancheng Wang",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8ef",
          "name": "Yiheng Lu",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8f0",
          "name": "Haoyue Zhan",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8f1",
          "name": "Pengyuan Xie",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8f2",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "689c33fbfab6fdd2e52ac8f3",
          "name": "Zhizheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T08:25:26.000Z",
      "submittedOnDailyAt": "2025-08-13T05:14:31.171Z",
      "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations",
      "submittedOnDailyBy": {
        "_id": "63072d60cd148dbc5e49f4dd",
        "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
        "isPro": false,
        "fullname": "Yuancheng Wang",
        "user": "Hecheng0625",
        "type": "user"
      },
      "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/.",
      "upvotes": 0,
      "discussionId": "689c33fcfab6fdd2e52ac8f4",
      "ai_summary": "NVSpeech is a pipeline that integrates the recognition and synthesis of paralinguistic vocalizations in Mandarin, using a large annotated dataset and models that treat these cues as decodable tokens.",
      "ai_keywords": [
        "paralinguistic vocalizations",
        "ASR modeling",
        "TTS",
        "paralinguistic-aware ASR",
        "zero-shot TTS",
        "word-level paralinguistic categories",
        "paralinguistic cues",
        "lexical and non-verbal transcription",
        "human-like speech synthesis",
        "expressive speech modeling"
      ]
    },
    "publishedAt": "2025-08-06T04:25:26.000Z",
    "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations",
    "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04195.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63072d60cd148dbc5e49f4dd",
      "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
      "fullname": "Yuancheng Wang",
      "name": "Hecheng0625",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  }
]