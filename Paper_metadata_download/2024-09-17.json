[
    {
        "paper": {
            "id": "2409.09214",
            "authors": [
                {
                    "_id": "66e8f414cd955ad711aa021d",
                    "name": "Ye Bai",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa021e",
                    "name": "Haonan Chen",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa021f",
                    "name": "Jitong Chen",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0220",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0221",
                    "name": "Yi Deng",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0222",
                    "name": "Xiaohong Dong",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0223",
                    "name": "Lamtharn Hantrakul",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0224",
                    "user": {
                        "_id": "62fd76854723285c5e16562e",
                        "avatarUrl": "/avatars/792bb34fbde9a3f1614946cdb8d2d62d.svg",
                        "isPro": false,
                        "fullname": "Hao",
                        "user": "Weituo",
                        "type": "user"
                    },
                    "name": "Weituo Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:29:17.750Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0225",
                    "name": "Qingqing Huang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0226",
                    "user": {
                        "_id": "6460ec941db65f8785148710",
                        "avatarUrl": "/avatars/4b5c9dc17cd8f038bb99317f80c52c0e.svg",
                        "isPro": false,
                        "fullname": "huangzhongyi",
                        "user": "smileezzz",
                        "type": "user"
                    },
                    "name": "Zhongyi Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:28:59.692Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0227",
                    "name": "Dongya Jia",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0228",
                    "name": "Feihu La",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0229",
                    "user": {
                        "_id": "645cf0d401f4eaab2a071099",
                        "avatarUrl": "/avatars/619706a56c79498768b2d9965074b49c.svg",
                        "isPro": false,
                        "fullname": "Duc Le",
                        "user": "ducle",
                        "type": "user"
                    },
                    "name": "Duc Le",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:30:29.776Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022a",
                    "name": "Bochen Li",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022b",
                    "user": {
                        "_id": "64355a3bf81a16e743614ea5",
                        "avatarUrl": "/avatars/9104a301a17f77f7544a079f9b167853.svg",
                        "isPro": false,
                        "fullname": "lichuming",
                        "user": "lich-ming",
                        "type": "user"
                    },
                    "name": "Chumin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:31:02.839Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022c",
                    "name": "Hui Li",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022d",
                    "user": {
                        "_id": "644686ea21eff8427f78a0b9",
                        "avatarUrl": "/avatars/3d20c70324c567150473c95a74b92cb7.svg",
                        "isPro": false,
                        "fullname": "li",
                        "user": "lixingxing",
                        "type": "user"
                    },
                    "name": "Xingxing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:31:22.114Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022e",
                    "user": {
                        "_id": "65db1dd1ab2f64915ceb6bfc",
                        "avatarUrl": "/avatars/6c9c54ca7de453555df498ff26e85079.svg",
                        "isPro": false,
                        "fullname": "liushouda",
                        "user": "vsooda",
                        "type": "user"
                    },
                    "name": "Shouda Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:35:12.239Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa022f",
                    "name": "Wei-Tsung Lu",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0230",
                    "name": "Yiqing Lu",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0231",
                    "name": "Andrew Shaw",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0232",
                    "name": "Janne Spijkervet",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0233",
                    "name": "Yakun Sun",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0234",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0235",
                    "name": "Ju-Chiang Wang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0236",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0237",
                    "user": {
                        "_id": "6694e5344a34b68738070c07",
                        "avatarUrl": "/avatars/f33ec08ec9822bb353020000a2fcbd84.svg",
                        "isPro": false,
                        "fullname": "Yuxuan WANG",
                        "user": "AlbusYuxuanWang",
                        "type": "user"
                    },
                    "name": "Yuxuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:32:12.188Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0238",
                    "name": "Ling Xu",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0239",
                    "user": {
                        "_id": "66541f311b1ce9f4a6c8e7a6",
                        "avatarUrl": "/avatars/1062a9887325c426863afa05620a81f8.svg",
                        "isPro": false,
                        "fullname": "YIFENG YANG",
                        "user": "cnyangyifeng",
                        "type": "user"
                    },
                    "name": "Yifeng Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:34:06.836Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023a",
                    "name": "Chao Yao",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023b",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023c",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023d",
                    "name": "Yilin Zhang",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023e",
                    "name": "Hang Zhao",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa023f",
                    "user": {
                        "_id": "65d590ac493aeaf0bb5b3443",
                        "avatarUrl": "/avatars/6b1cc5c1a50571c8ecccb56832e768f9.svg",
                        "isPro": false,
                        "fullname": "Ziyi Zhao",
                        "user": "Ziyi98",
                        "type": "user"
                    },
                    "name": "Ziyi Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:33:34.131Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0240",
                    "name": "Dejian Zhong",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0241",
                    "name": "Shicen Zhou",
                    "hidden": false
                },
                {
                    "_id": "66e8f414cd955ad711aa0242",
                    "name": "Pei Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-13T22:06:18.000Z",
            "title": "Seed-Music: A Unified Framework for High Quality and Controlled Music\n  Generation",
            "summary": "We introduce Seed-Music, a suite of music generation systems capable of\nproducing high-quality music with fine-grained style control. Our unified\nframework leverages both auto-regressive language modeling and diffusion\napproaches to support two key music creation workflows: controlled\nmusic generation and post-production editing. For controlled music\ngeneration, our system enables vocal music generation with performance controls\nfrom multi-modal inputs, including style descriptions, audio references,\nmusical scores, and voice prompts. For post-production editing, it offers\ninteractive tools for editing lyrics and vocal melodies directly in the\ngenerated audio.\n  We encourage readers to listen to demo audio examples at\nhttps://team.doubao.com/seed-music .",
            "upvotes": 20,
            "discussionId": "66e8f415cd955ad711aa028e"
        },
        "publishedAt": "2024-09-17T01:44:33.307Z",
        "title": "Seed-Music: A Unified Framework for High Quality and Controlled Music Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09214.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.10516",
            "authors": [
                {
                    "_id": "66e8f30d6646c8f0e8b11d93",
                    "name": "Di Liu",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d94",
                    "user": {
                        "_id": "64ace004b2f83dec96121d9e",
                        "avatarUrl": "/avatars/0b85db4bdf41f627026b0f1aef132b85.svg",
                        "isPro": false,
                        "fullname": "Meng Chen",
                        "user": "Matchyc",
                        "type": "user"
                    },
                    "name": "Meng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T09:30:37.669Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d95",
                    "user": {
                        "_id": "667135bdcca06def1c2599a6",
                        "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
                        "isPro": false,
                        "fullname": "Baotong Lu",
                        "user": "baotonglu",
                        "type": "user"
                    },
                    "name": "Baotong Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:39:33.609Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d96",
                    "user": {
                        "_id": "6278bd42541f3d2dfa77ea70",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
                        "isPro": true,
                        "fullname": "Huiqiang Jiang",
                        "user": "iofu728",
                        "type": "user"
                    },
                    "name": "Huiqiang Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:39:21.178Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d97",
                    "user": {
                        "_id": "64cc9b96e60d2cddfadca2c8",
                        "avatarUrl": "/avatars/a07755847ec8d05052221d351a3ae20f.svg",
                        "isPro": false,
                        "fullname": "Zhenhua Han",
                        "user": "hzhua",
                        "type": "user"
                    },
                    "name": "Zhenhua Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:39:59.765Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d98",
                    "user": {
                        "_id": "6699d087fbc9ba8a121a6205",
                        "avatarUrl": "/avatars/9c2ffb1f78ef8d29290a6875253b3a1a.svg",
                        "isPro": false,
                        "fullname": "Zhang Qianxi",
                        "user": "zqx123",
                        "type": "user"
                    },
                    "name": "Qianxi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:40:10.853Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d99",
                    "name": "Qi Chen",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9a",
                    "user": {
                        "_id": "64646896884f2e3e1ced3cd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64646896884f2e3e1ced3cd5/86-t8V8LGMNaPQRXnADiD.png",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "Chengruidong",
                        "type": "user"
                    },
                    "name": "Chengruidong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:40:30.129Z",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9b",
                    "name": "Bailu Ding",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9c",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9d",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9e",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11d9f",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "66e8f30d6646c8f0e8b11da0",
                    "name": "Lili Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-16T17:59:52.000Z",
            "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
            "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
            "upvotes": 11,
            "discussionId": "66e8f30e6646c8f0e8b11e00"
        },
        "publishedAt": "2024-09-17T01:49:09.975Z",
        "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10516.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
            "fullname": "Huiqiang Jiang",
            "name": "iofu728",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.09213",
            "authors": [
                {
                    "_id": "66e8ee1ee39084cfc7f233c9",
                    "name": "Sreyan Ghosh",
                    "hidden": false
                },
                {
                    "_id": "66e8ee1ee39084cfc7f233ca",
                    "user": {
                        "_id": "65203f31d6dec04046139874",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65203f31d6dec04046139874/1LDWW4KblYeIDve9F_kUA.png",
                        "isPro": false,
                        "fullname": "Sonal Kumar",
                        "user": "sonalkum",
                        "type": "user"
                    },
                    "name": "Sonal Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T15:08:14.641Z",
                    "hidden": false
                },
                {
                    "_id": "66e8ee1ee39084cfc7f233cb",
                    "name": "Chandra Kiran Reddy Evuru",
                    "hidden": false
                },
                {
                    "_id": "66e8ee1ee39084cfc7f233cc",
                    "user": {
                        "_id": "657a29d7d3e458405b8dc146",
                        "avatarUrl": "/avatars/1fe425dcf3133462fb6376e8e1550b4a.svg",
                        "isPro": false,
                        "fullname": "Oriol Nieto",
                        "user": "urinieto",
                        "type": "user"
                    },
                    "name": "Oriol Nieto",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:50:57.540Z",
                    "hidden": false
                },
                {
                    "_id": "66e8ee1ee39084cfc7f233cd",
                    "user": {
                        "_id": "65b8f0339b7250e205100190",
                        "avatarUrl": "/avatars/29df91154c5911a44ef503bb3580ac81.svg",
                        "isPro": false,
                        "fullname": "Ramani Duraiswami",
                        "user": "RamaniD",
                        "type": "user"
                    },
                    "name": "Ramani Duraiswami",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:50:51.405Z",
                    "hidden": false
                },
                {
                    "_id": "66e8ee1ee39084cfc7f233ce",
                    "user": {
                        "_id": "6537a569568d8be8fa096b8c",
                        "avatarUrl": "/avatars/bfda5cb252d8b5bc3ad737d99c0d7f49.svg",
                        "isPro": false,
                        "fullname": "Dinesh Manocha",
                        "user": "manocha",
                        "type": "user"
                    },
                    "name": "Dinesh Manocha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:50:44.680Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-13T21:58:20.000Z",
            "title": "ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds",
            "summary": "Open-vocabulary audio-language models, like CLAP, offer a promising approach\nfor zero-shot audio classification (ZSAC) by enabling classification with any\narbitrary set of categories specified with natural language prompts. In this\npaper, we propose a simple but effective method to improve ZSAC with CLAP.\nSpecifically, we shift from the conventional method of using prompts with\nabstract category labels (e.g., Sound of an organ) to prompts that describe\nsounds using their inherent descriptive features in a diverse context (e.g.,The\norgan's deep and resonant tones filled the cathedral.). To achieve this, we\nfirst propose ReCLAP, a CLAP model trained with rewritten audio captions for\nimproved understanding of sounds in the wild. These rewritten captions describe\neach sound event in the original caption using their unique discriminative\ncharacteristics. ReCLAP outperforms all baselines on both multi-modal\naudio-text retrieval and ZSAC. Next, to improve zero-shot audio classification\nwith ReCLAP, we propose prompt augmentation. In contrast to the traditional\nmethod of employing hand-written template prompts, we generate custom prompts\nfor each unique label in the dataset. These custom prompts first describe the\nsound event in the label and then employ them in diverse scenes. Our proposed\nmethod improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all\nbaselines by 1% - 55%.",
            "upvotes": 6,
            "discussionId": "66e8ee1fe39084cfc7f2342c"
        },
        "publishedAt": "2024-09-17T02:24:11.605Z",
        "title": "ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09213.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
            "fullname": "Ghosh",
            "name": "Sreyan88",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.06277",
            "authors": [
                {
                    "_id": "66e4dfaa5c06b7719c362746",
                    "user": {
                        "_id": "66123816d7dfcea8ae55a751",
                        "avatarUrl": "/avatars/3f24468b63e4babd7d9a0c926ca01b23.svg",
                        "isPro": false,
                        "fullname": "Shu Yao",
                        "user": "ZCODE0",
                        "type": "user"
                    },
                    "name": "Yao Shu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-16T07:02:53.062Z",
                    "hidden": false
                },
                {
                    "_id": "66e4dfaa5c06b7719c362747",
                    "name": "Wenyang Hu",
                    "hidden": false
                },
                {
                    "_id": "66e4dfaa5c06b7719c362748",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "66e4dfaa5c06b7719c362749",
                    "name": "Bryan Kian Hsiang Low",
                    "hidden": false
                },
                {
                    "_id": "66e4dfaa5c06b7719c36274a",
                    "name": "Fei Richard Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-10T07:28:13.000Z",
            "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models",
            "summary": "Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.",
            "upvotes": 5,
            "discussionId": "66e4dfab5c06b7719c362792"
        },
        "publishedAt": "2024-09-17T10:27:38.292Z",
        "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66123816d7dfcea8ae55a751/kJVtyZecZB8d6vZJGV8pa.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06277.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/3f24468b63e4babd7d9a0c926ca01b23.svg",
            "fullname": "Shu Yao",
            "name": "ZCODE0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.09269",
            "authors": [
                {
                    "_id": "66e9108fcfd2f509914b1e94",
                    "user": {
                        "_id": "6639a17d7256327c68a0841e",
                        "avatarUrl": "/avatars/28fcd935149c2d89661c6f642842c310.svg",
                        "isPro": false,
                        "fullname": "Neelabh Sinha",
                        "user": "neelabhsinha",
                        "type": "user"
                    },
                    "name": "Neelabh Sinha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T07:45:52.171Z",
                    "hidden": false
                },
                {
                    "_id": "66e9108fcfd2f509914b1e95",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "66e9108fcfd2f509914b1e96",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T07:11:18.674Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-14T02:29:36.000Z",
            "title": "Guiding Vision-Language Model Selection for Visual Question-Answering\n  Across Tasks, Domains, and Knowledge Types",
            "summary": "Visual Question-Answering (VQA) has become a key use-case in several\napplications to aid user experience, particularly after Vision-Language Models\n(VLMs) achieving good results in zero-shot inference. But evaluating different\nVLMs for an application requirement using a standardized framework in practical\nsettings is still challenging. This paper introduces a comprehensive framework\nfor evaluating VLMs tailored to VQA tasks in practical settings. We present a\nnovel dataset derived from established VQA benchmarks, annotated with task\ntypes, application domains, and knowledge types, three key practical aspects on\nwhich tasks can vary. We also introduce GoEval, a multimodal evaluation metric\ndeveloped using GPT-4o, achieving a correlation factor of 56.71% with human\njudgments. Our experiments with ten state-of-the-art VLMs reveals that no\nsingle model excelling universally, making appropriate selection a key design\ndecision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally\noutperform others, though open-source models like InternVL-2-8B and\nCogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts,\nwhile providing additional advantages. This study guides the selection of VLMs\nbased on specific task requirements and resource constraints, and can also be\nextended to other vision-language tasks.",
            "upvotes": 5,
            "discussionId": "66e91090cfd2f509914b1f20"
        },
        "publishedAt": "2024-09-17T03:53:25.182Z",
        "title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09269.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.09502",
            "authors": [
                {
                    "_id": "66e90e49955eeec85e4f6842",
                    "user": {
                        "_id": "6065a9cbe43e52694178ed78",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617275312190-noauth.png",
                        "isPro": false,
                        "fullname": "Emanuele Vivoli",
                        "user": "emanuelevivoli",
                        "type": "user"
                    },
                    "name": "Emanuele Vivoli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T09:30:07.682Z",
                    "hidden": false
                },
                {
                    "_id": "66e90e49955eeec85e4f6843",
                    "name": "Andrey Barsky",
                    "hidden": false
                },
                {
                    "_id": "66e90e49955eeec85e4f6844",
                    "user": {
                        "_id": "66e98dcf899bdb384bd951d6",
                        "avatarUrl": "/avatars/782744017a7a51c546fca81aa1272b94.svg",
                        "isPro": false,
                        "fullname": "Mohamed Ali Souibgui",
                        "user": "msouibgui",
                        "type": "user"
                    },
                    "name": "Mohamed Ali Souibgui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T15:08:12.305Z",
                    "hidden": false
                },
                {
                    "_id": "66e90e49955eeec85e4f6845",
                    "user": {
                        "_id": "630f3e904dbbbb6f667eceb7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630f3e904dbbbb6f667eceb7/GENlN7kDSoGhmIL7N3C8h.png",
                        "isPro": false,
                        "fullname": "Artemis Llabrés",
                        "user": "Llabres",
                        "type": "user"
                    },
                    "name": "Artemis LLabres",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T11:31:49.342Z",
                    "hidden": false
                },
                {
                    "_id": "66e90e49955eeec85e4f6846",
                    "user": {
                        "_id": "6319e9b78b8425c8fe653dd6",
                        "avatarUrl": "/avatars/b1abd8bc31e7e39d7d563cc275656e6c.svg",
                        "isPro": false,
                        "fullname": "Marco Bertini",
                        "user": "MarcoBertini",
                        "type": "user"
                    },
                    "name": "Marco Bertini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T09:31:38.153Z",
                    "hidden": false
                },
                {
                    "_id": "66e90e49955eeec85e4f6847",
                    "name": "Dimosthenis Karatzas",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-14T18:26:26.000Z",
            "title": "One missing piece in Vision and Language: A Survey on Comics\n  Understanding",
            "summary": "Vision-language models have recently evolved into versatile systems capable\nof high performance across a range of tasks, such as document understanding,\nvisual question answering, and grounding, often in zero-shot settings. Comics\nUnderstanding, a complex and multifaceted field, stands to greatly benefit from\nthese advances. Comics, as a medium, combine rich visual and textual\nnarratives, challenging AI models with tasks that span image classification,\nobject detection, instance segmentation, and deeper narrative comprehension\nthrough sequential panels. However, the unique structure of comics --\ncharacterized by creative variations in style, reading order, and non-linear\nstorytelling -- presents a set of challenges distinct from those in other\nvisual-language domains. In this survey, we present a comprehensive review of\nComics Understanding from both dataset and task perspectives. Our contributions\nare fivefold: (1) We analyze the structure of the comics medium, detailing its\ndistinctive compositional elements; (2) We survey the widely used datasets and\ntasks in comics research, emphasizing their role in advancing the field; (3) We\nintroduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy\nthat redefines vision-language tasks within comics and lays the foundation for\nfuture work; (4) We provide a detailed review and categorization of existing\nmethods following the LoCU framework; (5) Finally, we highlight current\nresearch challenges and propose directions for future exploration, particularly\nin the context of vision-language models applied to comics. This survey is the\nfirst to propose a task-oriented framework for comics intelligence and aims to\nguide future research by addressing critical gaps in data availability and task\ndefinition. A project associated with this survey is available at\nhttps://github.com/emanuelevivoli/awesome-comics-understanding.",
            "upvotes": 4,
            "discussionId": "66e90e4d955eeec85e4f69c5"
        },
        "publishedAt": "2024-09-17T07:55:34.483Z",
        "title": "One missing piece in Vision and Language: A Survey on Comics Understanding",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6065a9cbe43e52694178ed78/99uGxw8bLvrAg_fIZyUx5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09502.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617275312190-noauth.png",
            "fullname": "Emanuele Vivoli",
            "name": "emanuelevivoli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.10173",
            "authors": [
                {
                    "_id": "66e926caff154e75c55e597f",
                    "user": {
                        "_id": "64c23f6d569648a60737eddb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c23f6d569648a60737eddb/iZq7bp-yYaGl5VBVoN5Dg.jpeg",
                        "isPro": false,
                        "fullname": "Saba Sturua",
                        "user": "jupyterjazz",
                        "type": "user"
                    },
                    "name": "Saba Sturua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:08:29.709Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5980",
                    "user": {
                        "_id": "64a830cd6cc1a9a131f62619",
                        "avatarUrl": "/avatars/e7b0ad62e8fb8c99c8d75edf45688995.svg",
                        "isPro": false,
                        "fullname": "Isabelle Mohr",
                        "user": "isacat",
                        "type": "user"
                    },
                    "name": "Isabelle Mohr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:08:44.633Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5981",
                    "user": {
                        "_id": "64d22f33032a420d1863b6ea",
                        "avatarUrl": "/avatars/ed3eaf4bab70dd6ab9a2b67b5928e4fb.svg",
                        "isPro": false,
                        "fullname": "Mohammad Kalim Akram",
                        "user": "makram93",
                        "type": "user"
                    },
                    "name": "Mohammad Kalim Akram",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:08:51.647Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5982",
                    "user": {
                        "_id": "6476ff2699a5ce743ccea3fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6476ff2699a5ce743ccea3fc/zmFmF8tXXDaAGcl8RYiRr.jpeg",
                        "isPro": false,
                        "fullname": "Michael Günther",
                        "user": "michael-guenther",
                        "type": "user"
                    },
                    "name": "Michael Günther",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:09:15.549Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5983",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5984",
                    "user": {
                        "_id": "651e707ca92910108b12ecf4",
                        "avatarUrl": "/avatars/140ef2d6980aa2d7fec1550d245fc33e.svg",
                        "isPro": false,
                        "fullname": "Markus Krimmel",
                        "user": "Markus28",
                        "type": "user"
                    },
                    "name": "Markus Krimmel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:10:04.475Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5985",
                    "name": "Feng Wang",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5986",
                    "user": {
                        "_id": "64f8620e492828088373ddf9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8620e492828088373ddf9/g6XQmUzEPNMQYNc34gYpR.jpeg",
                        "isPro": false,
                        "fullname": "Georgios Mastrapas",
                        "user": "gmastrapas",
                        "type": "user"
                    },
                    "name": "Georgios Mastrapas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:10:19.566Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5987",
                    "user": {
                        "_id": "651e7084570ba4662812114b",
                        "avatarUrl": "/avatars/1438e5caa483f63dc0da5ee7508ef7eb.svg",
                        "isPro": false,
                        "fullname": "Andreas Koukounas",
                        "user": "koukandre",
                        "type": "user"
                    },
                    "name": "Andreas Koukounas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:10:25.511Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5988",
                    "user": {
                        "_id": "651e7084570ba4662812114b",
                        "avatarUrl": "/avatars/1438e5caa483f63dc0da5ee7508ef7eb.svg",
                        "isPro": false,
                        "fullname": "Andreas Koukounas",
                        "user": "koukandre",
                        "type": "user"
                    },
                    "name": "Andreas Koukounas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:10:31.608Z",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e5989",
                    "name": "Nan Wang",
                    "hidden": false
                },
                {
                    "_id": "66e926caff154e75c55e598a",
                    "user": {
                        "_id": "603763514de52ff951d89793",
                        "avatarUrl": "/avatars/4484b361bb887fc73a6d38d953bfe9e0.svg",
                        "isPro": false,
                        "fullname": "Han Xiao",
                        "user": "hanxiao",
                        "type": "user"
                    },
                    "name": "Han Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:14:54.542Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-16T11:10:29.000Z",
            "title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
            "summary": "We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Additionally, Matryoshka\nRepresentation Learning is integrated into the training process, allowing\nflexible truncation of embedding dimensions without compromising performance.\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the\nlatest proprietary embeddings from OpenAI and Cohere on English tasks, while\nachieving superior performance compared to multilingual-e5-large-instruct\nacross all multilingual tasks.",
            "upvotes": 3,
            "discussionId": "66e926cbff154e75c55e59a7"
        },
        "publishedAt": "2024-09-17T12:41:33.374Z",
        "title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10173.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.10038",
            "authors": [
                {
                    "_id": "66e928df2ff641e80cf92362",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T07:11:13.884Z",
                    "hidden": false
                },
                {
                    "_id": "66e928df2ff641e80cf92363",
                    "name": "Yang Yuan",
                    "hidden": false
                },
                {
                    "_id": "66e928df2ff641e80cf92364",
                    "name": "Andrew Chi-Chih Yao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-16T07:01:41.000Z",
            "title": "On the Diagram of Thought",
            "summary": "We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought.",
            "upvotes": 3,
            "discussionId": "66e928df2ff641e80cf923a7"
        },
        "publishedAt": "2024-09-17T05:38:59.316Z",
        "title": "On the Diagram of Thought",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/WejVqeof7UZsIr_TZW0Dx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10038.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.08199",
            "authors": [
                {
                    "_id": "66e949bd1fd0c4f1a55cc24a",
                    "user": {
                        "_id": "631974d51328b6caf9fe328f",
                        "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
                        "isPro": false,
                        "fullname": "Hyunjong Ok",
                        "user": "HJOK",
                        "type": "user"
                    },
                    "name": "Hyunjong Ok",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T09:30:25.970Z",
                    "hidden": false
                },
                {
                    "_id": "66e949bd1fd0c4f1a55cc24b",
                    "user": {
                        "_id": "651930780e3a5553d4a04c66",
                        "avatarUrl": "/avatars/35f1a91a2ed920253274c010abc6741d.svg",
                        "isPro": false,
                        "fullname": "suho yoo",
                        "user": "uso7d0",
                        "type": "user"
                    },
                    "name": "Suho Yoo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T09:39:29.808Z",
                    "hidden": false
                },
                {
                    "_id": "66e949bd1fd0c4f1a55cc24c",
                    "name": "Jaeho Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-12T16:36:39.000Z",
            "title": "AudioBERT: Audio Knowledge Augmented Language Model",
            "summary": "Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, e.g., colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the auditory knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at https://github.com/HJ-Ok/AudioBERT.",
            "upvotes": 2,
            "discussionId": "66e949be1fd0c4f1a55cc27f"
        },
        "publishedAt": "2024-09-17T08:03:20.222Z",
        "title": "AudioBERT: Audio Knowledge Augmented Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08199.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg",
            "fullname": "Hyunjong Ok",
            "name": "HJOK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.06957",
            "authors": [
                {
                    "_id": "66e984604e8e48237d978b46",
                    "user": {
                        "_id": "6468823272d9180d4ac90bdf",
                        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                        "isPro": false,
                        "fullname": "Wei Shen",
                        "user": "Swtheking",
                        "type": "user"
                    },
                    "name": "Wei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:16:03.982Z",
                    "hidden": false
                },
                {
                    "_id": "66e984604e8e48237d978b47",
                    "user": {
                        "_id": "64c32727c370f29a10334d35",
                        "avatarUrl": "/avatars/a3284bdd2e51f437433b89a96a904448.svg",
                        "isPro": false,
                        "fullname": "ZhangChuheng",
                        "user": "zhangchuheng123",
                        "type": "user"
                    },
                    "name": "Chuheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T15:16:16.744Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-11T02:40:38.000Z",
            "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
            "summary": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination (R^2) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.",
            "upvotes": 1,
            "discussionId": "66e984614e8e48237d978b6c"
        },
        "publishedAt": "2024-09-17T12:01:11.612Z",
        "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06957.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "fullname": "Wei Shen",
            "name": "Swtheking",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.07012",
            "authors": [
                {
                    "_id": "66e3bf06dba1e4fee4b33434",
                    "user": {
                        "_id": "645cd00f5ebf379fd6d7a4c1",
                        "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
                        "isPro": false,
                        "fullname": "Daeun Kyung",
                        "user": "dek924",
                        "type": "user"
                    },
                    "name": "Daeun Kyung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T11:33:57.894Z",
                    "hidden": false
                },
                {
                    "_id": "66e3bf06dba1e4fee4b33435",
                    "user": {
                        "_id": "64675818a9b4610868a134a5",
                        "avatarUrl": "/avatars/3a4fb9b70934fa2fbbf1aa21582bf77d.svg",
                        "isPro": false,
                        "fullname": "Junu Kim",
                        "user": "starmpcc",
                        "type": "user"
                    },
                    "name": "Junu Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T11:34:03.399Z",
                    "hidden": false
                },
                {
                    "_id": "66e3bf06dba1e4fee4b33436",
                    "name": "Tackeun Kim",
                    "hidden": false
                },
                {
                    "_id": "66e3bf06dba1e4fee4b33437",
                    "user": {
                        "_id": "65e8fb912dcdb39d1934260c",
                        "avatarUrl": "/avatars/358504b57f2a600753e9bc459091061b.svg",
                        "isPro": false,
                        "fullname": "Edward Choi",
                        "user": "forgetnight",
                        "type": "user"
                    },
                    "name": "Edward Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T11:34:29.235Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-11T04:49:44.000Z",
            "title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images\n  based on Electronic Health Records",
            "summary": "Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals\nto assess patient conditions and monitor changes over time. Generative models,\nspecifically diffusion-based models, have shown promise in generating realistic\nsynthetic X-rays. However, these models mainly focus on conditional generation\nusing single-time-point data, i.e., typically CXRs taken at a specific time\nwith their corresponding reports, limiting their clinical utility, particularly\nfor capturing temporal changes. To address this limitation, we propose a novel\nframework, EHRXDiff, which predicts future CXR images by integrating previous\nCXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.\nOur framework dynamically tracks and predicts disease progression based on a\nlatent diffusion model, conditioned on the previous CXR image and a history of\nmedical events. We comprehensively evaluate the performance of our framework\nacross three key aspects, including clinical consistency, demographic\nconsistency, and visual realism. We demonstrate that our framework generates\nhigh-quality, realistic future images that capture potential temporal changes,\nsuggesting its potential for further development as a clinical simulation tool.\nThis could offer valuable insights for patient monitoring and treatment\nplanning in the medical field.",
            "upvotes": 1,
            "discussionId": "66e3bf07dba1e4fee4b33493"
        },
        "publishedAt": "2024-09-17T08:57:16.774Z",
        "title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.07012.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
            "fullname": "Daeun Kyung",
            "name": "dek924",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.08831",
            "authors": [
                {
                    "_id": "66e98d0914066ff97210db29",
                    "user": {
                        "_id": "662a3289fb04f23fa920f4f0",
                        "avatarUrl": "/avatars/13c51dc7930845b8f2db9ca5f868e15c.svg",
                        "isPro": false,
                        "fullname": "Andreas Plesner",
                        "user": "aplesner-eth",
                        "type": "user"
                    },
                    "name": "Andreas Plesner",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-09-17T14:15:43.822Z",
                    "hidden": false
                },
                {
                    "_id": "66e98d0914066ff97210db2a",
                    "name": "Tobias Vontobel",
                    "hidden": false
                },
                {
                    "_id": "66e98d0914066ff97210db2b",
                    "user": {
                        "_id": "64afd0c109d6573c1d89dbf7",
                        "avatarUrl": "/avatars/ed6bd7133517590f46847a0930d7a049.svg",
                        "isPro": false,
                        "fullname": "Roger Wattenhofer",
                        "user": "Rogrrr",
                        "type": "user"
                    },
                    "name": "Roger Wattenhofer",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-09-17T14:07:06.724Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-13T13:47:12.000Z",
            "title": "Breaking reCAPTCHAv2",
            "summary": "Our work examines the efficacy of employing advanced machine learning methods\nto solve captchas from Google's reCAPTCHAv2 system. We evaluate the\neffectiveness of automated systems in solving captchas by utilizing advanced\nYOLO models for image segmentation and classification. Our main result is that\nwe can solve 100% of the captchas, while previous work only solved 68-71%.\nFurthermore, our findings suggest that there is no significant difference in\nthe number of challenges humans and bots must solve to pass the captchas in\nreCAPTCHAv2. This implies that current AI technologies can exploit advanced\nimage-based captchas. We also look under the hood of reCAPTCHAv2, and find\nevidence that reCAPTCHAv2 is heavily based on cookie and browser history data\nwhen evaluating whether a user is human or not. The code is provided alongside\nthis paper.",
            "upvotes": 0,
            "discussionId": "66e98d0a14066ff97210db6d"
        },
        "publishedAt": "2024-09-17T12:37:18.012Z",
        "title": "Breaking reCAPTCHAv2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08831.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.10309",
            "authors": [
                {
                    "_id": "66e939a1218585b91b9c81d6",
                    "user": {
                        "_id": "6641d8f7f448effbae1a32ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6641d8f7f448effbae1a32ca/mgT3E_Q9rWgHGpeLKHex7.png",
                        "isPro": false,
                        "fullname": "Vojtěch Vančura",
                        "user": "beeformer",
                        "type": "user"
                    },
                    "name": "Vojtěch Vančura",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-17T09:30:33.019Z",
                    "hidden": false
                },
                {
                    "_id": "66e939a1218585b91b9c81d7",
                    "name": "Pavel Kordík",
                    "hidden": false
                },
                {
                    "_id": "66e939a1218585b91b9c81d8",
                    "user": {
                        "_id": "607c806188160e14e4e2e4c8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1618772050171-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Milan Straka",
                        "user": "foxik",
                        "type": "user"
                    },
                    "name": "Milan Straka",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-17T11:34:56.923Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-16T14:15:42.000Z",
            "title": "beeFormer: Bridging the Gap Between Semantic and Interaction Similarity\n  in Recommender Systems",
            "summary": "Recommender systems often use text-side information to improve their\npredictions, especially in cold-start or zero-shot recommendation scenarios,\nwhere traditional collaborative filtering approaches cannot be used. Many\napproaches to text-mining side information for recommender systems have been\nproposed over recent years, with sentence Transformers being the most prominent\none. However, these models are trained to predict semantic similarity without\nutilizing interaction data with hidden patterns specific to recommender\nsystems. In this paper, we propose beeFormer, a framework for training sentence\nTransformer models with interaction data. We demonstrate that our models\ntrained with beeFormer can transfer knowledge between datasets while\noutperforming not only semantic similarity sentence Transformers but also\ntraditional collaborative filtering methods. We also show that training on\nmultiple datasets from different domains accumulates knowledge in a single\nmodel, unlocking the possibility of training universal, domain-agnostic\nsentence Transformer models to mine text representations for recommender\nsystems. We release the source code, trained models, and additional details\nallowing replication of our experiments at\nhttps://github.com/recombee/beeformer.",
            "upvotes": 0,
            "discussionId": "66e939a1218585b91b9c8200"
        },
        "publishedAt": "2024-09-17T09:57:52.745Z",
        "title": "beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10309.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6641d8f7f448effbae1a32ca/mgT3E_Q9rWgHGpeLKHex7.png",
            "fullname": "Vojtěch Vančura",
            "name": "beeformer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]