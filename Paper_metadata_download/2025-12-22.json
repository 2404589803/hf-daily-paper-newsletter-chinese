[
  {
    "paper": {
      "id": "2512.16793",
      "authors": [
        {
          "_id": "6944ba3efbf17e708e185f60",
          "user": {
            "_id": "6944c28d3cd5eeb7a7838663",
            "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg",
            "isPro": false,
            "fullname": "lin",
            "user": "birdxp",
            "type": "user"
          },
          "name": "Xiaopeng Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-19T08:58:14.947Z",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f61",
          "user": {
            "_id": "65ec01fd770aa0e25d9374dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
            "isPro": false,
            "fullname": "Shijie Lian",
            "user": "LiamLian0727",
            "type": "user"
          },
          "name": "Shijie Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-19T08:58:18.093Z",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f62",
          "name": "Bin Yu",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f63",
          "name": "Ruoqi Yang",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f64",
          "name": "Changti Wu",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f65",
          "name": "Yuzhuo Miao",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f66",
          "name": "Yurun Jin",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f67",
          "name": "Yukun Shi",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f68",
          "name": "Cong Huang",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f69",
          "name": "Bojun Cheng",
          "hidden": false
        },
        {
          "_id": "6944ba3efbf17e708e185f6a",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T17:27:03.000Z",
      "submittedOnDailyAt": "2025-12-22T00:59:36.650Z",
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "submittedOnDailyBy": {
        "_id": "6944c28d3cd5eeb7a7838663",
        "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg",
        "isPro": false,
        "fullname": "lin",
        "user": "birdxp",
        "type": "user"
      },
      "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
      "upvotes": 45,
      "discussionId": "6944ba3efbf17e708e185f6b",
      "projectPage": "https://zgc-embodyai.github.io/PhysBrain/",
      "ai_summary": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.",
      "ai_keywords": [
        "Egocentric2Embodiment",
        "VQA supervision",
        "evidence grounding",
        "temporal consistency",
        "Egocentric2Embodiment dataset",
        "E2E-3M",
        "PhysBrain",
        "egocentric-aware",
        "VLA fine-tuning",
        "SimplerEnv success rates"
      ],
      "organization": {
        "_id": "6948d884070dda0c2ae35a78",
        "name": "DeepCybo",
        "fullname": "DeepCybo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"
      }
    },
    "publishedAt": "2025-12-18T12:27:03.000Z",
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6944c28d3cd5eeb7a7838663",
      "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg",
      "fullname": "lin",
      "name": "birdxp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "6948d884070dda0c2ae35a78",
      "name": "DeepCybo",
      "fullname": "DeepCybo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.16969",
      "authors": [
        {
          "_id": "6948b09934f46eaf46cbb214",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb215",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb216",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb217",
          "name": "Qinglong Cao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb218",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb219",
          "name": "Jia Bu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21a",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21b",
          "name": "Yixin Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21c",
          "name": "Xuming He",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21d",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21e",
          "name": "Xiang Zhuang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21f",
          "name": "Fengxiang Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb220",
          "name": "Zhiwang Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb221",
          "name": "Qiantai Feng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb222",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb223",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb224",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb225",
          "name": "Yuejin Yang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb226",
          "name": "Guangshuai Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb227",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb228",
          "name": "Ziyan Huang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb229",
          "name": "Xinyao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22a",
          "name": "Jiyao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22b",
          "name": "Cheng Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22c",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22d",
          "name": "Ying Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22e",
          "name": "Junzhi Ning",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22f",
          "name": "Pengfei Jiang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb230",
          "name": "Chenglong Ma",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb231",
          "name": "Ye Du",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb232",
          "name": "Changkai Ji",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb233",
          "name": "Huihui Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb234",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb235",
          "name": "Jiangbin Zheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb236",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb237",
          "name": "Yucheng Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb238",
          "name": "Feifei Jiang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb239",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23a",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23b",
          "name": "Yuchen Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23c",
          "name": "Yingzhou Lu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23d",
          "name": "Yuanyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23e",
          "name": "Lihao Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23f",
          "name": "Chengbo Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb240",
          "name": "Jinzhe Ma",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb241",
          "name": "Wanhao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb242",
          "name": "Yating Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb243",
          "name": "Kuo-Cheng Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb244",
          "name": "Shengdu Chai",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb245",
          "name": "Yizhou Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb246",
          "name": "Ouwen Zhangjin",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb247",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb248",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb249",
          "name": "Wenbo Cao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24a",
          "name": "Junjie Ren",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24b",
          "name": "Taoyong Cui",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24c",
          "name": "Zhouheng Yao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24d",
          "name": "Juntao Deng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24e",
          "name": "Yijie Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24f",
          "name": "Feng Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb250",
          "name": "Wangxu Wei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb251",
          "name": "Jingyi Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb252",
          "name": "Zhangrui Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb253",
          "name": "Junchao Gong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb254",
          "name": "Zijie Guo",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb255",
          "name": "Zhiyu Yao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb256",
          "name": "Zaoyu Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb257",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb258",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb259",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25a",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25b",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25c",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25d",
          "name": "Fenghua Ling",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25e",
          "name": "Yan Lu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25f",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb260",
          "name": "Ben Fei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb261",
          "name": "Zhen Zhao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb262",
          "name": "Xinyu Gu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb263",
          "name": "Rui Su",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb264",
          "name": "Xiao-Ming Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb265",
          "name": "Weikang Si",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb266",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb267",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb268",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb269",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26a",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26b",
          "name": "Jiamin Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26c",
          "name": "Qihao Zheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26d",
          "name": "Chenhui Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26e",
          "name": "Zhiqiang Gao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26f",
          "name": "Hao Kong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb270",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb271",
          "name": "Mao Su",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb272",
          "name": "Tianfan Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb273",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb274",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb275",
          "name": "Nanqing Dong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb276",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb277",
          "name": "Huazhu Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb278",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb279",
          "name": "Lijing Cheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27a",
          "name": "Jintai Lin",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27b",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27c",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27d",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27e",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T12:44:36.000Z",
      "submittedOnDailyAt": "2025-12-22T00:14:52.424Z",
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
      "upvotes": 45,
      "discussionId": "6948b09934f46eaf46cbb27f",
      "projectPage": "https://internscience.github.io/SGI-Page/",
      "githubRepo": "https://github.com/InternScience/SGI-Bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.",
      "ai_keywords": [
        "Scientific General Intelligence",
        "SGI",
        "Practical Inquiry Model",
        "PIM",
        "deep research",
        "idea generation",
        "dry experiments",
        "wet experiments",
        "experimental reasoning",
        "SGI-Bench",
        "Big Questions",
        "Low exact match",
        "feasibility",
        "detail",
        "code executability",
        "execution result accuracy",
        "sequence fidelity",
        "multimodal comparative-reasoning",
        "Test-Time Reinforcement Learning",
        "TTRL",
        "retrieval-augmented novelty rewards",
        "hypothesis novelty"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-12-18T07:44:36.000Z",
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17901",
      "authors": [
        {
          "_id": "6948af7534f46eaf46cbb1da",
          "name": "Junyu Zhang",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1db",
          "name": "Yifan Sun",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1dc",
          "name": "Tianang Leng",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1dd",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1de",
          "name": "Liu Ziyin",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1df",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1e0",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T18:59:11.000Z",
      "submittedOnDailyAt": "2025-12-22T00:10:07.525Z",
      "title": "When Reasoning Meets Its Laws",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
      "upvotes": 36,
      "discussionId": "6948af7534f46eaf46cbb1e1",
      "projectPage": "https://lore-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/LoRe",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.",
      "ai_keywords": [
        "Laws of Reasoning",
        "LoRe",
        "compute law",
        "accuracy law",
        "question complexity",
        "monotonicity",
        "compositionality",
        "LoRe-Bench",
        "finetuning approach",
        "compute-law compositionality"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-12-19T13:59:11.000Z",
    "title": "When Reasoning Meets Its Laws",
    "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17260",
      "authors": [
        {
          "_id": "6948afc434f46eaf46cbb1f1",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f2",
          "name": "Wenxiang Chen",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f3",
          "name": "Jiacheng Du",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f4",
          "name": "Jinyi Hu",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f5",
          "name": "Zhicheng Jiang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f6",
          "name": "Allan Jie",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f7",
          "name": "Xiaoran Jin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f8",
          "name": "Xing Jin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f9",
          "name": "Chenggang Li",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fa",
          "name": "Wenlei Shi",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fb",
          "name": "Zhihong Wang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fc",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fd",
          "name": "Chenrui Wei",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fe",
          "name": "Shufa Wei",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1ff",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb200",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb201",
          "name": "Weihao Gao",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb202",
          "name": "Zheng Yuan",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb203",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb204",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb205",
          "name": "Tianxi Zhou",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb206",
          "name": "Thomas Hanwen Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T06:19:55.000Z",
      "submittedOnDailyAt": "2025-12-22T00:11:16.085Z",
      "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
      "upvotes": 30,
      "discussionId": "6948afc534f46eaf46cbb207",
      "ai_summary": "Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.",
      "ai_keywords": [
        "large language models",
        "theorem proving",
        "formal languages",
        "Lean",
        "reinforcement learning",
        "test-time scaling",
        "PutnamBench",
        "Fate-H",
        "Fate-X",
        "formal mathematical reasoning"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-12-19T01:19:55.000Z",
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17260.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17012",
      "authors": [
        {
          "_id": "6948aa3d34f46eaf46cbb1cb",
          "name": "Chiao-An Yang",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cc",
          "name": "Ryo Hachiuma",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cd",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1ce",
          "name": "Subhashree Radhakrishnan",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cf",
          "name": "Raymond A. Yeh",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1d0",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1d1",
          "name": "Min-Hung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T19:13:44.000Z",
      "submittedOnDailyAt": "2025-12-22T00:08:35.679Z",
      "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
      "upvotes": 24,
      "discussionId": "6948aa3d34f46eaf46cbb1d2",
      "projectPage": "https://ca-joe-yang.github.io/resource/projects/4D_RGPT",
      "ai_summary": "4D-RGPT, a specialized multimodal LLM, enhances 4D perception in video inputs through Perceptual 4D Distillation and is evaluated on R4D-Bench, a new benchmark for depth-aware dynamic scenes.",
      "ai_keywords": [
        "4D-RGPT",
        "Multimodal LLMs",
        "4D Video Question Answering",
        "4D representations",
        "Perceptual 4D Distillation",
        "R4D-Bench",
        "depth-aware",
        "dynamic scenes",
        "region-level prompting"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-12-18T14:13:44.000Z",
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17909",
      "authors": [
        {
          "_id": "6948d11034f46eaf46cbb338",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb339",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33a",
          "name": "Zhifei Zhang",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33b",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33c",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33d",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33e",
          "name": "Mengwei Ren",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb33f",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb340",
          "name": "Yuqian Zhou",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb341",
          "name": "Qing Liu",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb342",
          "name": "Daniil Pakhomov",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb343",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb344",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "6948d11034f46eaf46cbb345",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T18:59:57.000Z",
      "submittedOnDailyAt": "2025-12-22T03:13:08.364Z",
      "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "6424ffce46d202ad3d918a67",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg",
        "isPro": false,
        "fullname": "Shilong Zhang",
        "user": "shilongz",
        "type": "user"
      },
      "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
      "upvotes": 16,
      "discussionId": "6948d11034f46eaf46cbb346",
      "projectPage": "https://jshilong.github.io/PS-VAE-PAGE/",
      "ai_summary": "A systematic framework adapts understanding-oriented encoder features for generative tasks by introducing a semantic-pixel reconstruction objective, enabling state-of-the-art image reconstruction and generation.",
      "ai_keywords": [
        "Latent Diffusion Models",
        "VAE",
        "latent space",
        "generative latents",
        "discriminative feature space",
        "off-manifold latents",
        "pixel-level reconstruction",
        "semantic-pixel reconstruction objective",
        "Text-to-Image",
        "image editing",
        "feature spaces"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-12-19T13:59:57.000Z",
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17909.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424ffce46d202ad3d918a67",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg",
      "fullname": "Shilong Zhang",
      "name": "shilongz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.16041",
      "authors": [
        {
          "_id": "6944c0a4fbf17e708e186021",
          "name": "Yuanning Feng",
          "hidden": false
        },
        {
          "_id": "6944c0a4fbf17e708e186022",
          "name": "Sinan Wang",
          "hidden": false
        },
        {
          "_id": "6944c0a4fbf17e708e186023",
          "name": "Zhengxiang Cheng",
          "hidden": false
        },
        {
          "_id": "6944c0a4fbf17e708e186024",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "6944c0a4fbf17e708e186025",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643be8879f5d314db2d9ed23/HvOcmtVvwfWr0mRw_E8Uz.png"
      ],
      "publishedAt": "2025-12-17T23:49:55.000Z",
      "submittedOnDailyAt": "2025-12-22T01:17:34.013Z",
      "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
      "upvotes": 16,
      "discussionId": "6944c0a4fbf17e708e186026",
      "ai_summary": "Sage is a human-free evaluation suite for LLM-as-a-Judge, using rational choice theory to assess local and global consistency, revealing significant reliability issues with current LLM judges.",
      "ai_keywords": [
        "LLM-as-a-Judge",
        "human-annotated ground truth",
        "human bias",
        "scalability constraints",
        "rational choice theory",
        "local self-consistency",
        "global logical consistency",
        "pairwise preference stability",
        "transitivity",
        "LLMBar",
        "RewardBench2",
        "reliability problems",
        "scoring",
        "pairwise settings",
        "situational preference",
        "finetuned LLM-as-a-Judge",
        "panel-based judge",
        "deep reasoning",
        "human judgments"
      ],
      "organization": {
        "_id": "6822dcdbce3e59dd136b24fa",
        "name": "ONE-Lab",
        "fullname": "ONE Lab"
      }
    },
    "publishedAt": "2025-12-17T18:49:55.000Z",
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643be8879f5d314db2d9ed23/HvOcmtVvwfWr0mRw_E8Uz.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "6822dcdbce3e59dd136b24fa",
      "name": "ONE-Lab",
      "fullname": "ONE Lab"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17351",
      "authors": [
        {
          "_id": "6948bb2734f46eaf46cbb2ec",
          "name": "Zeyuan Allen-Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T08:47:28.000Z",
      "submittedOnDailyAt": "2025-12-22T01:04:44.279Z",
      "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
      "submittedOnDailyBy": {
        "_id": "6316d14c0db6c679cf77fe1f",
        "avatarUrl": "/avatars/b33dcadc0ff8941da6aa850284be3207.svg",
        "isPro": false,
        "fullname": "Zeyuan Allen-Zhu",
        "user": "zhuzeyuan",
        "type": "user"
      },
      "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
      "upvotes": 5,
      "discussionId": "6948bb2834f46eaf46cbb2ed",
      "projectPage": "https://physics.allen-zhu.com/part-4-architecture-design/part-4-1",
      "githubRepo": "https://github.com/facebookresearch/PhysicsLM4",
      "githubRepoAddedBy": "user",
      "ai_summary": "Canon layers, lightweight architectural components, enhance reasoning depth and breadth in language models, improving performance in synthetic and real-world pretraining tasks.",
      "ai_keywords": [
        "CANON LAYERS",
        "horizontal information flow",
        "weighted sums",
        "Transformer",
        "linear attention",
        "state-space models",
        "sequence architecture",
        "reasoning depth",
        "reasoning breadth",
        "knowledge manipulation",
        "NoPE",
        "RoPE",
        "Mamba2",
        "GDN",
        "synthetic tasks",
        "academic-scale pretraining",
        "future architectures",
        "data curation",
        "RL-based post-training",
        "hierarchical inference"
      ],
      "githubStars": 275,
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-12-19T03:47:28.000Z",
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6316d14c0db6c679cf77fe1f",
      "avatarUrl": "/avatars/b33dcadc0ff8941da6aa850284be3207.svg",
      "fullname": "Zeyuan Allen-Zhu",
      "name": "zhuzeyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 28
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17008",
      "authors": [
        {
          "_id": "6948bf7734f46eaf46cbb30e",
          "name": "Junbo Li",
          "hidden": false
        },
        {
          "_id": "6948bf7734f46eaf46cbb30f",
          "name": "Peng Zhou",
          "hidden": false
        },
        {
          "_id": "6948bf7734f46eaf46cbb310",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "6948bf7734f46eaf46cbb311",
          "name": "Meet P. Vadera",
          "hidden": false
        },
        {
          "_id": "6948bf7734f46eaf46cbb312",
          "name": "Lihong Li",
          "hidden": false
        },
        {
          "_id": "6948bf7734f46eaf46cbb313",
          "name": "Yang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T19:07:25.000Z",
      "submittedOnDailyAt": "2025-12-22T01:19:20.014Z",
      "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
      "submittedOnDailyBy": {
        "_id": "6317d681743def9b3612ebeb",
        "avatarUrl": "/avatars/9940b35d5f8ae5df417bb020b609a8d0.svg",
        "isPro": false,
        "fullname": "Junbo Li",
        "user": "ljb121002",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
      "upvotes": 4,
      "discussionId": "6948bf7734f46eaf46cbb314",
      "ai_summary": "Turn-PPO, a variant of PPO tailored for multi-turn RL tasks, improves advantage estimation and performance over GRPO in environments requiring long-horizon reasoning.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "Proximal Policy Optimization (PPO)",
        "turn-PPO",
        "multi-turn tasks",
        "long-horizon reasoning",
        "MDP formulation"
      ]
    },
    "publishedAt": "2025-12-18T14:07:25.000Z",
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17008.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6317d681743def9b3612ebeb",
      "avatarUrl": "/avatars/9940b35d5f8ae5df417bb020b609a8d0.svg",
      "fullname": "Junbo Li",
      "name": "ljb121002",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.14870",
      "authors": [
        {
          "_id": "6943df7e47055eb55aade016",
          "name": "Dan Ben-Ami",
          "hidden": false
        },
        {
          "_id": "6943df7e47055eb55aade017",
          "name": "Gabriele Serussi",
          "hidden": false
        },
        {
          "_id": "6943df7e47055eb55aade018",
          "name": "Kobi Cohen",
          "hidden": false
        },
        {
          "_id": "6943df7e47055eb55aade019",
          "name": "Chaim Baskin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-16T19:34:47.000Z",
      "submittedOnDailyAt": "2025-12-22T03:11:14.787Z",
      "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
      "submittedOnDailyBy": {
        "_id": "66214a94bbe70ad73f95b99b",
        "avatarUrl": "/avatars/fb16480dbb4eab9e1324cc73259a276c.svg",
        "isPro": false,
        "fullname": "Gabriele Serussi",
        "user": "GSerussi",
        "type": "user"
      },
      "summary": "Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.",
      "upvotes": 4,
      "discussionId": "6943df7e47055eb55aade01a",
      "projectPage": "https://herbench.github.io/",
      "githubRepo": "https://github.com/DanBenAmi/HERBench",
      "githubRepoAddedBy": "user",
      "ai_summary": "HERBench is a VideoQA benchmark that tests models' ability to integrate multiple, temporally separated visual cues, revealing significant challenges in current Video-LLMs.",
      "ai_keywords": [
        "Video Large Language Models",
        "VideoQA",
        "HERBench",
        "multi-evidence integration",
        "Minimum Required Frame-Set",
        "frame selectors",
        "fusion deficit",
        "video understanding"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "673d95462697317020d017a4",
        "name": "Insight-bgu",
        "fullname": "INSIGHT Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/671d53d3bc8e45af41e16f39/mnH2Th56cpCab0SlLJLx7.png"
      }
    },
    "publishedAt": "2025-12-16T14:34:47.000Z",
    "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
    "summary": "Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14870.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66214a94bbe70ad73f95b99b",
      "avatarUrl": "/avatars/fb16480dbb4eab9e1324cc73259a276c.svg",
      "fullname": "Gabriele Serussi",
      "name": "GSerussi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "673d95462697317020d017a4",
      "name": "Insight-bgu",
      "fullname": "INSIGHT Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/671d53d3bc8e45af41e16f39/mnH2Th56cpCab0SlLJLx7.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17495",
      "authors": [
        {
          "_id": "6948dc2634f46eaf46cbb34d",
          "name": "Rang Li",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb34e",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb34f",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb350",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb351",
          "name": "Shuhao Gu",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb352",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb353",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb354",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb355",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb356",
          "name": "Zhe Yang",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb357",
          "name": "Jingyuan Ma",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb358",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "6948dc2634f46eaf46cbb359",
          "name": "Fuli Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T12:06:25.000Z",
      "submittedOnDailyAt": "2025-12-22T03:21:37.716Z",
      "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
      "upvotes": 3,
      "discussionId": "6948dc2734f46eaf46cbb35a",
      "projectPage": "https://groundingme.github.io/",
      "ai_summary": "GroundingME benchmark evaluates multimodal large language models' visual grounding capabilities across complexity dimensions, revealing significant gaps and proposing strategies for improvement.",
      "ai_keywords": [
        "multimodal large language models",
        "visual grounding",
        "GroundingME",
        "discriminative",
        "spatial",
        "limited",
        "rejection",
        "thinking trajectory",
        "data-mixture training"
      ],
      "organization": {
        "_id": "680cb4c37f289defb2210940",
        "name": "XiaomiMiMo",
        "fullname": "Xiaomi MiMo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
      }
    },
    "publishedAt": "2025-12-19T07:06:25.000Z",
    "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17495.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "organization": {
      "_id": "680cb4c37f289defb2210940",
      "name": "XiaomiMiMo",
      "fullname": "Xiaomi MiMo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17419",
      "authors": [
        {
          "_id": "6948b03934f46eaf46cbb209",
          "name": "Lilin Wang",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20a",
          "name": "Lucas Ramalho",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20b",
          "name": "Alan Celestino",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20c",
          "name": "Phuc Anthony Pham",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20d",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20e",
          "name": "Umang Kumar Sinha",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20f",
          "name": "Andres Portillo",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb210",
          "name": "Onassis Osunwa",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb211",
          "name": "Gabriel Maduekwe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T10:16:51.000Z",
      "submittedOnDailyAt": "2025-12-22T00:13:29.948Z",
      "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
      "upvotes": 3,
      "discussionId": "6948b03a34f46eaf46cbb212",
      "projectPage": "https://research.turing.com/swebench",
      "ai_summary": "SWE-Bench++ is an automated framework generating repository-level coding tasks from live GitHub pull requests, offering a scalable, multilingual benchmark for evaluating and improving code generation models.",
      "ai_keywords": [
        "Large Language Models",
        "SWE-bench",
        "SWE-Bench++",
        "GitHub pull requests",
        "programmatic sourcing",
        "environment synthesis",
        "test oracle extraction",
        "quality assurance",
        "hint-guided trajectory synthesis",
        "pass@10",
        "SWE-bench Multilingual"
      ],
      "organization": {
        "_id": "64f91c328a234f114e40735d",
        "name": "TuringEnterprises",
        "fullname": "Turing Inc.",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f8ef1db0f25f3fe3c07b08/1fw2pc1Vl9LMGTL4Jirgi.jpeg"
      }
    },
    "publishedAt": "2025-12-19T05:16:51.000Z",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "organization": {
      "_id": "64f91c328a234f114e40735d",
      "name": "TuringEnterprises",
      "fullname": "Turing Inc.",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f8ef1db0f25f3fe3c07b08/1fw2pc1Vl9LMGTL4Jirgi.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.16483",
      "authors": [
        {
          "_id": "6948c3ee34f46eaf46cbb31d",
          "name": "Senmao Li",
          "hidden": false
        },
        {
          "_id": "6948c3ee34f46eaf46cbb31e",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6948c3ee34f46eaf46cbb31f",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "6948c3ee34f46eaf46cbb320",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6948c3ee34f46eaf46cbb321",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6948c3ee34f46eaf46cbb322",
          "name": "Yaxing Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T12:51:19.000Z",
      "submittedOnDailyAt": "2025-12-22T01:38:32.273Z",
      "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.",
      "upvotes": 3,
      "discussionId": "6948c3ef34f46eaf46cbb323",
      "githubRepo": "https://github.com/sen-mao/StageVAR",
      "githubRepoAddedBy": "user",
      "ai_summary": "StageVAR accelerates visual autoregressive models by selectively pruning or approximating less critical later stages, achieving significant speedup with minimal quality loss.",
      "ai_keywords": [
        "Visual Autoregressive (VAR) modeling",
        "next-token prediction",
        "next-scale prediction",
        "computational complexity",
        "stage-aware acceleration",
        "semantic consistency",
        "structural consistency",
        "plug-and-play acceleration",
        "semantic irrelevance",
        "low-rank properties",
        "speedup",
        "GenEval",
        "DPG",
        "efficient visual autoregressive image generation"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "661ceb12e7b0ab12bc5f8b8d",
        "name": "NankaiUniversity",
        "fullname": "Nankai University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/f6J0Jh9AWN1FQja8TYosr.png"
      }
    },
    "publishedAt": "2025-12-18T07:51:19.000Z",
    "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
    "summary": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "661ceb12e7b0ab12bc5f8b8d",
      "name": "NankaiUniversity",
      "fullname": "Nankai University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/f6J0Jh9AWN1FQja8TYosr.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17532",
      "authors": [
        {
          "_id": "6948b67134f46eaf46cbb2cc",
          "name": "Jiaqi Tang",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2cd",
          "name": "Jianmin Chen",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2ce",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2cf",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d0",
          "name": "Runtao Liu",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d1",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d2",
          "name": "Qipeng Xie",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d3",
          "name": "Jiafei Wu",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d4",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6948b67134f46eaf46cbb2d5",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T12:56:17.000Z",
      "submittedOnDailyAt": "2025-12-22T00:46:11.400Z",
      "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
      "submittedOnDailyBy": {
        "_id": "642e7a12ccdcf5da7f9657a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png",
        "isPro": true,
        "fullname": "Jiaqi Tang",
        "user": "Jiaqi-hkust",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
      "upvotes": 2,
      "discussionId": "6948b67134f46eaf46cbb2d6",
      "projectPage": "https://jqt.me/index.html",
      "ai_summary": "A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks.",
      "ai_keywords": [
        "multimodal large language models",
        "visual degradations",
        "visual encoder generalization",
        "supervised fine-tuning",
        "reward-driven alignment",
        "dynamic reasoning depth scaling",
        "R-Bench",
        "MMMB",
        "MMStar",
        "RealWorldQA"
      ]
    },
    "publishedAt": "2025-12-19T07:56:17.000Z",
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "summary": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e7a12ccdcf5da7f9657a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png",
      "fullname": "Jiaqi Tang",
      "name": "Jiaqi-hkust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17796",
      "authors": [
        {
          "_id": "6948b1b934f46eaf46cbb284",
          "name": "Yitong Wang",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb285",
          "name": "Fangyun Wei",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb286",
          "name": "Hongyang Zhang",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb287",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb288",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Uv0mhNSuq57dAOMvyE3w0.mp4"
      ],
      "publishedAt": "2025-12-18T18:59:18.000Z",
      "submittedOnDailyAt": "2025-12-22T00:24:30.770Z",
      "title": "Animate Any Character in Any World",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
      "upvotes": 1,
      "discussionId": "6948b1b934f46eaf46cbb289",
      "projectPage": "https://snowflakewang.github.io/AniX/",
      "githubRepo": "https://github.com/snowflakewang/AniX",
      "githubRepoAddedBy": "user",
      "ai_summary": "AniX synthesizes temporally coherent videos by extending controllable-entity models to support diverse, user-defined character interactions in static 3D environments using conditional autoregressive video generation.",
      "ai_keywords": [
        "world models",
        "3D environments",
        "static world generation",
        "controllable-entity models",
        "AniX",
        "3DGS scene",
        "natural language",
        "object-centric interactions",
        "conditional autoregressive video generation",
        "video generator",
        "motion dynamics",
        "generalization",
        "visual quality",
        "character consistency",
        "action controllability",
        "long-horizon coherence"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-12-18T13:59:18.000Z",
    "title": "Animate Any Character in Any World",
    "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Uv0mhNSuq57dAOMvyE3w0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.11362",
      "authors": [
        {
          "_id": "69422ade5d5b2dc1052748a1",
          "name": "Chao Xu",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a2",
          "name": "Suyu Zhang",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a4",
          "name": "Baigui Sun",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a5",
          "name": "Weihong Chen",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a6",
          "name": "Bo Xu",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a7",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a8",
          "name": "Juncheng Wang",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748a9",
          "name": "Shujun Wang",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748aa",
          "name": "Shan Luo",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748ab",
          "name": "Jan Peters",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748ac",
          "name": "Athanasios V. Vasilakos",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748ad",
          "name": "Stefanos Zafeiriou",
          "hidden": false
        },
        {
          "_id": "69422ade5d5b2dc1052748ae",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-12T08:22:03.000Z",
      "submittedOnDailyAt": "2025-12-22T05:18:10.003Z",
      "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
      "submittedOnDailyBy": {
        "_id": "691ae6cbc8e384e92365db5a",
        "avatarUrl": "/avatars/5ac2cb6bbf08de52353510fdd1983376.svg",
        "isPro": false,
        "fullname": "Chao Xu",
        "user": "Chaoxu0309",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/Survery/{project page}.",
      "upvotes": 0,
      "discussionId": "69422adf5d5b2dc1052748b1",
      "projectPage": "https://suyuz1.github.io/Survery/",
      "ai_summary": "This survey provides a structured overview of Vision-Language-Action models, detailing key challenges and opportunities in areas such as representation, execution, generalization, safety, and datasets in the field of robotics.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "embodied intelligence",
        "representation",
        "execution",
        "generalization",
        "safety",
        "datasets",
        "evaluation"
      ],
      "organization": {
        "_id": "6948f703e9247bfe31e25945",
        "name": "IRootech",
        "fullname": "IROOTECH TECHNOLOGY",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6890841645d3dd024dbfc7d0/qRm4Qap3hdUgBng28aeZt.png"
      }
    },
    "publishedAt": "2025-12-12T03:22:03.000Z",
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/Survery/{project page}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "691ae6cbc8e384e92365db5a",
      "avatarUrl": "/avatars/5ac2cb6bbf08de52353510fdd1983376.svg",
      "fullname": "Chao Xu",
      "name": "Chaoxu0309",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6948f703e9247bfe31e25945",
      "name": "IRootech",
      "fullname": "IROOTECH TECHNOLOGY",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6890841645d3dd024dbfc7d0/qRm4Qap3hdUgBng28aeZt.png"
    },
    "isAuthorParticipating": false
  }
]