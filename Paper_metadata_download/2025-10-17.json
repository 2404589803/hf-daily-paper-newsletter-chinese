[
  {
    "paper": {
      "id": "2510.14545",
      "authors": [
        {
          "_id": "68f1a44e6e0bef323a68fd02",
          "user": {
            "_id": "61cd4b833dd34ba1985e0753",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
            "isPro": false,
            "fullname": "KABI",
            "user": "dongguanting",
            "type": "user"
          },
          "name": "Guanting Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:43.546Z",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd03",
          "user": {
            "_id": "6690cf4c5d0a25eea5e3dfbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690cf4c5d0a25eea5e3dfbc/zqY3kl0uZaXrOPJvfmRAn.jpeg",
            "isPro": false,
            "fullname": "baolicheng",
            "user": "blc0910",
            "type": "user"
          },
          "name": "Licheng Bao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:38.484Z",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd04",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd05",
          "name": "Kangzhi Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd06",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd07",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd08",
          "name": "Jinghan Yang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd09",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0a",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0b",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0c",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0d",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0e",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0f",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T10:40:52.000Z",
      "submittedOnDailyAt": "2025-10-17T00:37:49.509Z",
      "title": "Agentic Entropy-Balanced Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
      "upvotes": 61,
      "discussionId": "68f1a44e6e0bef323a68fd10",
      "githubRepo": "https://github.com/RUC-NLPIR/ARPO/blob/main/",
      "ai_summary": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.",
      "ai_keywords": [
        "Agentic Reinforcement Learning",
        "AEPO",
        "entropy",
        "rollout",
        "policy update",
        "dynamic entropy-balanced rollout",
        "branch penalty",
        "stop-gradient operation",
        "high-entropy clipping",
        "entropy-aware advantage estimation",
        "rollout sampling diversity",
        "policy entropy"
      ],
      "githubStars": 663,
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2025-10-16T06:40:52.000Z",
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14545.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 49
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14975",
      "authors": [
        {
          "_id": "68f1a2616e0bef323a68fcd1",
          "user": {
            "_id": "634bde123d11eaedd889e277",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665916392312-noauth.png",
            "isPro": false,
            "fullname": "Hengyuan Xu",
            "user": "DobyXu",
            "type": "user"
          },
          "name": "Hengyuan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:54.829Z",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd2",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:13.677Z",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd3",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd4",
          "user": {
            "_id": "647469b9a51711a3b58bda2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
            "isPro": false,
            "fullname": "Yixiao Fang",
            "user": "fangyixiao",
            "type": "user"
          },
          "name": "Yixiao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:11.270Z",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd5",
          "name": "Shuhan Wu",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd6",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd7",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd8",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd9",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcda",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcdb",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:54.000Z",
      "submittedOnDailyAt": "2025-10-17T00:27:22.105Z",
      "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
      "upvotes": 53,
      "discussionId": "68f1a2616e0bef323a68fcdc",
      "projectPage": "https://doby-xu.github.io/WithAnyone/",
      "githubRepo": "https://github.com/Doby-Xu/WithAnyone",
      "ai_summary": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.",
      "ai_keywords": [
        "identity-consistent generation",
        "text-to-image",
        "reconstruction-based training",
        "copy-paste",
        "MultiID-2M",
        "contrastive identity loss",
        "diffusion-based model",
        "identity fidelity",
        "variation",
        "perceptual quality"
      ],
      "githubStars": 50,
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:54.000Z",
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14975.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 128
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14359",
      "authors": [
        {
          "_id": "68f1a6b16e0bef323a68fd1a",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1b",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1c",
          "user": {
            "_id": "6806464ed918f6d2fee2bc8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
            "isPro": false,
            "fullname": "Chenfei Liao",
            "user": "Chenfei-Liao",
            "type": "user"
          },
          "name": "Chenfei Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:34:38.296Z",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1d",
          "name": "Boxue Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1e",
          "user": {
            "_id": "656ae4088fb1ddf0d5ec9ac5",
            "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
            "isPro": false,
            "fullname": "Junxian Li",
            "user": "Duke-de-Artois",
            "type": "user"
          },
          "name": "Junxian Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:29.247Z",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1f",
          "name": "Weifeng Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd20",
          "name": "Haocong He",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd21",
          "user": {
            "_id": "68d3f778e41c80eb120e52e0",
            "avatarUrl": "/avatars/ad8de30deec15cf0f29134f3a1e73078.svg",
            "isPro": false,
            "fullname": "Bolong Feng",
            "user": "Fabulous1496",
            "type": "user"
          },
          "name": "Bolong Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:31.321Z",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd22",
          "name": "Xuyang Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd23",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd24",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd25",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd26",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T06:55:28.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:23.869Z",
      "title": "AI for Service: Proactive Assistance with AI Glasses",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
      "upvotes": 53,
      "discussionId": "68f1a6b16e0bef323a68fd27",
      "ai_summary": "Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.",
      "ai_keywords": [
        "AI4Service",
        "Alpha-Service",
        "egocentric video streams",
        "multi-agent system",
        "AI glasses",
        "Input Unit",
        "Central Processing Unit",
        "Arithmetic Logic Unit",
        "Memory Unit",
        "Output Unit",
        "natural human interaction"
      ],
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-10-16T02:55:28.000Z",
    "title": "AI for Service: Proactive Assistance with AI Glasses",
    "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14979",
      "authors": [
        {
          "_id": "68f19c5d6e0bef323a68fc30",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc31",
          "name": "Mingxuan Li",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc32",
          "name": "Silei Wu",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc33",
          "name": "Linjun Dai",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc34",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc35",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc36",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc37",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc38",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:58.000Z",
      "submittedOnDailyAt": "2025-10-17T00:18:30.032Z",
      "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
      "submittedOnDailyBy": {
        "_id": "64b4a717aa03b6520839e9b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
        "isPro": false,
        "fullname": "Haiwen Diao",
        "user": "Paranioar",
        "type": "user"
      },
      "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
      "upvotes": 42,
      "discussionId": "68f19c5e6e0bef323a68fc39",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/NEO",
      "ai_summary": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.",
      "ai_keywords": [
        "Vision-Language Models",
        "native VLMs",
        "modular VLMs",
        "pixel and word representations",
        "semantic space",
        "cross-modal properties",
        "visual perception",
        "vision-language conflicts",
        "monolithic model",
        "reusable components"
      ],
      "githubStars": 23,
      "organization": {
        "_id": "63203b617196f93bc94b73a2",
        "name": "SenseTime",
        "fullname": "SenseTime",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:58.000Z",
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
    "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14979.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "63203b617196f93bc94b73a2",
      "name": "SenseTime",
      "fullname": "SenseTime",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14943",
      "authors": [
        {
          "_id": "68f19d716e0bef323a68fc51",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:28.203Z",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc52",
          "name": "Weijie Liu",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc53",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc54",
          "name": "Yiju Guo",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc55",
          "name": "Lulu Wu",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc56",
          "name": "Saiyong Yang",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc57",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:55:11.000Z",
      "submittedOnDailyAt": "2025-10-17T00:25:17.471Z",
      "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
      "upvotes": 27,
      "discussionId": "68f19d716e0bef323a68fc58",
      "githubRepo": "https://github.com/RUCBM/LaSeR",
      "ai_summary": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "self-verification",
        "RLVR",
        "last-token self-rewarding",
        "policy model",
        "next-token log-probability",
        "KL coefficient",
        "MSE loss",
        "reasoning performance",
        "inference-time scaling"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2025-10-16T13:55:11.000Z",
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14967",
      "authors": [
        {
          "_id": "68f1a04a6e0bef323a68fcac",
          "name": "Guoqing Wang",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcad",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcae",
          "name": "Guangze Ye",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcaf",
          "name": "Zeyu Gan",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb0",
          "name": "Wei Yao",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb1",
          "name": "Yong Deng",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb2",
          "name": "Xiaofeng Wu",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb3",
          "name": "Zhenzhe Ying",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:32.000Z",
      "submittedOnDailyAt": "2025-10-17T00:19:11.288Z",
      "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
      "upvotes": 25,
      "discussionId": "68f1a04a6e0bef323a68fcb4",
      "ai_summary": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "tool use",
        "multi-turn reasoning",
        "knowledge acquisition",
        "reward sparsity",
        "advantage collapse",
        "fine-grained credit assignment",
        "Information Gain-based Policy Optimization",
        "intrinsic rewards",
        "belief updates",
        "dense reward trajectories",
        "sample efficiency"
      ],
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-16T13:59:32.000Z",
    "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
    "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14972",
      "authors": [
        {
          "_id": "68f19f5f6e0bef323a68fc99",
          "user": {
            "_id": "675764b1a55640463e079271",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6EMvM7M8uP9K7EeVMqAz-.png",
            "isPro": false,
            "fullname": "Yinxi Li",
            "user": "Yinxxx",
            "type": "user"
          },
          "name": "Yinxi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:23.022Z",
          "hidden": false
        },
        {
          "_id": "68f19f5f6e0bef323a68fc9a",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:56.808Z",
          "hidden": false
        },
        {
          "_id": "68f19f5f6e0bef323a68fc9b",
          "user": {
            "_id": "6643bdd9bc86c9465b551357",
            "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
            "isPro": false,
            "fullname": "Pengyu Nie",
            "user": "pengyunie",
            "type": "user"
          },
          "name": "Pengyu Nie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:25.906Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6643bdd9bc86c9465b551357/Zsnh3cU3Ix3vn65geRT0r.gif"
      ],
      "publishedAt": "2025-10-16T17:59:45.000Z",
      "submittedOnDailyAt": "2025-10-17T01:35:08.569Z",
      "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
      "submittedOnDailyBy": {
        "_id": "6643bdd9bc86c9465b551357",
        "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
        "isPro": false,
        "fullname": "Pengyu Nie",
        "user": "pengyunie",
        "type": "user"
      },
      "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
      "upvotes": 24,
      "discussionId": "68f19f606e0bef323a68fc9c",
      "githubRepo": "https://github.com/uw-swag/tokdrift",
      "ai_summary": "Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.",
      "ai_keywords": [
        "large language models",
        "code LLMs",
        "subword tokenizers",
        "byte-pair encoding",
        "BPE",
        "semantic-preserving rewrite rules",
        "TokDrift",
        "tokenization",
        "early embeddings",
        "grammar token boundaries",
        "code understanding",
        "code generation"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "6230c3ced93e84e2338765f3",
        "name": "UWaterloo",
        "fullname": "University of Waterloo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:45.000Z",
    "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
    "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6643bdd9bc86c9465b551357/Zsnh3cU3Ix3vn65geRT0r.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14972.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6643bdd9bc86c9465b551357",
      "avatarUrl": "/avatars/98f04dc2263e79f170e662a9ed113fc5.svg",
      "fullname": "Pengyu Nie",
      "name": "pengyunie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "6230c3ced93e84e2338765f3",
      "name": "UWaterloo",
      "fullname": "University of Waterloo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14847",
      "authors": [
        {
          "_id": "68f1d8076e0bef323a68ffd3",
          "name": "Meiqi Wu",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd4",
          "user": {
            "_id": "6682775501c30ad93ec5e500",
            "avatarUrl": "/avatars/971ee2028589f6089559306b40a58da0.svg",
            "isPro": false,
            "fullname": "Jiashu Zhu",
            "user": "Jiashuz",
            "type": "user"
          },
          "name": "Jiashu Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:13.761Z",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd5",
          "name": "Xiaokun Feng",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd6",
          "name": "Chubin Chen",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd7",
          "name": "Chen Zhu",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd8",
          "name": "Bingze Song",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffd9",
          "name": "Fangyuan Mao",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffda",
          "name": "Jiahong Wu",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffdb",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "68f1d8076e0bef323a68ffdc",
          "name": "Kaiqi Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T16:19:13.000Z",
      "submittedOnDailyAt": "2025-10-17T05:35:39.801Z",
      "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
      "upvotes": 22,
      "discussionId": "68f1d8076e0bef323a68ffdd",
      "githubRepo": "https://github.com/AMAP-ML/ImagerySearch",
      "ai_summary": "ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.",
      "ai_keywords": [
        "prompt-guided",
        "adaptive test-time search",
        "inference search space",
        "reward function",
        "long-distance semantic prompts",
        "LDT-Bench",
        "VBench",
        "creative generation capabilities"
      ],
      "organization": {
        "_id": "67d11771890254196d3174e5",
        "name": "GD-ML",
        "fullname": "AMAP-ML",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
      }
    },
    "publishedAt": "2025-10-16T12:19:13.000Z",
    "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
    "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14847.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67d11771890254196d3174e5",
      "name": "GD-ML",
      "fullname": "AMAP-ML",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14973",
      "authors": [
        {
          "_id": "68f1a7886e0bef323a68fd6a",
          "user": {
            "_id": "651517d381ce32e6e142a10e",
            "avatarUrl": "/avatars/1e87fddecf913a75b6c24fca08f7cd5b.svg",
            "isPro": false,
            "fullname": "Nguyen Tri Quan",
            "user": "NguyenTriQuan",
            "type": "user"
          },
          "name": "Quan Nguyen-Tri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:24.539Z",
          "hidden": false
        },
        {
          "_id": "68f1a7886e0bef323a68fd6b",
          "user": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "isPro": true,
            "fullname": "Mukul Ranjan",
            "user": "mukul54",
            "type": "user"
          },
          "name": "Mukul Ranjan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:26.806Z",
          "hidden": false
        },
        {
          "_id": "68f1a7886e0bef323a68fd6c",
          "name": "Zhiqiang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:48.000Z",
      "submittedOnDailyAt": "2025-10-17T01:02:05.861Z",
      "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "65262a396b41932089fd7bae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
        "isPro": true,
        "fullname": "Mukul Ranjan",
        "user": "mukul54",
        "type": "user"
      },
      "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant {bf MASK} tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n{bf Elastic-Cache}, a training-free, architecture-agnostic strategy that\njointly decides {when} to refresh (via an attention-aware drift test on the\nmost-attended token) and {where} to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences,\nand 4.8times on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n(6.8times on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
      "upvotes": 18,
      "discussionId": "68f1a7886e0bef323a68fd6d",
      "projectPage": "https://vila-lab.github.io/elastic-cache-webpage/",
      "ai_summary": "Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.",
      "ai_keywords": [
        "diffusion large language models",
        "KV caches",
        "QKV",
        "denoising steps",
        "attention-aware drift test",
        "depth-aware schedule",
        "Elastic-Cache",
        "adaptive cache updates",
        "generation quality",
        "throughput"
      ],
      "organization": {
        "_id": "61fb9e24dc607a42af5f193f",
        "name": "MBZUAI",
        "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
      }
    },
    "publishedAt": "2025-10-16T13:59:48.000Z",
    "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant {bf MASK} tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n{bf Elastic-Cache}, a training-free, architecture-agnostic strategy that\njointly decides {when} to refresh (via an attention-aware drift test on the\nmost-attended token) and {where} to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences,\nand 4.8times on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n(6.8times on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14973.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65262a396b41932089fd7bae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
      "fullname": "Mukul Ranjan",
      "name": "mukul54",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "61fb9e24dc607a42af5f193f",
      "name": "MBZUAI",
      "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14528",
      "authors": [
        {
          "_id": "68f1a43f6e0bef323a68fcee",
          "name": "Cheng Cui",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcef",
          "user": {
            "_id": "63d7b8ee07cd1aa3c49a2026",
            "avatarUrl": "/avatars/a2c640ae334a8e78f27c4e13b8554150.svg",
            "isPro": false,
            "fullname": "Ting Sun",
            "user": "sunflowerting78",
            "type": "user"
          },
          "name": "Ting Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:51.310Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf0",
          "name": "Suyin Liang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf1",
          "user": {
            "_id": "681c1ecd9539bdde5ae1733c",
            "avatarUrl": "/avatars/23322876ba0812bdabc288cc2381a776.svg",
            "isPro": false,
            "fullname": "Tingquan Gao",
            "user": "Tingquan",
            "type": "user"
          },
          "name": "Tingquan Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:58.135Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf2",
          "name": "Zelun Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf3",
          "name": "Jiaxuan Liu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf4",
          "name": "Xueqing Wang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf5",
          "user": {
            "_id": "68c23142fc7c68f3c1be9788",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68c23142fc7c68f3c1be9788/jDAZ0p5tDC9M1MOvhHY0J.png",
            "isPro": false,
            "fullname": "chagndazhou",
            "user": "vvvic0415",
            "type": "user"
          },
          "name": "Changda Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:53.661Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf6",
          "user": {
            "_id": "68493f0616e67d38f02f138a",
            "avatarUrl": "/avatars/f0ece4f914440fd652ef0ff935f61c9c.svg",
            "isPro": false,
            "fullname": "Liu",
            "user": "Hongen1",
            "type": "user"
          },
          "name": "Hongen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:49.116Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf7",
          "user": {
            "_id": "67d96e68939c3823ab2e06a5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJx_Nf_5aD1w1s0RqbyGn.png",
            "isPro": false,
            "fullname": "Manhui Lin",
            "user": "gggdddfff",
            "type": "user"
          },
          "name": "Manhui Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:55.920Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf8",
          "user": {
            "_id": "68c23ee406c83a13a583843f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0Sgn8aro1cQeLlY9G7ju9.png",
            "isPro": false,
            "fullname": "Yue Zhang",
            "user": "xiaohei66",
            "type": "user"
          },
          "name": "Yue Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:46.078Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf9",
          "name": "Yubo Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfa",
          "name": "Handong Zheng",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfb",
          "user": {
            "_id": "652b2e9166313ebb6197e706",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b2e9166313ebb6197e706/0qAVPmsc_fRp8OmlaC2S2.png",
            "isPro": false,
            "fullname": "AlexZhang",
            "user": "AlexTransformer",
            "type": "user"
          },
          "name": "Jing Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:00.280Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfc",
          "user": {
            "_id": "64f187a2cc1c03340ac30498",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f187a2cc1c03340ac30498/dMTUFA5Ul35v595JPKCMw.jpeg",
            "isPro": false,
            "fullname": "Jun Zhang",
            "user": "jzhang533",
            "type": "user"
          },
          "name": "Jun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:02.869Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfd",
          "user": {
            "_id": "6501403fea5699b59abacf15",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6501403fea5699b59abacf15/Sh9ouC68HWHsF1Jh_0CGi.jpeg",
            "isPro": false,
            "fullname": "Yi Liu",
            "user": "michaelowenliu",
            "type": "user"
          },
          "name": "Yi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:09.095Z",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfe",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcff",
          "name": "Yanjun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T10:18:48.000Z",
      "submittedOnDailyAt": "2025-10-17T00:35:15.163Z",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "upvotes": 14,
      "discussionId": "68f1a4406e0bef323a68fd00",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "ai_keywords": [
        "vision-language model",
        "NaViT-style",
        "dynamic resolution visual encoder",
        "ERNIE-4.5",
        "element recognition",
        "page-level document parsing",
        "element-level recognition",
        "inference speeds"
      ],
      "organization": {
        "_id": "62067d5d3906f102bc9658bd",
        "name": "PaddlePaddle",
        "fullname": "PaddlePaddle",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
      }
    },
    "publishedAt": "2025-10-16T06:18:48.000Z",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 128
    },
    "organization": {
      "_id": "62067d5d3906f102bc9658bd",
      "name": "PaddlePaddle",
      "fullname": "PaddlePaddle",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10518",
      "authors": [
        {
          "_id": "68f0de8336f8b025381e1b61",
          "user": {
            "_id": "67a2ed64ed6024cc7b4f8acf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a2ed64ed6024cc7b4f8acf/763S4FbWvnaYl2D8KyIsp.jpeg",
            "isPro": false,
            "fullname": "Qunzhong WANG",
            "user": "qunwang13",
            "type": "user"
          },
          "name": "Qunzhong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:12:54.291Z",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b62",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b63",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b64",
          "name": "Yilei Jiang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b65",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b66",
          "name": "Jinyuan Chen",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b67",
          "name": "Yaozhi Zheng",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b68",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b69",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b6a",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b6b",
          "name": "Jiaheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T09:29:50.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:35.185Z",
      "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
      "upvotes": 13,
      "discussionId": "68f0de8336f8b025381e1b6c",
      "githubRepo": "https://github.com/qunzhongwang/vr-thinker",
      "ai_summary": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.",
      "ai_keywords": [
        "multimodal reward models",
        "visual generative models",
        "visual reasoning operations",
        "visual memory window",
        "reinforcement fine-tuning",
        "curated visual chain-of-thought data",
        "Rejection sampling Fine-Tuning",
        "Group Relative Policy Optimization",
        "VideoGen Reward",
        "GenAI-Bench",
        "MJ-Bench-Video"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "68edc767abe005ac1b354573",
        "name": "NJU-LINK",
        "fullname": "NJU-LINK Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
      }
    },
    "publishedAt": "2025-10-12T05:29:50.000Z",
    "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
    "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "organization": {
      "_id": "68edc767abe005ac1b354573",
      "name": "NJU-LINK",
      "fullname": "NJU-LINK Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13998",
      "authors": [
        {
          "_id": "68f1b1f06e0bef323a68fde5",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fde6",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fde7",
          "name": "Wenhui Wang",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fde8",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fde9",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fdea",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68f1b1f06e0bef323a68fdeb",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632bd2f72d6a805eeb4bc601/DDH8WmVsFnYPWUzk8tXdy.png"
      ],
      "publishedAt": "2025-10-15T18:28:12.000Z",
      "submittedOnDailyAt": "2025-10-17T01:34:38.483Z",
      "title": "BitNet Distillation",
      "submittedOnDailyBy": {
        "_id": "632bd2f72d6a805eeb4bc601",
        "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
        "isPro": false,
        "fullname": "HUANG SHAOHAN",
        "user": "buaahsh",
        "type": "user"
      },
      "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
      "upvotes": 12,
      "discussionId": "68f1b1f06e0bef323a68fdec",
      "githubRepo": "https://github.com/microsoft/BitNet",
      "ai_summary": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.",
      "ai_keywords": [
        "BitNet Distillation",
        "BitDistill",
        "SubLN",
        "multi-head attention distillation",
        "continual pre-training",
        "LLMs",
        "Qwen",
        "ternary weights",
        "memory savings",
        "inference speed"
      ],
      "githubStars": 24192,
      "organization": {
        "_id": "68151d0f51add3813f3f7d1b",
        "name": "MicrosoftResearch",
        "fullname": "Microsoft Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
      }
    },
    "publishedAt": "2025-10-15T14:28:12.000Z",
    "title": "BitNet Distillation",
    "summary": "In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632bd2f72d6a805eeb4bc601/DDH8WmVsFnYPWUzk8tXdy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632bd2f72d6a805eeb4bc601",
      "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
      "fullname": "HUANG SHAOHAN",
      "name": "buaahsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14958",
      "authors": [
        {
          "_id": "68f1a3346e0bef323a68fcde",
          "name": "Weikang Shi",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fcdf",
          "name": "Aldrich Yu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce0",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce1",
          "name": "Houxing Ren",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce2",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce3",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce4",
          "name": "Changyao Tian",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce5",
          "name": "Xinyu Fu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce6",
          "name": "Yuxuan Hu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce7",
          "name": "Zimu Lu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce8",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce9",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fcea",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fceb",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:58:58.000Z",
      "submittedOnDailyAt": "2025-10-17T00:30:38.950Z",
      "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
      "upvotes": 10,
      "discussionId": "68f1a3346e0bef323a68fcec",
      "projectPage": "https://mathcanvas.github.io/",
      "githubRepo": "https://github.com/shiwk24/MathCanvas",
      "ai_summary": "MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "Visual Chain-of-Thought",
        "Large Multimodal Models",
        "Visual Manipulation",
        "Strategic Visual-Aided Reasoning",
        "MathCanvas-Imagen",
        "MathCanvas-Edit",
        "MathCanvas-Instruct",
        "MathCanvas-Bench",
        "BAGEL-Canvas"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-16T13:58:58.000Z",
    "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
    "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14763",
      "authors": [
        {
          "_id": "68f1d66a6e0bef323a68ffbe",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffbf",
          "name": "Shuangshuang Ying",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc0",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc1",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc2",
          "name": "Sheng Jin",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc3",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc4",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc5",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc6",
          "name": "Xeron Du",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc7",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc8",
          "name": "Jiajun Shi",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffc9",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffca",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffcb",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffcc",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffcd",
          "name": "Stephen Huang",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffce",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffcf",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "68f1d66a6e0bef323a68ffd0",
          "name": "Eli Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T15:01:19.000Z",
      "submittedOnDailyAt": "2025-10-17T04:09:34.642Z",
      "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
      "submittedOnDailyBy": {
        "_id": "64a948d9723beceb2f13f5eb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
        "isPro": false,
        "fullname": "XinLi",
        "user": "XINLI1997",
        "type": "user"
      },
      "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
      "upvotes": 10,
      "discussionId": "68f1d66b6e0bef323a68ffd1",
      "projectPage": "https://COIG-Writer.github.io/",
      "githubRepo": "https://github.com/COIG-Writer/COIG-Writer",
      "ai_summary": "COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.",
      "ai_keywords": [
        "COIG-Writer",
        "creative writing dataset",
        "reverse-engineered prompt",
        "creative reasoning",
        "narrative logic",
        "linguistic expression",
        "process supervision",
        "general-purpose data",
        "cultural-bound capabilities",
        "cross-lingual transfer",
        "lexical diversity",
        "TTR paradox"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "6384ee7fdfffab482400b938",
        "name": "m-a-p",
        "fullname": "Multimodal Art Projection",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg"
      }
    },
    "publishedAt": "2025-10-16T11:01:19.000Z",
    "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
    "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a948d9723beceb2f13f5eb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
      "fullname": "XinLi",
      "name": "XINLI1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6384ee7fdfffab482400b938",
      "name": "m-a-p",
      "fullname": "Multimodal Art Projection",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63839e9962badff4326cf360/k4Q7R4XLDMp_1VF4C6GEd.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14616",
      "authors": [
        {
          "_id": "68f1b22d6e0bef323a68fdf4",
          "name": "Shuangshuang Ying",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdf5",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdf6",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdf7",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdf8",
          "name": "Sheng Jin",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdf9",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdfa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdfb",
          "name": "Xeron Du",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdfc",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdfd",
          "name": "Yichi Zhang",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdfe",
          "name": "Letian Ni",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fdff",
          "name": "Yuyang Cheng",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe00",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe01",
          "name": "Jingzhe Ding",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe02",
          "name": "Shengda Long",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe03",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe04",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe05",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe06",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe07",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe08",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe09",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "68f1b22d6e0bef323a68fe0a",
          "name": "Chenghua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T12:23:13.000Z",
      "submittedOnDailyAt": "2025-10-17T04:10:42.518Z",
      "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
      "submittedOnDailyBy": {
        "_id": "64a948d9723beceb2f13f5eb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
        "isPro": false,
        "fullname": "XinLi",
        "user": "XINLI1997",
        "type": "user"
      },
      "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.",
      "upvotes": 9,
      "discussionId": "68f1b22d6e0bef323a68fe0b",
      "projectPage": "https://WritingPreferenceBench.github.io/",
      "githubRepo": "https://github.com/WritingPreferenceBench/Writing-Preference-Bench",
      "ai_summary": "Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.",
      "ai_keywords": [
        "sequence-based reward models",
        "zero-shot language models",
        "generative reward models",
        "explicit reasoning chains",
        "RLHF",
        "preference learning",
        "creative writing genres",
        "objective correctness",
        "factual accuracy",
        "subjective quality preferences",
        "creativity",
        "stylistic flair",
        "emotional resonance"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-16T08:23:13.000Z",
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
    "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14616.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a948d9723beceb2f13f5eb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a948d9723beceb2f13f5eb/UQMO7LczJpabtbcoNt1zQ.jpeg",
      "fullname": "XinLi",
      "name": "XINLI1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13217",
      "authors": [
        {
          "_id": "68f19ecf6e0bef323a68fc75",
          "name": "Nilesh Gupta",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc76",
          "name": "Wei-Cheng Chang",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc77",
          "name": "Ngot Bui",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc78",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc79",
          "name": "Inderjit S. Dhillon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T07:05:17.000Z",
      "submittedOnDailyAt": "2025-10-17T00:12:11.927Z",
      "title": "LLM-guided Hierarchical Retrieval",
      "submittedOnDailyBy": {
        "_id": "6527151d5606f146974d60d8",
        "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
        "isPro": false,
        "fullname": "Nilesh Gupta",
        "user": "quicktensor",
        "type": "user"
      },
      "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
      "upvotes": 8,
      "discussionId": "68f19ed06e0bef323a68fc7a",
      "projectPage": "https://nilesh2797.github.io/publications/lattice/",
      "githubRepo": "https://github.com/nilesh2797/lattice",
      "ai_summary": "LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.",
      "ai_keywords": [
        "LLM-based IR",
        "retrieve-then-rerank",
        "embedding-based retrieval",
        "parametric generative approaches",
        "long-context methods",
        "hierarchical retrieval framework",
        "semantic tree structure",
        "offline phase",
        "bottom-up agglomerative strategy",
        "top-down divisive strategy",
        "multi-level summaries",
        "online traversal phase",
        "search LLM",
        "relevance judgments",
        "latent relevance scores",
        "global path relevance metric",
        "zero-shot performance",
        "BRIGHT benchmark",
        "Recall@100",
        "nDCG@10",
        "DIVER-v2"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-15T03:05:17.000Z",
    "title": "LLM-guided Hierarchical Retrieval",
    "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527151d5606f146974d60d8",
      "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
      "fullname": "Nilesh Gupta",
      "name": "quicktensor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09033",
      "authors": [
        {
          "_id": "68f1a6f96e0bef323a68fd64",
          "user": {
            "_id": "6454f1486f4ae9965626409c",
            "avatarUrl": "/avatars/ac9d471b230f8babd03a1f35378d990e.svg",
            "isPro": true,
            "fullname": "Chi Seng Cheang",
            "user": "chiseng-cheang",
            "type": "user"
          },
          "name": "Chi Seng Cheang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:34:35.821Z",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd65",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd66",
          "name": "Wenxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd67",
          "user": {
            "_id": "68f1c7bc2faaae2d56c10bff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xWST1Lh10BK7SRKBLjkFR.jpeg",
            "isPro": false,
            "fullname": "Yang Deng",
            "user": "ydeng17",
            "type": "user"
          },
          "name": "Yang Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:48.685Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T06:09:04.000Z",
      "submittedOnDailyAt": "2025-10-17T00:49:36.375Z",
      "title": "Large Language Models Do NOT Really Know What They Don't Know",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
      "upvotes": 8,
      "discussionId": "68f1a6fa6e0bef323a68fd68",
      "ai_summary": "LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "hidden states",
        "attention weights",
        "token probabilities",
        "factual queries",
        "hallucinations",
        "subject information",
        "internal recall process",
        "hidden-state geometries",
        "truthfulness",
        "knowledge recall"
      ],
      "organization": {
        "_id": "6296f43820dc74838613d1ba",
        "name": "SingaporeManagementUniversity",
        "fullname": "Singapore Management University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
      }
    },
    "publishedAt": "2025-10-10T02:09:04.000Z",
    "title": "Large Language Models Do NOT Really Know What They Don't Know",
    "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "organization": {
      "_id": "6296f43820dc74838613d1ba",
      "name": "SingaporeManagementUniversity",
      "fullname": "Singapore Management University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14300",
      "authors": [
        {
          "_id": "68f1c62c6e0bef323a68fe9a",
          "user": {
            "_id": "674d8f71556d9ccbaee9f55a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
            "isPro": false,
            "fullname": "Shen Weijie",
            "user": "shenweijie",
            "type": "user"
          },
          "name": "Weijie Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:34:33.583Z",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fe9b",
          "name": "Yitian Liu",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fe9c",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fe9d",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fe9e",
          "name": "Sijia Gu",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fe9f",
          "name": "Dehui Wang",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea0",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea1",
          "name": "Lei Xu",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea2",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea3",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea4",
          "name": "Xinping Guan",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea5",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "68f1c62c6e0bef323a68fea6",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T04:52:57.000Z",
      "submittedOnDailyAt": "2025-10-17T03:12:34.734Z",
      "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
      "submittedOnDailyBy": {
        "_id": "674d8f71556d9ccbaee9f55a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
        "isPro": false,
        "fullname": "Shen Weijie",
        "user": "shenweijie",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
      "upvotes": 7,
      "discussionId": "68f1c62d6e0bef323a68fea7",
      "ai_summary": "AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoE",
        "feedforward layers",
        "sparsely activated MoE layers",
        "decoupling technique",
        "scale adapter",
        "router",
        "expert selection",
        "expert weighting",
        "collaborative expert utilization",
        "LIBERO",
        "RoboTwin"
      ]
    },
    "publishedAt": "2025-10-16T00:52:57.000Z",
    "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for\n  Vision-Language-Action Learning",
    "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14300.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674d8f71556d9ccbaee9f55a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VyZeljrRkqXCpeE1gkbAJ.png",
      "fullname": "Shen Weijie",
      "name": "shenweijie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14276",
      "authors": [
        {
          "_id": "68f1a6d46e0bef323a68fd29",
          "name": "Haiquan Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2a",
          "name": "Chenhan Yuan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2b",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2c",
          "name": "Xiaomeng Hu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2e",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2f",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd30",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd31",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd32",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd33",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd34",
          "name": "Chen Cheng",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd35",
          "name": "Jialong Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd36",
          "name": "Jiandong Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd37",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd38",
          "name": "Jijie Xu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd39",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3a",
          "name": "Minmin Sun",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3b",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3c",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3d",
          "name": "Qiaoyu Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3e",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3f",
          "name": "Rong Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd40",
          "name": "Shibin Wu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd41",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd42",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd43",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd44",
          "name": "Tingyu Xia",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd45",
          "name": "Wei Liao",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd46",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd47",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd48",
          "name": "Wenmeng Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd49",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4a",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4b",
          "name": "Xiaodong Deng",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4c",
          "name": "Xiaodong Xu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4f",
          "name": "Yeqiu Li",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd50",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd51",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd52",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd53",
          "name": "Yuxin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T04:00:18.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:03.670Z",
      "title": "Qwen3Guard Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
      "upvotes": 7,
      "discussionId": "68f1a6d56e0bef323a68fd54",
      "githubRepo": "https://github.com/QwenLM/Qwen3Guard",
      "ai_summary": "Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.",
      "ai_keywords": [
        "large language models",
        "safety guardrail models",
        "binary labels",
        "safety policies",
        "safety tolerances",
        "streaming inference",
        "Generative Qwen3Guard",
        "Stream Qwen3Guard",
        "instruction-following task",
        "tri-class judgments",
        "token-level classification",
        "multilingual",
        "prompt safety",
        "response safety",
        "Apache 2.0 license"
      ],
      "githubStars": 282,
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2025-10-16T00:00:18.000Z",
    "title": "Qwen3Guard Technical Report",
    "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 128
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14978",
      "authors": [
        {
          "_id": "68f1a20a6e0bef323a68fcc5",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc6",
          "name": "Sheng-Yu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc7",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc8",
          "name": "Yotam Nitzan",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc9",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcca",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccb",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccc",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccd",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcce",
          "name": "Xun Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-17T00:25:36.586Z",
      "title": "Learning an Image Editing Model without Image Editing Pairs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
      "upvotes": 5,
      "discussionId": "68f1a20a6e0bef323a68fccf",
      "ai_summary": "A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.",
      "ai_keywords": [
        "diffusion model",
        "unrolling",
        "vision-language models",
        "distribution matching loss",
        "image manifold",
        "few-step setting",
        "RL-based techniques",
        "Flow-GRPO"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:57.000Z",
    "title": "Learning an Image Editing Model without Image Editing Pairs",
    "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14978.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 128
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14211",
      "authors": [
        {
          "_id": "68f1ccce6e0bef323a68ff16",
          "name": "Beomseok Kang",
          "hidden": false
        },
        {
          "_id": "68f1ccce6e0bef323a68ff17",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:29.645Z",
          "hidden": false
        },
        {
          "_id": "68f1ccce6e0bef323a68ff18",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T01:37:39.000Z",
      "submittedOnDailyAt": "2025-10-17T03:29:50.976Z",
      "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
      "submittedOnDailyBy": {
        "_id": "662672eaebdfec5cfdf1d034",
        "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
        "isPro": false,
        "fullname": "Jiwon Song",
        "user": "jiwonsong",
        "type": "user"
      },
      "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
      "upvotes": 5,
      "discussionId": "68f1cccf6e0bef323a68ff19",
      "ai_summary": "LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.",
      "ai_keywords": [
        "multi-stage reasoning",
        "layer skipping",
        "stage-wise variation",
        "skip sensitivity",
        "redundant output tokens",
        "confidence-based generation",
        "early exit",
        "OBQA",
        "CSQA",
        "StrategyQA"
      ],
      "organization": {
        "_id": "6304ac9f5d136debcecba4fd",
        "name": "snu",
        "fullname": "Seoul National University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"
      }
    },
    "publishedAt": "2025-10-15T21:37:39.000Z",
    "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
    "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662672eaebdfec5cfdf1d034",
      "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
      "fullname": "Jiwon Song",
      "name": "jiwonsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "6304ac9f5d136debcecba4fd",
      "name": "snu",
      "fullname": "Seoul National University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13054",
      "authors": [
        {
          "_id": "68f0817e36f8b025381e1a20",
          "name": "Ankit Goyal",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a21",
          "name": "Hugo Hadfield",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a22",
          "name": "Xuning Yang",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a23",
          "name": "Valts Blukis",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a24",
          "name": "Fabio Ramos",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
      ],
      "publishedAt": "2025-10-15T00:31:10.000Z",
      "submittedOnDailyAt": "2025-10-17T00:51:30.866Z",
      "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
      "submittedOnDailyBy": {
        "_id": "666c6f67bc840e6748554a91",
        "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
        "isPro": false,
        "fullname": "Ankit Goyal",
        "user": "ankgoyal",
        "type": "user"
      },
      "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
      "upvotes": 5,
      "discussionId": "68f0817e36f8b025381e1a25",
      "ai_summary": "A simple VLA model, VLA-0, outperforms more complex models on robotic manipulation tasks by representing actions as text without additional modifications or large-scale training.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Vision-Language Model",
        "action tokens",
        "action heads",
        "LIBERO",
        "$\\pi_0.5$-KI",
        "OpenVLA-OFT",
        "SmolVLA",
        "GR00T-N1",
        "MolmoAct"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-14T20:31:10.000Z",
    "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
    "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666c6f67bc840e6748554a91",
      "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
      "fullname": "Ankit Goyal",
      "name": "ankgoyal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14969",
      "authors": [
        {
          "_id": "68f19da26e0bef323a68fc68",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc69",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6a",
          "name": "Yuedong Cui",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6b",
          "name": "Ruichen Zheng",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6c",
          "name": "Zhiqian Li",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6d",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6e",
          "name": "Di Wu",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6f",
          "name": "Xueqing Wu",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc70",
          "name": "Chenchen Ye",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc71",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc72",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
      ],
      "publishedAt": "2025-10-16T17:59:38.000Z",
      "submittedOnDailyAt": "2025-10-17T00:20:18.195Z",
      "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
      "submittedOnDailyBy": {
        "_id": "634e4670a51d5df8c2d92fce",
        "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
        "isPro": false,
        "fullname": "Da Yin",
        "user": "DaYin",
        "type": "user"
      },
      "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
      "upvotes": 4,
      "discussionId": "68f19da26e0bef323a68fc73",
      "ai_summary": "UI-Simulator generates diverse UI trajectories for digital agents using a scalable paradigm, improving robustness and performance with targeted scaling strategies.",
      "ai_keywords": [
        "UI-Simulator",
        "digital world simulator",
        "guided rollout process",
        "trajectory wrapper",
        "UI-Simulator-Grow",
        "WebArena",
        "AndroidWorld",
        "Llama-3-70B-Instruct",
        "Llama-3-8B-Instruct"
      ],
      "organization": {
        "_id": "60b492a78e9589ce25930346",
        "name": "uclanlp",
        "fullname": "UCLA NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:38.000Z",
    "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
    "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14969.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "organization": {
      "_id": "60b492a78e9589ce25930346",
      "name": "uclanlp",
      "fullname": "UCLA NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13454",
      "authors": [
        {
          "_id": "68f19cc36e0bef323a68fc3b",
          "name": "Hyojun Go",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3c",
          "name": "Dominik Narnhofer",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3d",
          "name": "Goutam Bhat",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3e",
          "name": "Prune Truong",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3f",
          "name": "Federico Tombari",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc40",
          "name": "Konrad Schindler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T11:55:08.000Z",
      "submittedOnDailyAt": "2025-10-17T00:09:34.641Z",
      "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
      "submittedOnDailyBy": {
        "_id": "649f65a4ca03a1a35e3dac14",
        "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
        "isPro": false,
        "fullname": "Hyojun GO",
        "user": "HJGO",
        "type": "user"
      },
      "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
      "upvotes": 4,
      "discussionId": "68f19cc86e0bef323a68fc41",
      "projectPage": "https://gohyojun15.github.io/VIST3A/",
      "githubRepo": "https://github.com/gohyojun15/VIST3A",
      "ai_summary": "VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.",
      "ai_keywords": [
        "latent text-to-video model",
        "3D reconstruction system",
        "model stitching",
        "direct reward finetuning",
        "Gaussian splats",
        "text-to-pointmap generation"
      ],
      "githubStars": 18
    },
    "publishedAt": "2025-10-15T07:55:08.000Z",
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
    "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f65a4ca03a1a35e3dac14",
      "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
      "fullname": "Hyojun GO",
      "name": "HJGO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14955",
      "authors": [
        {
          "_id": "68f1e1d76e0bef323a68fffc",
          "name": "Guo Cheng",
          "hidden": false
        },
        {
          "_id": "68f1e1d76e0bef323a68fffd",
          "name": "Danni Yang",
          "hidden": false
        },
        {
          "_id": "68f1e1d76e0bef323a68fffe",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "68f1e1d76e0bef323a68ffff",
          "name": "Jianlou Si",
          "hidden": false
        },
        {
          "_id": "68f1e1d76e0bef323a690000",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "68f1e1d76e0bef323a690001",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:58:25.000Z",
      "submittedOnDailyAt": "2025-10-17T05:22:05.938Z",
      "title": "RealDPO: Real or Not Real, that is the Preference",
      "submittedOnDailyBy": {
        "_id": "60efe7fa0d920bc7805cada5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
        "isPro": false,
        "fullname": "Ziqi Huang",
        "user": "Ziqi",
        "type": "user"
      },
      "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
      "upvotes": 3,
      "discussionId": "68f1e1d86e0bef323a690002",
      "ai_summary": "RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.",
      "ai_keywords": [
        "RealDPO",
        "Direct Preference Optimization",
        "preference learning",
        "motion synthesis",
        "video generative models",
        "RealAction-5K",
        "text alignment",
        "motion realism"
      ]
    },
    "publishedAt": "2025-10-16T13:58:25.000Z",
    "title": "RealDPO: Real or Not Real, that is the Preference",
    "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60efe7fa0d920bc7805cada5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
      "fullname": "Ziqi Huang",
      "name": "Ziqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14880",
      "authors": [
        {
          "_id": "68f1b4f86e0bef323a68fe21",
          "name": "Rikiya Takehi",
          "hidden": false
        },
        {
          "_id": "68f1b4f86e0bef323a68fe22",
          "user": {
            "_id": "5ff60d4352c26e9bc240badd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
            "isPro": true,
            "fullname": "Benjamin Clavi",
            "user": "bclavie",
            "type": "user"
          },
          "name": "Benjamin Clavi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:11.390Z",
          "hidden": false
        },
        {
          "_id": "68f1b4f86e0bef323a68fe23",
          "name": "Sean Lee",
          "hidden": false
        },
        {
          "_id": "68f1b4f86e0bef323a68fe24",
          "name": "Aamir Shakir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:00:35.000Z",
      "submittedOnDailyAt": "2025-10-17T01:47:16.250Z",
      "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
      "submittedOnDailyBy": {
        "_id": "5ff60d4352c26e9bc240badd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
        "isPro": true,
        "fullname": "Benjamin Clavi",
        "user": "bclavie",
        "type": "user"
      },
      "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency.",
      "upvotes": 3,
      "discussionId": "68f1b4f96e0bef323a68fe25",
      "ai_summary": "mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.",
      "ai_keywords": [
        "mxbai-edge-colbert-v0",
        "retrieval",
        "late-interaction models",
        "distillation",
        "ablation studies",
        "BEIR",
        "ColBERTv2",
        "long-context tasks"
      ],
      "organization": {
        "_id": "6579b8e83db8c022afd86d63",
        "name": "mixedbread-ai",
        "fullname": "Mixedbread",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643ee0870d1194da249bd7fe/voYdYlFgQH5vyMwyMNluZ.png"
      }
    },
    "publishedAt": "2025-10-16T13:00:35.000Z",
    "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
    "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14880.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ff60d4352c26e9bc240badd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff60d4352c26e9bc240badd/HzoknJibrSasc1ZzU71XA.png",
      "fullname": "Benjamin Clavi",
      "name": "bclavie",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 67
    },
    "organization": {
      "_id": "6579b8e83db8c022afd86d63",
      "name": "mixedbread-ai",
      "fullname": "Mixedbread",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643ee0870d1194da249bd7fe/voYdYlFgQH5vyMwyMNluZ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.13996",
      "authors": [
        {
          "_id": "68f1d4a56e0bef323a68ff90",
          "name": "Lukas Gienapp",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff91",
          "name": "Christopher Schrder",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff92",
          "user": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "isPro": true,
            "fullname": "Stefan Schweter",
            "user": "stefan-it",
            "type": "user"
          },
          "name": "Stefan Schweter",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:03.613Z",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff93",
          "name": "Christopher Akiki",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff94",
          "name": "Ferdinand Schlatt",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff95",
          "name": "Arden Zimmermann",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff96",
          "name": "Phillipe Gent",
          "hidden": false
        },
        {
          "_id": "68f1d4a56e0bef323a68ff97",
          "name": "Martin Potthast",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/jPBWPo-iRgevY5QK6u_Jn.png"
      ],
      "publishedAt": "2025-10-15T18:24:26.000Z",
      "submittedOnDailyAt": "2025-10-17T04:22:19.366Z",
      "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Large language model development relies on large-scale training corpora, yet\nmost contain data of unclear licensing status, limiting the development of\ntruly open models. This problem is exacerbated for non-English languages, where\nopenly licensed text remains critically scarce. We introduce the German\nCommons, the largest collection of openly licensed German text to date. It\ncompiles data from 41 sources across seven domains, encompassing legal,\nscientific, cultural, political, news, economic, and web text. Through\nsystematic sourcing from established data providers with verifiable licensing,\nit yields 154.56 billion tokens of high-quality text for language model\ntraining. Our processing pipeline implements comprehensive quality filtering,\ndeduplication, and text formatting fixes, ensuring consistent quality across\nheterogeneous text sources. All domain subsets feature licenses of at least\nCC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and\nredistribution. The German Commons therefore addresses the critical gap in\nopenly licensed German pretraining data, and enables the development of truly\nopen German language models. We also release code for corpus construction and\ndata filtering tailored to German language text, rendering the German Commons\nfully reproducible and extensible.",
      "upvotes": 3,
      "discussionId": "68f1d4a56e0bef323a68ff98",
      "ai_summary": "The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.",
      "ai_keywords": [
        "language model",
        "training corpora",
        "licensing status",
        "openly licensed text",
        "German Commons",
        "data sources",
        "legal",
        "scientific",
        "cultural",
        "political",
        "news",
        "economic",
        "web text",
        "high-quality text",
        "quality filtering",
        "deduplication",
        "text formatting",
        "CC-BY-SA 4.0",
        "legal compliance",
        "model training",
        "redistribution",
        "corpus construction",
        "data filtering"
      ],
      "organization": {
        "_id": "6819f812d0e7f0efa77658f8",
        "name": "coralnlp",
        "fullname": "CORAL NLP Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625026749d39e8be3166132f/4wCrPdv2fnR-gWBJVXYdo.png"
      }
    },
    "publishedAt": "2025-10-15T14:24:26.000Z",
    "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for\n  German Language Models",
    "summary": "Large language model development relies on large-scale training corpora, yet\nmost contain data of unclear licensing status, limiting the development of\ntruly open models. This problem is exacerbated for non-English languages, where\nopenly licensed text remains critically scarce. We introduce the German\nCommons, the largest collection of openly licensed German text to date. It\ncompiles data from 41 sources across seven domains, encompassing legal,\nscientific, cultural, political, news, economic, and web text. Through\nsystematic sourcing from established data providers with verifiable licensing,\nit yields 154.56 billion tokens of high-quality text for language model\ntraining. Our processing pipeline implements comprehensive quality filtering,\ndeduplication, and text formatting fixes, ensuring consistent quality across\nheterogeneous text sources. All domain subsets feature licenses of at least\nCC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and\nredistribution. The German Commons therefore addresses the critical gap in\nopenly licensed German pretraining data, and enables the development of truly\nopen German language models. We also release code for corpus construction and\ndata filtering tailored to German language text, rendering the German Commons\nfully reproducible and extensible.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/jPBWPo-iRgevY5QK6u_Jn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3395
    },
    "organization": {
      "_id": "6819f812d0e7f0efa77658f8",
      "name": "coralnlp",
      "fullname": "CORAL NLP Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625026749d39e8be3166132f/4wCrPdv2fnR-gWBJVXYdo.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14974",
      "authors": [
        {
          "_id": "68f19b086e0bef323a68fc1a",
          "user": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "isPro": false,
            "fullname": "Hansheng Chen",
            "user": "Lakonik",
            "type": "user"
          },
          "name": "Hansheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:30.593Z",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1b",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1c",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1d",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1e",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1f",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-17T00:12:55.031Z",
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
      "upvotes": 2,
      "discussionId": "68f19b086e0bef323a68fc20",
      "githubRepo": "https://github.com/Lakonik/piFlow",
      "ai_summary": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.",
      "ai_keywords": [
        "velocity-predicting teacher",
        "student flow model",
        "policy-based flow models",
        "$\\pi$-Flow",
        "dynamic flow velocities",
        "ODE integration",
        "imitation distillation",
        "$\\ell_2$ flow matching loss",
        "ImageNet 256$^2$",
        "FID",
        "MeanFlow",
        "DiT architecture",
        "FLUX.1-12B",
        "Qwen-Image-20B",
        "NFE"
      ],
      "githubStars": 13,
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:51.000Z",
    "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14949",
      "authors": [
        {
          "_id": "68f1d2d56e0bef323a68ff5e",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff5f",
          "name": "Sohyun An",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff60",
          "user": {
            "_id": "624cf6ba5ee9cb3d45588971",
            "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
            "isPro": false,
            "fullname": "Haikang Deng",
            "user": "hk",
            "type": "user"
          },
          "name": "Haikang Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:24.216Z",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff61",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff62",
          "name": "Clark Peng",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff63",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff64",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "68f1d2d56e0bef323a68ff65",
          "name": "Nanyun Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:56:55.000Z",
      "submittedOnDailyAt": "2025-10-17T04:38:12.186Z",
      "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
      "submittedOnDailyBy": {
        "_id": "624cf6ba5ee9cb3d45588971",
        "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
        "isPro": false,
        "fullname": "Haikang Deng",
        "user": "hk",
        "type": "user"
      },
      "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
      "upvotes": 2,
      "discussionId": "68f1d2d56e0bef323a68ff66",
      "projectPage": "https://dialectgen.github.io/",
      "githubRepo": "https://github.com/DialectGen/DialectGen",
      "ai_summary": "A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.",
      "ai_keywords": [
        "multimodal generative models",
        "dialectal textual input",
        "large-scale benchmark",
        "dialect speakers",
        "image and video generative models",
        "performance degradation",
        "fine-tuning",
        "prompt rewriting",
        "encoder-based mitigation strategy",
        "Stable Diffusion 1.5"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "60b492a78e9589ce25930346",
        "name": "uclanlp",
        "fullname": "UCLA NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
      }
    },
    "publishedAt": "2025-10-16T13:56:55.000Z",
    "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
    "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624cf6ba5ee9cb3d45588971",
      "avatarUrl": "/avatars/f0fc5e353951d3e501542dab559bbdb5.svg",
      "fullname": "Haikang Deng",
      "name": "hk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "60b492a78e9589ce25930346",
      "name": "uclanlp",
      "fullname": "UCLA NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14902",
      "authors": [
        {
          "_id": "68f1d24b6e0bef323a68ff4b",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "68f1d24b6e0bef323a68ff4c",
          "name": "Jiaxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f1d24b6e0bef323a68ff4d",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "68f1d24b6e0bef323a68ff4e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "68f1d24b6e0bef323a68ff4f",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:18:34.000Z",
      "submittedOnDailyAt": "2025-10-17T05:06:57.203Z",
      "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
      "submittedOnDailyBy": {
        "_id": "646dbbc8075bbcc48ddcecbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg",
        "isPro": false,
        "fullname": "Han Zhao",
        "user": "han1997",
        "type": "user"
      },
      "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
      "upvotes": 2,
      "discussionId": "68f1d24b6e0bef323a68ff50",
      "projectPage": "https://vla-2.github.io",
      "ai_summary": "A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.",
      "ai_keywords": [
        "vision-language-action models",
        "pre-trained models",
        "multi-task capabilities",
        "generalization",
        "out-of-distribution objects",
        "OpenVLA",
        "web retrieval",
        "object detection",
        "LIBERO simulation environment",
        "evaluation benchmark",
        "success rate",
        "in-domain tasks"
      ],
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2025-10-16T13:18:34.000Z",
    "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
    "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14902.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646dbbc8075bbcc48ddcecbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg",
      "fullname": "Han Zhao",
      "name": "han1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14252",
      "authors": [
        {
          "_id": "68f1a46e6e0bef323a68fd12",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:10:33.450Z",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd13",
          "name": "Zhiyuan Ji",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd14",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd15",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd16",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd17",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T03:09:51.000Z",
      "submittedOnDailyAt": "2025-10-17T00:42:57.762Z",
      "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
      "upvotes": 2,
      "discussionId": "68f1a46e6e0bef323a68fd18",
      "ai_summary": "The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.",
      "ai_keywords": [
        "document memory extraction",
        "Mixtures of scenario-aware document Memories (MoM)",
        "small language models (SLMs)",
        "large language models (LLMs)",
        "document logical outlines",
        "multi-path sampling",
        "multi-perspective evaluation",
        "reverse reasoning strategy",
        "three-layer document memory retrieval mechanism",
        "probabilistic modeling"
      ]
    },
    "publishedAt": "2025-10-15T23:09:51.000Z",
    "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
    "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14252.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.14976",
      "authors": [
        {
          "_id": "68f1c7cb6e0bef323a68febb",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "68f1c7cb6e0bef323a68febc",
          "name": "Chuan Guo",
          "hidden": false
        },
        {
          "_id": "68f1c7cb6e0bef323a68febd",
          "name": "Bing Zhou",
          "hidden": false
        },
        {
          "_id": "68f1c7cb6e0bef323a68febe",
          "name": "Jian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:56.000Z",
      "submittedOnDailyAt": "2025-10-17T03:12:25.526Z",
      "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
      "submittedOnDailyBy": {
        "_id": "670462c6fd5ef6902568d7bd",
        "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
        "isPro": false,
        "fullname": "Shaowei Liu",
        "user": "shaoweiliu",
        "type": "user"
      },
      "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
      "upvotes": 1,
      "discussionId": "68f1c7cb6e0bef323a68febf",
      "projectPage": "https://stevenlsw.github.io/ponimator/",
      "githubRepo": "https://github.com/stevenlsw/ponimator",
      "ai_summary": "Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.",
      "ai_keywords": [
        "conditional diffusion models",
        "pose animator",
        "pose generator",
        "interactive pose priors",
        "image-based interaction animation",
        "reaction animation",
        "text-to-interaction synthesis",
        "motion capture data"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "668450a2c1cbe5e008ac6515",
        "name": "Snapchat",
        "fullname": "Snapchat Inc.",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
      }
    },
    "publishedAt": "2025-10-16T13:59:56.000Z",
    "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
    "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670462c6fd5ef6902568d7bd",
      "avatarUrl": "/avatars/2d6ea275ccc289f6f1b07d0c8e860888.svg",
      "fullname": "Shaowei Liu",
      "name": "shaoweiliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "668450a2c1cbe5e008ac6515",
      "name": "Snapchat",
      "fullname": "Snapchat Inc.",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14351",
      "authors": [
        {
          "_id": "68f193bd6e0bef323a68fbcd",
          "name": "Perapard Ngokpol",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbce",
          "user": {
            "_id": "63a83c5432ed73936eb8363e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
            "isPro": false,
            "fullname": "kun kerdthaisong",
            "user": "augustus2011",
            "type": "user"
          },
          "name": "Kun Kerdthaisong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T04:11:35.817Z",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbcf",
          "name": "Pasin Buakhaw",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd0",
          "name": "Pitikorn Khlaisamniang",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd1",
          "name": "Supasate Vorathammathorn",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd2",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd3",
          "name": "Nutchanon Yongsatianchot",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T06:39:27.000Z",
      "submittedOnDailyAt": "2025-10-17T00:01:32.280Z",
      "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
      "submittedOnDailyBy": {
        "_id": "63a83c5432ed73936eb8363e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
        "isPro": false,
        "fullname": "kun kerdthaisong",
        "user": "augustus2011",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
      "upvotes": 1,
      "discussionId": "68f193be6e0bef323a68fbd4",
      "githubRepo": "https://github.com/Augustus2011/Beyond_One_World.git",
      "ai_summary": "Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "role-playing agents",
        "character-grounded roleplay",
        "Canon Events",
        "Moral Dilemmas",
        "canonical accuracy",
        "reasoning fidelity",
        "Think-Act Matching",
        "chain-of-thought prompting",
        "cross-version generalization"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "67aaba5d8d478dcb4b2f4281",
        "name": "Character-lab",
        "fullname": "Character-lab"
      }
    },
    "publishedAt": "2025-10-16T02:39:27.000Z",
    "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
    "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a83c5432ed73936eb8363e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
      "fullname": "kun kerdthaisong",
      "name": "augustus2011",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67aaba5d8d478dcb4b2f4281",
      "name": "Character-lab",
      "fullname": "Character-lab"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10390",
      "authors": [
        {
          "_id": "68f1bb1f6e0bef323a68fe35",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "68f1bb1f6e0bef323a68fe36",
          "name": "Leonardo F. R. Ribeiro",
          "hidden": false
        },
        {
          "_id": "68f1bb1f6e0bef323a68fe37",
          "name": "Markus Dreyer",
          "hidden": false
        },
        {
          "_id": "68f1bb1f6e0bef323a68fe38",
          "name": "Virginia Smith",
          "hidden": false
        },
        {
          "_id": "68f1bb1f6e0bef323a68fe39",
          "name": "Mona T. Diab",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T00:53:42.000Z",
      "submittedOnDailyAt": "2025-10-17T02:13:43.285Z",
      "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.",
      "upvotes": 1,
      "discussionId": "68f1bb206e0bef323a68fe3a",
      "ai_summary": "RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.",
      "ai_keywords": [
        "RAG systems",
        "language models",
        "selective refusal",
        "refusal accuracy",
        "dangerous overconfidence",
        "overcaution",
        "static benchmarks",
        "generative methodology",
        "controlled linguistic perturbation",
        "perturbation strategies",
        "informational uncertainty",
        "intensity levels",
        "RefusalBench-NQ",
        "RefusalBench-GaRAGe"
      ],
      "organization": {
        "_id": "674e3f226f1597e933139875",
        "name": "amazon-agi",
        "fullname": "Amazon AGI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
      }
    },
    "publishedAt": "2025-10-11T20:53:42.000Z",
    "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded\n  Language Models",
    "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10390.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "674e3f226f1597e933139875",
      "name": "amazon-agi",
      "fullname": "Amazon AGI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06694",
      "authors": [
        {
          "_id": "68f1e5d66e0bef323a690014",
          "name": "Jipeng Lyu",
          "hidden": false
        },
        {
          "_id": "68f1e5d66e0bef323a690015",
          "name": "Jiahua Dong",
          "hidden": false
        },
        {
          "_id": "68f1e5d66e0bef323a690016",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T06:39:33.000Z",
      "submittedOnDailyAt": "2025-10-17T05:17:07.221Z",
      "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
      "submittedOnDailyBy": {
        "_id": "6475b69ef60449361e556b78",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6475b69ef60449361e556b78/qkyWjWe7y1LruNm-uiFhm.png",
        "isPro": false,
        "fullname": "Jipeng Lyu",
        "user": "kedaxiaoqiu",
        "type": "user"
      },
      "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.",
      "upvotes": 1,
      "discussionId": "68f1e5d66e0bef323a690017",
      "ai_summary": "SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "cascaded optimization framework",
        "hierarchical patterns",
        "deformations",
        "self-supervised articulated object segmentation",
        "novel view synthesis",
        "dense point tracking"
      ],
      "organization": {
        "_id": "65448bef5b5d9185ba3202b9",
        "name": "UIUC-CS",
        "fullname": "University of Illinois at Urbana-Champaign",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
      }
    },
    "publishedAt": "2025-10-08T02:39:33.000Z",
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
    "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475b69ef60449361e556b78",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6475b69ef60449361e556b78/qkyWjWe7y1LruNm-uiFhm.png",
      "fullname": "Jipeng Lyu",
      "name": "kedaxiaoqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "65448bef5b5d9185ba3202b9",
      "name": "UIUC-CS",
      "fullname": "University of Illinois at Urbana-Champaign",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
    },
    "isAuthorParticipating": false
  }
]