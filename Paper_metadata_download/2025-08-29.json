[
  {
    "paper": {
      "id": "2508.20751",
      "authors": [
        {
          "_id": "68b10e71b19c54000148491a",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491b",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491c",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491d",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491e",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491f",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484920",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484921",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484922",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T13:11:24.000Z",
      "submittedOnDailyAt": "2025-08-29T00:53:42.823Z",
      "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
        "isPro": false,
        "fullname": "SII-Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
      "upvotes": 45,
      "discussionId": "68b10e71b19c540001484923",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/Pref-GRPO",
      "githubRepo": "https://github.com/CodeGoat24/Pref-GRPO",
      "ai_summary": "Pref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.",
      "ai_keywords": [
        "GRPO",
        "reinforcement learning",
        "text-to-image",
        "pointwise reward models",
        "reward hacking",
        "pairwise preference",
        "preference fitting",
        "win rate",
        "UniGenBench",
        "semantic consistency",
        "MLLM"
      ],
      "githubStars": 50
    },
    "publishedAt": "2025-08-28T09:11:24.000Z",
    "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
    "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20751.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
      "fullname": "SII-Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20722",
      "authors": [
        {
          "_id": "68b10494b19c5400014848cb",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cc",
          "name": "Yifei Liu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cd",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848ce",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cf",
          "name": "Weijiang Xu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d0",
          "name": "Xinyu Guan",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d1",
          "name": "Buze Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d2",
          "name": "Bingcheng Dong",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d3",
          "name": "Xudong Zhou",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d4",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d5",
          "name": "Ying Xin",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d6",
          "name": "Ziming Miao",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d7",
          "name": "Scarlett Li",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d8",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d9",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T12:45:25.000Z",
      "submittedOnDailyAt": "2025-08-29T00:25:20.345Z",
      "title": "rStar2-Agent: Agentic Reasoning Technical Report",
      "submittedOnDailyBy": {
        "_id": "62b0009c72043b05d29492b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
        "isPro": false,
        "fullname": "Li Lyna Zhang",
        "user": "lynazhang",
        "type": "user"
      },
      "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
      "upvotes": 34,
      "discussionId": "68b10495b19c5400014848da",
      "githubRepo": "https://github.com/microsoft/rStar",
      "ai_summary": "rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.",
      "ai_keywords": [
        "agentic reinforcement learning",
        "CoT",
        "Python coding tools",
        "GRPO-RoC",
        "Resample-on-Correct",
        "SFT",
        "multi-RL",
        "AIME24",
        "AIME25",
        "DeepSeek-R1",
        "alignment",
        "scientific reasoning",
        "agentic tool-use"
      ],
      "githubStars": 639
    },
    "publishedAt": "2025-08-28T08:45:25.000Z",
    "title": "rStar2-Agent: Agentic Reasoning Technical Report",
    "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20722.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18966",
      "authors": [
        {
          "_id": "68ae8011364411bea07df7fd",
          "user": {
            "_id": "660114b38ae190912a61be5d",
            "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
            "isPro": false,
            "fullname": "ShaojinWu",
            "user": "fenfan",
            "type": "user"
          },
          "name": "Shaojin Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:26.369Z",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df7fe",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df7ff",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df800",
          "user": {
            "_id": "635634171c93c1ef4e9eb1c2",
            "avatarUrl": "/avatars/66b31b801960612057ecfd1e26410075.svg",
            "isPro": false,
            "fullname": "wuwenxu",
            "user": "wuwx",
            "type": "user"
          },
          "name": "Wenxu Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-28T08:55:56.250Z",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df801",
          "name": "Jiahe Tian",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df802",
          "name": "Yiming Luo",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df803",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "68ae8011364411bea07df804",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T12:10:24.000Z",
      "submittedOnDailyAt": "2025-08-29T01:32:11.306Z",
      "title": "USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning",
      "submittedOnDailyBy": {
        "_id": "660114b38ae190912a61be5d",
        "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
        "isPro": false,
        "fullname": "ShaojinWu",
        "user": "fenfan",
        "type": "user"
      },
      "summary": "Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO",
      "upvotes": 23,
      "discussionId": "68ae8011364411bea07df805",
      "projectPage": "https://bytedance.github.io/USO/",
      "githubRepo": "https://github.com/bytedance/USO",
      "ai_summary": "USO, a unified model, achieves state-of-the-art performance in both style similarity and subject consistency by disentangling and re-composing content and style through a disentangled learning scheme and style reward-learning paradigm.",
      "ai_keywords": [
        "disentangled learning",
        "style-alignment training",
        "content-style disentanglement training",
        "style reward-learning",
        "USO-Bench"
      ],
      "githubStars": 111
    },
    "publishedAt": "2025-08-26T08:10:24.000Z",
    "title": "USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning",
    "summary": "Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660114b38ae190912a61be5d",
      "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
      "fullname": "ShaojinWu",
      "name": "fenfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.20404",
      "authors": [
        {
          "_id": "68b11548b19c540001484981",
          "name": "Chengyue Yu",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484982",
          "name": "Siyuan Lu",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484983",
          "name": "Chenyi Zhuang",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484984",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484985",
          "name": "Qintong Wu",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484986",
          "name": "Zongyue Li",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484987",
          "name": "Runsheng Gan",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484988",
          "name": "Chunfeng Wang",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484989",
          "name": "Siqi Hou",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498a",
          "name": "Gaochi Huang",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498b",
          "name": "Wenlong Yan",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498c",
          "name": "Lifeng Hong",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498d",
          "name": "Aohui Xue",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498e",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "68b11548b19c54000148498f",
          "name": "Jinjie Gu",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484990",
          "name": "David Tsai",
          "hidden": false
        },
        {
          "_id": "68b11548b19c540001484991",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T04:04:30.000Z",
      "submittedOnDailyAt": "2025-08-29T02:20:06.800Z",
      "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
      "submittedOnDailyBy": {
        "_id": "64e847ab5ddcace745b8f5b1",
        "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
        "isPro": true,
        "fullname": "chenyi zhuang",
        "user": "chengle",
        "type": "user"
      },
      "summary": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that significantly outperforms its base model, increasing\nits overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most\nchallenging levels, our agent achieves a score of 16.33%, surpassing the\nperformance of leading proprietary models. Our open-source system and resulting\nagent provide a practical blueprint for a complete agentic AI training\npipeline, from efficient interaction to demonstrable model improvement.",
      "upvotes": 17,
      "discussionId": "68b11548b19c540001484992",
      "githubRepo": "https://github.com/inclusionAI/AWorld/tree/main",
      "ai_summary": "AWorld, an open-source system for large-scale agent-environment interaction, accelerates experience collection and enhances reinforcement learning, leading to significant improvements in agentic AI performance on complex benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "Qwen3-32B",
        "GAIA benchmark",
        "agent-environment interaction",
        "experience generation",
        "distributed tasks",
        "cluster",
        "sequential execution",
        "agentic AI",
        "model improvement"
      ],
      "githubStars": 624
    },
    "publishedAt": "2025-08-28T00:04:30.000Z",
    "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
    "summary": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that significantly outperforms its base model, increasing\nits overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most\nchallenging levels, our agent achieves a score of 16.33%, surpassing the\nperformance of leading proprietary models. Our open-source system and resulting\nagent provide a practical blueprint for a complete agentic AI training\npipeline, from efficient interaction to demonstrable model improvement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e847ab5ddcace745b8f5b1",
      "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
      "fullname": "chenyi zhuang",
      "name": "chengle",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20374",
      "authors": [
        {
          "_id": "68b11ef0b19c5400014849ae",
          "name": "Simin Ma",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849af",
          "name": "Shujian Liu",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b0",
          "name": "Jun Tan",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b1",
          "name": "Yebowen Hu",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b2",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b3",
          "name": "Sathish Reddy Indurthi",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b4",
          "name": "Sanqiang Zhao",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b5",
          "name": "Liwei Wu",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b6",
          "name": "Jianbing Han",
          "hidden": false
        },
        {
          "_id": "68b11ef0b19c5400014849b7",
          "name": "Kaiqiang Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T02:42:10.000Z",
      "submittedOnDailyAt": "2025-08-29T02:02:23.288Z",
      "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning",
      "submittedOnDailyBy": {
        "_id": "67d0574cc2ef59a2ab641b80",
        "avatarUrl": "/avatars/31362733e6b8e745ac3d0586909a7d55.svg",
        "isPro": false,
        "fullname": "Shujian",
        "user": "shujian2025",
        "type": "user"
      },
      "summary": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications.",
      "upvotes": 16,
      "discussionId": "68b11ef0b19c5400014849b8",
      "ai_summary": "Task Centric Instruction Augmentation (TCIA) enhances large language models' performance on specific tasks while maintaining general instruction-following ability.",
      "ai_keywords": [
        "instruction tuning",
        "large language models",
        "instruction dataset",
        "data diversity",
        "task-specific knowledge",
        "instruction augmentation",
        "discrete query-constraints space",
        "task-relevant instructions",
        "open-source LLMs",
        "closed-source models"
      ]
    },
    "publishedAt": "2025-08-27T22:42:10.000Z",
    "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning",
    "summary": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20374.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67d0574cc2ef59a2ab641b80",
      "avatarUrl": "/avatars/31362733e6b8e745ac3d0586909a7d55.svg",
      "fullname": "Shujian",
      "name": "shujian2025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21058",
      "authors": [
        {
          "_id": "68b10915b19c5400014848fb",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fc",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fd",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fe",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848ff",
          "name": "Junfei Xiao",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484900",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484901",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484902",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484903",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484904",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484905",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484906",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484907",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:57:55.000Z",
      "submittedOnDailyAt": "2025-08-29T00:28:28.135Z",
      "title": "Mixture of Contexts for Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "66a1da7cc9e703d2af5ad742",
        "avatarUrl": "/avatars/f9cd9ae3407e249ab4569479200feb1f.svg",
        "isPro": true,
        "fullname": "Shengqu Cai",
        "user": "primecai",
        "type": "user"
      },
      "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
      "upvotes": 8,
      "discussionId": "68b10915b19c540001484908",
      "projectPage": "https://primecai.github.io/moc/",
      "ai_summary": "Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.",
      "ai_keywords": [
        "diffusion transformers",
        "self-attention",
        "long-context video generation",
        "internal information retrieval",
        "Mixture of Contexts",
        "sparse attention routing",
        "causal routing",
        "salient history",
        "memory",
        "consistency"
      ]
    },
    "publishedAt": "2025-08-28T13:57:55.000Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a1da7cc9e703d2af5ad742",
      "avatarUrl": "/avatars/f9cd9ae3407e249ab4569479200feb1f.svg",
      "fullname": "Shengqu Cai",
      "name": "primecai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20453",
      "authors": [
        {
          "_id": "68b10726b19c5400014848e6",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e7",
          "name": "Qi Chang",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e8",
          "name": "Hemani Patel",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e9",
          "name": "Shashank Biju",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ea",
          "name": "Cheng-En Wu",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848eb",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ec",
          "name": "Aolin Ding",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ed",
          "name": "Alireza Rezazadeh",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ee",
          "name": "Ankit Shah",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ef",
          "name": "Yujia Bao",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848f0",
          "name": "Eugene Siow",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T05:58:57.000Z",
      "submittedOnDailyAt": "2025-08-29T00:22:20.147Z",
      "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "We introduce MCP-Bench, a benchmark for evaluating large language models\n(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool\ncoordination, precise parameter control, and planning/reasoning for solving\ntasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28\nrepresentative live MCP servers spanning 250 tools across domains such as\nfinance, traveling, scientific computing, and academic search. Unlike prior\nAPI-based benchmarks, each MCP server provides a set of complementary tools\ndesigned to work together, enabling the construction of authentic, multi-step\ntasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability\nto retrieve relevant tools from fuzzy instructions without explicit tool names,\nplan multi-hop execution trajectories for complex objectives, ground responses\nin intermediate tool outputs, and orchestrate cross-domain workflows -\ncapabilities not adequately evaluated by existing benchmarks that rely on\nexplicit tool specifications, shallow few-step workflows, and isolated domain\noperations. We propose a multi-faceted evaluation framework covering tool-level\nschema understanding and usage, trajectory-level planning, and task completion.\nExperiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code\nand data: https://github.com/Accenture/mcp-bench.",
      "upvotes": 5,
      "discussionId": "68b10726b19c5400014848f1",
      "ai_summary": "MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.",
      "ai_keywords": [
        "large language models",
        "MCP-Bench",
        "Model Context Protocol",
        "MCP servers",
        "multi-step tasks",
        "tool use",
        "cross-tool coordination",
        "parameter control",
        "planning",
        "reasoning",
        "schema understanding",
        "trajectory-level planning",
        "task completion"
      ]
    },
    "publishedAt": "2025-08-28T01:58:57.000Z",
    "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers",
    "summary": "We introduce MCP-Bench, a benchmark for evaluating large language models\n(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool\ncoordination, precise parameter control, and planning/reasoning for solving\ntasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28\nrepresentative live MCP servers spanning 250 tools across domains such as\nfinance, traveling, scientific computing, and academic search. Unlike prior\nAPI-based benchmarks, each MCP server provides a set of complementary tools\ndesigned to work together, enabling the construction of authentic, multi-step\ntasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability\nto retrieve relevant tools from fuzzy instructions without explicit tool names,\nplan multi-hop execution trajectories for complex objectives, ground responses\nin intermediate tool outputs, and orchestrate cross-domain workflows -\ncapabilities not adequately evaluated by existing benchmarks that rely on\nexplicit tool specifications, shallow few-step workflows, and isolated domain\noperations. We propose a multi-faceted evaluation framework covering tool-level\nschema understanding and usage, trajectory-level planning, and task completion.\nExperiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code\nand data: https://github.com/Accenture/mcp-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21046",
      "authors": [
        {
          "_id": "68b12b08b19c5400014849bf",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68b12b08b19c5400014849c0",
          "name": "Renshan Zhang",
          "hidden": false
        },
        {
          "_id": "68b12b08b19c5400014849c1",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "68b12b08b19c5400014849c2",
          "name": "Jie He",
          "hidden": false
        },
        {
          "_id": "68b12b08b19c5400014849c3",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:50:58.000Z",
      "submittedOnDailyAt": "2025-08-29T02:52:32.640Z",
      "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
      "upvotes": 4,
      "discussionId": "68b12b08b19c5400014849c4",
      "githubRepo": "https://github.com/JiuTian-VL/CogVLA",
      "ai_summary": "CogVLA, a Cognition-Aligned Vision-Language-Action framework, enhances efficiency and performance through instruction-driven routing and sparsification, achieving state-of-the-art results with reduced computational costs.",
      "ai_keywords": [
        "Encoder-FiLM",
        "Aggregation Routing",
        "EFA-Routing",
        "LLM-FiLM",
        "Pruning Routing",
        "LFP-Routing",
        "V-L-A Coupled Attention",
        "CAtten",
        "causal vision-language attention",
        "bidirectional action parallel decoding"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-08-28T13:50:58.000Z",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
    "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 97
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21061",
      "authors": [
        {
          "_id": "68b12b5eb19c5400014849c6",
          "name": "Adam Coscia",
          "hidden": false
        },
        {
          "_id": "68b12b5eb19c5400014849c7",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "68b12b5eb19c5400014849c8",
          "name": "Eunyee Koh",
          "hidden": false
        },
        {
          "_id": "68b12b5eb19c5400014849c9",
          "name": "Alex Endert",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:58:29.000Z",
      "submittedOnDailyAt": "2025-08-29T02:54:09.934Z",
      "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
      "upvotes": 2,
      "discussionId": "68b12b5eb19c5400014849ca",
      "ai_summary": "OnGoal, an LLM chat interface with goal tracking, improves user efficiency and engagement in complex dialogues by providing real-time feedback and visualizations.",
      "ai_keywords": [
        "LLM chat interface",
        "goal alignment",
        "LLM-assisted evaluation",
        "goal progression",
        "writing task",
        "cognitive load",
        "interactivity",
        "feedback"
      ]
    },
    "publishedAt": "2025-08-28T13:58:29.000Z",
    "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models",
    "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 97
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21070",
      "authors": [
        {
          "_id": "68b12ba4b19c5400014849d5",
          "name": "Jun-Kun Chen",
          "hidden": false
        },
        {
          "_id": "68b12ba4b19c5400014849d6",
          "name": "Aayush Bansal",
          "hidden": false
        },
        {
          "_id": "68b12ba4b19c5400014849d7",
          "name": "Minh Phuoc Vo",
          "hidden": false
        },
        {
          "_id": "68b12ba4b19c5400014849d8",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:59:55.000Z",
      "submittedOnDailyAt": "2025-08-29T02:55:21.415Z",
      "title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Dress&Dance, a video diffusion framework that generates high\nquality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a\nuser wearing desired garments while moving in accordance with a given reference\nvideo. Our approach requires a single user image and supports a range of tops,\nbottoms, and one-piece garments, as well as simultaneous tops and bottoms\ntry-on in a single pass. Key to our framework is CondNet, a novel conditioning\nnetwork that leverages attention to unify multi-modal inputs (text, images, and\nvideos), thereby enhancing garment registration and motion fidelity. CondNet is\ntrained on heterogeneous training data, combining limited video data and a\nlarger, more readily available image dataset, in a multistage progressive\nmanner. Dress&Dance outperforms existing open source and commercial solutions\nand enables a high quality and flexible try-on experience.",
      "upvotes": 1,
      "discussionId": "68b12ba4b19c5400014849d9",
      "ai_summary": "A video diffusion framework generates high-quality virtual try-on videos using a conditioning network that unifies multi-modal inputs, outperforming existing solutions.",
      "ai_keywords": [
        "video diffusion",
        "CondNet",
        "attention",
        "multi-modal inputs",
        "heterogeneous training data",
        "multistage progressive training"
      ]
    },
    "publishedAt": "2025-08-28T13:59:55.000Z",
    "title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
    "summary": "We present Dress&Dance, a video diffusion framework that generates high\nquality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a\nuser wearing desired garments while moving in accordance with a given reference\nvideo. Our approach requires a single user image and supports a range of tops,\nbottoms, and one-piece garments, as well as simultaneous tops and bottoms\ntry-on in a single pass. Key to our framework is CondNet, a novel conditioning\nnetwork that leverages attention to unify multi-modal inputs (text, images, and\nvideos), thereby enhancing garment registration and motion fidelity. CondNet is\ntrained on heterogeneous training data, combining limited video data and a\nlarger, more readily available image dataset, in a multistage progressive\nmanner. Dress&Dance outperforms existing open source and commercial solutions\nand enables a high quality and flexible try-on experience.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 97
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21052",
      "authors": [
        {
          "_id": "68b12b7fb19c5400014849cc",
          "name": "Gaetan Brison",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849cd",
          "name": "Soobash Daiboo",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849ce",
          "name": "Samy Aimeur",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849cf",
          "name": "Awais Hussain Sani",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849d0",
          "name": "Xi Wang",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849d1",
          "name": "Gianni Franchi",
          "hidden": false
        },
        {
          "_id": "68b12b7fb19c5400014849d2",
          "name": "Vicky Kalogeiton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:55:14.000Z",
      "submittedOnDailyAt": "2025-08-29T02:54:47.608Z",
      "title": "FakeParts: a New Family of AI-Generated DeepFakes",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.",
      "upvotes": 1,
      "discussionId": "68b12b80b19c5400014849d3",
      "ai_summary": "FakeParts introduces a new type of deepfake with subtle, localized manipulations that are difficult to detect, and FakePartsBench provides a large-scale dataset to evaluate detection methods.",
      "ai_keywords": [
        "deepfakes",
        "FakeParts",
        "partial manipulations",
        "facial expressions",
        "object substitutions",
        "background modifications",
        "FakePartsBench",
        "pixel-level",
        "frame-level",
        "manipulation annotations",
        "deepfake detection"
      ]
    },
    "publishedAt": "2025-08-28T13:55:14.000Z",
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "summary": "We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21052.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 97
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20766",
      "authors": [
        {
          "_id": "68b1442eb19c5400014849ff",
          "name": "Harethah Abu Shairah",
          "hidden": false
        },
        {
          "_id": "68b1442eb19c540001484a00",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "68b1442eb19c540001484a01",
          "name": "George Turkiyyah",
          "hidden": false
        },
        {
          "_id": "68b1442eb19c540001484a02",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/B8S-dF8lICOcN_JPx2OQ0.jpeg"
      ],
      "publishedAt": "2025-08-28T13:22:33.000Z",
      "submittedOnDailyAt": "2025-08-29T04:41:43.731Z",
      "title": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": false,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
      "upvotes": 1,
      "discussionId": "68b1442fb19c540001484a03",
      "ai_summary": "Rank-One Safety Injection (ROSI) enhances Large Language Model safety by amplifying refusal-mediating subspace activations without fine-tuning.",
      "ai_keywords": [
        "Rank-One Safety Injection",
        "ROSI",
        "residual stream write matrices",
        "Llama Guard 3",
        "MMLU",
        "HellaSwag",
        "Arc",
        "latent safety directions"
      ]
    },
    "publishedAt": "2025-08-28T09:22:33.000Z",
    "title": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection",
    "summary": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/B8S-dF8lICOcN_JPx2OQ0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20755",
      "authors": [
        {
          "_id": "68b12c01b19c5400014849db",
          "name": "Sam Houliston",
          "hidden": false
        },
        {
          "_id": "68b12c01b19c5400014849dc",
          "name": "Ambroise Odonnat",
          "hidden": false
        },
        {
          "_id": "68b12c01b19c5400014849dd",
          "name": "Charles Arnal",
          "hidden": false
        },
        {
          "_id": "68b12c01b19c5400014849de",
          "name": "Vivien Cabannes",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T13:12:19.000Z",
      "submittedOnDailyAt": "2025-08-29T02:56:50.777Z",
      "title": "Provable Benefits of In-Tool Learning for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.",
      "upvotes": 1,
      "discussionId": "68b12c02b19c5400014849df",
      "ai_summary": "Tool-augmented language models achieve better factual recall and scalability through external retrieval rather than memorization in their weights.",
      "ai_keywords": [
        "in-tool learning",
        "in-weight learning",
        "factual recall",
        "parameter count",
        "tool-use",
        "unbounded factual recall",
        "circuit construction",
        "pretrained large language models",
        "teaching tool-use",
        "general rules",
        "finetuning"
      ]
    },
    "publishedAt": "2025-08-28T09:12:19.000Z",
    "title": "Provable Benefits of In-Tool Learning for Large Language Models",
    "summary": "Tool-augmented language models, equipped with retrieval, memory, or external\nAPIs, are reshaping AI, yet their theoretical advantages remain underexplored.\nIn this paper, we address this question by demonstrating the benefits of\nin-tool learning (external retrieval) over in-weight learning (memorization)\nfor factual recall. We show that the number of facts a model can memorize\nsolely in its weights is fundamentally limited by its parameter count. In\ncontrast, we prove that tool-use enables unbounded factual recall via a simple\nand efficient circuit construction. These results are validated in controlled\nexperiments, where tool-using models consistently outperform memorizing ones.\nWe further show that for pretrained large language models, teaching tool-use\nand general rules is more effective than finetuning facts into memory. Our work\nprovides both a theoretical and empirical foundation, establishing why\ntool-augmented workflows are not just practical, but provably more scalable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 97
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18633",
      "authors": [
        {
          "_id": "68ae7a62364411bea07df79e",
          "name": "Chenxuan Miao",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df79f",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a0",
          "name": "Jianshu Zeng",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a1",
          "name": "Zixiang Gao",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a2",
          "name": "Hantang Liu",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a3",
          "name": "Yunfeng Yan",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a4",
          "name": "Donglian Qi",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a6",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68ae7a62364411bea07df7a7",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/kRyCjgivIKhGGy48qd2ZC.gif"
      ],
      "publishedAt": "2025-08-26T03:18:31.000Z",
      "submittedOnDailyAt": "2025-08-29T01:16:23.845Z",
      "title": "ROSE: Remove Objects with Side Effects in Videos",
      "submittedOnDailyBy": {
        "_id": "64a54e468cfaa458bd6844bf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
        "isPro": false,
        "fullname": "Yutong Feng",
        "user": "fengyutong",
        "type": "user"
      },
      "summary": "Video object removal has achieved advanced performance due to the recent\nsuccess of video generative models. However, when addressing the side effects\nof objects, e.g., their shadows and reflections, existing works struggle to\neliminate these effects for the scarcity of paired video data as supervision.\nThis paper presents ROSE, termed Remove Objects with Side Effects, a framework\nthat systematically studies the object's effects on environment, which can be\ncategorized into five common cases: shadows, reflections, light, translucency\nand mirror. Given the challenges of curating paired videos exhibiting the\naforementioned effects, we leverage a 3D rendering engine for synthetic data\ngeneration. We carefully construct a fully-automatic pipeline for data\npreparation, which simulates a large-scale paired dataset with diverse scenes,\nobjects, shooting angles, and camera trajectories. ROSE is implemented as an\nvideo inpainting model built on diffusion transformer. To localize all\nobject-correlated areas, the entire video is fed into the model for\nreference-based erasing. Moreover, additional supervision is introduced to\nexplicitly predict the areas affected by side effects, which can be revealed\nthrough the differential mask between the paired videos. To fully investigate\nthe model performance on various side effect removal, we presents a new\nbenchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five\nspecial side effects for comprehensive evaluation. Experimental results\ndemonstrate that ROSE achieves superior performance compared to existing video\nobject erasing models and generalizes well to real-world video scenarios. The\nproject page is https://rose2025-inpaint.github.io/.",
      "upvotes": 1,
      "discussionId": "68ae7a63364411bea07df7a8",
      "projectPage": "https://rose2025-inpaint.github.io/",
      "githubRepo": "https://github.com/Kunbyte-AI/ROSE",
      "ai_summary": "ROSE, a video inpainting framework using diffusion transformers, effectively removes objects and their side effects like shadows and reflections by leveraging synthetic data and differential masks.",
      "ai_keywords": [
        "video generative models",
        "shadows",
        "reflections",
        "light",
        "translucency",
        "mirror",
        "3D rendering engine",
        "diffusion transformer",
        "video inpainting",
        "differential mask",
        "ROSE-Bench"
      ],
      "githubStars": 18
    },
    "publishedAt": "2025-08-25T23:18:31.000Z",
    "title": "ROSE: Remove Objects with Side Effects in Videos",
    "summary": "Video object removal has achieved advanced performance due to the recent\nsuccess of video generative models. However, when addressing the side effects\nof objects, e.g., their shadows and reflections, existing works struggle to\neliminate these effects for the scarcity of paired video data as supervision.\nThis paper presents ROSE, termed Remove Objects with Side Effects, a framework\nthat systematically studies the object's effects on environment, which can be\ncategorized into five common cases: shadows, reflections, light, translucency\nand mirror. Given the challenges of curating paired videos exhibiting the\naforementioned effects, we leverage a 3D rendering engine for synthetic data\ngeneration. We carefully construct a fully-automatic pipeline for data\npreparation, which simulates a large-scale paired dataset with diverse scenes,\nobjects, shooting angles, and camera trajectories. ROSE is implemented as an\nvideo inpainting model built on diffusion transformer. To localize all\nobject-correlated areas, the entire video is fed into the model for\nreference-based erasing. Moreover, additional supervision is introduced to\nexplicitly predict the areas affected by side effects, which can be revealed\nthrough the differential mask between the paired videos. To fully investigate\nthe model performance on various side effect removal, we presents a new\nbenchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five\nspecial side effects for comprehensive evaluation. Experimental results\ndemonstrate that ROSE achieves superior performance compared to existing video\nobject erasing models and generalizes well to real-world video scenarios. The\nproject page is https://rose2025-inpaint.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/kRyCjgivIKhGGy48qd2ZC.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a54e468cfaa458bd6844bf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
      "fullname": "Yutong Feng",
      "name": "fengyutong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15228",
      "authors": [
        {
          "_id": "68b11155b19c54000148492d",
          "name": "Ziang Cao",
          "hidden": false
        },
        {
          "_id": "68b11155b19c54000148492e",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68b11155b19c54000148492f",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "68b11155b19c540001484930",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T04:31:14.000Z",
      "submittedOnDailyAt": "2025-08-29T01:16:24.295Z",
      "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation",
      "submittedOnDailyBy": {
        "_id": "65af6f6b52e1b2aae437af2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
        "isPro": false,
        "fullname": "Ziang Cao",
        "user": "Caoza",
        "type": "user"
      },
      "summary": "3D content inherently encompasses multi-modal characteristics and can be\nprojected into different modalities (e.g., RGB images, RGBD, and point clouds).\nEach modality exhibits distinct advantages in 3D asset modeling: RGB images\ncontain vivid 3D textures, whereas point clouds define fine-grained 3D\ngeometries. However, most existing 3D-native generative architectures either\noperate predominantly within single-modality paradigms-thus overlooking the\ncomplementary benefits of multi-modality data-or restrict themselves to 3D\nstructures, thereby limiting the scope of available training datasets. To\nholistically harness multi-modalities for 3D modeling, we present TriMM, the\nfirst feed-forward 3D-native generative model that learns from basic\nmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM\nfirst introduces collaborative multi-modal coding, which integrates\nmodality-specific features while preserving their unique representational\nstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to\nraise the robustness and performance of multi-modal coding. 3) Based on the\nembedded multi-modal code, TriMM employs a triplane latent diffusion model to\ngenerate 3D assets of superior quality, enhancing both the texture and the\ngeometric detail. Extensive experiments on multiple well-known datasets\ndemonstrate that TriMM, by effectively leveraging multi-modality, achieves\ncompetitive performance with models trained on large-scale datasets, despite\nutilizing a small amount of training data. Furthermore, we conduct additional\nexperiments on recent RGB-D datasets, verifying the feasibility of\nincorporating other multi-modal datasets into 3D generation.",
      "upvotes": 1,
      "discussionId": "68b11155b19c540001484931",
      "ai_summary": "TriMM, a feed-forward 3D-native generative model, integrates multi-modal data (RGB, RGBD, point clouds) to enhance 3D asset generation quality and robustness.",
      "ai_keywords": [
        "collaborative multi-modal coding",
        "auxiliary 2D and 3D supervision",
        "triplane latent diffusion model"
      ]
    },
    "publishedAt": "2025-08-21T00:31:14.000Z",
    "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation",
    "summary": "3D content inherently encompasses multi-modal characteristics and can be\nprojected into different modalities (e.g., RGB images, RGBD, and point clouds).\nEach modality exhibits distinct advantages in 3D asset modeling: RGB images\ncontain vivid 3D textures, whereas point clouds define fine-grained 3D\ngeometries. However, most existing 3D-native generative architectures either\noperate predominantly within single-modality paradigms-thus overlooking the\ncomplementary benefits of multi-modality data-or restrict themselves to 3D\nstructures, thereby limiting the scope of available training datasets. To\nholistically harness multi-modalities for 3D modeling, we present TriMM, the\nfirst feed-forward 3D-native generative model that learns from basic\nmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM\nfirst introduces collaborative multi-modal coding, which integrates\nmodality-specific features while preserving their unique representational\nstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to\nraise the robustness and performance of multi-modal coding. 3) Based on the\nembedded multi-modal code, TriMM employs a triplane latent diffusion model to\ngenerate 3D assets of superior quality, enhancing both the texture and the\ngeometric detail. Extensive experiments on multiple well-known datasets\ndemonstrate that TriMM, by effectively leveraging multi-modality, achieves\ncompetitive performance with models trained on large-scale datasets, despite\nutilizing a small amount of training data. Furthermore, we conduct additional\nexperiments on recent RGB-D datasets, verifying the feasibility of\nincorporating other multi-modal datasets into 3D generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65af6f6b52e1b2aae437af2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
      "fullname": "Ziang Cao",
      "name": "Caoza",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21060",
      "authors": [
        {
          "_id": "68b145d7b19c540001484a0c",
          "name": "Frano Raji",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a0d",
          "name": "Haofei Xu",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a0e",
          "name": "Marko Mihajlovic",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a0f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a10",
          "name": "Irem Demir",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a11",
          "name": "Emircan Gndodu",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a12",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a13",
          "name": "Sergey Prokudin",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a14",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "68b145d7b19c540001484a15",
          "name": "Siyu Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a40013aab6329074397dfe/fa2AvgEhvlMJvy5merEU7.gif"
      ],
      "publishedAt": "2025-08-28T17:58:20.000Z",
      "submittedOnDailyAt": "2025-08-29T05:28:15.013Z",
      "title": "Multi-View 3D Point Tracking",
      "submittedOnDailyBy": {
        "_id": "64a40013aab6329074397dfe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I13uQeHFl9ynuS_Sur6rY.png",
        "isPro": false,
        "fullname": "Frano Raji",
        "user": "m43",
        "type": "user"
      },
      "summary": "We introduce the first data-driven multi-view 3D point tracker, designed to\ntrack arbitrary points in dynamic scenes using multiple camera views. Unlike\nexisting monocular trackers, which struggle with depth ambiguities and\nocclusion, or prior multi-camera methods that require over 20 cameras and\ntedious per-sequence optimization, our feed-forward model directly predicts 3D\ncorrespondences using a practical number of cameras (e.g., four), enabling\nrobust and accurate online tracking. Given known camera poses and either\nsensor-based or estimated multi-view depth, our tracker fuses multi-view\nfeatures into a unified point cloud and applies k-nearest-neighbors correlation\nalongside a transformer-based update to reliably estimate long-range 3D\ncorrespondences, even under occlusion. We train on 5K synthetic multi-view\nKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and\nDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.\nOur method generalizes well to diverse camera setups of 1-8 views with varying\nvantage points and video lengths of 24-150 frames. By releasing our tracker\nalongside training and evaluation datasets, we aim to set a new standard for\nmulti-view 3D tracking research and provide a practical tool for real-world\napplications. Project page available at https://ethz-vlg.github.io/mvtracker.",
      "upvotes": 0,
      "discussionId": "68b145d8b19c540001484a16",
      "ai_summary": "A multi-view 3D point tracker using a feed-forward model with transformers and k-nearest-neighbors achieves robust tracking with fewer cameras and less optimization compared to existing methods.",
      "ai_keywords": [
        "multi-view 3D point tracker",
        "feed-forward model",
        "transformer-based update",
        "k-nearest-neighbors",
        "multi-view features",
        "unified point cloud",
        "long-range 3D correspondences",
        "Panoptic Studio",
        "DexYCB",
        "synthetic multi-view Kubric sequences"
      ]
    },
    "publishedAt": "2025-08-28T13:58:20.000Z",
    "title": "Multi-View 3D Point Tracking",
    "summary": "We introduce the first data-driven multi-view 3D point tracker, designed to\ntrack arbitrary points in dynamic scenes using multiple camera views. Unlike\nexisting monocular trackers, which struggle with depth ambiguities and\nocclusion, or prior multi-camera methods that require over 20 cameras and\ntedious per-sequence optimization, our feed-forward model directly predicts 3D\ncorrespondences using a practical number of cameras (e.g., four), enabling\nrobust and accurate online tracking. Given known camera poses and either\nsensor-based or estimated multi-view depth, our tracker fuses multi-view\nfeatures into a unified point cloud and applies k-nearest-neighbors correlation\nalongside a transformer-based update to reliably estimate long-range 3D\ncorrespondences, even under occlusion. We train on 5K synthetic multi-view\nKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and\nDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.\nOur method generalizes well to diverse camera setups of 1-8 views with varying\nvantage points and video lengths of 24-150 frames. By releasing our tracker\nalongside training and evaluation datasets, we aim to set a new standard for\nmulti-view 3D tracking research and provide a practical tool for real-world\napplications. Project page available at https://ethz-vlg.github.io/mvtracker.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a40013aab6329074397dfe/fa2AvgEhvlMJvy5merEU7.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a40013aab6329074397dfe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I13uQeHFl9ynuS_Sur6rY.png",
      "fullname": "Frano Raji",
      "name": "m43",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]