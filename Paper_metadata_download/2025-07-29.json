[
  {
    "paper": {
      "id": "2507.19849",
      "authors": [
        {
          "_id": "688836e8af872d625c10c60d",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c60e",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c60f",
          "name": "Kai Ma",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c610",
          "name": "Licheng Bao",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c611",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c612",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c613",
          "name": "Zhongxia Chen",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c614",
          "name": "Jiazhen Du",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c615",
          "name": "Huiyang Wang",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c616",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c617",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c618",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c619",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "688836e8af872d625c10c61a",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T07:53:11.000Z",
      "submittedOnDailyAt": "2025-07-29T01:41:53.469Z",
      "title": "Agentic Reinforced Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO",
      "upvotes": 40,
      "discussionId": "688836e9af872d625c10c61b",
      "projectPage": "https://github.com/dongguanting/ARPO",
      "githubRepo": "https://github.com/dongguanting/ARPO",
      "ai_summary": "Agentic Reinforced Policy Optimization (ARPO) is a novel RL algorithm that enhances multi-turn LLM-based agents by adaptive uncertainty management and advantage attribution, outperforming trajectory-level RL algorithms with reduced resource usage.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "LLMs",
        "agentic RL",
        "Agentic Reinforced Policy Optimization",
        "entropy-based adaptive rollout mechanism",
        "advantage attribution estimation",
        "computational reasoning",
        "knowledge reasoning",
        "deep search domains",
        "global trajectory sampling",
        "step-level sampling",
        "trajectory-level RL algorithms"
      ],
      "githubStars": 56
    },
    "publishedAt": "2025-07-26T03:53:11.000Z",
    "title": "Agentic Reinforced Policy Optimization",
    "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20939",
      "authors": [
        {
          "_id": "68882c7eaf872d625c10c5c0",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c1",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c2",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c3",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c4",
          "name": "Junfu Pu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c5",
          "name": "Yizhuo Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c6",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c7",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c8",
          "name": "Lisheng Duan",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c9",
          "name": "Xinyu Zuo",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5ca",
          "name": "Jinwen Luo",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cb",
          "name": "Weibo Gu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cc",
          "name": "Zexuan Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cd",
          "name": "Xiaojing Zhang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5ce",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cf",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5d0",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5d1",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T15:52:36.000Z",
      "submittedOnDailyAt": "2025-07-29T00:45:07.316Z",
      "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
      "submittedOnDailyBy": {
        "_id": "6455cc8f654d8bccae50e4d4",
        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
        "isPro": false,
        "fullname": "Yuying Ge",
        "user": "tttoaster",
        "type": "user"
      },
      "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
      "upvotes": 32,
      "discussionId": "68882c7eaf872d625c10c5d2",
      "projectPage": "https://tencentarc.github.io/posts/arc-video-announcement/",
      "githubRepo": "https://github.com/TencentARC/ARC-Hunyuan-Video-7B",
      "ai_summary": "A multimodal model that processes visual, audio, and textual signals for structured comprehension of real-world short videos improves video search, recommendation, and engagement.",
      "ai_keywords": [
        "multimodal model",
        "timestamped video captioning",
        "summarization",
        "video question answering",
        "temporal video grounding",
        "video reasoning",
        "instruction fine-tuning",
        "reinforcement learning",
        "ShortVid-Bench",
        "zero-shot learning",
        "H20 GPU"
      ],
      "githubStars": 42
    },
    "publishedAt": "2025-07-28T11:52:36.000Z",
    "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
    "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20939.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6455cc8f654d8bccae50e4d4",
      "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
      "fullname": "Yuying Ge",
      "name": "tttoaster",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21045",
      "authors": [
        {
          "_id": "68882eddaf872d625c10c5d4",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d5",
          "name": "Jiahao Lu",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d6",
          "name": "Zhisheng Huang",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d7",
          "name": "Zhuowei Shen",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d8",
          "name": "Chengfeng Zhao",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d9",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5da",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5db",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5dc",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5dd",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5de",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-29T00:46:42.998Z",
      "title": "Reconstructing 4D Spatial Intelligence: A Survey",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
      "upvotes": 21,
      "discussionId": "68882eddaf872d625c10c5df",
      "githubRepo": "https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence",
      "ai_summary": "A survey organizes methods for reconstructing 4D spatial intelligence from visual observations into five progressive levels, offering analysis and identifying future research directions.",
      "ai_keywords": [
        "3D representations",
        "deep learning architectures",
        "4D scene reconstruction",
        "3D attributes",
        "depth",
        "pose",
        "point maps",
        "3D scene components",
        "objects",
        "humans",
        "structures",
        "4D dynamic scenes",
        "modeling of interactions",
        "physical laws",
        "constraints"
      ],
      "githubStars": 50
    },
    "publishedAt": "2025-07-28T13:59:02.000Z",
    "title": "Reconstructing 4D Spatial Intelligence: A Survey",
    "summary": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21045.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21049",
      "authors": [
        {
          "_id": "68884427af872d625c10c66b",
          "user": {
            "_id": "6594d390674349122ce6f368",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
            "isPro": false,
            "fullname": "Zedong Wang (Jacky)",
            "user": "ZedongWangAI",
            "type": "user"
          },
          "name": "Zedong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:39.228Z",
          "hidden": false
        },
        {
          "_id": "68884427af872d625c10c66c",
          "user": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "isPro": false,
            "fullname": "Siyuan Li",
            "user": "Lupin1998",
            "type": "user"
          },
          "name": "Siyuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:42.285Z",
          "hidden": false
        },
        {
          "_id": "68884427af872d625c10c66d",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:59:28.000Z",
      "submittedOnDailyAt": "2025-07-29T02:28:08.749Z",
      "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for\n  Multi-Task Learning",
      "submittedOnDailyBy": {
        "_id": "6594d390674349122ce6f368",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
        "isPro": false,
        "fullname": "Zedong Wang (Jacky)",
        "user": "ZedongWangAI",
        "type": "user"
      },
      "summary": "Despite the promise of Multi-Task Learning in leveraging complementary\nknowledge across tasks, existing multi-task optimization (MTO) techniques\nremain fixated on resolving conflicts via optimizer-centric loss scaling and\ngradient manipulation strategies, yet fail to deliver consistent gains. In this\npaper, we argue that the shared representation space, where task interactions\nnaturally occur, offers rich information and potential for operations\ncomplementary to existing optimizers, especially for facilitating the\ninter-task complementarity, which is rarely explored in MTO. This intuition\nleads to Rep-MTL, which exploits the representation-level task saliency to\nquantify interactions between task-specific optimization and shared\nrepresentation learning. By steering these saliencies through entropy-based\npenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate\nnegative transfer by maintaining the effective training of individual tasks\ninstead pure conflict-solving, while explicitly promoting complementary\ninformation sharing. Experiments are conducted on four challenging MTL\nbenchmarks covering both task-shift and domain-shift scenarios. The results\nshow that Rep-MTL, even paired with the basic equal weighting policy, achieves\ncompetitive performance gains with favorable efficiency. Beyond standard\nperformance metrics, Power Law exponent analysis demonstrates Rep-MTL's\nefficacy in balancing task-specific learning and cross-task sharing. The\nproject page is available at HERE.",
      "upvotes": 20,
      "discussionId": "68884427af872d625c10c66e",
      "projectPage": "https://jacky1128.github.io/RepMTL/",
      "githubRepo": "https://github.com/Jacky1128/Rep-MTL",
      "ai_summary": "Rep-MTL optimizes multi-task learning by leveraging task saliency in shared representations to promote complementarity and reduce negative transfer.",
      "ai_keywords": [
        "Multi-Task Learning",
        "MTO",
        "shared representation space",
        "task-specific optimization",
        "entropy-based penalization",
        "sample-wise cross-task alignment",
        "negative transfer",
        "task-shift",
        "domain-shift",
        "Power Law exponent analysis"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-07-28T13:59:28.000Z",
    "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for\n  Multi-Task Learning",
    "summary": "Despite the promise of Multi-Task Learning in leveraging complementary\nknowledge across tasks, existing multi-task optimization (MTO) techniques\nremain fixated on resolving conflicts via optimizer-centric loss scaling and\ngradient manipulation strategies, yet fail to deliver consistent gains. In this\npaper, we argue that the shared representation space, where task interactions\nnaturally occur, offers rich information and potential for operations\ncomplementary to existing optimizers, especially for facilitating the\ninter-task complementarity, which is rarely explored in MTO. This intuition\nleads to Rep-MTL, which exploits the representation-level task saliency to\nquantify interactions between task-specific optimization and shared\nrepresentation learning. By steering these saliencies through entropy-based\npenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate\nnegative transfer by maintaining the effective training of individual tasks\ninstead pure conflict-solving, while explicitly promoting complementary\ninformation sharing. Experiments are conducted on four challenging MTL\nbenchmarks covering both task-shift and domain-shift scenarios. The results\nshow that Rep-MTL, even paired with the basic equal weighting policy, achieves\ncompetitive performance gains with favorable efficiency. Beyond standard\nperformance metrics, Power Law exponent analysis demonstrates Rep-MTL's\nefficacy in balancing task-specific learning and cross-task sharing. The\nproject page is available at HERE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6594d390674349122ce6f368",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
      "fullname": "Zedong Wang (Jacky)",
      "name": "ZedongWangAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20984",
      "authors": [
        {
          "_id": "68883e4aaf872d625c10c63f",
          "user": {
            "_id": "642924f916d4d8293c93af08",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642924f916d4d8293c93af08/Olp4zFPgGu2E6mFFPXfQO.jpeg",
            "isPro": false,
            "fullname": "Yixin Song",
            "user": "yixinsong",
            "type": "user"
          },
          "name": "Yixin Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:50.992Z",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c640",
          "name": "Zhenliang Xue",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c641",
          "name": "Dongliang Wei",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c642",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c643",
          "name": "Jianxiang Gao",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c644",
          "user": {
            "_id": "666a8735f5d8ceba613494b9",
            "avatarUrl": "/avatars/3feec27d0b041d28af0fcc3426d1bd82.svg",
            "isPro": false,
            "fullname": "Junchen",
            "user": "Sorrymaker2024",
            "type": "user"
          },
          "name": "Junchen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:44.672Z",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c645",
          "name": "Hangyu Liang",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c646",
          "user": {
            "_id": "6709d31dbb2498474f4b11ad",
            "avatarUrl": "/avatars/e507fb237ed9f0b098e494e75127988e.svg",
            "isPro": false,
            "fullname": "Guangshuo Qin",
            "user": "qsstcl",
            "type": "user"
          },
          "name": "Guangshuo Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:53.325Z",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c647",
          "name": "Chengrong Tian",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c648",
          "name": "Bo Wen",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c649",
          "name": "Longyu Zhao",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c64a",
          "name": "Xinrui Zheng",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c64b",
          "name": "Zeyu Mi",
          "hidden": false
        },
        {
          "_id": "68883e4aaf872d625c10c64c",
          "name": "Haibo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T16:45:14.000Z",
      "submittedOnDailyAt": "2025-07-29T01:52:55.485Z",
      "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
      "submittedOnDailyBy": {
        "_id": "642924f916d4d8293c93af08",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642924f916d4d8293c93af08/Olp4zFPgGu2E6mFFPXfQO.jpeg",
        "isPro": false,
        "fullname": "Yixin Song",
        "user": "yixinsong",
        "type": "user"
      },
      "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
      "upvotes": 19,
      "discussionId": "68883e4aaf872d625c10c64d",
      "ai_summary": "SmallThinker, designed for localdevices with limited resources, uses advanced architectural innovations to achieve high performance without requiring GPU hardware.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoE",
        "sparse feed-forward networks",
        "pre-attention router",
        "NoPE-RoPE hybrid sparse attention mechanism",
        "KV cache"
      ]
    },
    "publishedAt": "2025-07-28T12:45:14.000Z",
    "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
    "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642924f916d4d8293c93af08",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642924f916d4d8293c93af08/Olp4zFPgGu2E6mFFPXfQO.jpeg",
      "fullname": "Yixin Song",
      "name": "yixinsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.21033",
      "authors": [
        {
          "_id": "688836d8af872d625c10c604",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c605",
          "name": "Siwei Yang",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c606",
          "user": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "isPro": true,
            "fullname": "Bingchen Zhao",
            "user": "tennant",
            "type": "user"
          },
          "name": "Bingchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:19.454Z",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c607",
          "name": "Letian Zhang",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c608",
          "name": "Qing Liu",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c609",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "688836d8af872d625c10c60a",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:54:04.000Z",
      "submittedOnDailyAt": "2025-07-29T01:47:07.013Z",
      "title": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
      "submittedOnDailyBy": {
        "_id": "645eb61da3c5cd8a16efffff",
        "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
        "isPro": false,
        "fullname": "Cihang Xie",
        "user": "cihangxie",
        "type": "user"
      },
      "summary": "Recent advancements in large multimodal models like GPT-4o have set a new\nstandard for high-fidelity, instruction-guided image editing. However, the\nproprietary nature of these models and their training data creates a\nsignificant barrier for open-source research. To bridge this gap, we introduce\nGPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus\ncontaining more than 1.5 million high-quality triplets (instruction, source\nimage, edited image). We systematically construct this dataset by leveraging\nthe versatile capabilities of GPT-4o to unify and refine three popular\nimage-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our\nmethodology involves 1) regenerating output images to enhance visual quality\nand instruction alignment, and 2) selectively rewriting prompts to improve\nsemantic clarity. To validate the efficacy of our dataset, we fine-tune\nadvanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are\nexciting, e.g., the fine-tuned FluxKontext achieves highly competitive\nperformance across a comprehensive suite of benchmarks, including 7.24 on\nGEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger\ninstruction following and higher perceptual quality while maintaining identity.\nThese scores markedly exceed all previously published open-source methods and\nsubstantially narrow the gap to leading proprietary models. We hope the full\nrelease of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in\ninstruction-guided image editing.",
      "upvotes": 10,
      "discussionId": "688836d8af872d625c10c60b",
      "projectPage": "https://ucsc-vlaa.github.io/GPT-Image-Edit/",
      "githubRepo": "https://github.com/wyhlovecpp/GPT-Image-Edit",
      "githubStars": 22
    },
    "publishedAt": "2025-07-28T13:54:04.000Z",
    "title": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
    "summary": "Recent advancements in large multimodal models like GPT-4o have set a new\nstandard for high-fidelity, instruction-guided image editing. However, the\nproprietary nature of these models and their training data creates a\nsignificant barrier for open-source research. To bridge this gap, we introduce\nGPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus\ncontaining more than 1.5 million high-quality triplets (instruction, source\nimage, edited image). We systematically construct this dataset by leveraging\nthe versatile capabilities of GPT-4o to unify and refine three popular\nimage-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our\nmethodology involves 1) regenerating output images to enhance visual quality\nand instruction alignment, and 2) selectively rewriting prompts to improve\nsemantic clarity. To validate the efficacy of our dataset, we fine-tune\nadvanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are\nexciting, e.g., the fine-tuned FluxKontext achieves highly competitive\nperformance across a comprehensive suite of benchmarks, including 7.24 on\nGEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger\ninstruction following and higher perceptual quality while maintaining identity.\nThese scores markedly exceed all previously published open-source methods and\nsubstantially narrow the gap to leading proprietary models. We hope the full\nrelease of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in\ninstruction-guided image editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645eb61da3c5cd8a16efffff",
      "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
      "fullname": "Cihang Xie",
      "name": "cihangxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20673",
      "authors": [
        {
          "_id": "68883a8eaf872d625c10c631",
          "name": "Yuzhong Zhao",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c632",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c633",
          "user": {
            "_id": "6512a2e284bedffb5ac5a511",
            "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
            "isPro": false,
            "fullname": "Junpeng Liu",
            "user": "jeepliu",
            "type": "user"
          },
          "name": "Junpeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:16.611Z",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c634",
          "name": "Jingye Chen",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c635",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c636",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c637",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c638",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c639",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c63a",
          "name": "Qixiang Ye",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c63b",
          "name": "Fang Wan",
          "hidden": false
        },
        {
          "_id": "68883a8eaf872d625c10c63c",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T09:54:05.000Z",
      "submittedOnDailyAt": "2025-07-29T01:39:36.823Z",
      "title": "Geometric-Mean Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6512a2e284bedffb5ac5a511",
        "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
        "isPro": false,
        "fullname": "Junpeng Liu",
        "user": "jeepliu",
        "type": "user"
      },
      "summary": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO.",
      "upvotes": 10,
      "discussionId": "68883a8eaf872d625c10c63d",
      "githubRepo": "https://github.com/callsys/GMPO",
      "ai_summary": "Geometric-Mean Policy Optimization (GMPO) stabilizes policy updates in large language models by maximizing the geometric mean of token-level rewards, improving performance on mathematical and multimodal reasoning benchmarks.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "token-level rewards",
        "policy updates",
        "outlier importance-weighted rewards",
        "importance sampling ratio",
        "Geometric-Mean Policy Optimization (GMPO)",
        "AIME24",
        "AMC",
        "MATH500",
        "OlympiadBench",
        "Minerva",
        "Geometry3K"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-28T05:54:05.000Z",
    "title": "Geometric-Mean Policy Optimization",
    "summary": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6512a2e284bedffb5ac5a511",
      "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
      "fullname": "Junpeng Liu",
      "name": "jeepliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20025",
      "authors": [
        {
          "_id": "68883088af872d625c10c5eb",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ec",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:26.917Z",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ed",
          "user": {
            "_id": "6478679d7b370854241b2ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
            "isPro": false,
            "fullname": "xiangan",
            "user": "xiangan",
            "type": "user"
          },
          "name": "Xiang An",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:29.232Z",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ee",
          "user": {
            "_id": "64f597588b6d053c709debd9",
            "avatarUrl": "/avatars/85d046a412c503f883e16324030a457b.svg",
            "isPro": false,
            "fullname": "Kun",
            "user": "Athinklo",
            "type": "user"
          },
          "name": "Kun Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:24.529Z",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ef",
          "name": "Yongle Zhao",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f0",
          "name": "Weimo Deng",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f1",
          "name": "Zimin Ran",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f2",
          "name": "Yumeng Wang",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f4",
          "name": "Roy Miles",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f5",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f6",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T17:47:09.000Z",
      "submittedOnDailyAt": "2025-07-29T00:56:42.617Z",
      "title": "Region-based Cluster Discrimination for Visual Representation Learning",
      "submittedOnDailyBy": {
        "_id": "6478679d7b370854241b2ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
        "isPro": false,
        "fullname": "xiangan",
        "user": "xiangan",
        "type": "user"
      },
      "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
      "upvotes": 10,
      "discussionId": "68883088af872d625c10c5f7",
      "githubRepo": "https://github.com/deepglint/MVT",
      "ai_summary": "RICE enhances region-level visual and OCR capabilities through a novel Region Transformer and cluster discrimination loss, achieving superior performance across dense prediction and perception tasks.",
      "ai_keywords": [
        "Region-Aware Cluster Discrimination (RICE)",
        "Region Transformer",
        "cluster discrimination loss",
        "segmentation",
        "dense detection",
        "Multimodal Large Language Models (MLLMs)"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-26T13:47:09.000Z",
    "title": "Region-based Cluster Discrimination for Visual Representation Learning",
    "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20025.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6478679d7b370854241b2ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
      "fullname": "xiangan",
      "name": "xiangan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.21046",
      "authors": [
        {
          "_id": "68886bb5af872d625c10c6ba",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6bb",
          "name": "Jiayi Geng",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6bc",
          "name": "Wenyue Hua",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6bd",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6be",
          "name": "Xinzhe Juan",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6bf",
          "user": {
            "_id": "6632160088f75d987d1a156f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6632160088f75d987d1a156f/mYlMQfK1BGWeEbOSMmeSb.jpeg",
            "isPro": false,
            "fullname": "Hongzhang Liu",
            "user": "Alphamasterliu",
            "type": "user"
          },
          "name": "Hongzhang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:22.183Z",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c0",
          "name": "Shilong Liu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c1",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c2",
          "name": "Xuan Qi",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c3",
          "name": "Yiran Wu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c4",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c5",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c6",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c7",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c8",
          "user": {
            "_id": "65f40e83653c231cbaf7defe",
            "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg",
            "isPro": false,
            "fullname": "Jiayi Zhang",
            "user": "didiforhugface",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:24.616Z",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6c9",
          "user": {
            "_id": "649ea7106282cb41e77760bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649ea7106282cb41e77760bc/HlWjaqxr03ob93vdKg_LQ.jpeg",
            "isPro": false,
            "fullname": "Isaac",
            "user": "XiangJinYu",
            "type": "user"
          },
          "name": "Jinyu Xiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:19.166Z",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6ca",
          "name": "Yixiong Fang",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6cb",
          "name": "Qiwen Zhao",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6cc",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6cd",
          "name": "Qihan Ren",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6ce",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6cf",
          "name": "Zhenghailong Wang",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6d0",
          "name": "Minda Hu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6d1",
          "name": "Huazheng Wang",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6d2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6d3",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68886bb5af872d625c10c6d4",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:59:05.000Z",
      "submittedOnDailyAt": "2025-07-29T05:06:58.475Z",
      "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
      "submittedOnDailyBy": {
        "_id": "65271df5abd7795aaa1fd86e",
        "avatarUrl": "/avatars/99e1aabaefcf5e7f438817375cd1ddef.svg",
        "isPro": false,
        "fullname": "Jiahao Qiu",
        "user": "jiahaoq",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
      "upvotes": 9,
      "discussionId": "68886bb6af872d625c10c6d5",
      "ai_summary": "This survey reviews architectures and methods for self-evolving agents in continual learning environments, examining different components, adaptation stages, and design considerations.",
      "ai_keywords": [
        "continual learning",
        "adaptive reasoning",
        "real-time adaptation",
        "self-evolving agents",
        "evolutionary mechanisms",
        "intra-test-time",
        "inter-test-time",
        "scalar rewards",
        "textual feedback",
        "single-agent systems",
        "multi-agent systems",
        "evaluation metrics",
        "benchmarks",
        "applications",
        "challenges",
        "scalability",
        "co-evolutionary dynamics",
        "Artificial Super Intelligence (ASI)"
      ]
    },
    "publishedAt": "2025-07-28T13:59:05.000Z",
    "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65271df5abd7795aaa1fd86e",
      "avatarUrl": "/avatars/99e1aabaefcf5e7f438817375cd1ddef.svg",
      "fullname": "Jiahao Qiu",
      "name": "jiahaoq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.19766",
      "authors": [
        {
          "_id": "6888299caf872d625c10c5b2",
          "user": {
            "_id": "651eaaad567de56557e7bde6",
            "avatarUrl": "/avatars/7ddf08fe4459fa2b7b3796be4eb4f43d.svg",
            "isPro": false,
            "fullname": "duu",
            "user": "dongdongdongdu",
            "type": "user"
          },
          "name": "Dong Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:53.985Z",
          "hidden": false
        },
        {
          "_id": "6888299caf872d625c10c5b3",
          "user": {
            "_id": "67cffe3d275d90f8097ae10a",
            "avatarUrl": "/avatars/eb4d06abb06c677e4d1be77685fc23f6.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "forestliutc",
            "type": "user"
          },
          "name": "Shulin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:57.247Z",
          "hidden": false
        },
        {
          "_id": "6888299caf872d625c10c5b4",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "6888299caf872d625c10c5b5",
          "name": "Shaohua Chen",
          "hidden": false
        },
        {
          "_id": "6888299caf872d625c10c5b6",
          "name": "Yang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T03:42:33.000Z",
      "submittedOnDailyAt": "2025-07-29T05:44:29.213Z",
      "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing\n  Large Language Models' Reasoning Abilities",
      "submittedOnDailyBy": {
        "_id": "67cffe3d275d90f8097ae10a",
        "avatarUrl": "/avatars/eb4d06abb06c677e4d1be77685fc23f6.svg",
        "isPro": false,
        "fullname": "liu",
        "user": "forestliutc",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.",
      "upvotes": 8,
      "discussionId": "6888299caf872d625c10c5b7",
      "ai_summary": "A novel reinforcement learning approach for large language models addresses inefficiencies in handling ultra-long outputs, enhancing performance and training speed through segmentation and dynamic masking techniques.",
      "ai_keywords": [
        "large language models",
        "reinforcement learning",
        "verifiable rewards",
        "ultra-long output reinforcement learning",
        "segment rollout",
        "dynamic masking",
        "well-Mastered Positive Tokens",
        "entropy collapse",
        "AIME2025",
        "BeyondAIME"
      ]
    },
    "publishedAt": "2025-07-25T23:42:33.000Z",
    "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing\n  Large Language Models' Reasoning Abilities",
    "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67cffe3d275d90f8097ae10a",
      "avatarUrl": "/avatars/eb4d06abb06c677e4d1be77685fc23f6.svg",
      "fullname": "liu",
      "name": "forestliutc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.19804",
      "authors": [
        {
          "_id": "68883519af872d625c10c5f9",
          "name": "Peng Cai",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fa",
          "name": "Qiang Li",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fb",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:21.957Z",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fc",
          "name": "Dong Guo",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fd",
          "name": "Jia Li",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fe",
          "name": "Nan Zhou",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5ff",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c600",
          "name": "Ninghua Yang",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c601",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T05:36:48.000Z",
      "submittedOnDailyAt": "2025-07-29T01:14:03.210Z",
      "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
      "upvotes": 5,
      "discussionId": "68883519af872d625c10c602",
      "githubRepo": "https://github.com/caipeng328/ForCenNet",
      "ai_summary": "A Foreground-Centric Network for document image rectification improves state-of-the-art by effectively handling foreground elements and layout distortions.",
      "ai_keywords": [
        "Foreground-Centric Network",
        "ForCenNet",
        "curvature consistency loss"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-26T01:36:48.000Z",
    "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
    "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.19058",
      "authors": [
        {
          "_id": "688771dec8db48c925156ed0",
          "user": {
            "_id": "676c04f44464f476aaa53d1c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/k488J1893F3JGwEMvaeuh.png",
            "isPro": false,
            "fullname": "Chong Xia",
            "user": "xiac24",
            "type": "user"
          },
          "name": "Chong Xia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:10:02.445Z",
          "hidden": false
        },
        {
          "_id": "688771dec8db48c925156ed1",
          "name": "Shengjun Zhang",
          "hidden": false
        },
        {
          "_id": "688771dec8db48c925156ed2",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "688771dec8db48c925156ed3",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "688771dec8db48c925156ed4",
          "name": "Khodchaphun Hirunyaratsameewong",
          "hidden": false
        },
        {
          "_id": "688771dec8db48c925156ed5",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-25T08:21:12.000Z",
      "submittedOnDailyAt": "2025-07-29T05:42:17.033Z",
      "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with\n  Concept Relation Alignment",
      "submittedOnDailyBy": {
        "_id": "676c04f44464f476aaa53d1c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/k488J1893F3JGwEMvaeuh.png",
        "isPro": false,
        "fullname": "Chong Xia",
        "user": "xiac24",
        "type": "user"
      },
      "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.",
      "upvotes": 2,
      "discussionId": "688771dec8db48c925156ed6",
      "projectPage": "https://xiac20.github.io/ScenePainter/",
      "githubRepo": "https://github.com/xiac20/ScenePainter",
      "ai_summary": "ScenePainter framework uses a hierarchical graph structure to ensure semantically consistent 3D scene generation by addressing the semantic drift problem in successive view expansion.",
      "ai_keywords": [
        "outpainting",
        "semantic drift",
        "ScenePainter",
        "SceneConceptGraph",
        "hierarchical graph structure"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-25T04:21:12.000Z",
    "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with\n  Concept Relation Alignment",
    "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "676c04f44464f476aaa53d1c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/k488J1893F3JGwEMvaeuh.png",
      "fullname": "Chong Xia",
      "name": "xiac24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.21035",
      "authors": [
        {
          "_id": "68886a70af872d625c10c6b5",
          "user": {
            "_id": "67e894cc0afe9169e8bac4aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e894cc0afe9169e8bac4aa/pnGZMwKlil9dpOb91cbEs.jpeg",
            "isPro": false,
            "fullname": "Haoyang Liu",
            "user": "Liu-Hy",
            "type": "user"
          },
          "name": "Haoyang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:07:27.474Z",
          "hidden": false
        },
        {
          "_id": "68886a70af872d625c10c6b6",
          "name": "Yijiang Li",
          "hidden": false
        },
        {
          "_id": "68886a70af872d625c10c6b7",
          "name": "Haohan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:55:08.000Z",
      "submittedOnDailyAt": "2025-07-29T05:07:21.058Z",
      "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
      "submittedOnDailyBy": {
        "_id": "67e894cc0afe9169e8bac4aa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e894cc0afe9169e8bac4aa/pnGZMwKlil9dpOb91cbEs.jpeg",
        "isPro": false,
        "fullname": "Haoyang Liu",
        "user": "Liu-Hy",
        "type": "user"
      },
      "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
      "upvotes": 1,
      "discussionId": "68886a71af872d625c10c6b8",
      "githubRepo": "https://github.com/Liu-Hy/GenoMAS",
      "ai_summary": "A system using LLM-based agents enhances gene expression analysis by integrating workflow reliability and autonomous adaptability to improve preprocessing and identification accuracy while uncovering biologically meaningful associations.",
      "ai_keywords": [
        "LLM-based scientists",
        "typed message-passing protocols",
        "Action Units",
        "guided-planning framework",
        "Composite Similarity Correlation",
        "F$_1$",
        "gene identification",
        "GenoTEX benchmark",
        "gene-phenotype associations",
        "latent confounders"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-28T13:55:08.000Z",
    "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
    "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21035.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e894cc0afe9169e8bac4aa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e894cc0afe9169e8bac4aa/pnGZMwKlil9dpOb91cbEs.jpeg",
      "fullname": "Haoyang Liu",
      "name": "Liu-Hy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20900",
      "authors": [
        {
          "_id": "68883026af872d625c10c5e1",
          "name": "Yonghyun Kim",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e2",
          "name": "Wayne Chi",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e3",
          "name": "Anastasios N. Angelopoulos",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e4",
          "name": "Wei-Lin Chiang",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e5",
          "name": "Koichi Saito",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e6",
          "name": "Shinji Watanabe",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e7",
          "name": "Yuki Mitsufuji",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e8",
          "user": {
            "_id": "63b1b5f034b91b8272aac6c8",
            "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
            "isPro": false,
            "fullname": "Chris Donahue",
            "user": "chrisdonahue",
            "type": "user"
          },
          "name": "Chris Donahue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T02:34:38.516Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T14:52:57.000Z",
      "submittedOnDailyAt": "2025-07-29T01:01:25.876Z",
      "title": "Music Arena: Live Evaluation for Text-to-Music",
      "submittedOnDailyBy": {
        "_id": "63b1b5f034b91b8272aac6c8",
        "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
        "isPro": false,
        "fullname": "Chris Donahue",
        "user": "chrisdonahue",
        "type": "user"
      },
      "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
      "upvotes": 1,
      "discussionId": "68883027af872d625c10c5e9",
      "ai_summary": "Music Arena provides a scalable, interactive platform for evaluating text-to-music models through user-generated preferences and detailed feedback.",
      "ai_keywords": [
        "LLM-based routing system",
        "text-to-music models",
        "live evaluation",
        "heterogeneous type signatures",
        "detailed preferences",
        "listening data",
        "natural language feedback",
        "rolling data release policy",
        "user privacy guarantees",
        "standardized evaluation protocol",
        "transparent data access policies"
      ]
    },
    "publishedAt": "2025-07-28T10:52:57.000Z",
    "title": "Music Arena: Live Evaluation for Text-to-Music",
    "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b1b5f034b91b8272aac6c8",
      "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
      "fullname": "Chris Donahue",
      "name": "chrisdonahue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20880",
      "authors": [
        {
          "_id": "688826c8af872d625c10c5a1",
          "name": "Renhang Liu",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a2",
          "user": {
            "_id": "62864cdf011a13fe5f84d746",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62864cdf011a13fe5f84d746/5yPYgAtGJTaUq4fHhh9Pr.jpeg",
            "isPro": false,
            "fullname": "Hung Chia Yu",
            "user": "hungchiayu",
            "type": "user"
          },
          "name": "Chia-Yu Hung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T07:08:59.724Z",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a3",
          "name": "Navonil Majumder",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a4",
          "name": "Taylor Gautreaux",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a5",
          "name": "Amir Ali Bagherzadeh",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a6",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a7",
          "name": "Dorien Herremans",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a8",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T14:34:02.000Z",
      "submittedOnDailyAt": "2025-07-29T00:39:10.042Z",
      "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability\n  and Aesthetic Alignment",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
      "upvotes": 1,
      "discussionId": "688826c8af872d625c10c5a9",
      "projectPage": "https://declare-lab.github.io/jamify",
      "githubRepo": "https://github.com/declare-lab/jamify",
      "ai_summary": "A flow-matching-based model enhances lyrics-to-song generation by providing word-level control over vocal timing and duration, improving quality through aesthetic alignment and surpassing current models in music-specific attributes.",
      "ai_keywords": [
        "diffusion models",
        "flow-matching models",
        "text-to-audio generation",
        "lyrics-to-song models",
        "DiffRhythm",
        "ACE-Step",
        "LeVo",
        "word-level timing",
        "duration control",
        "fine-grained vocal control",
        "Direct Preference Optimization",
        "aesthetic alignment",
        "public evaluation dataset",
        "JAME"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-07-28T10:34:02.000Z",
    "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability\n  and Aesthetic Alignment",
    "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20880.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  }
]