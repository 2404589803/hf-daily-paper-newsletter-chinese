[
    {
        "paper": {
            "id": "2407.04842",
            "authors": [
                {
                    "_id": "668cbe15bddba3c354e0e7bc",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d448d3d095077728f7c740/4CFkiVoXXDsu7uQPTU0n3.jpeg",
                        "isPro": false,
                        "fullname": "Zhaorun Chen",
                        "user": "Zhaorun",
                        "type": "user"
                    },
                    "name": "Zhaorun Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T07:41:52.006Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7bd",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e1d3451e5a4f34b7a728ef/kHE5JHF9iZJOG0uBkvJr-.jpeg",
                        "isPro": false,
                        "fullname": "yichaodu",
                        "user": "yichaodu",
                        "type": "user"
                    },
                    "name": "Yichao Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T07:41:59.325Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7be",
                    "user": {
                        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                        "isPro": false,
                        "fullname": "Zichen Wen",
                        "user": "zichenwen",
                        "type": "user"
                    },
                    "name": "Zichen Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:34:41.704Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7bf",
                    "user": {
                        "avatarUrl": "/avatars/f3c5560d500c699e452986a6a45ba3ee.svg",
                        "isPro": false,
                        "fullname": "Yiyang Zhou",
                        "user": "YiyangAiLab",
                        "type": "user"
                    },
                    "name": "Yiyang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:35:14.296Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c0",
                    "user": {
                        "avatarUrl": "/avatars/baefd153c59f3a3238fd6034c6daf0d4.svg",
                        "isPro": false,
                        "fullname": "ChenhangCui",
                        "user": "Chenhangcui",
                        "type": "user"
                    },
                    "name": "Chenhang Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:35:28.734Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c1",
                    "user": {
                        "avatarUrl": "/avatars/579d083fdbfba7331b07941f0bef83a4.svg",
                        "isPro": false,
                        "fullname": "Zhenzhen Weng",
                        "user": "zzweng",
                        "type": "user"
                    },
                    "name": "Zhenzhen Weng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:35:35.672Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c2",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Haoqin Tu",
                        "user": "PahaII",
                        "type": "user"
                    },
                    "name": "Haoqin Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:35:41.704Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c3",
                    "user": {
                        "avatarUrl": "/avatars/3c33cf7bac9fab07aa200e553eb832e9.svg",
                        "isPro": false,
                        "fullname": "Chaoqi Wang",
                        "user": "alecwangcq",
                        "type": "user"
                    },
                    "name": "Chaoqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:35:48.911Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c4",
                    "name": "Zhengwei Tong",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c5",
                    "user": {
                        "avatarUrl": "/avatars/0fd23bc7db683a18cac8d2cc52131840.svg",
                        "isPro": false,
                        "fullname": "hql",
                        "user": "huangqinglan",
                        "type": "user"
                    },
                    "name": "Qinglan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:36:09.433Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c6",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483af58571c2dcfa98cae82/d8Vnh_EG1KxeEoNAAQXHZ.jpeg",
                        "isPro": false,
                        "fullname": "Canyu Chen",
                        "user": "canyuchen",
                        "type": "user"
                    },
                    "name": "Canyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T07:41:55.281Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c7",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/0lncTpIHXn6suB0p-oSma.jpeg",
                        "isPro": false,
                        "fullname": "QinghaoYe",
                        "user": "MAGAer13",
                        "type": "user"
                    },
                    "name": "Qinghao Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T14:05:11.232Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c8",
                    "user": {
                        "avatarUrl": "/avatars/f862e7f98cfc3ffb820309ffe7bc73d3.svg",
                        "isPro": false,
                        "fullname": "zhihong zhu",
                        "user": "PKUGGBond",
                        "type": "user"
                    },
                    "name": "Zhihong Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:36:25.102Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7c9",
                    "name": "Yuqing Zhang",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7ca",
                    "name": "Jiawei Zhou",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7cb",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63af25605fe9db73f67a0fb7/mcQHbTdJVxi2GdHzRB9ad.jpeg",
                        "isPro": false,
                        "fullname": "Zhuokai Zhao",
                        "user": "zhuokai",
                        "type": "user"
                    },
                    "name": "Zhuokai Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:37:03.316Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7cc",
                    "user": {
                        "avatarUrl": "/avatars/df3a52bd4bc374d7cb4061f449f75c07.svg",
                        "isPro": false,
                        "fullname": "Rafael Rafailov",
                        "user": "rmrafailov",
                        "type": "user"
                    },
                    "name": "Rafael Rafailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:37:13.962Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7cd",
                    "user": {
                        "avatarUrl": "/avatars/fcac4912678ad3cb6e817d40bdee9aea.svg",
                        "isPro": false,
                        "fullname": "Chelsea Finn",
                        "user": "cbfinn",
                        "type": "user"
                    },
                    "name": "Chelsea Finn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:37:19.483Z",
                    "hidden": false
                },
                {
                    "_id": "668cbe15bddba3c354e0e7ce",
                    "user": {
                        "avatarUrl": "/avatars/ed10c2cf2ba3fde6d7da93f076961607.svg",
                        "isPro": false,
                        "fullname": "Yao",
                        "user": "Huaxiu",
                        "type": "user"
                    },
                    "name": "Huaxiu Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:37:27.914Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T20:03:16.000Z",
            "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for\n  Text-to-Image Generation?",
            "summary": "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench.",
            "upvotes": 27
        },
        "publishedAt": "2024-07-09T06:57:50.769Z",
        "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04842.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e1d3451e5a4f34b7a728ef/kHE5JHF9iZJOG0uBkvJr-.jpeg",
            "fullname": "yichaodu",
            "name": "yichaodu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.05975",
            "authors": [
                {
                    "_id": "668cab02a249df5a139d0ec1",
                    "name": "Yinquan Lu",
                    "hidden": false
                },
                {
                    "_id": "668cab02a249df5a139d0ec2",
                    "user": {
                        "avatarUrl": "/avatars/b444240770d4025dea41871cf38126dc.svg",
                        "isPro": false,
                        "fullname": "Wenhao Zhu",
                        "user": "Wenhao97",
                        "type": "user"
                    },
                    "name": "Wenhao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T09:18:40.502Z",
                    "hidden": false
                },
                {
                    "_id": "668cab02a249df5a139d0ec3",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "668cab02a249df5a139d0ec4",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "668cab02a249df5a139d0ec5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
                        "isPro": false,
                        "fullname": "FeiYuan",
                        "user": "FeYuan",
                        "type": "user"
                    },
                    "name": "Fei Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:27:51.394Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T14:18:28.000Z",
            "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation\n  Capabilities Beyond 100 Languages",
            "summary": "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\url{https://github.com/CONE-MT/LLaMAX/.} and\nmodels~\\url{https://huggingface.co/LLaMAX/.} are publicly available.",
            "upvotes": 22
        },
        "publishedAt": "2024-07-09T01:59:21.178Z",
        "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05975.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
            "fullname": "FeiYuan",
            "name": "FeYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04841",
            "authors": [
                {
                    "_id": "668cee02f8e6657dff5baeb7",
                    "name": "Ivan Rodkin",
                    "hidden": false
                },
                {
                    "_id": "668cee02f8e6657dff5baeb8",
                    "user": {
                        "avatarUrl": "/avatars/e4a9394057dc0813880e4a567293f34f.svg",
                        "isPro": false,
                        "fullname": "Yury Kuratov",
                        "user": "yurakuratov",
                        "type": "user"
                    },
                    "name": "Yuri Kuratov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T08:17:45.419Z",
                    "hidden": false
                },
                {
                    "_id": "668cee02f8e6657dff5baeb9",
                    "name": "Aydar Bulatov",
                    "hidden": false
                },
                {
                    "_id": "668cee02f8e6657dff5baeba",
                    "name": "Mikhail Burtsev",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T19:57:49.000Z",
            "title": "Associative Recurrent Memory Transformer",
            "summary": "This paper addresses the challenge of creating a neural architecture for very\nlong sequences that requires constant time for processing new information at\neach time step. Our approach, Associative Recurrent Memory Transformer (ARMT),\nis based on transformer self-attention for local context and segment-level\nrecurrence for storage of task specific information distributed over a long\ncontext. We demonstrate that ARMT outperfors existing alternatives in\nassociative retrieval tasks and sets a new performance record in the recent\nBABILong multi-task long-context benchmark by answering single-fact questions\nover 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.",
            "upvotes": 12
        },
        "publishedAt": "2024-07-09T13:35:29.960Z",
        "title": "Associative Recurrent Memory Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04841.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "fullname": "MIKHAIL BURTSEV",
            "name": "mbur",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.06135",
            "authors": [
                {
                    "_id": "668cf5f763d1bc005ba9d481",
                    "user": {
                        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
                        "isPro": false,
                        "fullname": "Ethan Chern",
                        "user": "ethanchern",
                        "type": "user"
                    },
                    "name": "Ethan Chern",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T09:18:31.173Z",
                    "hidden": false
                },
                {
                    "_id": "668cf5f763d1bc005ba9d482",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65437c82d3db5d5336af93ff/YDfpywk15IigQwcAoMu5n.jpeg",
                        "isPro": false,
                        "fullname": "Jiadi Su",
                        "user": "JoyBoy-Su",
                        "type": "user"
                    },
                    "name": "Jiadi Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T09:34:27.601Z",
                    "hidden": false
                },
                {
                    "_id": "668cf5f763d1bc005ba9d483",
                    "name": "Yan Ma",
                    "hidden": false
                },
                {
                    "_id": "668cf5f763d1bc005ba9d484",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
                        "isPro": true,
                        "fullname": "Pengfei Liu",
                        "user": "Pengfei",
                        "type": "user"
                    },
                    "name": "Pengfei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T09:35:09.039Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T17:08:02.000Z",
            "title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation",
            "summary": "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.",
            "upvotes": 11
        },
        "publishedAt": "2024-07-09T07:09:14.626Z",
        "title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/JFoSoQzAsUvdu5SYvgdWN.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06135.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "fullname": "Ethan Chern",
            "name": "ethanchern",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.05700",
            "authors": [
                {
                    "_id": "668c9c719f4aead59cf00fef",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e42be971139393aa3dd90a/N7CwgOiU6rx5FTIfLO1Yo.png",
                        "isPro": false,
                        "fullname": "Yutong Wu",
                        "user": "wyt2000",
                        "type": "user"
                    },
                    "name": "Yutong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T07:42:06.880Z",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff0",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff1",
                    "name": "Wenxuan Shi",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff2",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff3",
                    "user": {
                        "avatarUrl": "/avatars/ff9131485c3797671a38ed45af9de807.svg",
                        "isPro": false,
                        "fullname": "Lingzhe Gao",
                        "user": "youlitaiglz",
                        "type": "user"
                    },
                    "name": "Lingzhe Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:39:58.699Z",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff4",
                    "name": "Shihao Liu",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff5",
                    "name": "Ziyuan Nan",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff6",
                    "name": "Kaizhao Yuan",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff7",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff8",
                    "name": "Xishan Zhang",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ff9",
                    "name": "Zidong Du",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ffa",
                    "name": "Qi Guo",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ffb",
                    "name": "Yewen Pu",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ffc",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ffd",
                    "name": "Xing Hu",
                    "hidden": false
                },
                {
                    "_id": "668c9c719f4aead59cf00ffe",
                    "name": "Yunji Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T08:00:05.000Z",
            "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with\n  Inverse-Instruct",
            "summary": "Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation.",
            "upvotes": 4
        },
        "publishedAt": "2024-07-09T06:36:42.575Z",
        "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05700.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e42be971139393aa3dd90a/N7CwgOiU6rx5FTIfLO1Yo.png",
            "fullname": "Yutong Wu",
            "name": "wyt2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.06191",
            "authors": [
                {
                    "_id": "668cb6cc61b6eff5a82aeba2",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c129c155686a357c99ea0b/1iy8_aLVU4FflO-d1fozO.jpeg",
                        "isPro": true,
                        "fullname": "Qi Zhangyang (Alex Severns)",
                        "user": "alexzyqi",
                        "type": "user"
                    },
                    "name": "Zhangyang Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:05:53.230Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba3",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
                        "isPro": false,
                        "fullname": "Yunhan Yang",
                        "user": "yhyang-myron",
                        "type": "user"
                    },
                    "name": "Yunhan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:06:02.450Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba4",
                    "user": {
                        "avatarUrl": "/avatars/afb532f8aa9396463881416d2f761891.svg",
                        "isPro": false,
                        "fullname": "Zhang Mengchen",
                        "user": "Dubhe-zmc",
                        "type": "user"
                    },
                    "name": "Mengchen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:06:16.585Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba5",
                    "name": "Long Xing",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba6",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoyang Wu",
                        "user": "Gofinge",
                        "type": "user"
                    },
                    "name": "Xiaoyang Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:09:24.638Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba7",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba8",
                    "user": {
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:08:43.006Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aeba9",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aebaa",
                    "user": {
                        "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
                        "isPro": false,
                        "fullname": "JIaqi",
                        "user": "Jiaqiwang",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:07:59.442Z",
                    "hidden": false
                },
                {
                    "_id": "668cb6cc61b6eff5a82aebab",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T17:59:55.000Z",
            "title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side\n  Images",
            "summary": "Recent advances in 3D AIGC have shown promise in directly creating 3D objects\nfrom text and images, offering significant cost savings in animation and\nproduct design. However, detailed edit and customization of 3D assets remains a\nlong-standing challenge. Specifically, 3D Generation methods lack the ability\nto follow finely detailed instructions as precisely as their 2D image creation\ncounterparts. Imagine you can get a toy through 3D AIGC but with undesired\naccessories and dressing. To tackle this challenge, we propose a novel pipeline\ncalled Tailor3D, which swiftly creates customized 3D assets from editable\ndual-side images. We aim to emulate a tailor's ability to locally change\nobjects or perform overall style transfer. Unlike creating 3D assets from\nmultiple views, using dual-side images eliminates conflicts on overlapping\nareas that occur when editing individual views. Specifically, it begins by\nediting the front view, then generates the back view of the object through\nmulti-view diffusion. Afterward, it proceeds to edit the back views. Finally, a\nDual-sided LRM is proposed to seamlessly stitch together the front and back 3D\nfeatures, akin to a tailor sewing together the front and back of a garment. The\nDual-sided LRM rectifies imperfect consistencies between the front and back\nviews, enhancing editing capabilities and reducing memory burdens while\nseamlessly integrating them into a unified 3D representation with the LoRA\nTriplane Transformer. Experimental results demonstrate Tailor3D's effectiveness\nacross various 3D generation and editing tasks, including 3D generative fill\nand style transfer. It provides a user-friendly, efficient solution for editing\n3D assets, with each editing step taking only seconds to complete.",
            "upvotes": 4
        },
        "publishedAt": "2024-07-09T04:38:03.983Z",
        "title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/UABm3HnHCutmBzNcG5lox.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06191.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.05463",
            "authors": [
                {
                    "_id": "668d62cdc04604b679487253",
                    "name": "Jiaxin Ge",
                    "hidden": false
                },
                {
                    "_id": "668d62cdc04604b679487254",
                    "name": "Xueying Jia",
                    "hidden": false
                },
                {
                    "_id": "668d62cdc04604b679487255",
                    "name": "Vijay Viswanathan",
                    "hidden": false
                },
                {
                    "_id": "668d62cdc04604b679487256",
                    "name": "Hongyin Luo",
                    "hidden": false
                },
                {
                    "_id": "668d62cdc04604b679487257",
                    "name": "Graham Neubig",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-07T18:27:59.000Z",
            "title": "Training Task Experts through Retrieval Based Distillation",
            "summary": "One of the most reliable ways to create deployable models for specialized\ntasks is to obtain an adequate amount of high-quality task-specific data.\nHowever, for specialized tasks, often such datasets do not exist. Existing\nmethods address this by creating such data from large language models (LLMs)\nand then distilling such knowledge into smaller models. However, these methods\nare limited by the quality of the LLMs output, and tend to generate repetitive\nor incorrect data. In this work, we present Retrieval Based Distillation\n(ReBase), a method that first retrieves data from rich online sources and then\ntransforms them into domain-specific data. This method greatly enhances data\ndiversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills\nthe reasoning capacity of LLMs. We test our method on 4 benchmarks and results\nshow that our method significantly improves performance by up to 7.8% on SQuAD,\n1.37% on MNLI, and 1.94% on BigBench-Hard.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-09T14:53:13.025Z",
        "title": "Training Task Experts through Retrieval Based Distillation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bc495016897fa76cdcda18/bIcuGH9KT2Qn70sTv5noR.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/62bc495016897fa76cdcda18/TehbOz5Dp08Mvu6xT9FzY.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05463.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656506661693-noauth.jpeg",
            "fullname": "Hongyin Luo",
            "name": "luohy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.06027",
            "authors": [
                {
                    "_id": "668d0d05781c9e63c563d46e",
                    "name": "Miao Zheng",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d46f",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d470",
                    "user": {
                        "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
                        "isPro": false,
                        "fullname": "FanYang",
                        "user": "fairyang",
                        "type": "user"
                    },
                    "name": "Fan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T14:05:12.708Z",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d471",
                    "name": "Haoze Sun",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d472",
                    "name": "Tianpeng Li",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d473",
                    "name": "Lingchu Xiong",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d474",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d475",
                    "name": "Yozhen Wu",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d476",
                    "name": "Kun Li",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d477",
                    "name": "Yanjun Sheng",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d478",
                    "name": "Mingan Lin",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d479",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47a",
                    "user": {
                        "avatarUrl": "/avatars/4e198c35a9f09bbc6551c34148aaf560.svg",
                        "isPro": false,
                        "fullname": "guosheng dong",
                        "user": "dongguosheng",
                        "type": "user"
                    },
                    "name": "Guosheng Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T14:05:04.202Z",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47b",
                    "name": "Yujing Qiao",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47c",
                    "name": "Kun Fang",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47d",
                    "name": "Weipeng Chen",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47e",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d47f",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "668d0d05781c9e63c563d480",
                    "name": "Zenan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T15:25:33.000Z",
            "title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
            "summary": "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-09T08:44:11.526Z",
        "title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06027.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
            "fullname": "FanYang",
            "name": "fairyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.05282",
            "authors": [
                {
                    "_id": "668d5fd3ed63008dfaad9325",
                    "name": "Haozhe Zhao",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad9326",
                    "name": "Xiaojian Ma",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad9327",
                    "name": "Liang Chen",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad9328",
                    "name": "Shuzheng Si",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad9329",
                    "name": "Rujie Wu",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad932a",
                    "name": "Kaikai An",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad932b",
                    "name": "Peiyu Yu",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad932c",
                    "name": "Minjia Zhang",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad932d",
                    "name": "Qing Li",
                    "hidden": false
                },
                {
                    "_id": "668d5fd3ed63008dfaad932e",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-07T06:50:22.000Z",
            "title": "UltraEdit: Instruction-based Fine-Grained Image Editing at Scale",
            "summary": "This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-09T14:35:46.086Z",
        "title": "UltraEdit: Instruction-based Fine-Grained Image Editing at Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05282.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04604",
            "authors": [
                {
                    "_id": "668bec5cb37c6ac0b09d9657",
                    "user": {
                        "avatarUrl": "/avatars/b6210993b51bb650ea8aa8c04e5fc351.svg",
                        "isPro": false,
                        "fullname": "kamwoh",
                        "user": "kamwoh",
                        "type": "user"
                    },
                    "name": "Kam Woh Ng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:35:36.484Z",
                    "hidden": false
                },
                {
                    "_id": "668bec5cb37c6ac0b09d9658",
                    "user": {
                        "avatarUrl": "/avatars/dc23e15acf7ee7f3b9f1a68c2716a66d.svg",
                        "isPro": false,
                        "fullname": "Xiatian Zhu",
                        "user": "Xiatian-Zhu",
                        "type": "user"
                    },
                    "name": "Xiatian Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:35:42.899Z",
                    "hidden": false
                },
                {
                    "_id": "668bec5cb37c6ac0b09d9659",
                    "user": {
                        "avatarUrl": "/avatars/00264325fdf277e290aac5a1206ba73f.svg",
                        "isPro": false,
                        "fullname": "YIZHE SONG",
                        "user": "JackeySong",
                        "type": "user"
                    },
                    "name": "Yi-Zhe Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:35:56.664Z",
                    "hidden": false
                },
                {
                    "_id": "668bec5cb37c6ac0b09d965a",
                    "name": "Tao Xiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T15:53:04.000Z",
            "title": "PartCraft: Crafting Creative Objects by Parts",
            "summary": "This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-09T07:12:03.684Z",
        "title": "PartCraft: Crafting Creative Objects by Parts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04604.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b6210993b51bb650ea8aa8c04e5fc351.svg",
            "fullname": "kamwoh",
            "name": "kamwoh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.06192",
            "authors": [
                {
                    "_id": "668d5680f4ab25acfedd9bea",
                    "name": "Xuweiyi Chen",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9beb",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bec",
                    "name": "Xuejun Zhang",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bed",
                    "name": "Sihan Xu",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bee",
                    "name": "Shengyi Qian",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bef",
                    "name": "Jianing Yang",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bf0",
                    "name": "David F. Fouhey",
                    "hidden": false
                },
                {
                    "_id": "668d5680f4ab25acfedd9bf1",
                    "name": "Joyce Chai",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-08T17:59:57.000Z",
            "title": "Multi-Object Hallucination in Vision-Language Models",
            "summary": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-09T13:56:40.159Z",
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06192.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/LsDQPftQrnp_Gvanzim4J.jpeg",
            "fullname": "Jed Yang",
            "name": "jedyang97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.06182",
            "authors": [
                {
                    "_id": "668c9d937dbe0ab21d9a65f3",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "668c9d937dbe0ab21d9a65f4",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-10T16:27:47.000Z",
            "title": "Compositional Video Generation as Flow Equalization",
            "summary": "Large-scale Text-to-Video (T2V) diffusion models have recently demonstrated\nunprecedented capability to transform natural language descriptions into\nstunning and photorealistic videos. Despite the promising results, a\nsignificant challenge remains: these models struggle to fully grasp complex\ncompositional interactions between multiple concepts and actions. This issue\narises when some words dominantly influence the final video, overshadowing\nother concepts.To tackle this problem, we introduce Vico, a generic\nframework for compositional video generation that explicitly ensures all\nconcepts are represented properly. At its core, Vico analyzes how input tokens\ninfluence the generated video, and adjusts the model to prevent any single\nconcept from dominating. Specifically, Vico extracts attention weights from all\nlayers to build a spatial-temporal attention graph, and then estimates the\ninfluence as the max-flow from the source text token to the video target\ntoken. Although the direct computation of attention flow in diffusion models is\ntypically infeasible, we devise an efficient approximation based on subgraph\nflows and employ a fast and vectorized implementation, which in turn makes the\nflow computation manageable and differentiable. By updating the noisy latent to\nbalance these flows, Vico captures complex interactions and consequently\nproduces videos that closely adhere to textual descriptions. We apply our\nmethod to multiple diffusion-based video models for compositional T2V and video\nediting. Empirical results demonstrate that our framework significantly\nenhances the compositional richness and accuracy of the generated videos. Visit\nour website\nat~https://adamdad.github.io/vico/{https://adamdad.github.io/vico/}.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-09T12:50:36.437Z",
        "title": "Compositional Video Generation as Flow Equalization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06182.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04020",
            "authors": [
                {
                    "_id": "668cf1b27f96b83037411b4a",
                    "user": {
                        "avatarUrl": "/avatars/3e3f2886bd4a730ec19b13aecc99279f.svg",
                        "isPro": false,
                        "fullname": "Amy Xin",
                        "user": "amyxx2001",
                        "type": "user"
                    },
                    "name": "Amy Xin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T08:17:42.259Z",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b4b",
                    "user": {
                        "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
                        "isPro": false,
                        "fullname": "Qi Yunjia",
                        "user": "Kikkk",
                        "type": "user"
                    },
                    "name": "Yunjia Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:38:07.751Z",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b4c",
                    "user": {
                        "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
                        "isPro": false,
                        "fullname": "Zijun",
                        "user": "TranSirius",
                        "type": "user"
                    },
                    "name": "Zijun Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T08:17:43.791Z",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b4d",
                    "user": {
                        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
                        "isPro": false,
                        "fullname": "Fangwei Zhu",
                        "user": "soliz1998",
                        "type": "user"
                    },
                    "name": "Fangwei Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:38:13.246Z",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b4e",
                    "name": "Kaisheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b4f",
                    "name": "Xu Bin",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b50",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "668cf1b27f96b83037411b51",
                    "user": {
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T08:39:00.208Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T15:55:13.000Z",
            "title": "LLMAEL: Large Language Models are Good Context Augmenters for Entity\n  Linking",
            "summary": "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-09T06:45:56.632Z",
        "title": "LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04020.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
            "fullname": "Zijun",
            "name": "TranSirius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04693",
            "authors": [
                {
                    "_id": "668cb372e7cc32ad4d3a6837",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                        "isPro": false,
                        "fullname": "yuzhe gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-09T07:42:02.824Z",
                    "hidden": false
                },
                {
                    "_id": "668cb372e7cc32ad4d3a6838",
                    "user": {
                        "avatarUrl": "/avatars/3d2ceb17e02967aea5ef9118d8514136.svg",
                        "isPro": false,
                        "fullname": "Ziwei Ji",
                        "user": "ruguodayu",
                        "type": "user"
                    },
                    "name": "Ziwei Ji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:37:27.463Z",
                    "hidden": false
                },
                {
                    "_id": "668cb372e7cc32ad4d3a6839",
                    "user": {
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:37:33.406Z",
                    "hidden": false
                },
                {
                    "_id": "668cb372e7cc32ad4d3a683a",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "668cb372e7cc32ad4d3a683b",
                    "user": {
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-09T10:37:50.609Z",
                    "hidden": false
                },
                {
                    "_id": "668cb372e7cc32ad4d3a683c",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T17:56:38.000Z",
            "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models",
            "summary": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-09T06:14:54.575Z",
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04693.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "fullname": "yuzhe gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]