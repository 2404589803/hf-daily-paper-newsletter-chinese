[
    {
        "paper": {
            "id": "2410.17856",
            "authors": [
                {
                    "_id": "6719e7d84dfd79aa9f3f6c69",
                    "user": {
                        "_id": "6578459d62d3ac1817ed79fe",
                        "avatarUrl": "/avatars/afaa47a0a15f3216ffee5e90a602cbf9.svg",
                        "isPro": true,
                        "fullname": "Shaofei Cai",
                        "user": "phython96",
                        "type": "user"
                    },
                    "name": "Shaofei Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:25:08.953Z",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6a",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6b",
                    "user": {
                        "_id": "64c232e577655fcf3ff06082",
                        "avatarUrl": "/avatars/8a44d5561e97cf94df7de3973874645c.svg",
                        "isPro": false,
                        "fullname": "Kewei Lian",
                        "user": "kevinLian",
                        "type": "user"
                    },
                    "name": "Kewei Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:24:52.514Z",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6c",
                    "user": {
                        "_id": "648f0c64ddc0620d54e199e9",
                        "avatarUrl": "/avatars/ff3fd6e11d451b03f82183e53fc48613.svg",
                        "isPro": false,
                        "fullname": "ZhancunMu",
                        "user": "Zhancun",
                        "type": "user"
                    },
                    "name": "Zhancun Mu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:17:23.469Z",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6d",
                    "user": {
                        "_id": "60dd0e36a15ddd7d2006d2e9",
                        "avatarUrl": "/avatars/8bd98177a79efbf295be8f6457683297.svg",
                        "isPro": true,
                        "fullname": "Xiaojian Ma",
                        "user": "jeasinema",
                        "type": "user"
                    },
                    "name": "Xiaojian Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:17:12.954Z",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6e",
                    "user": {
                        "_id": "66757955b3882fd587d5f363",
                        "avatarUrl": "/avatars/41fdfa19057ad3ea5ece29b94f163218.svg",
                        "isPro": false,
                        "fullname": "Anji Liu",
                        "user": "anjiliu",
                        "type": "user"
                    },
                    "name": "Anji Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T15:16:10.809Z",
                    "hidden": false
                },
                {
                    "_id": "6719e7d84dfd79aa9f3f6c6f",
                    "name": "Yitao Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-23T13:26:59.000Z",
            "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context\n  Prompting",
            "summary": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. A key issue is the difficulty in smoothly connecting individual\nentities in low-level observations with abstract concepts required for\nplanning. A common approach to address this problem is through the use of\nhierarchical agents, where VLMs serve as high-level reasoners that break down\ntasks into executable sub-tasks, typically specified using language and\nimagined observations. However, language often fails to effectively convey\nspatial information, while generating future images with sufficient accuracy\nremains challenging. To address these limitations, we propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy\nmodels. This protocol leverages object segmentation from both past and present\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, with real-time object tracking\nprovided by SAM-2. Our method unlocks the full potential of VLMs\nvisual-language reasoning abilities, enabling them to solve complex creative\ntasks, especially those heavily reliant on spatial understanding. Experiments\nin Minecraft demonstrate that our approach allows agents to accomplish\npreviously unattainable tasks, highlighting the effectiveness of\nvisual-temporal context prompting in embodied decision-making. Codes and demos\nwill be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
            "upvotes": 30,
            "discussionId": "6719e7db4dfd79aa9f3f6d18"
        },
        "publishedAt": "2024-10-28T02:54:44.530Z",
        "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6578459d62d3ac1817ed79fe/qwPSKAEEaYYo-R3TmnmUr.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17856.png",
        "numComments": 5,
        "submittedBy": {
            "avatarUrl": "/avatars/afaa47a0a15f3216ffee5e90a602cbf9.svg",
            "fullname": "Shaofei Cai",
            "name": "phython96",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2410.16048",
            "authors": [
                {
                    "_id": "671753f39feb1b7d3a5f10c6",
                    "name": "Arnon Turetzky",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10c7",
                    "user": {
                        "_id": "62bedff7304b82a773bf8c1b",
                        "avatarUrl": "/avatars/f9e79dc196caa95c220127c6212e9944.svg",
                        "isPro": false,
                        "fullname": "Nimrod Shabtay",
                        "user": "NimrodShabtay1986",
                        "type": "user"
                    },
                    "name": "Nimrod Shabtay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:25:20.632Z",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10c8",
                    "user": {
                        "_id": "66e9d97456fb59c436a1bbc1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1y4m9Y9RZJ9lrw85pwELm.png",
                        "isPro": false,
                        "fullname": "Slava Shechtman",
                        "user": "slavashe",
                        "type": "user"
                    },
                    "name": "Slava Shechtman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:17:44.202Z",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10c9",
                    "user": {
                        "_id": "643425b4a4c9c55871a7a02b",
                        "avatarUrl": "/avatars/eb2f357888159f5120bbf70a40cb089d.svg",
                        "isPro": false,
                        "fullname": "Hagai Aronowitz",
                        "user": "hagaia",
                        "type": "user"
                    },
                    "name": "Hagai Aronowitz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:17:50.097Z",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10ca",
                    "user": {
                        "_id": "66f59c2051064e3444ff38c1",
                        "avatarUrl": "/avatars/b64fb3ab1910dc8746e243083a3ef549.svg",
                        "isPro": false,
                        "fullname": "David Haws",
                        "user": "dchaws",
                        "type": "user"
                    },
                    "name": "David Haws",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:17:55.258Z",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10cb",
                    "name": "Ron Hoory",
                    "hidden": false
                },
                {
                    "_id": "671753f39feb1b7d3a5f10cc",
                    "user": {
                        "_id": "63b7e09359060ca9f4c4de35",
                        "avatarUrl": "/avatars/3d8e6d46d9af81527e504dbd1324e4a1.svg",
                        "isPro": false,
                        "fullname": "Avihu Dekel",
                        "user": "Avihu",
                        "type": "user"
                    },
                    "name": "Avihu Dekel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-22T07:55:44.222Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-21T14:23:46.000Z",
            "title": "Continuous Speech Synthesis using per-token Latent Diffusion",
            "summary": "The success of autoregressive transformer models with discrete tokens has\ninspired quantization-based approaches for continuous modalities, though these\noften limit reconstruction quality. We therefore introduce SALAD, a per-token\nlatent diffusion model for zero-shot text-to-speech, that operates on\ncontinuous representations. SALAD builds upon the recently proposed expressive\ndiffusion head for image generation, and extends it to generate variable-length\noutputs. Our approach utilizes semantic tokens for providing contextual\ninformation and determining the stopping condition. We suggest three continuous\nvariants for our method, extending popular discrete speech synthesis\ntechniques. Additionally, we implement discrete baselines for each variant and\nconduct a comparative analysis of discrete versus continuous speech modeling\ntechniques. Our results demonstrate that both continuous and discrete\napproaches are highly competent, and that SALAD achieves a superior\nintelligibility score while obtaining speech quality and speaker similarity on\npar with the ground-truth audio.",
            "upvotes": 20,
            "discussionId": "671753f49feb1b7d3a5f1124"
        },
        "publishedAt": "2024-10-28T05:10:35.134Z",
        "title": "Continuous Speech Synthesis using per-token Latent Diffusion",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b7e09359060ca9f4c4de35/pECXdDk07ucMYUwmgCIFR.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16048.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/3d8e6d46d9af81527e504dbd1324e4a1.svg",
            "fullname": "Avihu Dekel",
            "name": "Avihu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.19008",
            "authors": [
                {
                    "_id": "671ee90c77035878c52e67b2",
                    "user": {
                        "_id": "656fd85f039133e875431ed3",
                        "avatarUrl": "/avatars/9dda19397d3ce1266176ce402e5a977f.svg",
                        "isPro": false,
                        "fullname": "Ruoqi Liu",
                        "user": "RuoqiLiu",
                        "type": "user"
                    },
                    "name": "Ruoqi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:18:19.340Z",
                    "hidden": false
                },
                {
                    "_id": "671ee90c77035878c52e67b3",
                    "name": "Yuelin Bai",
                    "hidden": false
                },
                {
                    "_id": "671ee90c77035878c52e67b4",
                    "user": {
                        "_id": "6230d750d93e84e233882dbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Yue",
                        "user": "yuexiang96",
                        "type": "user"
                    },
                    "name": "Xiang Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:18:37.003Z",
                    "hidden": false
                },
                {
                    "_id": "671ee90c77035878c52e67b5",
                    "user": {
                        "_id": "671f96c070f231ff42ceadc6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671f96c070f231ff42ceadc6/lIRWnc9-zItr6QoGs0N3u.jpeg",
                        "isPro": false,
                        "fullname": "Ping Zhang",
                        "user": "aidhlab",
                        "type": "user"
                    },
                    "name": "Ping Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:19:05.468Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-21T20:26:41.000Z",
            "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
            "summary": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for\nassessing cardiac conditions. Existing automatic interpretation methods suffer\nfrom limited generalizability, focusing on a narrow range of cardiac\nconditions, and typically depend on raw physiological signals, which may not be\nreadily available in resource-limited settings where only printed or digital\nECG images are accessible. Recent advancements in multimodal large language\nmodels (MLLMs) present promising opportunities for addressing these challenges.\nHowever, the application of MLLMs to ECG image interpretation remains\nchallenging due to the lack of instruction tuning datasets and well-established\nECG image benchmarks for quantitative evaluation. To address these challenges,\nwe introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset\nof over one million samples, covering a wide range of ECG-related tasks from\ndiverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for\nECG image comprehension. In addition, we curate ECGBench, a new evaluation\nbenchmark covering four key ECG image interpretation tasks across nine\ndifferent datasets. Our experiments show that PULSE sets a new\nstate-of-the-art, outperforming general MLLMs with an average accuracy\nimprovement of 15% to 30%. This work highlights the potential of PULSE to\nenhance ECG interpretation in clinical practice.",
            "upvotes": 16,
            "discussionId": "671ee90e77035878c52e681e"
        },
        "publishedAt": "2024-10-28T00:09:51.184Z",
        "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19008.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
            "fullname": "Xiang Yue",
            "name": "yuexiang96",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 24
        }
    },
    {
        "paper": {
            "id": "2410.19355",
            "authors": [
                {
                    "_id": "671f085f943be9e8dfcd3e6b",
                    "user": {
                        "_id": "63243004dedcbcf92830e76b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63243004dedcbcf92830e76b/Ftv8UFhHQFMi3utTCoDVF.png",
                        "isPro": false,
                        "fullname": "Zhengyao Lu",
                        "user": "zlu",
                        "type": "user"
                    },
                    "name": "Zhengyao Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:22:46.709Z",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e6c",
                    "user": {
                        "_id": "635f8ed47c05eb9f59963d3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                        "isPro": false,
                        "fullname": "ChenyangSi",
                        "user": "ChenyangSi",
                        "type": "user"
                    },
                    "name": "Chenyang Si",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:24:46.068Z",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e6d",
                    "user": {
                        "_id": "653a88011f981e43044ccd7d",
                        "avatarUrl": "/avatars/58c24cc39091f4e2180f59ab9056f868.svg",
                        "isPro": false,
                        "fullname": "Junhao",
                        "user": "JunhaoSong",
                        "type": "user"
                    },
                    "name": "Junhao Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:24:38.601Z",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e6e",
                    "name": "Zhenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e6f",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e70",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:23:53.295Z",
                    "hidden": false
                },
                {
                    "_id": "671f085f943be9e8dfcd3e71",
                    "name": "Kwan-Yee K. Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-25T07:24:38.000Z",
            "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
            "summary": "In this paper, we present \\textit{FasterCache}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67times\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
            "upvotes": 11,
            "discussionId": "671f0862943be9e8dfcd3f7e"
        },
        "publishedAt": "2024-10-28T02:14:39.137Z",
        "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19355.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 4984
        }
    },
    {
        "paper": {
            "id": "2410.18558",
            "authors": [
                {
                    "_id": "671afdd1ffcdeeb655cbe21b",
                    "user": {
                        "_id": "642e72cec1b0f8e4e76af16d",
                        "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
                        "isPro": false,
                        "fullname": "shuhao gu",
                        "user": "gsh33",
                        "type": "user"
                    },
                    "name": "Shuhao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:19:19.039Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe21c",
                    "user": {
                        "_id": "640ed5f1b0ee289c857d464c",
                        "avatarUrl": "/avatars/2517a793cabb2d4c24962c0409c63f6e.svg",
                        "isPro": false,
                        "fullname": "Jialing Zhang",
                        "user": "jlzhang",
                        "type": "user"
                    },
                    "name": "Jialing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:19:25.379Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe21d",
                    "name": "Siyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe21e",
                    "name": "Kevin Yu",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe21f",
                    "user": {
                        "_id": "632c2100ea6e62428ab201e9",
                        "avatarUrl": "/avatars/5bd355e095af93261928198e3d6d5696.svg",
                        "isPro": false,
                        "fullname": "Xingzhaohu",
                        "user": "xingzhaohu",
                        "type": "user"
                    },
                    "name": "Zhaohu Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:20:13.818Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe220",
                    "user": {
                        "_id": "63a11ce02fabbbb899a01d58",
                        "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
                        "isPro": false,
                        "fullname": "ldwang",
                        "user": "ldwang",
                        "type": "user"
                    },
                    "name": "Liangdong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-25T09:30:05.708Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe221",
                    "name": "Zhou Cao",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe222",
                    "user": {
                        "_id": "6695dc41b91490da0249797e",
                        "avatarUrl": "/avatars/59df94f34fb4967e39b8237aac669546.svg",
                        "isPro": false,
                        "fullname": "Jintao Jia",
                        "user": "jjtnb",
                        "type": "user"
                    },
                    "name": "Jintao Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:20:22.639Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe223",
                    "name": "Zhuoyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe224",
                    "user": {
                        "_id": "66a0a5721bb371390c70f5a5",
                        "avatarUrl": "/avatars/9403480ef1277ac789ad2d375f398113.svg",
                        "isPro": false,
                        "fullname": "Grace Wang",
                        "user": "Gracecc",
                        "type": "user"
                    },
                    "name": "Yixuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T16:55:00.224Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe225",
                    "user": {
                        "_id": "669600f8cebc15b84aff0337",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N755P9kTEM4Vu_UUY_lAU.jpeg",
                        "isPro": false,
                        "fullname": "胡振崇",
                        "user": "zhenchonghu",
                        "type": "user"
                    },
                    "name": "Zhenchong Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:21:14.573Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe226",
                    "user": {
                        "_id": "6335113375bed9932474315e",
                        "avatarUrl": "/avatars/aa1837a3a07514b89c48de888934a7b2.svg",
                        "isPro": false,
                        "fullname": "bowenzhang",
                        "user": "bowen92",
                        "type": "user"
                    },
                    "name": "Bo-Wen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:04:53.176Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe227",
                    "user": {
                        "_id": "65e00e64092e556a9a7dd429",
                        "avatarUrl": "/avatars/57959174ef1b0a9dbb14e53d4aea58a8.svg",
                        "isPro": false,
                        "fullname": "lijijie",
                        "user": "Avoczh",
                        "type": "user"
                    },
                    "name": "Jijie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:21:27.092Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe228",
                    "name": "Dong Liang",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe229",
                    "name": "Yingli Zhao",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe22a",
                    "user": {
                        "_id": "64f3eb4fd41e83d74a0173ec",
                        "avatarUrl": "/avatars/461f804bb5a7c5b31f862cfa27e52de1.svg",
                        "isPro": false,
                        "fullname": "Yulong Ao",
                        "user": "aoyulong",
                        "type": "user"
                    },
                    "name": "Yulong Ao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:22:10.950Z",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe22b",
                    "name": "Yaoqi Liu",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe22c",
                    "name": "Fangxiang Feng",
                    "hidden": false
                },
                {
                    "_id": "671afdd1ffcdeeb655cbe22d",
                    "user": {
                        "_id": "632c234f42c386ebd2710434",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
                        "isPro": false,
                        "fullname": "Guang Liu",
                        "user": "ZacLiu",
                        "type": "user"
                    },
                    "name": "Guang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:22:42.703Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-24T09:03:48.000Z",
            "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and\n  High-Quality Instruction Data",
            "summary": "Vision-Language Models (VLMs) have recently made significant progress, but\nthe limited scale and quality of open-source instruction data hinder their\nperformance compared to closed-source models. In this work, we address this\nlimitation by introducing Infinity-MM, a large-scale multimodal instruction\ndataset with 40 million samples, enhanced through rigorous quality filtering\nand deduplication. We also propose a synthetic instruction generation method\nbased on open-source VLMs, using detailed image annotations and diverse\nquestion generation. Using this data, we trained a 2-billion-parameter VLM,\nAquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of\nsimilar scale. This demonstrates that expanding instruction data and generating\nsynthetic data can significantly improve the performance of open-source models.",
            "upvotes": 11,
            "discussionId": "671afdd2ffcdeeb655cbe281"
        },
        "publishedAt": "2024-10-28T00:50:14.002Z",
        "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18558.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
            "fullname": "ldwang",
            "name": "ldwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2410.18889",
            "authors": [
                {
                    "_id": "671f8971915aa3d2d1691b9b",
                    "user": {
                        "_id": "663511a0efbee9a1fde43940",
                        "avatarUrl": "/avatars/ad124ac100829359f67052c730e0a0d4.svg",
                        "isPro": false,
                        "fullname": "Omer Nahum",
                        "user": "omer6nahum",
                        "type": "user"
                    },
                    "name": "Omer Nahum",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:37:31.730Z",
                    "hidden": false
                },
                {
                    "_id": "671f8971915aa3d2d1691b9c",
                    "user": {
                        "_id": "62d6a0c18faee0ac953c51fa",
                        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
                        "isPro": false,
                        "fullname": "Nitay Calderon",
                        "user": "nitay",
                        "type": "user"
                    },
                    "name": "Nitay Calderon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:37:37.337Z",
                    "hidden": false
                },
                {
                    "_id": "671f8971915aa3d2d1691b9d",
                    "user": {
                        "_id": "66f00fe216bffdccd51832e2",
                        "avatarUrl": "/avatars/6d2330d2353ed40cbff6bd4a71fb7865.svg",
                        "isPro": false,
                        "fullname": "Orgad Keller",
                        "user": "orgad",
                        "type": "user"
                    },
                    "name": "Orgad Keller",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:37:42.433Z",
                    "hidden": false
                },
                {
                    "_id": "671f8971915aa3d2d1691b9e",
                    "name": "Idan Szpektor",
                    "hidden": false
                },
                {
                    "_id": "671f8971915aa3d2d1691b9f",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-24T16:27:03.000Z",
            "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance",
            "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance.",
            "upvotes": 9,
            "discussionId": "671f8972915aa3d2d1691c21"
        },
        "publishedAt": "2024-10-28T11:55:06.161Z",
        "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/663511a0efbee9a1fde43940/ArpJqGcPwxXG6ktuok6X8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18889.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ad124ac100829359f67052c730e0a0d4.svg",
            "fullname": "Omer Nahum",
            "name": "omer6nahum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.19168",
            "authors": [
                {
                    "_id": "671ef25002737c628d38c1a3",
                    "name": "S Sakshi",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a4",
                    "name": "Utkarsh Tyagi",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a5",
                    "user": {
                        "_id": "65203f31d6dec04046139874",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65203f31d6dec04046139874/1LDWW4KblYeIDve9F_kUA.png",
                        "isPro": false,
                        "fullname": "Sonal Kumar",
                        "user": "sonalkum",
                        "type": "user"
                    },
                    "name": "Sonal Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T15:40:32.700Z",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a6",
                    "name": "Ashish Seth",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a7",
                    "name": "Ramaneswaran Selvakumar",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a8",
                    "user": {
                        "_id": "657a29d7d3e458405b8dc146",
                        "avatarUrl": "/avatars/1fe425dcf3133462fb6376e8e1550b4a.svg",
                        "isPro": false,
                        "fullname": "Oriol Nieto",
                        "user": "urinieto",
                        "type": "user"
                    },
                    "name": "Oriol Nieto",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:28:06.888Z",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1a9",
                    "user": {
                        "_id": "65b8f0339b7250e205100190",
                        "avatarUrl": "/avatars/29df91154c5911a44ef503bb3580ac81.svg",
                        "isPro": false,
                        "fullname": "Ramani Duraiswami",
                        "user": "RamaniD",
                        "type": "user"
                    },
                    "name": "Ramani Duraiswami",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:28:01.454Z",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1aa",
                    "user": {
                        "_id": "62c9664eb34e600d7eaa4beb",
                        "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
                        "isPro": false,
                        "fullname": "Ghosh",
                        "user": "Sreyan88",
                        "type": "user"
                    },
                    "name": "Sreyan Ghosh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T15:36:42.210Z",
                    "hidden": false
                },
                {
                    "_id": "671ef25002737c628d38c1ab",
                    "user": {
                        "_id": "6537a569568d8be8fa096b8c",
                        "avatarUrl": "/avatars/bfda5cb252d8b5bc3ad737d99c0d7f49.svg",
                        "isPro": false,
                        "fullname": "Dinesh Manocha",
                        "user": "manocha",
                        "type": "user"
                    },
                    "name": "Dinesh Manocha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:27:27.197Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-24T21:20:10.000Z",
            "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
            "summary": "The ability to comprehend audio--which includes speech, non-speech sounds,\nand music--is crucial for AI agents to interact effectively with the world. We\npresent MMAU, a novel benchmark designed to evaluate multimodal audio\nunderstanding models on tasks requiring expert-level knowledge and complex\nreasoning. MMAU comprises 10k carefully curated audio clips paired with\nhuman-annotated natural language questions and answers spanning speech,\nenvironmental sounds, and music. It includes information extraction and\nreasoning questions, requiring models to demonstrate 27 distinct skills across\nunique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes\nadvanced perception and reasoning with domain-specific knowledge, challenging\nmodels to tackle tasks akin to those faced by experts. We assess 18 open-source\nand proprietary (Large) Audio-Language Models, demonstrating the significant\nchallenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5\nachieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio\nachieves only 52.50%, highlighting considerable room for improvement. We\nbelieve MMAU will drive the audio and multimodal research community to develop\nmore advanced audio understanding models capable of solving complex audio\ntasks.",
            "upvotes": 9,
            "discussionId": "671ef25502737c628d38c3fb"
        },
        "publishedAt": "2024-10-28T00:39:59.197Z",
        "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19168.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
            "fullname": "Ghosh",
            "name": "Sreyan88",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2410.19290",
            "authors": [
                {
                    "_id": "671f0d24941d8e30b6a43180",
                    "user": {
                        "_id": "615025173e89795099b6fcd9",
                        "avatarUrl": "/avatars/9e2ebea7c5c2d38c95e68af13cc9e382.svg",
                        "isPro": false,
                        "fullname": "Yujian Liu",
                        "user": "yujianll",
                        "type": "user"
                    },
                    "name": "Yujian Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-28T04:09:03.357Z",
                    "hidden": false
                },
                {
                    "_id": "671f0d24941d8e30b6a43181",
                    "user": {
                        "_id": "6696d191d79ce5b27da89c95",
                        "avatarUrl": "/avatars/820c2631c1715a50edb628493656b64f.svg",
                        "isPro": false,
                        "fullname": "Shiyu Chang",
                        "user": "code-terminator",
                        "type": "user"
                    },
                    "name": "Shiyu Chang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-28T13:57:57.969Z",
                    "hidden": false
                },
                {
                    "_id": "671f0d24941d8e30b6a43182",
                    "name": "Tommi Jaakkola",
                    "hidden": false
                },
                {
                    "_id": "671f0d24941d8e30b6a43183",
                    "name": "Yang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-25T03:48:51.000Z",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite\n  Learning",
            "summary": "Recent studies have identified one aggravating factor of LLM hallucinations\nas the knowledge inconsistency between pre-training and fine-tuning, where\nunfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong\noutputs. In this paper, we propose a novel fine-tuning strategy called\nPrereq-Tune to address this knowledge inconsistency and reduce hallucinations.\nFundamentally, Prereq-Tune disentangles the learning of skills and knowledge,\nso the model learns only the task skills without being impacted by the\nknowledge inconsistency. To achieve this, Prereq-Tune introduces an additional\nprerequisite learning stage to learn the necessary knowledge for SFT, allowing\nsubsequent SFT to focus only on task skills. Prereq-Tune can also be combined\nwith fictitious synthetic data to enhance the grounding of LLM outputs to their\ninternal knowledge. Experiments show that Prereq-Tune outperforms existing\nbaselines in improving LLM's factuality across short QA and long-form\ngeneration tasks. It also opens new possibilities for knowledge-controlled\ngeneration in LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
            "upvotes": 4,
            "discussionId": "671f0d24941d8e30b6a431c7"
        },
        "publishedAt": "2024-10-28T02:46:08.877Z",
        "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19290.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9e2ebea7c5c2d38c95e68af13cc9e382.svg",
            "fullname": "Yujian Liu",
            "name": "yujianll",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.19133",
            "authors": [
                {
                    "_id": "671ed82fb91f5a4a244a9d87",
                    "user": {
                        "_id": "634e20a0c1ce28f1de920cc4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666064515342-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Lj V. Miranda",
                        "user": "ljvmiranda921",
                        "type": "user"
                    },
                    "name": "Lester James V. Miranda",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T10:33:55.165Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d88",
                    "user": {
                        "_id": "6269be67d1ac0cde592aba29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6269be67d1ac0cde592aba29/EP3zZbD7-jWK9ITvHGbnZ.jpeg",
                        "isPro": false,
                        "fullname": "Yizhong Wang",
                        "user": "yizhongw",
                        "type": "user"
                    },
                    "name": "Yizhong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:40:01.833Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d89",
                    "user": {
                        "_id": "623ca115a795593324c4353f",
                        "avatarUrl": "/avatars/bf11fe728df2786d52ed4d2de12b48d3.svg",
                        "isPro": false,
                        "fullname": "Yanai Elazar",
                        "user": "yanaiela",
                        "type": "user"
                    },
                    "name": "Yanai Elazar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:40:23.763Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8a",
                    "user": {
                        "_id": "63f24d2d7ddf724fbcc0ea9c",
                        "avatarUrl": "/avatars/3e24c1aa9c1b4066d2dd56aeb4b0f62e.svg",
                        "isPro": false,
                        "fullname": "sachin kumar",
                        "user": "sachinkumar",
                        "type": "user"
                    },
                    "name": "Sachin Kumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:41:48.584Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8b",
                    "user": {
                        "_id": "6556cff80e7a7067a934445f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6556cff80e7a7067a934445f/PoT7qQ6tVqLGGrYvGBbr8.jpeg",
                        "isPro": false,
                        "fullname": "Valentina Pyatkin",
                        "user": "valpy",
                        "type": "user"
                    },
                    "name": "Valentina Pyatkin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:41:55.289Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8c",
                    "user": {
                        "_id": "65282b8d578679aac7888aec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65282b8d578679aac7888aec/dibBkhH-z1c70mJZZxJ7u.jpeg",
                        "isPro": false,
                        "fullname": "Faeze Brahman",
                        "user": "faezeb",
                        "type": "user"
                    },
                    "name": "Faeze Brahman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:42:00.686Z",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8d",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8e",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "671ed82fb91f5a4a244a9d8f",
                    "user": {
                        "_id": "6408fcc93461c51cf735a61e",
                        "avatarUrl": "/avatars/619f3653911d111f046a5a6c30fc8319.svg",
                        "isPro": false,
                        "fullname": "Pradeep Dasigi",
                        "user": "pradeepd",
                        "type": "user"
                    },
                    "name": "Pradeep Dasigi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:42:06.735Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-24T20:04:15.000Z",
            "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI\n  Feedback",
            "summary": "Learning from human feedback has enabled the alignment of language models\n(LMs) with human preferences. However, directly collecting human preferences\ncan be expensive, time-consuming, and can have high variance. An appealing\nalternative is to distill preferences from LMs as a source of synthetic\nannotations as they are more consistent, cheaper, and scale better than human\nannotation; however, they are also prone to biases and errors. In this work, we\nintroduce a routing framework that combines inputs from humans and LMs to\nachieve better annotation quality, while reducing the total cost of human\nannotation. The crux of our approach is to identify preference instances that\nwill benefit from human annotations. We formulate this as an optimization\nproblem: given a preference dataset and an evaluation metric, we train a\nperformance prediction model to predict a reward model's performance on an\narbitrary combination of human and LM annotations and employ a routing strategy\nthat selects a combination that maximizes predicted performance. We train the\nperformance prediction model on MultiPref, a new preference dataset with 10K\ninstances paired with human and LM labels. We show that the selected hybrid\nmixture of LM and direct human preferences using our routing framework achieves\nbetter reward model performance compared to using either one exclusively. We\nsimulate selective human preference collection on three other datasets and show\nthat our method generalizes well to all three. We analyze features from the\nrouting model to identify characteristics of instances that can benefit from\nhuman feedback, e.g., prompts with a moderate safety concern or moderate intent\ncomplexity. We release the dataset, annotation platform, and source code used\nin this study to foster more efficient and accurate preference collection in\nthe future.",
            "upvotes": 4,
            "discussionId": "671ed830b91f5a4a244a9dfb"
        },
        "publishedAt": "2024-10-28T02:40:29.063Z",
        "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19133.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666064515342-noauth.jpeg",
            "fullname": "Lj V. Miranda",
            "name": "ljvmiranda921",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        }
    },
    {
        "paper": {
            "id": "2410.16090",
            "authors": [
                {
                    "_id": "6717ea90b954d4a1d5926d0e",
                    "user": {
                        "_id": "654a500d9b8bd6406d431c0d",
                        "avatarUrl": "/avatars/a6b76441bbc6f4b71d49c52e454c9ef7.svg",
                        "isPro": false,
                        "fullname": "Yu Zhao",
                        "user": "yuzhaouoe",
                        "type": "user"
                    },
                    "name": "Yu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-23T07:33:06.204Z",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d0f",
                    "name": "Xiaotang Du",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d10",
                    "name": "Giwon Hong",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d11",
                    "user": {
                        "_id": "644f895e23d7eb05ca695054",
                        "avatarUrl": "/avatars/3fb04dd8544b403262bf98507de05453.svg",
                        "isPro": false,
                        "fullname": "Aryo Pradipta Gema",
                        "user": "aryopg",
                        "type": "user"
                    },
                    "name": "Aryo Pradipta Gema",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T11:25:18.371Z",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d12",
                    "name": "Alessio Devoto",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d13",
                    "name": "Hongru Wang",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d14",
                    "name": "Xuanli He",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d15",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                },
                {
                    "_id": "6717ea90b954d4a1d5926d16",
                    "name": "Pasquale Minervini",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-21T15:12:51.000Z",
            "title": "Analysing the Residual Stream of Language Models Under Knowledge\n  Conflicts",
            "summary": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context. Such conflicts can lead to\nundesirable model behaviour, such as reliance on outdated or incorrect\ninformation. In this work, we investigate whether LLMs can identify knowledge\nconflicts and whether it is possible to know which source of knowledge the\nmodel will rely on by analysing the residual stream of the LLM. Through probing\ntasks, we find that LLMs can internally register the signal of knowledge\nconflict in the residual stream, which can be accurately detected by probing\nthe intermediate model activations. This allows us to detect conflicts within\nthe residual stream before generating the answers without modifying the input\nor model parameters. Moreover, we find that the residual stream shows\nsignificantly different patterns when the model relies on contextual knowledge\nversus parametric knowledge to resolve conflicts. This pattern can be employed\nto estimate the behaviour of LLMs when conflict happens and prevent unexpected\nanswers before producing the answers. Our analysis offers insights into how\nLLMs internally manage knowledge conflicts and provides a foundation for\ndeveloping methods to control the knowledge selection processes.",
            "upvotes": 3,
            "discussionId": "6717ea92b954d4a1d5926de3"
        },
        "publishedAt": "2024-10-28T14:03:26.892Z",
        "title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16090.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a6b76441bbc6f4b71d49c52e454c9ef7.svg",
            "fullname": "Yu Zhao",
            "name": "yuzhaouoe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        }
    },
    {
        "paper": {
            "id": "2410.19730",
            "authors": [
                {
                    "_id": "671f2fe7aadccdbd12aa688d",
                    "user": {
                        "_id": "656553d89bf6665f10e3a92d",
                        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                        "isPro": false,
                        "fullname": "xiang wyatt zhang",
                        "user": "Wyattz23",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-28T15:41:18.496Z",
                    "hidden": false
                },
                {
                    "_id": "671f2fe7aadccdbd12aa688e",
                    "name": "Juntai Cao",
                    "hidden": false
                },
                {
                    "_id": "671f2fe7aadccdbd12aa688f",
                    "user": {
                        "_id": "6466d463060756d2854ab3e1",
                        "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
                        "isPro": false,
                        "fullname": "Chenyu You",
                        "user": "Charlesyooo",
                        "type": "user"
                    },
                    "name": "Chenyu You",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:39:39.305Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-25T17:56:24.000Z",
            "title": "Counting Ability of Large Language Models and Impact of Tokenization",
            "summary": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC^0, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.",
            "upvotes": 3,
            "discussionId": "671f2fe8aadccdbd12aa68dc"
        },
        "publishedAt": "2024-10-28T05:02:39.866Z",
        "title": "Counting Ability of Large Language Models and Impact of Tokenization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19730.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.17655",
            "authors": [
                {
                    "_id": "6719f2f4defdd700b13520a4",
                    "name": "Dairazalia Sánchez-Cortés",
                    "hidden": false
                },
                {
                    "_id": "6719f2f4defdd700b13520a5",
                    "user": {
                        "_id": "61d626d9e3a234cfcb617dca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61d626d9e3a234cfcb617dca/kJD3oxckDgiaEnYwGth6p.jpeg",
                        "isPro": false,
                        "fullname": "Sergio Burdisso",
                        "user": "sergioburdisso",
                        "type": "user"
                    },
                    "name": "Sergio Burdisso",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-24T07:15:17.790Z",
                    "hidden": false
                },
                {
                    "_id": "6719f2f4defdd700b13520a6",
                    "user": {
                        "_id": "661fed2e9e3858af5d7037dc",
                        "avatarUrl": "/avatars/07b8256344e093e179ebf9e724ae5075.svg",
                        "isPro": false,
                        "fullname": "Esau Villatoro-Tello",
                        "user": "evillatoro",
                        "type": "user"
                    },
                    "name": "Esaú Villatoro-Tello",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:49:51.940Z",
                    "hidden": false
                },
                {
                    "_id": "6719f2f4defdd700b13520a7",
                    "name": "Petr Motlicek",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-23T08:18:26.000Z",
            "title": "Mapping the Media Landscape: Predicting Factual Reporting and Political\n  Bias Through Web Interactions",
            "summary": "Bias assessment of news sources is paramount for professionals,\norganizations, and researchers who rely on truthful evidence for information\ngathering and reporting. While certain bias indicators are discernible from\ncontent analysis, descriptors like political bias and fake news pose greater\nchallenges. In this paper, we propose an extension to a recently presented news\nmedia reliability estimation method that focuses on modeling outlets and their\nlongitudinal web interactions. Concretely, we assess the classification\nperformance of four reinforcement learning strategies on a large news media\nhyperlink graph. Our experiments, targeting two challenging bias descriptors,\nfactual reporting and political bias, showed a significant performance\nimprovement at the source media level. Additionally, we validate our methods on\nthe CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in\nboth, F1-score and the official MAE metric. Furthermore, we contribute by\nreleasing the largest annotated dataset of news source media, categorized with\nfactual reporting and political bias labels. Our findings suggest that\nprofiling news media sources based on their hyperlink interactions over time is\nfeasible, offering a bird's-eye view of evolving media landscapes.",
            "upvotes": 2,
            "discussionId": "6719f2f5defdd700b1352104"
        },
        "publishedAt": "2024-10-28T14:02:25.191Z",
        "title": "Mapping the Media Landscape: Predicting Factual Reporting and Political Bias Through Web Interactions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17655.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61d626d9e3a234cfcb617dca/kJD3oxckDgiaEnYwGth6p.jpeg",
            "fullname": "Sergio Burdisso",
            "name": "sergioburdisso",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.18912",
            "authors": [
                {
                    "_id": "671c6ae3a7c9769d4c7f7b55",
                    "user": {
                        "_id": "671c6a3e255aa50ebb504fc5",
                        "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
                        "isPro": false,
                        "fullname": "Mingtong Zhang",
                        "user": "MingtongZ",
                        "type": "user"
                    },
                    "name": "Mingtong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T10:36:59.663Z",
                    "hidden": false
                },
                {
                    "_id": "671c6ae3a7c9769d4c7f7b56",
                    "name": "Kaifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "671c6ae3a7c9769d4c7f7b57",
                    "user": {
                        "_id": "64a46562c641afb468a3dc04",
                        "avatarUrl": "/avatars/bc54e5c13f657251b05bf2725086e56d.svg",
                        "isPro": false,
                        "fullname": "Yunzhu Li",
                        "user": "yunzhuli",
                        "type": "user"
                    },
                    "name": "Yunzhu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:55:25.973Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-24T17:02:52.000Z",
            "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
            "summary": "Videos of robots interacting with objects encode rich information about the\nobjects' dynamics. However, existing video prediction approaches typically do\nnot explicitly account for the 3D information from videos, such as robot\nactions and objects' 3D states, limiting their use in real-world robotic\napplications. In this work, we introduce a framework to learn object dynamics\ndirectly from multi-view RGB videos by explicitly considering the robot's\naction trajectories and their effects on scene dynamics. We utilize the 3D\nGaussian representation of 3D Gaussian Splatting (3DGS) to train a\nparticle-based dynamics model using Graph Neural Networks. This model operates\non sparse control particles downsampled from the densely tracked 3D Gaussian\nreconstructions. By learning the neural dynamics model on offline robot\ninteraction data, our method can predict object motions under varying initial\nconfigurations and unseen robot actions. The 3D transformations of Gaussians\ncan be interpolated from the motions of control particles, enabling the\nrendering of predicted future object states and achieving action-conditioned\nvideo prediction. The dynamics model can also be applied to model-based\nplanning frameworks for object manipulation tasks. We conduct experiments on\nvarious kinds of deformable materials, including ropes, clothes, and stuffed\nanimals, demonstrating our framework's ability to model complex shapes and\ndynamics. Our project page is available at https://gs-dynamics.github.io.",
            "upvotes": 2,
            "discussionId": "671c6ae5a7c9769d4c7f7c31"
        },
        "publishedAt": "2024-10-28T09:11:06.573Z",
        "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18912.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
            "fullname": "Mingtong Zhang",
            "name": "MingtongZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.16270",
            "authors": [
                {
                    "_id": "671f35fc2750014020ef8e8f",
                    "user": {
                        "_id": "65c1d9413601dac1372b0ffe",
                        "avatarUrl": "/avatars/109f2af09a29b16a884b77c930bb7f48.svg",
                        "isPro": false,
                        "fullname": "Lingyu Li",
                        "user": "LingyuLi",
                        "type": "user"
                    },
                    "name": "Lingyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-28T08:51:28.542Z",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e90",
                    "name": "Yixu Wang",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e91",
                    "user": {
                        "_id": "64fecf8dd30d99e75e0a557c",
                        "avatarUrl": "/avatars/6f6ba57711f7717bcab8e3db05a849cb.svg",
                        "isPro": false,
                        "fullname": "Haiquan Zhao",
                        "user": "haidequanbu",
                        "type": "user"
                    },
                    "name": "Haiquan Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:45:27.152Z",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e92",
                    "name": "Shuqi Kong",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e93",
                    "name": "Yan Teng",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e94",
                    "name": "Chunbo Li",
                    "hidden": false
                },
                {
                    "_id": "671f35fc2750014020ef8e95",
                    "user": {
                        "_id": "66f108310ea83b350d0d7411",
                        "avatarUrl": "/avatars/93a68a13b016a8ae81a3453840addfc3.svg",
                        "isPro": false,
                        "fullname": "Yingchun WANG",
                        "user": "DaenerysW",
                        "type": "user"
                    },
                    "name": "Yingchun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:45:58.257Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-21T17:59:50.000Z",
            "title": "Reflection-Bench: probing AI intelligence with reflection",
            "summary": "The ability to adapt beliefs or behaviors in response to unexpected outcomes,\nreflection, is fundamental to intelligent systems' interaction with the world.\nFrom a cognitive science perspective, this serves as a core principle of\nintelligence applicable to both human and AI systems. To address the debate on\nthe intelligence of large language models (LLMs), we propose Reflection-Bench,\na comprehensive benchmark comprising 7 tasks spanning core cognitive functions\ncrucial for reflection, including perception, memory, belief updating,\ndecision-making, prediction, counterfactual thinking, and meta-reflection. We\nevaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude\n3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory\nreflection ability. We discuss the underlying causes of these results and\nsuggest potential avenues for future research. In conclusion, Reflection-Bench\noffers both evaluation tools and inspiration for developing AI capable of\nreliably interacting with the environment. Our data and code are available at\nhttps://github.com/YabYum/ReflectionBench.",
            "upvotes": 2,
            "discussionId": "671f35fd2750014020ef8f19"
        },
        "publishedAt": "2024-10-28T07:28:30.515Z",
        "title": "Reflection-Bench: probing AI intelligence with reflection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16270.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/109f2af09a29b16a884b77c930bb7f48.svg",
            "fullname": "Lingyu Li",
            "name": "LingyuLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.18076",
            "authors": [
                {
                    "_id": "6719e6dccc4568fb62d392ec",
                    "user": {
                        "_id": "6568d51a16053f9a304585f9",
                        "avatarUrl": "/avatars/2dc0596e321c978b9513cf5f67141c2a.svg",
                        "isPro": false,
                        "fullname": "Max Wilcoxson",
                        "user": "mwilcoxson",
                        "type": "user"
                    },
                    "name": "Max Wilcoxson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:49:01.754Z",
                    "hidden": false
                },
                {
                    "_id": "6719e6dccc4568fb62d392ed",
                    "name": "Qiyang Li",
                    "hidden": false
                },
                {
                    "_id": "6719e6dccc4568fb62d392ee",
                    "user": {
                        "_id": "6321e71ca97afe3c4c66f7b5",
                        "avatarUrl": "/avatars/02cf0d5779417b37e34cc6340bffacad.svg",
                        "isPro": false,
                        "fullname": "Kevin Frans",
                        "user": "kvfrans",
                        "type": "user"
                    },
                    "name": "Kevin Frans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:48:46.738Z",
                    "hidden": false
                },
                {
                    "_id": "6719e6dccc4568fb62d392ef",
                    "user": {
                        "_id": "665ce54120a307a3754849dd",
                        "avatarUrl": "/avatars/e698726e9be61dd50ce2efe372ed5dac.svg",
                        "isPro": false,
                        "fullname": "Sergey Levine",
                        "user": "svlevine",
                        "type": "user"
                    },
                    "name": "Sergey Levine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-28T15:48:55.303Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-23T17:58:45.000Z",
            "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online\n  Exploration",
            "summary": "Unsupervised pretraining has been transformative in many supervised domains.\nHowever, applying such ideas to reinforcement learning (RL) presents a unique\nchallenge in that fine-tuning does not involve mimicking task-specific data,\nbut rather exploring and locating the solution through iterative\nself-improvement. In this work, we study how unlabeled prior trajectory data\ncan be leveraged to learn efficient exploration strategies. While prior data\ncan be used to pretrain a set of low-level skills, or as additional off-policy\ndata for online RL, it has been unclear how to combine these ideas effectively\nfor online exploration. Our method SUPE (Skills from Unlabeled Prior data for\nExploration) demonstrates that a careful combination of these ideas compounds\ntheir benefits. Our method first extracts low-level skills using a variational\nautoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an\noptimistic reward model, transforming prior data into high-level, task-relevant\nexamples. Finally, SUPE uses these transformed examples as additional\noff-policy data for online RL to learn a high-level policy that composes\npretrained low-level skills to explore efficiently. We empirically show that\nSUPE reliably outperforms prior strategies, successfully solving a suite of\nlong-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
            "upvotes": 2,
            "discussionId": "6719e6ddcc4568fb62d39383"
        },
        "publishedAt": "2024-10-28T00:57:59.565Z",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18076.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        }
    }
]