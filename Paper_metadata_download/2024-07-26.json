[
    {
        "paper": {
            "id": "2407.16982",
            "authors": [
                {
                    "_id": "66a1dad004ab5290ec1b405e",
                    "user": {
                        "id": "661639c4dd9284ce5d361f0f",
                        "avatarUrl": "/avatars/9a0f8f725d7b90c30dfdb64313761155.svg",
                        "isPro": true,
                        "fullname": "Lirui Zhao",
                        "user": "LiruiZhao",
                        "type": "user"
                    },
                    "name": "Lirui Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-25T07:38:19.597Z",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b405f",
                    "name": "Tianshuo Yang",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4060",
                    "user": {
                        "id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T07:58:24.207Z",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4061",
                    "name": "Yuxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4062",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4063",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4064",
                    "user": {
                        "id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:06:11.679Z",
                    "hidden": false
                },
                {
                    "_id": "66a1dad004ab5290ec1b4065",
                    "name": "Rongrong Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-24T03:58:58.000Z",
            "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
            "summary": "This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.",
            "upvotes": 11
        },
        "publishedAt": "2024-07-26T02:23:32.225Z",
        "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16982.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9a0f8f725d7b90c30dfdb64313761155.svg",
            "fullname": "Lirui Zhao",
            "name": "LiruiZhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.17490",
            "authors": [
                {
                    "_id": "66a30b583c7f0decff91b8a6",
                    "user": {
                        "id": "6458ce236fa580137af5aa95",
                        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Chai",
                        "user": "Yuxiang007",
                        "type": "user"
                    },
                    "name": "Yuxiang Chai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T07:40:16.403Z",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8a7",
                    "user": {
                        "id": "63c7a33121bd95f80ed74652",
                        "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "thuhsy",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T07:45:40.847Z",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8a8",
                    "name": "Yazhe Niu",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8a9",
                    "name": "Han Xiao",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8aa",
                    "name": "Liang Liu",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8ab",
                    "name": "Dingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8ac",
                    "name": "Peng Gao",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8ad",
                    "name": "Shuai Ren",
                    "hidden": false
                },
                {
                    "_id": "66a30b583c7f0decff91b8ae",
                    "user": {
                        "id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T07:48:40.057Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-03T17:59:58.000Z",
            "title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
            "summary": "AI agents have drawn increasing attention mostly on their ability to perceive\nenvironments, understand tasks, and autonomously achieve goals. To advance\nresearch on AI agents in mobile scenarios, we introduce the Android\nMulti-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for\ngeneralist mobile GUI-control agents. Their capabilities of completing complex\ntasks by directly interacting with the graphical user interface (GUI) on mobile\ndevices are trained and evaluated with the proposed dataset. AMEX comprises\nover 104K high-resolution screenshots from 110 popular mobile applications,\nwhich are annotated at multiple levels. Unlike existing mobile device-control\ndatasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations:\nGUI interactive element grounding, GUI screen and element functionality\ndescriptions, and complex natural language instructions, each averaging 13\nsteps with stepwise GUI-action chains. We develop this dataset from a more\ninstructive and detailed perspective, complementing the general settings of\nexisting datasets. Additionally, we develop a baseline model SPHINX Agent and\ncompare its performance across state-of-the-art agents trained on other\ndatasets. To facilitate further research, we open-source our dataset, models,\nand relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/",
            "upvotes": 11
        },
        "publishedAt": "2024-07-26T01:06:16.361Z",
        "title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17490.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.16637",
            "authors": [
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b4",
                    "user": {
                        "id": "64ed86a5b6d4b4b02a9190cf",
                        "avatarUrl": "/avatars/8b92a24566ecb3489a3adac0586d4dd6.svg",
                        "isPro": false,
                        "fullname": "Rongwu Xu",
                        "user": "pillowsofwind",
                        "type": "user"
                    },
                    "name": "Rongwu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-26T10:26:39.119Z",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b5",
                    "name": "Yishuo Cai",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b6",
                    "name": "Zhenhong Zhou",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b7",
                    "user": {
                        "id": "668657b4c9ffbedc86831006",
                        "avatarUrl": "/avatars/0d766ccde2e9890c1d61cc5d04e1d692.svg",
                        "isPro": false,
                        "fullname": "Renjie Gu",
                        "user": "csuJC",
                        "type": "user"
                    },
                    "name": "Renjie Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T12:20:33.035Z",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b8",
                    "user": {
                        "id": "63fef648c0ec83fda43da041",
                        "avatarUrl": "/avatars/a24ce8fc339f9b36237d241f741472ea.svg",
                        "isPro": false,
                        "fullname": "Haiqin Weng",
                        "user": "haiqin",
                        "type": "user"
                    },
                    "name": "Haiqin Weng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T12:20:26.728Z",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2b9",
                    "name": "Yan Liu",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2ba",
                    "name": "Tianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2bb",
                    "name": "Wei Xu",
                    "hidden": false
                },
                {
                    "_id": "66a378e49b6c8a5bd7ddf2bc",
                    "name": "Han Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-23T16:54:28.000Z",
            "title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
            "summary": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of course-correction,\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the C^2-Eval benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\nC^2-Syn, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, Llama2-Chat 7B and\nQwen2 7B, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
            "upvotes": 7
        },
        "publishedAt": "2024-07-26T09:15:33.250Z",
        "title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/yptaDZVg9c1LQRYaxazFA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/SFn1hryYtLJqxLPB_8oZb.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/EvgtjrznVOyHGqfRIVWbW.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/OpqUwBokOqgdnRtrWn9y2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/ot80c7cfLCbvHBFF4IVWn.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/5FfYXhfhil52-761QZISU.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ed86a5b6d4b4b02a9190cf/Qd2yLNMnX_EjHE0tHEWmB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16637.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/8b92a24566ecb3489a3adac0586d4dd6.svg",
            "fullname": "Rongwu Xu",
            "name": "pillowsofwind",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.17535",
            "authors": [
                {
                    "_id": "66a31decc1886a250bbda980",
                    "user": {
                        "id": "64c0e071e9263c783d548178",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c0e071e9263c783d548178/VFwxjv7HFPwIoKIVGWLDg.png",
                        "isPro": false,
                        "fullname": "Maojun SUN",
                        "user": "Stephen-smj",
                        "type": "user"
                    },
                    "name": "Maojun Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T07:48:55.476Z",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda981",
                    "name": "Ruijian Han",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda982",
                    "name": "Binyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda983",
                    "name": "Houduo Qi",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda984",
                    "name": "Defeng Sun",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda985",
                    "name": "Yancheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "66a31decc1886a250bbda986",
                    "name": "Jian Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-24T06:26:36.000Z",
            "title": "LAMBDA: A Large Model Based Data Agent",
            "summary": "We introduce ``LAMBDA,\" a novel open-source, code-free multi-agent data\nanalysis system that that harnesses the power of large models. LAMBDA is\ndesigned to address data analysis challenges in complex data-driven\napplications through the use of innovatively designed data agents that operate\niteratively and generatively using natural language. At the core of LAMBDA are\ntwo key agent roles: the programmer and the inspector, which are engineered to\nwork together seamlessly. Specifically, the programmer generates code based on\nthe user's instructions and domain-specific knowledge, enhanced by advanced\nmodels. Meanwhile, the inspector debugs the code when necessary. To ensure\nrobustness and handle adverse scenarios, LAMBDA features a user interface that\nallows direct user intervention in the operational loop. Additionally, LAMBDA\ncan flexibly integrate external models and algorithms through our knowledge\nintegration mechanism, catering to the needs of customized data analysis.\nLAMBDA has demonstrated strong performance on various machine learning\ndatasets. It has the potential to enhance data science practice and analysis\nparadigm by seamlessly integrating human and artificial intelligence, making it\nmore accessible, effective, and efficient for individuals from diverse\nbackgrounds. The strong performance of LAMBDA in solving data science problems\nis demonstrated in several case studies, which are presented at\nhttps://www.polyu.edu.hk/ama/cmfai/lambda.html.",
            "upvotes": 7
        },
        "publishedAt": "2024-07-26T02:24:29.379Z",
        "title": "LAMBDA: A Large Model Based Data Agent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17535.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.17789",
            "authors": [
                {
                    "_id": "66a31f39a74b20aad777e7e7",
                    "user": {
                        "id": "64101b4ca92fedb0e8559b91",
                        "avatarUrl": "/avatars/c7453a5641a94d04a75de9b7d4272bc2.svg",
                        "isPro": false,
                        "fullname": "Xuchen Pan",
                        "user": "panxuchen",
                        "type": "user"
                    },
                    "name": "Xuchen Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:30:22.586Z",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7e8",
                    "user": {
                        "id": "642e79a83a486fec83426a49",
                        "avatarUrl": "/avatars/a003d27177cee6234348008263933cfd.svg",
                        "isPro": false,
                        "fullname": "dawei gao",
                        "user": "DavdGao",
                        "type": "user"
                    },
                    "name": "Dawei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:30:28.568Z",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7e9",
                    "name": "Yuexiang Xie",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7ea",
                    "name": "Zhewei Wei",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7eb",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7ec",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7ed",
                    "user": {
                        "id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:32:01.623Z",
                    "hidden": false
                },
                {
                    "_id": "66a31f39a74b20aad777e7ee",
                    "user": {
                        "id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:32:09.560Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-25T05:50:46.000Z",
            "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
            "summary": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, centralized workflow\norchestration, and both inter-agent and agent-environment interactions among\nagents. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of the\nproposed enhancements in AgentScope, and provide detailed observations and\ndiscussions to highlight the great potential of applying multi-agent systems in\nlarge-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope to inspire further research and\ndevelopment in large-scale multi-agent simulations.",
            "upvotes": 6
        },
        "publishedAt": "2024-07-26T02:30:00.260Z",
        "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17789.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.18121",
            "authors": [
                {
                    "_id": "66a3aa35ee3de8c56edca432",
                    "user": {
                        "id": "64f001bfabd9fb1914398bd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
                        "isPro": false,
                        "fullname": "liuzuyan",
                        "user": "Zuyan",
                        "type": "user"
                    },
                    "name": "Zuyan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T14:03:37.394Z",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca433",
                    "name": "Benlin Liu",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca434",
                    "name": "Jiahui Wang",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca435",
                    "user": {
                        "id": "652965773a416e1f2173443b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/XOeZ_8S4Pi9Dp57KwMdvD.png",
                        "isPro": false,
                        "fullname": "Yuhao Dong",
                        "user": "THUdyh",
                        "type": "user"
                    },
                    "name": "Yuhao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T14:06:03.783Z",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca436",
                    "user": {
                        "id": "64975e18a08b5c7ea0163be7",
                        "avatarUrl": "/avatars/88963b6ae00ec3f10f4a40322f75944a.svg",
                        "isPro": false,
                        "fullname": "Guangyi Chen",
                        "user": "CHENGY12",
                        "type": "user"
                    },
                    "name": "Guangyi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T14:06:11.477Z",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca437",
                    "user": {
                        "id": "63e4865354f51ea342d45d78",
                        "avatarUrl": "/avatars/2e7eccc878751331ca8b282f53e38899.svg",
                        "isPro": false,
                        "fullname": "Yongming Rao",
                        "user": "raoyongming",
                        "type": "user"
                    },
                    "name": "Yongming Rao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T14:06:19.070Z",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca438",
                    "user": {
                        "id": "66429868ab89e3a3a85668b0",
                        "avatarUrl": "/avatars/170e0daa454838deee2bf946f7118651.svg",
                        "isPro": false,
                        "fullname": "Ranjay Krishna",
                        "user": "ranjaykrishna",
                        "type": "user"
                    },
                    "name": "Ranjay Krishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T14:06:25.297Z",
                    "hidden": false
                },
                {
                    "_id": "66a3aa35ee3de8c56edca439",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-25T15:29:05.000Z",
            "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
            "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
            "upvotes": 4
        },
        "publishedAt": "2024-07-26T12:24:07.862Z",
        "title": "Efficient Inference of Vision Instruction-Following Models with Elastic Cache",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.18121.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/XOeZ_8S4Pi9Dp57KwMdvD.png",
            "fullname": "Yuhao Dong",
            "name": "THUdyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.17952",
            "authors": [
                {
                    "_id": "66a355b0e88715bf1bb3094e",
                    "user": {
                        "id": "657dc1576dc01435cd9029d8",
                        "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
                        "isPro": false,
                        "fullname": "Xiang Zhang",
                        "user": "XiangZ",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-26T10:26:32.542Z",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb3094f",
                    "user": {
                        "id": "63d90391da4f72339244c2a8",
                        "avatarUrl": "/avatars/eb0e0259c391d59739c1a205c36bb539.svg",
                        "isPro": false,
                        "fullname": "Ke",
                        "user": "Bingxin",
                        "type": "user"
                    },
                    "name": "Bingxin Ke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:39:19.926Z",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30950",
                    "user": {
                        "id": "6332ab213c1494be1137bd58",
                        "avatarUrl": "/avatars/f65dac80d7f5148c46781055ddbdae51.svg",
                        "isPro": false,
                        "fullname": "hayko riemenschneider",
                        "user": "chrysmun",
                        "type": "user"
                    },
                    "name": "Hayko Riemenschneider",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:39:27.141Z",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30951",
                    "user": {
                        "id": "63a5785a8fb23d08bb2d0291",
                        "avatarUrl": "/avatars/758b06dae06e9eee6fced10ce682aef1.svg",
                        "isPro": false,
                        "fullname": "Nando Metzger",
                        "user": "nandometzger",
                        "type": "user"
                    },
                    "name": "Nando Metzger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T08:39:32.523Z",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30952",
                    "user": {
                        "id": "62f93abbc4817cfc0756b6f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg",
                        "isPro": false,
                        "fullname": "Anton Obukhov",
                        "user": "toshas",
                        "type": "user"
                    },
                    "name": "Anton Obukhov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-26T08:05:55.086Z",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30953",
                    "name": "Markus Gross",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30954",
                    "name": "Konrad Schindler",
                    "hidden": false
                },
                {
                    "_id": "66a355b0e88715bf1bb30955",
                    "name": "Christopher Schroers",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-25T11:16:37.000Z",
            "title": "BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular\n  Depth Estimation",
            "summary": "By training over large-scale datasets, zero-shot monocular depth estimation\n(MDE) methods show robust performance in the wild but often suffer from\ninsufficiently precise details. Although recent diffusion-based MDE approaches\nexhibit appealing detail extraction ability, they still struggle in\ngeometrically challenging scenes due to the difficulty of gaining robust\ngeometric priors from diverse datasets. To leverage the complementary merits of\nboth worlds, we propose BetterDepth to efficiently achieve geometrically\ncorrect affine-invariant MDE performance while capturing fine-grained details.\nSpecifically, BetterDepth is a conditional diffusion-based refiner that takes\nthe prediction from pre-trained MDE models as depth conditioning, in which the\nglobal depth context is well-captured, and iteratively refines details based on\nthe input image. For the training of such a refiner, we propose global\npre-alignment and local patch masking methods to ensure the faithfulness of\nBetterDepth to depth conditioning while learning to capture fine-grained scene\ndetails. By efficient training on small-scale synthetic datasets, BetterDepth\nachieves state-of-the-art zero-shot MDE performance on diverse public datasets\nand in-the-wild scenes. Moreover, BetterDepth can improve the performance of\nother MDE models in a plug-and-play manner without additional re-training.",
            "upvotes": 4
        },
        "publishedAt": "2024-07-26T06:30:39.206Z",
        "title": "BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/AHQqo1k1Wd6s4hSngBAkw.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17952.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg",
            "fullname": "Anton Obukhov",
            "name": "toshas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.16607",
            "authors": [
                {
                    "_id": "66a0f22917555c302003b936",
                    "user": {
                        "id": "62f68be78465979f627b94b6",
                        "avatarUrl": "/avatars/2b8e7ddd6c364ba9f1b4806e256e9e91.svg",
                        "isPro": false,
                        "fullname": "Jonathan Hayase",
                        "user": "Jhayase",
                        "type": "user"
                    },
                    "name": "Jonathan Hayase",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T12:18:19.787Z",
                    "hidden": false
                },
                {
                    "_id": "66a0f22917555c302003b937",
                    "user": {
                        "id": "62449da46b55ef458d430371",
                        "avatarUrl": "/avatars/b95954a88c7b15524a3d7776a4abe083.svg",
                        "isPro": false,
                        "fullname": "Alisa Liu",
                        "user": "alisawuffles",
                        "type": "user"
                    },
                    "name": "Alisa Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T12:18:55.764Z",
                    "hidden": false
                },
                {
                    "_id": "66a0f22917555c302003b938",
                    "user": {
                        "id": "64d42729f63b01b7f676b176",
                        "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                        "isPro": false,
                        "fullname": "Yejin Choi",
                        "user": "yejinchoinka",
                        "type": "user"
                    },
                    "name": "Yejin Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-26T12:19:14.024Z",
                    "hidden": false
                },
                {
                    "_id": "66a0f22917555c302003b939",
                    "name": "Sewoong Oh",
                    "hidden": false
                },
                {
                    "_id": "66a0f22917555c302003b93a",
                    "name": "Noah A. Smith",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-23T16:13:22.000Z",
            "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?",
            "summary": "The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-26T10:10:04.824Z",
        "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16607.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.18129",
            "authors": [
                {
                    "_id": "66a3c1fd4366ebab2c8a0291",
                    "name": "Fakhraddin Alwajih",
                    "hidden": false
                },
                {
                    "_id": "66a3c1fd4366ebab2c8a0292",
                    "name": "Gagan Bhatia",
                    "hidden": false
                },
                {
                    "_id": "66a3c1fd4366ebab2c8a0293",
                    "name": "Muhammad Abdul-Mageed",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-25T15:36:48.000Z",
            "title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
            "summary": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-26T14:04:48.382Z",
        "title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.18129.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
            "fullname": "Gagan Bhatia",
            "name": "gagan3012",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]