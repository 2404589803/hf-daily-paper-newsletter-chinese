[
  {
    "paper": {
      "id": "2509.02547",
      "authors": [
        {
          "_id": "68b7bd09295f15ff60911362",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911363",
          "name": "Hejia Geng",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911364",
          "name": "Xiaohang Yu",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911365",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911366",
          "name": "Zaibin Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911367",
          "name": "Zelin Tan",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911368",
          "name": "Heng Zhou",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911369",
          "name": "Zhongzhi Li",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136a",
          "name": "Xiangyuan Xue",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136b",
          "name": "Yijiang Li",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136c",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136d",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136e",
          "name": "Chen Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091136f",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911370",
          "name": "Zihu Wang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911371",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911372",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911373",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911374",
          "name": "Mengyue Yang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911375",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911376",
          "name": "Michael Littman",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911377",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911378",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff60911379",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68b7bd09295f15ff6091137a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T17:46:26.000Z",
      "submittedOnDailyAt": "2025-09-03T02:32:28.813Z",
      "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
      "submittedOnDailyBy": {
        "_id": "660d17d6c9be0dcd31a30b3d",
        "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg",
        "isPro": false,
        "fullname": "Zhou Heng",
        "user": "henggg",
        "type": "user"
      },
      "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.",
      "upvotes": 35,
      "discussionId": "68b7bd09295f15ff6091137b",
      "githubRepo": "https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers",
      "ai_summary": "Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.",
      "ai_keywords": [
        "agentic reinforcement learning",
        "LLM RL",
        "Markov Decision Processes",
        "POMDPs",
        "planning",
        "tool use",
        "memory",
        "reasoning",
        "self-improvement",
        "perception",
        "reinforcement learning",
        "open-source environments",
        "benchmarks",
        "frameworks"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-09-02T13:46:26.000Z",
    "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
    "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660d17d6c9be0dcd31a30b3d",
      "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg",
      "fullname": "Zhou Heng",
      "name": "henggg",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00676",
      "authors": [
        {
          "_id": "68b7c0f7295f15ff609113cb",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113cc",
          "name": "Chunyuan Li",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113cd",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113ce",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113cf",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113d0",
          "name": "Tianyi Xiong",
          "hidden": false
        },
        {
          "_id": "68b7c0f7295f15ff609113d1",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-31T03:08:02.000Z",
      "submittedOnDailyAt": "2025-09-03T02:45:59.278Z",
      "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In vision-language modeling, critic models are typically trained to evaluate\noutputs -- assigning scalar scores or pairwise preferences -- rather than to\ngenerate responses. This separation from policy models, which produce the\nresponses, is so entrenched that critics are rarely considered for direct\npolicy use. In this work, we challenge this convention. We propose to\nreorganize preference-labeled critic datasets into verifiable training signals\nand perform reinforcement learning directly on a base generative model,\nproducing LLaVA-Critic-R1, a multimodal critic trained to optimize preference\njudgments while retaining full generation ability. Surprisingly,\nLLaVA-Critic-R1 emerges not only as a top-performing critic but also as a\ncompetitive policy model -- matching or surpassing specialized reasoning VLMs\ntrained with in-domain data across 26 visual reasoning and understanding\nbenchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).\nExtending this approach to existing strong reasoning VLMs yields\nLLaVA-Critic-R1+, which further advances policy performance without sacrificing\ncritic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.\nFinally, we show that the enhanced critic ability benefits inference: applying\nself-critique at test time yields an average +13.8% improvement on five\nrepresentative reasoning tasks without additional training. Our results reveal\nthat RL training on critic data can produce a unified model excelling at both\nevaluation and generation, offering a simple path toward scalable,\nself-improving multimodal systems.",
      "upvotes": 34,
      "discussionId": "68b7c0f8295f15ff609113d2",
      "githubRepo": "https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/llava-critic-r1",
      "ai_summary": "Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.",
      "ai_keywords": [
        "critic models",
        "preference-labeled datasets",
        "reinforcement learning",
        "generative model",
        "LLaVA-Critic-R1",
        "multimodal critic",
        "visual reasoning",
        "understanding benchmarks",
        "Qwen-2.5-VL-7B",
        "LLaVA-Critic-R1+",
        "self-critique",
        "MMMU"
      ],
      "githubStars": 4163
    },
    "publishedAt": "2025-08-30T23:08:02.000Z",
    "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
    "summary": "In vision-language modeling, critic models are typically trained to evaluate\noutputs -- assigning scalar scores or pairwise preferences -- rather than to\ngenerate responses. This separation from policy models, which produce the\nresponses, is so entrenched that critics are rarely considered for direct\npolicy use. In this work, we challenge this convention. We propose to\nreorganize preference-labeled critic datasets into verifiable training signals\nand perform reinforcement learning directly on a base generative model,\nproducing LLaVA-Critic-R1, a multimodal critic trained to optimize preference\njudgments while retaining full generation ability. Surprisingly,\nLLaVA-Critic-R1 emerges not only as a top-performing critic but also as a\ncompetitive policy model -- matching or surpassing specialized reasoning VLMs\ntrained with in-domain data across 26 visual reasoning and understanding\nbenchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).\nExtending this approach to existing strong reasoning VLMs yields\nLLaVA-Critic-R1+, which further advances policy performance without sacrificing\ncritic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.\nFinally, we show that the enhanced critic ability benefits inference: applying\nself-critique at test time yields an average +13.8% improvement on five\nrepresentative reasoning tasks without additional training. Our results reveal\nthat RL training on critic data can produce a unified model excelling at both\nevaluation and generation, offering a simple path toward scalable,\nself-improving multimodal systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00676.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02479",
      "authors": [
        {
          "_id": "68b7b9f8295f15ff6091130d",
          "name": "Zhenghai Xue",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff6091130e",
          "name": "Longtao Zheng",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff6091130f",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff60911310",
          "name": "Yingru Li",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff60911311",
          "name": "Xiaosen Zheng",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff60911312",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68b7b9f8295f15ff60911313",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/8Ju5mD0aVNHopWdipRWTa.png"
      ],
      "publishedAt": "2025-09-02T16:30:19.000Z",
      "submittedOnDailyAt": "2025-09-03T02:21:40.676Z",
      "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation.",
      "upvotes": 30,
      "discussionId": "68b7b9f8295f15ff60911314",
      "projectPage": "https://simpletir.notion.site/report",
      "githubRepo": "https://github.com/ltzheng/SimpleTIR",
      "ai_summary": "SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.",
      "ai_keywords": [
        "Tool-Integrated Reasoning",
        "TIR",
        "Reinforcement Learning",
        "RL",
        "distributional drift",
        "low-probability tokens",
        "gradient norm explosions",
        "SimpleTIR",
        "void turns",
        "policy update",
        "AIME24",
        "Qwen2.5-7B",
        "self-correction",
        "cross-validation"
      ],
      "githubStars": 191
    },
    "publishedAt": "2025-09-02T12:30:19.000Z",
    "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
    "summary": "Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/8Ju5mD0aVNHopWdipRWTa.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01215",
      "authors": [
        {
          "_id": "68b7bc71295f15ff60911355",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff60911356",
          "name": "Zhongyin Zhao",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff60911357",
          "name": "Le Tian",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff60911358",
          "name": "Haicheng Wang",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff60911359",
          "name": "Xubing Ye",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135a",
          "name": "Yangxiu You",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135b",
          "name": "Zilin Yu",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135c",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135d",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135e",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "68b7bc71295f15ff6091135f",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T07:54:18.000Z",
      "submittedOnDailyAt": "2025-09-03T02:28:03.644Z",
      "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion",
      "submittedOnDailyBy": {
        "_id": "64d47a7a508a6313e33faedd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg",
        "isPro": false,
        "fullname": "Yuan Liu",
        "user": "YuanLiuuuuuu",
        "type": "user"
      },
      "summary": "High-quality labeled data is essential for training accurate document\nconversion models, particularly in domains with complex formats such as tables,\nformulas, and multi-column text. However, manual annotation is both costly and\ntime-consuming, while automatic labeling using existing models often lacks\naccuracy in handling such challenging scenarios. Consequently, training student\nmodels by distilling outputs from teacher models can significantly limit their\nperformance in real-world applications. In this paper, we propose a fully\nautomated, distillation-free framework comprising two stages for constructing\nhigh-quality document extraction datasets and models capable of handling\ndiverse document formats and layouts. In the first stage, we introduce a method\nfor generating large-scale, diverse synthetic data, which enables a model to\nextract key elements in a unified format with strong initial performance. In\nthe second stage, we present a self-improvement approach that further adapts\nthe model, initially trained on synthetic data, to real-world documents.\nSpecifically, we first use the fine-tuned model to annotate real documents,\nthen apply a suite of filtering strategies to verify annotation quality, and\nfinally retrain the model on the verified dataset. By iteratively repeating\nthis process, we progressively enhance both the model's conversion capabilities\nand the quality of the generated data. We train a public POINTS-1.5 model to\nobtain POINTS-Reader, which surpasses many existing public and proprietary\nmodels of comparable or larger size. Our model is available at\nhttps://github.com/Tencent/POINTS-Reader.",
      "upvotes": 30,
      "discussionId": "68b7bc71295f15ff60911360",
      "ai_summary": "A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.",
      "ai_keywords": [
        "document conversion models",
        "synthetic data",
        "self-improvement",
        "annotation quality",
        "POINTS-1.5",
        "POINTS-Reader"
      ]
    },
    "publishedAt": "2025-09-01T03:54:18.000Z",
    "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion",
    "summary": "High-quality labeled data is essential for training accurate document\nconversion models, particularly in domains with complex formats such as tables,\nformulas, and multi-column text. However, manual annotation is both costly and\ntime-consuming, while automatic labeling using existing models often lacks\naccuracy in handling such challenging scenarios. Consequently, training student\nmodels by distilling outputs from teacher models can significantly limit their\nperformance in real-world applications. In this paper, we propose a fully\nautomated, distillation-free framework comprising two stages for constructing\nhigh-quality document extraction datasets and models capable of handling\ndiverse document formats and layouts. In the first stage, we introduce a method\nfor generating large-scale, diverse synthetic data, which enables a model to\nextract key elements in a unified format with strong initial performance. In\nthe second stage, we present a self-improvement approach that further adapts\nthe model, initially trained on synthetic data, to real-world documents.\nSpecifically, we first use the fine-tuned model to annotate real documents,\nthen apply a suite of filtering strategies to verify annotation quality, and\nfinally retrain the model on the verified dataset. By iteratively repeating\nthis process, we progressively enhance both the model's conversion capabilities\nand the quality of the generated data. We train a public POINTS-1.5 model to\nobtain POINTS-Reader, which surpasses many existing public and proprietary\nmodels of comparable or larger size. Our model is available at\nhttps://github.com/Tencent/POINTS-Reader.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d47a7a508a6313e33faedd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg",
      "fullname": "Yuan Liu",
      "name": "YuanLiuuuuuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02544",
      "authors": [
        {
          "_id": "68b7c7ff295f15ff609113ed",
          "name": "Haoming Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113ee",
          "name": "Haoyang Zou",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113ef",
          "name": "Huatong Song",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f0",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f1",
          "name": "Junjie Fang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f2",
          "name": "Junting Lu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f3",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f4",
          "name": "Qinyu Luo",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f5",
          "name": "Shihao Liang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f6",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f7",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f8",
          "name": "Yining Ye",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113f9",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113fa",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113fb",
          "name": "Yuxin Song",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113fc",
          "name": "Zhiyong Wu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113fd",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113fe",
          "name": "Chen Dun",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff609113ff",
          "name": "Chong Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911400",
          "name": "Fuxing Leng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911401",
          "name": "Hanbin Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911402",
          "name": "Hao Yu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911403",
          "name": "Haobin Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911404",
          "name": "Hongyi Guo",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911405",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911406",
          "name": "Jingjia Huang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911407",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911408",
          "name": "Kaiyu Shi",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911409",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140a",
          "name": "Peiyao Zhao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140b",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140c",
          "name": "Qinghao Ye",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140d",
          "name": "Renjie Zheng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140e",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091140f",
          "name": "Wen Heng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911410",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911411",
          "name": "Wenqian Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911412",
          "name": "Xiaobo Qin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911413",
          "name": "Yi Lin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911414",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911415",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911416",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911417",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911418",
          "name": "Xinchun Zhang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911419",
          "name": "Xujing Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141a",
          "name": "Yuanfan Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141b",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141c",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141d",
          "name": "Faming Wu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141e",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091141f",
          "name": "Jinlin Pang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911420",
          "name": "Li Han",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911421",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911422",
          "name": "Siyao Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911423",
          "name": "Songhua Cai",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911424",
          "name": "Wenqi Fu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911425",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911426",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911427",
          "name": "Bo Zhou",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911428",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911429",
          "name": "Jiajun Shi",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142a",
          "name": "Jiale Yang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142b",
          "name": "Jie Tang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142c",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142d",
          "name": "Taoran Lu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142e",
          "name": "Woyu Lin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091142f",
          "name": "Xiaokang Tong",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911430",
          "name": "Xinyao Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911431",
          "name": "Yichi Zhang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911432",
          "name": "Yu Miao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911433",
          "name": "Zhengxuan Jiang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911434",
          "name": "Zili Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911435",
          "name": "Ziyuan Zhao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911436",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911437",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911438",
          "name": "Feng Lin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911439",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143a",
          "name": "Haihua Yang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143b",
          "name": "Hangyu Guo",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143c",
          "name": "Hongda Zhu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143d",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143e",
          "name": "Junda Du",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091143f",
          "name": "Kai Cai",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911440",
          "name": "Kuanye Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911441",
          "name": "Lichen Yuan",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911442",
          "name": "Meilan Han",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911443",
          "name": "Minchao Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911444",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911445",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911446",
          "name": "Xiaobo Ma",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911447",
          "name": "Xiaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911448",
          "name": "Xiaolong Huang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911449",
          "name": "Xinjie Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144a",
          "name": "Yidi Du",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144b",
          "name": "Yilin Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144c",
          "name": "Yiwen Wang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144d",
          "name": "Zhaojian Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144e",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff6091144f",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911450",
          "name": "Chaolin Jin",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911451",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911452",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911453",
          "name": "Haoli Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911454",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911455",
          "name": "Qinghao Zhao",
          "hidden": false
        },
        {
          "_id": "68b7c7ff295f15ff60911456",
          "name": "Guang Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T17:44:45.000Z",
      "submittedOnDailyAt": "2025-09-03T03:16:36.062Z",
      "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64892d31cbda0d1cdb956897",
        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
        "isPro": false,
        "fullname": "Zehui Chen",
        "user": "lovesnowbest",
        "type": "user"
      },
      "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios.",
      "upvotes": 27,
      "discussionId": "68b7c7ff295f15ff60911457",
      "ai_summary": "UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "GUI",
        "data flywheel",
        "multi-turn RL",
        "hybrid GUI environment",
        "unified sandbox platform",
        "Online-Mind2Web",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld",
        "LMGame-Bench",
        "long-horizon information-seeking tasks",
        "software engineering benchmarks"
      ]
    },
    "publishedAt": "2025-09-02T13:44:45.000Z",
    "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
    "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64892d31cbda0d1cdb956897",
      "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
      "fullname": "Zehui Chen",
      "name": "lovesnowbest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01055",
      "authors": [
        {
          "_id": "68b7d137295f15ff60911475",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff60911476",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff60911477",
          "name": "Zhuofeng Li",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff60911478",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff60911479",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147a",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147b",
          "name": "Alex Su",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147c",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147d",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147e",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff6091147f",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "68b7d137295f15ff60911480",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/b9isKgdkpTlbjq2_X7mXc.png"
      ],
      "publishedAt": "2025-09-01T01:45:18.000Z",
      "submittedOnDailyAt": "2025-09-03T03:59:47.708Z",
      "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2times speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
      "upvotes": 25,
      "discussionId": "68b7d137295f15ff60911481",
      "githubRepo": "https://github.com/TIGER-AI-Lab/verl-tool",
      "ai_summary": "VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Agentic Reinforcement Learning with Tool use",
        "VerlTool",
        "upstream alignment",
        "VeRL",
        "standardized APIs",
        "asynchronous rollout execution",
        "multi-turn trajectories",
        "multi-modal observation tokens",
        "mathematical reasoning",
        "knowledge QA",
        "SQL generation",
        "visual reasoning",
        "web search",
        "software engineering",
        "modular plugin architecture"
      ],
      "githubStars": 358
    },
    "publishedAt": "2025-08-31T21:45:18.000Z",
    "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2times speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/b9isKgdkpTlbjq2_X7mXc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02208",
      "authors": [
        {
          "_id": "68b7ba3d295f15ff60911316",
          "name": "Baichuan-M2 Team",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911318",
          "name": "Chengfeng Dou",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911319",
          "name": "Chong Liu",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131a",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131b",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131c",
          "name": "Jiyuan Jia",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131d",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131e",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091131f",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911320",
          "name": "Shunya Dang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911321",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911322",
          "name": "Xiangrong Zeng",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911323",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911324",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911325",
          "name": "Da Pan",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911326",
          "name": "Fei Deng",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911327",
          "name": "Guangwei Ai",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911328",
          "name": "Guosheng Dong",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911329",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132a",
          "name": "Jinyang Tai",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132b",
          "name": "Jixiang Hong",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132c",
          "name": "Kai Lu",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132d",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132e",
          "name": "Peidong Guo",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff6091132f",
          "name": "Qian Ma",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911330",
          "name": "Rihui Xin",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911331",
          "name": "Shihui Yang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911332",
          "name": "Shusen Zhang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911333",
          "name": "Yichuan Mo",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911334",
          "name": "Zheng Liang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911335",
          "name": "Zhishou Zhang",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911336",
          "name": "Hengfu Cui",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911337",
          "name": "Zuyi Zhu",
          "hidden": false
        },
        {
          "_id": "68b7ba3d295f15ff60911338",
          "name": "Xiaochuan Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641c45c921964f8f6d451d16/B1K-fqXbuHaoSivgxjK3x.png"
      ],
      "publishedAt": "2025-09-02T11:23:35.000Z",
      "submittedOnDailyAt": "2025-09-03T04:34:24.083Z",
      "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
      "submittedOnDailyBy": {
        "_id": "641c45c921964f8f6d451d16",
        "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
        "isPro": false,
        "fullname": "FanYang",
        "user": "fairyang",
        "type": "user"
      },
      "summary": "As large language models (LLMs) advance in conversational and reasoning\ncapabilities, their practical application in healthcare has become a critical\nresearch focus. However, there is a notable gap between the performance of\nmedical LLMs on static benchmarks such as USMLE and their utility in real-world\nclinical decision-making. This discrepancy arises because traditional exams\nfail to capture the dynamic, interactive nature of medical consultations. To\naddress this challenge, we introduce a novel dynamic verification framework\nthat moves beyond static answer verifier, establishing a large-scale,\nhigh-fidelity interactive reinforcement learning system. Our framework\ncomprises two key components: a Patient Simulator that creates realistic\nclinical environments using de-identified medical records, and a Clinical\nRubrics Generator that dynamically produces multi-dimensional evaluation\nmetrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter\nmedical augmented reasoning model trained through a multi-stage reinforcement\nlearning strategy with an improved Group Relative Policy Optimization (GRPO)\nalgorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other\nopen-source models and most advanced closed-source counterparts, achieving a\nscore above 32 on the challenging HealthBench Hard benchmark-previously\nexceeded only by GPT-5. Our work demonstrates that robust dynamic verifier\nsystem is essential for aligning LLM capabilities with practical clinical\napplications, establishing a new Pareto front in the performance-parameter\ntrade-off for medical AI deployment.",
      "upvotes": 22,
      "discussionId": "68b7ba3d295f15ff60911339",
      "ai_summary": "A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "conversational capabilities",
        "reasoning capabilities",
        "healthcare",
        "medical LLMs",
        "USMLE",
        "real-world clinical decision-making",
        "dynamic verification framework",
        "interactive reinforcement learning",
        "Patient Simulator",
        "de-identified medical records",
        "Clinical Rubrics Generator",
        "multi-dimensional evaluation metrics",
        "Baichuan-M2",
        "32B-parameter",
        "medical augmented reasoning model",
        "multi-stage reinforcement learning",
        "Group Relative Policy Optimization",
        "GRPO",
        "HealthBench",
        "HealthBench Hard benchmark",
        "GPT-5",
        "Pareto front",
        "medical AI deployment"
      ]
    },
    "publishedAt": "2025-09-02T07:23:35.000Z",
    "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
    "summary": "As large language models (LLMs) advance in conversational and reasoning\ncapabilities, their practical application in healthcare has become a critical\nresearch focus. However, there is a notable gap between the performance of\nmedical LLMs on static benchmarks such as USMLE and their utility in real-world\nclinical decision-making. This discrepancy arises because traditional exams\nfail to capture the dynamic, interactive nature of medical consultations. To\naddress this challenge, we introduce a novel dynamic verification framework\nthat moves beyond static answer verifier, establishing a large-scale,\nhigh-fidelity interactive reinforcement learning system. Our framework\ncomprises two key components: a Patient Simulator that creates realistic\nclinical environments using de-identified medical records, and a Clinical\nRubrics Generator that dynamically produces multi-dimensional evaluation\nmetrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter\nmedical augmented reasoning model trained through a multi-stage reinforcement\nlearning strategy with an improved Group Relative Policy Optimization (GRPO)\nalgorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other\nopen-source models and most advanced closed-source counterparts, achieving a\nscore above 32 on the challenging HealthBench Hard benchmark-previously\nexceeded only by GPT-5. Our work demonstrates that robust dynamic verifier\nsystem is essential for aligning LLM capabilities with practical clinical\napplications, establishing a new Pareto front in the performance-parameter\ntrade-off for medical AI deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641c45c921964f8f6d451d16/B1K-fqXbuHaoSivgxjK3x.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02208.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641c45c921964f8f6d451d16",
      "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
      "fullname": "FanYang",
      "name": "fairyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02522",
      "authors": [
        {
          "_id": "68b7ad8a295f15ff609112bb",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112bc",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112bd",
          "name": "Ze Gong",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112be",
          "name": "Yukun Chen",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112bf",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112c0",
          "name": "Wanwei He",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112c1",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68b7ad8a295f15ff609112c2",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T17:22:46.000Z",
      "submittedOnDailyAt": "2025-09-03T02:03:35.135Z",
      "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
      "submittedOnDailyBy": {
        "_id": "6355eaf660c1b72f6269bc64",
        "avatarUrl": "/avatars/dd176b9d6db2ac63c19c3170566a3f35.svg",
        "isPro": false,
        "fullname": "Jiaming Li",
        "user": "Geaming",
        "type": "user"
      },
      "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose PACS, a\nnovel RLVR framework that achieves imPlicit Actor\nCritic coupling via a Supervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS.",
      "upvotes": 17,
      "discussionId": "68b7ad8a295f15ff609112c3",
      "githubRepo": "https://github.com/ritzz-ai/PACS",
      "ai_summary": "PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "large language models (LLMs)",
        "verifiable outcome rewards",
        "policy optimization",
        "sparse reward signals",
        "unstable policy gradient updates",
        "PACS",
        "Implicit Actor Critic coupling",
        "supervised learning",
        "score function",
        "cross-entropy loss",
        "policy gradient update",
        "AIME 2025",
        "pass@256",
        "PPO",
        "GRPO"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-09-02T13:22:46.000Z",
    "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
    "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose PACS, a\nnovel RLVR framework that achieves imPlicit Actor\nCritic coupling via a Supervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02522.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6355eaf660c1b72f6269bc64",
      "avatarUrl": "/avatars/dd176b9d6db2ac63c19c3170566a3f35.svg",
      "fullname": "Jiaming Li",
      "name": "Geaming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01563",
      "authors": [
        {
          "_id": "68b7bec1295f15ff60911384",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911385",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911386",
          "name": "Boyang Ding",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911387",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911388",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911389",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138a",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138b",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138c",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138d",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138e",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091138f",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911390",
          "name": "Guowang Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911391",
          "name": "Han Shen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911392",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911393",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911394",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911395",
          "name": "Hengrui Ju",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911396",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911397",
          "name": "Jiangxia Cao",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911398",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff60911399",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139e",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff6091139f",
          "name": "Qiang Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a0",
          "name": "Ruitao Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a1",
          "name": "Sen Na",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a2",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a3",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a4",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a5",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a6",
          "name": "Tingting Gao",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a7",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a8",
          "name": "Wei Yuan",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113a9",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113aa",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113ab",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113ac",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113ad",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113ae",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113af",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b0",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b1",
          "name": "Zhixin Ling",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b2",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b3",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b4",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b5",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b6",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b7",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b8",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113b9",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113ba",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113bb",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113bc",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113bd",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113be",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "68b7bec1295f15ff609113bf",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T15:46:58.000Z",
      "submittedOnDailyAt": "2025-09-03T02:36:34.622Z",
      "title": "Kwai Keye-VL 1.5 Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In recent years, the development of Large Language Models (LLMs) has\nsignificantly advanced, extending their capabilities to multimodal tasks\nthrough Multimodal Large Language Models (MLLMs). However, video understanding\nremains a challenging area due to the dynamic and information-dense nature of\nvideos. Existing models struggle with the trade-off between spatial resolution\nand temporal coverage when processing video content. We present Keye-VL-1.5,\nwhich addresses fundamental challenges in video comprehension through three key\ninnovations. First, we introduce a novel Slow-Fast video encoding strategy that\ndynamically allocates computational resources based on inter-frame similarity,\nprocessing key frames with significant visual changes at higher resolution\n(Slow pathway) while handling relatively static frames with increased temporal\ncoverage at lower resolution (Fast pathway). Second, we implement a progressive\nfour-stage pre-training methodology that systematically extends the model's\ncontext length from 8K to 128K tokens, enabling processing of longer videos and\nmore complex visual content. Third, we develop a comprehensive post-training\npipeline focusing on reasoning enhancement and human preference alignment,\nincorporating a 5-step chain-of-thought data construction process, iterative\nGSPO-based reinforcement learning with progressive prompt hinting for difficult\ncases, and alignment training. Through extensive evaluation on public\nbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates\nsignificant improvements over existing models, particularly excelling in video\nunderstanding tasks while maintaining competitive performance on general\nmultimodal benchmarks.",
      "upvotes": 17,
      "discussionId": "68b7bec2295f15ff609113c0",
      "ai_summary": "Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.",
      "ai_keywords": [
        "Slow-Fast video encoding strategy",
        "progressive four-stage pre-training",
        "chain-of-thought data construction",
        "GSPO-based reinforcement learning",
        "alignment training"
      ]
    },
    "publishedAt": "2025-09-01T11:46:58.000Z",
    "title": "Kwai Keye-VL 1.5 Technical Report",
    "summary": "In recent years, the development of Large Language Models (LLMs) has\nsignificantly advanced, extending their capabilities to multimodal tasks\nthrough Multimodal Large Language Models (MLLMs). However, video understanding\nremains a challenging area due to the dynamic and information-dense nature of\nvideos. Existing models struggle with the trade-off between spatial resolution\nand temporal coverage when processing video content. We present Keye-VL-1.5,\nwhich addresses fundamental challenges in video comprehension through three key\ninnovations. First, we introduce a novel Slow-Fast video encoding strategy that\ndynamically allocates computational resources based on inter-frame similarity,\nprocessing key frames with significant visual changes at higher resolution\n(Slow pathway) while handling relatively static frames with increased temporal\ncoverage at lower resolution (Fast pathway). Second, we implement a progressive\nfour-stage pre-training methodology that systematically extends the model's\ncontext length from 8K to 128K tokens, enabling processing of longer videos and\nmore complex visual content. Third, we develop a comprehensive post-training\npipeline focusing on reasoning enhancement and human preference alignment,\nincorporating a 5-step chain-of-thought data construction process, iterative\nGSPO-based reinforcement learning with progressive prompt hinting for difficult\ncases, and alignment training. Through extensive evaluation on public\nbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates\nsignificant improvements over existing models, particularly excelling in video\nunderstanding tasks while maintaining competitive performance on general\nmultimodal benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02534",
      "authors": [
        {
          "_id": "68b7b787295f15ff609112f8",
          "name": "Tianjian Li",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112f9",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112fa",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112fb",
          "name": "Swarnadeep Saha",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112fc",
          "name": "Daniel Khashabi",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112fd",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112fe",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "68b7b787295f15ff609112ff",
          "name": "Tianlu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T17:38:47.000Z",
      "submittedOnDailyAt": "2025-09-03T02:06:07.851Z",
      "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
      "submittedOnDailyBy": {
        "_id": "62bc72d464b7a623f3a242e9",
        "avatarUrl": "/avatars/db79848dc78a5c4927e27de4426c3b47.svg",
        "isPro": false,
        "fullname": "Tianjian Li",
        "user": "dogtooth",
        "type": "user"
      },
      "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses.",
      "upvotes": 8,
      "discussionId": "68b7b788295f15ff60911300",
      "ai_summary": "DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.",
      "ai_keywords": [
        "Diversity-Aware Reinforcement Learning",
        "DARLING",
        "learned partition function",
        "semantic diversity",
        "quality reward",
        "online reinforcement learning",
        "non-verifiable tasks",
        "verifiable tasks",
        "instruction following",
        "creative writing",
        "competition math",
        "pass@1",
        "pass@k",
        "exploration"
      ]
    },
    "publishedAt": "2025-09-02T13:38:47.000Z",
    "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
    "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc72d464b7a623f3a242e9",
      "avatarUrl": "/avatars/db79848dc78a5c4927e27de4426c3b47.svg",
      "fullname": "Tianjian Li",
      "name": "dogtooth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02460",
      "authors": [
        {
          "_id": "68b7e0a3295f15ff609114ba",
          "name": "Shuzhou Yang",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114bb",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114bc",
          "name": "Xiaodong Cun",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114bd",
          "name": "Guangzhi Wang",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114be",
          "name": "Lingen Li",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114bf",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "68b7e0a3295f15ff609114c0",
          "name": "Jian Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T16:10:13.000Z",
      "submittedOnDailyAt": "2025-09-03T05:03:44.984Z",
      "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
      "submittedOnDailyBy": {
        "_id": "66d8284b6bddfb32e77ddafb",
        "avatarUrl": "/avatars/edd371ecef6e7d58b945a30bbc6095ee.svg",
        "isPro": false,
        "fullname": "Xiaoyu Li",
        "user": "Xiaoyu521",
        "type": "user"
      },
      "summary": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
      "upvotes": 8,
      "discussionId": "68b7e0a3295f15ff609114c1",
      "projectPage": "https://gencompositor.github.io/",
      "githubRepo": "https://github.com/TencentARC/GenCompositor",
      "ai_summary": "A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.",
      "ai_keywords": [
        "Diffusion Transformer",
        "DiT",
        "masked token injection",
        "full self-attention",
        "foreground augmentation",
        "Extended Rotary Position Embedding",
        "ERoPE",
        "generative video compositing"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-09-02T12:10:13.000Z",
    "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
    "summary": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02460.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66d8284b6bddfb32e77ddafb",
      "avatarUrl": "/avatars/edd371ecef6e7d58b945a30bbc6095ee.svg",
      "fullname": "Xiaoyu Li",
      "name": "Xiaoyu521",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01644",
      "authors": [
        {
          "_id": "68b7beee295f15ff609113c2",
          "name": "Yanqing Liu",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c3",
          "name": "Xianhang Li",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c4",
          "name": "Letian Zhang",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c5",
          "name": "Zirui Wang",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c6",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c7",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "68b7beee295f15ff609113c8",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T17:38:21.000Z",
      "submittedOnDailyAt": "2025-09-03T02:40:07.039Z",
      "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning",
      "submittedOnDailyBy": {
        "_id": "654a771a2d2fcd6bf27ff0de",
        "avatarUrl": "/avatars/6312b23c642009f13c7993ce4658d06d.svg",
        "isPro": false,
        "fullname": "Liuyanqing",
        "user": "Yanqing0327",
        "type": "user"
      },
      "summary": "This paper provides a simplification on OpenVision's architecture and loss\ndesign for enhancing its training efficiency. Following the prior\nvision-language pretraining works CapPa and AIMv2, as well as modern multimodal\ndesigns like LLaVA, our changes are straightforward: we remove the text encoder\n(and therefore the contrastive loss), retaining only the captioning loss as a\npurely generative training signal. We name this new version OpenVision 2. The\ninitial results are promising: despite this simplification, OpenVision 2\ncompetitively matches the original model's performance on a broad set of\nmultimodal benchmarks while substantially cutting both training time and memory\nconsumption. For example, with ViT-L/14, it reduces training time by about 1.5x\n(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,\nequivalently allowing the maximum batch size to grow from 2k to 8k). This\nsuperior training efficiency also allows us to scale far beyond the largest\nvision encoder used in OpenVision, reaching more than 1 billion parameters. We\nhold a strong belief that this lightweight, generative-only paradigm is\ncompelling for future vision encoder development in multimodal foundation\nmodels.",
      "upvotes": 8,
      "discussionId": "68b7beef295f15ff609113c9",
      "projectPage": "https://ucsc-vlaa.github.io/OpenVision2/",
      "ai_summary": "OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.",
      "ai_keywords": [
        "OpenVision",
        "text encoder",
        "contrastive loss",
        "captioning loss",
        "multimodal benchmarks",
        "training time",
        "memory consumption",
        "ViT-L/14",
        "batch size",
        "vision encoder",
        "multimodal foundation models"
      ]
    },
    "publishedAt": "2025-09-01T13:38:21.000Z",
    "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning",
    "summary": "This paper provides a simplification on OpenVision's architecture and loss\ndesign for enhancing its training efficiency. Following the prior\nvision-language pretraining works CapPa and AIMv2, as well as modern multimodal\ndesigns like LLaVA, our changes are straightforward: we remove the text encoder\n(and therefore the contrastive loss), retaining only the captioning loss as a\npurely generative training signal. We name this new version OpenVision 2. The\ninitial results are promising: despite this simplification, OpenVision 2\ncompetitively matches the original model's performance on a broad set of\nmultimodal benchmarks while substantially cutting both training time and memory\nconsumption. For example, with ViT-L/14, it reduces training time by about 1.5x\n(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,\nequivalently allowing the maximum batch size to grow from 2k to 8k). This\nsuperior training efficiency also allows us to scale far beyond the largest\nvision encoder used in OpenVision, reaching more than 1 billion parameters. We\nhold a strong belief that this lightweight, generative-only paradigm is\ncompelling for future vision encoder development in multimodal foundation\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01644.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654a771a2d2fcd6bf27ff0de",
      "avatarUrl": "/avatars/6312b23c642009f13c7993ce4658d06d.svg",
      "fullname": "Liuyanqing",
      "name": "Yanqing0327",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02040",
      "authors": [
        {
          "_id": "68b7c566295f15ff609113e8",
          "name": "Guangzeng Han",
          "hidden": false
        },
        {
          "_id": "68b7c566295f15ff609113e9",
          "name": "Weisi Liu",
          "hidden": false
        },
        {
          "_id": "68b7c566295f15ff609113ea",
          "name": "Xiaolei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T07:35:20.000Z",
      "submittedOnDailyAt": "2025-09-03T03:06:01.701Z",
      "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation",
      "submittedOnDailyBy": {
        "_id": "63337b0073c07e8aebb3df0d",
        "avatarUrl": "/avatars/f0e55c3c61538d9521a28eda78cf31db.svg",
        "isPro": false,
        "fullname": "GUANGZENG HAN",
        "user": "kwangju",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications.",
      "upvotes": 7,
      "discussionId": "68b7c566295f15ff609113eb",
      "ai_summary": "Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.",
      "ai_keywords": [
        "Genetic Prompt",
        "genetic algorithms",
        "LLMs",
        "semantic text attributes",
        "gene sequences",
        "crossover",
        "mutation",
        "active learning",
        "synthetic data generation",
        "NLP tasks",
        "class-imbalanced scenarios"
      ]
    },
    "publishedAt": "2025-09-02T03:35:20.000Z",
    "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation",
    "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63337b0073c07e8aebb3df0d",
      "avatarUrl": "/avatars/f0e55c3c61538d9521a28eda78cf31db.svg",
      "fullname": "GUANGZENG HAN",
      "name": "kwangju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01363",
      "authors": [
        {
          "_id": "68b7e1c3295f15ff609114cd",
          "name": "Mohammad Zbeeb",
          "hidden": false
        },
        {
          "_id": "68b7e1c3295f15ff609114ce",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "68b7e1c3295f15ff609114cf",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/6l5eLx-fi_XKIhzhUVGmU.png"
      ],
      "publishedAt": "2025-09-01T11:04:51.000Z",
      "submittedOnDailyAt": "2025-09-03T05:07:51.524Z",
      "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": true,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: v_{reason} = theta_{GRPO} - theta_{SFT}. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments.",
      "upvotes": 7,
      "discussionId": "68b7e1c3295f15ff609114d0",
      "ai_summary": "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "group relative policy optimization",
        "reasoning vector",
        "instruction-tuned models",
        "GSM8K",
        "HumanEval",
        "SciQ",
        "BigBenchHard",
        "adversarial conditions"
      ]
    },
    "publishedAt": "2025-09-01T07:04:51.000Z",
    "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic",
    "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: v_{reason} = theta_{GRPO} - theta_{SFT}. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/6l5eLx-fi_XKIhzhUVGmU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01360",
      "authors": [
        {
          "_id": "68b7bc15295f15ff6091134b",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff6091134c",
          "name": "Zheng Jiang",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff6091134d",
          "name": "Chengyu Fang",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff6091134e",
          "name": "Heng Guo",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff6091134f",
          "name": "Yan-Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff60911350",
          "name": "Jiaqi Qu",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff60911351",
          "name": "Le Lu",
          "hidden": false
        },
        {
          "_id": "68b7bc15295f15ff60911352",
          "name": "Minfeng Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T10:59:39.000Z",
      "submittedOnDailyAt": "2025-09-03T02:26:05.504Z",
      "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision",
      "submittedOnDailyBy": {
        "_id": "631b9ff5824f2502e3557c7e",
        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
        "isPro": true,
        "fullname": "liu",
        "user": "che111",
        "type": "user"
      },
      "summary": "Medical image retrieval is essential for clinical decision-making and\ntranslational research, relying on discriminative visual representations. Yet,\ncurrent methods remain fragmented, relying on separate architectures and\ntraining strategies for 2D, 3D, and video-based medical data. This\nmodality-specific design hampers scalability and inhibits the development of\nunified representations. To enable unified learning, we curate a large-scale\nhybrid-modality dataset comprising 867,653 medical imaging samples, including\n2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging\nthis dataset, we train M3Ret, a unified visual encoder without any\nmodality-specific customization. It successfully learns transferable\nrepresentations using both generative (MAE) and contrastive (SimDINO)\nself-supervised learning (SSL) paradigms. Our approach sets a new\nstate-of-the-art in zero-shot image-to-image retrieval across all individual\nmodalities, surpassing strong baselines such as DINOv3 and the text-supervised\nBMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired\ndata, and the model generalizes to unseen MRI tasks, despite never observing\nMRI during pretraining, demonstrating the generalizability of purely visual\nself-supervision to unseen modalities. Comprehensive analyses further validate\nthe scalability of our framework across model and data sizes. These findings\ndeliver a promising signal to the medical imaging community, positioning M3Ret\nas a step toward foundation models for visual SSL in multimodal medical image\nunderstanding.",
      "upvotes": 7,
      "discussionId": "68b7bc15295f15ff60911353",
      "ai_summary": "M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.",
      "ai_keywords": [
        "visual encoder",
        "self-supervised learning",
        "SSL",
        "generative",
        "contrastive",
        "MAE",
        "SimDINO",
        "zero-shot image-to-image retrieval",
        "cross-modal alignment",
        "foundation models",
        "multimodal medical image understanding"
      ]
    },
    "publishedAt": "2025-09-01T06:59:39.000Z",
    "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision",
    "summary": "Medical image retrieval is essential for clinical decision-making and\ntranslational research, relying on discriminative visual representations. Yet,\ncurrent methods remain fragmented, relying on separate architectures and\ntraining strategies for 2D, 3D, and video-based medical data. This\nmodality-specific design hampers scalability and inhibits the development of\nunified representations. To enable unified learning, we curate a large-scale\nhybrid-modality dataset comprising 867,653 medical imaging samples, including\n2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging\nthis dataset, we train M3Ret, a unified visual encoder without any\nmodality-specific customization. It successfully learns transferable\nrepresentations using both generative (MAE) and contrastive (SimDINO)\nself-supervised learning (SSL) paradigms. Our approach sets a new\nstate-of-the-art in zero-shot image-to-image retrieval across all individual\nmodalities, surpassing strong baselines such as DINOv3 and the text-supervised\nBMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired\ndata, and the model generalizes to unseen MRI tasks, despite never observing\nMRI during pretraining, demonstrating the generalizability of purely visual\nself-supervision to unseen modalities. Comprehensive analyses further validate\nthe scalability of our framework across model and data sizes. These findings\ndeliver a promising signal to the medical imaging community, positioning M3Ret\nas a step toward foundation models for visual SSL in multimodal medical image\nunderstanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00244",
      "authors": [
        {
          "_id": "68b7ba7a295f15ff6091133b",
          "name": "Peter Belcak",
          "hidden": false
        },
        {
          "_id": "68b7ba7a295f15ff6091133c",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e8cccddd2c4effdd6283cf/dZVoR5UiXW8fg65nh5C-c.png"
      ],
      "publishedAt": "2025-08-29T21:22:19.000Z",
      "submittedOnDailyAt": "2025-09-03T02:20:50.120Z",
      "title": "Universal Deep Research: Bring Your Own Model and Strategy",
      "submittedOnDailyBy": {
        "_id": "63e8cccddd2c4effdd6283cf",
        "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
        "isPro": false,
        "fullname": "Peter Belcak",
        "user": "pbelcak",
        "type": "user"
      },
      "summary": "Deep research tools are among the most impactful and most commonly\nencountered agentic systems today. We observe, however, that each deep research\nagent introduced so far is hard-coded to carry out a particular research\nstrategy using a fixed choice of tools. We introduce Universal Deep Research\n(UDR), a generalist agentic system that wraps around any language model and\nenables the user to create, edit, and refine their own entirely custom deep\nresearch strategies without any need for additional training or finetuning. To\nshowcase the generality of our system, we equip UDR with example minimal,\nexpansive, and intensive research strategies, and provide a user interface to\nfacilitate experimentation with the system.",
      "upvotes": 4,
      "discussionId": "68b7ba7a295f15ff6091133d",
      "ai_summary": "Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.",
      "ai_keywords": [
        "Universal Deep Research",
        "UDR",
        "language model",
        "research strategies",
        "customization"
      ]
    },
    "publishedAt": "2025-08-29T17:22:19.000Z",
    "title": "Universal Deep Research: Bring Your Own Model and Strategy",
    "summary": "Deep research tools are among the most impactful and most commonly\nencountered agentic systems today. We observe, however, that each deep research\nagent introduced so far is hard-coded to carry out a particular research\nstrategy using a fixed choice of tools. We introduce Universal Deep Research\n(UDR), a generalist agentic system that wraps around any language model and\nenables the user to create, edit, and refine their own entirely custom deep\nresearch strategies without any need for additional training or finetuning. To\nshowcase the generality of our system, we equip UDR with example minimal,\nexpansive, and intensive research strategies, and provide a user interface to\nfacilitate experimentation with the system.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e8cccddd2c4effdd6283cf/dZVoR5UiXW8fg65nh5C-c.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00244.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e8cccddd2c4effdd6283cf",
      "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
      "fullname": "Peter Belcak",
      "name": "pbelcak",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01052",
      "authors": [
        {
          "_id": "68b7c8db295f15ff60911459",
          "name": "Jaewoo Ahn",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145a",
          "name": "Junseo Kim",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145b",
          "name": "Heeseung Yun",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145c",
          "name": "Jaehyeon Son",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145d",
          "name": "Dongmin Park",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145e",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "68b7c8db295f15ff6091145f",
          "name": "Gunhee Kim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64bb081c01f1983a863654dc/J79LOX_pbVsBCnlcRrw0U.mp4"
      ],
      "publishedAt": "2025-09-01T01:33:16.000Z",
      "submittedOnDailyAt": "2025-09-03T03:27:31.059Z",
      "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
      "submittedOnDailyBy": {
        "_id": "64bb081c01f1983a863654dc",
        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
        "isPro": false,
        "fullname": "Jaewoo Ahn",
        "user": "ahnpersie",
        "type": "user"
      },
      "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide.",
      "upvotes": 3,
      "discussionId": "68b7c8dc295f15ff60911460",
      "projectPage": "https://ahnjaewoo.github.io/flashadventure/",
      "githubRepo": "https://github.com/ahnjaewoo/FlashAdventure",
      "ai_summary": "FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.",
      "ai_keywords": [
        "LLMs",
        "GUI agents",
        "FlashAdventure",
        "CUA-as-a-Judge",
        "COAST",
        "long-term clue memory",
        "observation-behavior gap"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-08-31T21:33:16.000Z",
    "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
    "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64bb081c01f1983a863654dc/J79LOX_pbVsBCnlcRrw0U.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01052.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bb081c01f1983a863654dc",
      "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
      "fullname": "Jaewoo Ahn",
      "name": "ahnpersie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01984",
      "authors": [
        {
          "_id": "68b7b78e295f15ff60911302",
          "name": "Quan Dao",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911303",
          "name": "Xiaoxiao He",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911304",
          "name": "Ligong Han",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911305",
          "name": "Ngan Hoai Nguyen",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911306",
          "name": "Amin Heyrani Nobar",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911307",
          "name": "Faez Ahmed",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911308",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff60911309",
          "name": "Viet Anh Nguyen",
          "hidden": false
        },
        {
          "_id": "68b7b78e295f15ff6091130a",
          "name": "Dimitris Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T06:01:52.000Z",
      "submittedOnDailyAt": "2025-09-03T03:06:16.161Z",
      "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
      "submittedOnDailyBy": {
        "_id": "63e083e6f351dc0745745d17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
        "isPro": false,
        "fullname": "Quan Dao",
        "user": "quandao10",
        "type": "user"
      },
      "summary": "Visual autoregressive models (VAR) have recently emerged as a promising class\nof generative models, achieving performance comparable to diffusion models in\ntext-to-image generation tasks. While conditional generation has been widely\nexplored, the ability to perform prompt-guided image editing without additional\ntraining is equally critical, as it supports numerous practical real-world\napplications. This paper investigates the text-to-image editing capabilities of\nVAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise\ninversion-based editing technique designed explicitly for VAR models. VARIN\nleverages a novel pseudo-inverse function for argmax sampling, named\nLocation-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These\ninverse noises enable precise reconstruction of the source image and facilitate\ntargeted, controllable edits aligned with textual prompts. Extensive\nexperiments demonstrate that VARIN effectively modifies source images according\nto specified prompts while significantly preserving the original background and\nstructural details, thus validating its efficacy as a practical editing\napproach.",
      "upvotes": 2,
      "discussionId": "68b7b78f295f15ff6091130b",
      "ai_summary": "VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.",
      "ai_keywords": [
        "Visual autoregressive models",
        "VAR",
        "diffusion models",
        "text-to-image generation",
        "prompt-guided image editing",
        "Visual AutoRegressive Inverse Noise",
        "VARIN",
        "Location-aware Argmax Inversion",
        "LAI",
        "inverse Gumbel noises",
        "argmax sampling"
      ]
    },
    "publishedAt": "2025-09-02T02:01:52.000Z",
    "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
    "summary": "Visual autoregressive models (VAR) have recently emerged as a promising class\nof generative models, achieving performance comparable to diffusion models in\ntext-to-image generation tasks. While conditional generation has been widely\nexplored, the ability to perform prompt-guided image editing without additional\ntraining is equally critical, as it supports numerous practical real-world\napplications. This paper investigates the text-to-image editing capabilities of\nVAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise\ninversion-based editing technique designed explicitly for VAR models. VARIN\nleverages a novel pseudo-inverse function for argmax sampling, named\nLocation-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These\ninverse noises enable precise reconstruction of the source image and facilitate\ntargeted, controllable edits aligned with textual prompts. Extensive\nexperiments demonstrate that VARIN effectively modifies source images according\nto specified prompts while significantly preserving the original background and\nstructural details, thus validating its efficacy as a practical editing\napproach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e083e6f351dc0745745d17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
      "fullname": "Quan Dao",
      "name": "quandao10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02379",
      "authors": [
        {
          "_id": "68b7bad9295f15ff60911344",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "68b7bad9295f15ff60911345",
          "name": "Yizhou Wu",
          "hidden": false
        },
        {
          "_id": "68b7bad9295f15ff60911346",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "68b7bad9295f15ff60911347",
          "name": "Mingzhe Hu",
          "hidden": false
        },
        {
          "_id": "68b7bad9295f15ff60911348",
          "name": "Xiaofeng Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T14:44:43.000Z",
      "submittedOnDailyAt": "2025-09-03T02:20:01.649Z",
      "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting\nDINOv3 to medical segmentation. We first revisit plain ViTs and design a simple\nand effective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M\naxial CT slices, using a multi-stage DINOv3 recipe to learn robust dense\nfeatures. MedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.",
      "upvotes": 1,
      "discussionId": "68b7bad9295f15ff60911349",
      "githubRepo": "https://github.com/ricklisz/MedDINOv3",
      "ai_summary": "MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.",
      "ai_keywords": [
        "ViT",
        "CNN",
        "vision foundation models",
        "DINOv3",
        "multi-scale token aggregation",
        "domain-adaptive pretraining",
        "CT-3M",
        "dense features",
        "medical image segmentation"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-02T10:44:43.000Z",
    "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
    "summary": "Accurate segmentation of organs and tumors in CT and MRI scans is essential\nfor diagnosis, treatment planning, and disease monitoring. While deep learning\nhas advanced automated segmentation, most models remain task-specific, lacking\ngeneralizability across modalities and institutions. Vision foundation models\n(FMs) pretrained on billion-scale natural images offer powerful and\ntransferable representations. However, adapting them to medical imaging faces\ntwo key challenges: (1) the ViT backbone of most foundation models still\nunderperform specialized CNNs on medical image segmentation, and (2) the large\ndomain gap between natural and medical images limits transferability. We\nintroduce MedDINOv3, a simple and effective framework for adapting\nDINOv3 to medical segmentation. We first revisit plain ViTs and design a simple\nand effective architecture with multi-scale token aggregation. Then, we perform\ndomain-adaptive pretraining on CT-3M, a curated collection of 3.87M\naxial CT slices, using a multi-stage DINOv3 recipe to learn robust dense\nfeatures. MedDINOv3 matches or exceeds state-of-the-art performance across four\nsegmentation benchmarks, demonstrating the potential of vision foundation\nmodels as unified backbones for medical image segmentation. The code is\navailable at https://github.com/ricklisz/MedDINOv3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02379.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02133",
      "authors": [
        {
          "_id": "68b7e5ae295f15ff609114e0",
          "name": "Snehasis Mukhopadhyay",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e1",
          "name": "Aryan Kasat",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e2",
          "name": "Shivam Dubey",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e3",
          "name": "Rahul Karthikeyan",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e4",
          "name": "Dhruv Sood",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e5",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e6",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "68b7e5ae295f15ff609114e7",
          "name": "Amitava Das",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T09:33:30.000Z",
      "submittedOnDailyAt": "2025-09-03T05:22:47.900Z",
      "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/",
      "upvotes": 1,
      "discussionId": "68b7e5ae295f15ff609114e8",
      "ai_summary": "Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "Small Language Model",
        "SLM",
        "Constitution-Aware Decoding Layer",
        "speculative decoding",
        "fairness",
        "neutrality",
        "inclusion",
        "bias mitigation",
        "AI Constitution of India",
        "fairness-by-speculation"
      ]
    },
    "publishedAt": "2025-09-02T05:33:30.000Z",
    "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models",
    "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01610",
      "authors": [
        {
          "_id": "68b7be87295f15ff6091137d",
          "name": "Jefferson Hernandez",
          "hidden": false
        },
        {
          "_id": "68b7be87295f15ff6091137e",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "68b7be87295f15ff6091137f",
          "name": "Simon Jenni",
          "hidden": false
        },
        {
          "_id": "68b7be87295f15ff60911380",
          "name": "Vicente Ordonez",
          "hidden": false
        },
        {
          "_id": "68b7be87295f15ff60911381",
          "name": "Kushal Kafle",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T16:43:48.000Z",
      "submittedOnDailyAt": "2025-09-03T02:35:40.721Z",
      "title": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
      "upvotes": 1,
      "discussionId": "68b7be88295f15ff60911382",
      "ai_summary": "A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.",
      "ai_keywords": [
        "Large Vision and Language Models",
        "LVLMs",
        "Panel-of-Peers",
        "collaborative learning",
        "peer review",
        "iterative self-improvement",
        "benchmarks"
      ]
    },
    "publishedAt": "2025-09-01T12:43:48.000Z",
    "title": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
    "summary": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01250",
      "authors": [
        {
          "_id": "68b7ba97295f15ff6091133f",
          "name": "Xiangdong Zhang",
          "hidden": false
        },
        {
          "_id": "68b7ba97295f15ff60911340",
          "name": "Shaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "68b7ba97295f15ff60911341",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656d8d4b1f8d9b618de91369/oxyPkp9MWgCc5tPYs3GiZ.png"
      ],
      "publishedAt": "2025-09-01T08:42:17.000Z",
      "submittedOnDailyAt": "2025-09-03T02:21:39.408Z",
      "title": "Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
      "submittedOnDailyBy": {
        "_id": "656d8d4b1f8d9b618de91369",
        "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
        "isPro": false,
        "fullname": "Xiangdong Zhang",
        "user": "aHapBean",
        "type": "user"
      },
      "summary": "Point cloud learning, especially in a self-supervised way without manual\nlabels, has gained growing attention in both vision and learning communities\ndue to its potential utility in a wide range of applications. Most existing\ngenerative approaches for point cloud self-supervised learning focus on\nrecovering masked points from visible ones within a single view. Recognizing\nthat a two-view pre-training paradigm inherently introduces greater diversity\nand variance, it may thus enable more challenging and informative pre-training.\nInspired by this, we explore the potential of two-view learning in this domain.\nIn this paper, we propose Point-PQAE, a cross-reconstruction generative\nparadigm that first generates two decoupled point clouds/views and then\nreconstructs one from the other. To achieve this goal, we develop a crop\nmechanism for point cloud view generation for the first time and further\npropose a novel positional encoding to represent the 3D relative position\nbetween the two decoupled views. The cross-reconstruction significantly\nincreases the difficulty of pre-training compared to self-reconstruction, which\nenables our method to surpass previous single-modal self-reconstruction methods\nin 3D self-supervised learning. Specifically, it outperforms the\nself-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three\nvariants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is\navailable at https://github.com/aHapBean/Point-PQAE.",
      "upvotes": 1,
      "discussionId": "68b7ba97295f15ff60911342",
      "githubRepo": "https://github.com/aHapBean/Point-PQAE",
      "ai_summary": "Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.",
      "ai_keywords": [
        "point cloud learning",
        "self-supervised learning",
        "generative approaches",
        "two-view pre-training",
        "cross-reconstruction",
        "crop mechanism",
        "positional encoding",
        "3D relative position",
        "ScanObjectNN",
        "Mlp-Linear evaluation protocol"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-01T04:42:17.000Z",
    "title": "Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
    "summary": "Point cloud learning, especially in a self-supervised way without manual\nlabels, has gained growing attention in both vision and learning communities\ndue to its potential utility in a wide range of applications. Most existing\ngenerative approaches for point cloud self-supervised learning focus on\nrecovering masked points from visible ones within a single view. Recognizing\nthat a two-view pre-training paradigm inherently introduces greater diversity\nand variance, it may thus enable more challenging and informative pre-training.\nInspired by this, we explore the potential of two-view learning in this domain.\nIn this paper, we propose Point-PQAE, a cross-reconstruction generative\nparadigm that first generates two decoupled point clouds/views and then\nreconstructs one from the other. To achieve this goal, we develop a crop\nmechanism for point cloud view generation for the first time and further\npropose a novel positional encoding to represent the 3D relative position\nbetween the two decoupled views. The cross-reconstruction significantly\nincreases the difficulty of pre-training compared to self-reconstruction, which\nenables our method to surpass previous single-modal self-reconstruction methods\nin 3D self-supervised learning. Specifically, it outperforms the\nself-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three\nvariants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is\navailable at https://github.com/aHapBean/Point-PQAE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656d8d4b1f8d9b618de91369/oxyPkp9MWgCc5tPYs3GiZ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01250.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656d8d4b1f8d9b618de91369",
      "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
      "fullname": "Xiangdong Zhang",
      "name": "aHapBean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00581",
      "authors": [
        {
          "_id": "68b7e592295f15ff609114db",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "68b7e592295f15ff609114dc",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "68b7e592295f15ff609114dd",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-30T18:27:12.000Z",
      "submittedOnDailyAt": "2025-09-03T05:24:12.555Z",
      "title": "SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Converting natural language queries into SQL queries is a crucial challenge\nin both industry and academia, aiming to increase access to databases and\nlarge-scale applications. This work examines how in-context learning and\nchain-of-thought can be utilized to develop a robust solution for text-to-SQL\nsystems. We propose SQL-of-Thought: a multi-agent framework that decomposes the\nText2SQL task into schema linking, subproblem identification, query plan\ngeneration, SQL generation, and a guided correction loop. Unlike prior systems\nthat rely only on execution-based static correction, we introduce\ntaxonomy-guided dynamic error modification informed by in-context learning.\nSQL-of-Thought achieves state-of-the-art results on the Spider dataset and its\nvariants, combining guided error taxonomy with reasoning-based query planning.",
      "upvotes": 1,
      "discussionId": "68b7e593295f15ff609114de",
      "ai_summary": "A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.",
      "ai_keywords": [
        "in-context learning",
        "chain-of-thought",
        "SQL-of-Thought",
        "schema linking",
        "subproblem identification",
        "query plan generation",
        "SQL generation",
        "guided correction loop",
        "taxonomy-guided dynamic error modification",
        "reasoning-based query planning",
        "Spider dataset"
      ]
    },
    "publishedAt": "2025-08-30T14:27:12.000Z",
    "title": "SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction",
    "summary": "Converting natural language queries into SQL queries is a crucial challenge\nin both industry and academia, aiming to increase access to databases and\nlarge-scale applications. This work examines how in-context learning and\nchain-of-thought can be utilized to develop a robust solution for text-to-SQL\nsystems. We propose SQL-of-Thought: a multi-agent framework that decomposes the\nText2SQL task into schema linking, subproblem identification, query plan\ngeneration, SQL generation, and a guided correction loop. Unlike prior systems\nthat rely only on execution-based static correction, we introduce\ntaxonomy-guided dynamic error modification informed by in-context learning.\nSQL-of-Thought achieves state-of-the-art results on the Spider dataset and its\nvariants, combining guided error taxonomy with reasoning-based query planning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00578",
      "authors": [
        {
          "_id": "68b7d632295f15ff609114a9",
          "name": "Abdellah Zakaria Sellam",
          "hidden": false
        },
        {
          "_id": "68b7d632295f15ff609114aa",
          "name": "Ilyes Benaissa",
          "hidden": false
        },
        {
          "_id": "68b7d632295f15ff609114ab",
          "name": "Salah Eddine Bekhouche",
          "hidden": false
        },
        {
          "_id": "68b7d632295f15ff609114ac",
          "name": "Abdenour Hadid",
          "hidden": false
        },
        {
          "_id": "68b7d632295f15ff609114ad",
          "name": "Vito Renó",
          "hidden": false
        },
        {
          "_id": "68b7d632295f15ff609114ae",
          "name": "Cosimo Distante",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/ek5mY2wQD7nosUEcZPFaN.png"
      ],
      "publishedAt": "2025-08-30T18:06:06.000Z",
      "submittedOnDailyAt": "2025-09-03T04:18:15.695Z",
      "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection",
      "submittedOnDailyBy": {
        "_id": "63e7493fdb40d9e67feaccdd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
        "isPro": false,
        "fullname": "Salah Eddine Bekhouche",
        "user": "Bekhouche",
        "type": "user"
      },
      "summary": "Fine-grained object detection in challenging visual domains, such as vehicle\ndamage assessment, presents a formidable challenge even for human experts to\nresolve reliably. While DiffusionDet has advanced the state-of-the-art through\nconditional denoising diffusion, its performance remains limited by local\nfeature conditioning in context-dependent scenarios. We address this\nfundamental limitation by introducing Context-Aware Fusion (CAF), which\nleverages cross-attention mechanisms to integrate global scene context with\nlocal proposal features directly. The global context is generated using a\nseparate dedicated encoder that captures comprehensive environmental\ninformation, enabling each object proposal to attend to scene-level\nunderstanding. Our framework significantly enhances the generative detection\nparadigm by enabling each object proposal to attend to comprehensive\nenvironmental information. Experimental results demonstrate an improvement over\nstate-of-the-art models on the CarDD benchmark, establishing new performance\nbenchmarks for context-aware object detection in fine-grained domains",
      "upvotes": 1,
      "discussionId": "68b7d632295f15ff609114af",
      "ai_summary": "Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.",
      "ai_keywords": [
        "DiffusionDet",
        "conditional denoising diffusion",
        "Context-Aware Fusion",
        "cross-attention mechanisms",
        "global scene context",
        "local proposal features",
        "dedicated encoder",
        "generative detection paradigm",
        "CarDD benchmark"
      ]
    },
    "publishedAt": "2025-08-30T14:06:06.000Z",
    "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection",
    "summary": "Fine-grained object detection in challenging visual domains, such as vehicle\ndamage assessment, presents a formidable challenge even for human experts to\nresolve reliably. While DiffusionDet has advanced the state-of-the-art through\nconditional denoising diffusion, its performance remains limited by local\nfeature conditioning in context-dependent scenarios. We address this\nfundamental limitation by introducing Context-Aware Fusion (CAF), which\nleverages cross-attention mechanisms to integrate global scene context with\nlocal proposal features directly. The global context is generated using a\nseparate dedicated encoder that captures comprehensive environmental\ninformation, enabling each object proposal to attend to scene-level\nunderstanding. Our framework significantly enhances the generative detection\nparadigm by enabling each object proposal to attend to comprehensive\nenvironmental information. Experimental results demonstrate an improvement over\nstate-of-the-art models on the CarDD benchmark, establishing new performance\nbenchmarks for context-aware object detection in fine-grained domains",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/ek5mY2wQD7nosUEcZPFaN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e7493fdb40d9e67feaccdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
      "fullname": "Salah Eddine Bekhouche",
      "name": "Bekhouche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.00404",
      "authors": [
        {
          "_id": "68b7e917295f15ff609114ea",
          "name": "Hengjie Cao",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114eb",
          "name": "Mengyi Chen",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114ec",
          "name": "Yifeng Yang",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114ed",
          "name": "Ruijun Huang",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114ee",
          "name": "Fang Dong",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114ef",
          "name": "Jixian Zhou",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f0",
          "name": "Anrui Chen",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f1",
          "name": "Mingzhi Dong",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f2",
          "name": "Yujiang Wang",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f3",
          "name": "Jinlong Hou",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f4",
          "name": "Yuan Cheng",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f5",
          "name": "Fan Wu",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f6",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f7",
          "name": "Tun Lu",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f8",
          "name": "Ning Gu",
          "hidden": false
        },
        {
          "_id": "68b7e917295f15ff609114f9",
          "name": "Li Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-30T08:09:08.000Z",
      "submittedOnDailyAt": "2025-09-03T05:37:34.422Z",
      "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
      "submittedOnDailyBy": {
        "_id": "62d22496c58f969c152bcefd",
        "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
        "isPro": false,
        "fullname": "Tiezhen WANG",
        "user": "xianbao",
        "type": "user"
      },
      "summary": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/typename-yyf/Metis-quantization.",
      "upvotes": 1,
      "discussionId": "68b7e917295f15ff609114fa",
      "ai_summary": "Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.",
      "ai_keywords": [
        "anisotropic parameter distributions",
        "large language models",
        "low-bit quantization",
        "singular values",
        "block-wise quantization",
        "spectral decomposition",
        "random embedding",
        "adaptive learning rates",
        "dual-range regularizer",
        "FP8",
        "FP32"
      ]
    },
    "publishedAt": "2025-08-30T04:09:08.000Z",
    "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
    "summary": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/typename-yyf/Metis-quantization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d22496c58f969c152bcefd",
      "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
      "fullname": "Tiezhen WANG",
      "name": "xianbao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 129
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.02046",
      "authors": [
        {
          "_id": "68b7efa6295f15ff6091151e",
          "name": "Kaiyue Wen",
          "hidden": false
        },
        {
          "_id": "68b7efa6295f15ff6091151f",
          "name": "David Hall",
          "hidden": false
        },
        {
          "_id": "68b7efa6295f15ff60911520",
          "name": "Tengyu Ma",
          "hidden": false
        },
        {
          "_id": "68b7efa6295f15ff60911521",
          "name": "Percy Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T07:43:22.000Z",
      "submittedOnDailyAt": "2025-09-03T06:05:42.340Z",
      "title": "Fantastic Pretraining Optimizers and Where to Find Them",
      "submittedOnDailyBy": {
        "_id": "651e96991b97c9f33d26bde6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
        "isPro": true,
        "fullname": "Elie Bakouch",
        "user": "eliebak",
        "type": "user"
      },
      "summary": "AdamW has long been the dominant optimizer in language model pretraining,\ndespite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We\nposit that two methodological shortcomings have obscured fair comparisons and\nhindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited\nor misleading evaluation setups. To address these two issues, we conduct a\nsystematic study of ten deep learning optimizers across four model scales\n(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).\nWe find that fair and informative comparisons require rigorous hyperparameter\ntuning and evaluations across a range of model scales and data-to-model ratios,\nperformed at the end of training. First, optimal hyperparameters for one\noptimizer may be suboptimal for another, making blind hyperparameter transfer\nunfair. Second, the actual speedup of many proposed optimizers over well-tuned\nbaselines is lower than claimed and decreases with model size to only 1.1x for\n1.2B parameter models. Thirdly, comparing intermediate checkpoints before\nreaching the target training budgets can be misleading, as rankings between two\noptimizers can flip during training due to learning rate decay. Through our\nthorough investigation, we find that all the fastest optimizers such as Muon\nand Soap, use matrices as preconditioners -- multiplying gradients with\nmatrices rather than entry-wise scalars. However, the speedup of matrix-based\noptimizers is inversely proportional to model scale, decreasing from 1.4x over\nAdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.",
      "upvotes": 0,
      "discussionId": "68b7efa6295f15ff60911522",
      "ai_summary": "A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.",
      "ai_keywords": [
        "AdamW",
        "deep learning optimizers",
        "hyperparameter tuning",
        "evaluation setups",
        "model scales",
        "data-to-model ratios",
        "Muon",
        "Soap",
        "preconditioners",
        "learning rate decay"
      ]
    },
    "publishedAt": "2025-09-02T03:43:22.000Z",
    "title": "Fantastic Pretraining Optimizers and Where to Find Them",
    "summary": "AdamW has long been the dominant optimizer in language model pretraining,\ndespite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We\nposit that two methodological shortcomings have obscured fair comparisons and\nhindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited\nor misleading evaluation setups. To address these two issues, we conduct a\nsystematic study of ten deep learning optimizers across four model scales\n(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).\nWe find that fair and informative comparisons require rigorous hyperparameter\ntuning and evaluations across a range of model scales and data-to-model ratios,\nperformed at the end of training. First, optimal hyperparameters for one\noptimizer may be suboptimal for another, making blind hyperparameter transfer\nunfair. Second, the actual speedup of many proposed optimizers over well-tuned\nbaselines is lower than claimed and decreases with model size to only 1.1x for\n1.2B parameter models. Thirdly, comparing intermediate checkpoints before\nreaching the target training budgets can be misleading, as rankings between two\noptimizers can flip during training due to learning rate decay. Through our\nthorough investigation, we find that all the fastest optimizers such as Muon\nand Soap, use matrices as preconditioners -- multiplying gradients with\nmatrices rather than entry-wise scalars. However, the speedup of matrix-based\noptimizers is inversely proportional to model scale, decreasing from 1.4x over\nAdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651e96991b97c9f33d26bde6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
      "fullname": "Elie Bakouch",
      "name": "eliebak",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 277
    },
    "isAuthorParticipating": false
  }
]