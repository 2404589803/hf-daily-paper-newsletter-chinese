[
  {
    "paper": {
      "id": "2507.02813",
      "authors": [
        {
          "_id": "686735e69db35afc9c304ce1",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce2",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce3",
          "name": "Jiawei Chi",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce4",
          "user": {
            "_id": "65c38f6c137aba2aee524989",
            "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
            "isPro": false,
            "fullname": "Hanyang Wang",
            "user": "hanyang-21",
            "type": "user"
          },
          "name": "Hanyang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:31.644Z",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce5",
          "name": "Minghui Yang",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce6",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce7",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/GMjqJ0RyCYifjpsev_YPY.mp4"
      ],
      "publishedAt": "2025-07-03T17:21:23.000Z",
      "submittedOnDailyAt": "2025-07-04T00:35:47.996Z",
      "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.",
      "upvotes": 34,
      "discussionId": "686735e69db35afc9c304ce8",
      "projectPage": "https://liuff19.github.io/LangScene-X/",
      "githubRepo": "https://github.com/liuff19/LangScene-X/",
      "githubStars": 32
    },
    "publishedAt": "2025-07-03T13:21:23.000Z",
    "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion",
    "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/GMjqJ0RyCYifjpsev_YPY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02025",
      "authors": [
        {
          "_id": "686738019db35afc9c304cf3",
          "name": "The IntFold Team",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf4",
          "name": "Leon Qiao",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf5",
          "name": "Wayne Bai",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf6",
          "name": "He Yan",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf7",
          "user": {
            "_id": "63dd2bacea4d39995f581222",
            "avatarUrl": "/avatars/c117b66fdc1bcf3eed218b0b66e958cb.svg",
            "isPro": false,
            "fullname": "Liu",
            "user": "FuxuLiu",
            "type": "user"
          },
          "name": "Gary Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:15.140Z",
          "hidden": true
        },
        {
          "_id": "686738019db35afc9c304cf8",
          "name": "Nova Xi",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf9",
          "name": "Xiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:09:47.000Z",
      "submittedOnDailyAt": "2025-07-04T03:52:28.488Z",
      "title": "IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction",
      "submittedOnDailyBy": {
        "_id": "67d3a1a5943a965360fcae51",
        "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
        "isPro": false,
        "fullname": "Siqi Sun",
        "user": "siqisun",
        "type": "user"
      },
      "summary": "We introduce IntFold, a controllable foundation model for both general and\nspecialized biomolecular structure prediction. IntFold demonstrates predictive\naccuracy comparable to the state-of-the-art AlphaFold3, while utilizing a\nsuperior customized attention kernel. Beyond standard structure prediction,\nIntFold can be adapted to predict allosteric states, constrained structures,\nand binding affinity through the use of individual adapters. Furthermore, we\nintroduce a novel confidence head to estimate docking quality, offering a more\nnuanced assessment for challenging targets such as antibody-antigen complexes.\nFinally, we share insights gained during the training process of this\ncomputationally intensive model.",
      "upvotes": 27,
      "discussionId": "686738029db35afc9c304cfa",
      "projectPage": "https://server.intfold.com/",
      "githubRepo": "https://github.com/IntelliGen-AI/IntFold",
      "githubStars": 5
    },
    "publishedAt": "2025-07-02T12:09:47.000Z",
    "title": "IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction",
    "summary": "We introduce IntFold, a controllable foundation model for both general and\nspecialized biomolecular structure prediction. IntFold demonstrates predictive\naccuracy comparable to the state-of-the-art AlphaFold3, while utilizing a\nsuperior customized attention kernel. Beyond standard structure prediction,\nIntFold can be adapted to predict allosteric states, constrained structures,\nand binding affinity through the use of individual adapters. Furthermore, we\nintroduce a novel confidence head to estimate docking quality, offering a more\nnuanced assessment for challenging targets such as antibody-antigen complexes.\nFinally, we share insights gained during the training process of this\ncomputationally intensive model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d3a1a5943a965360fcae51",
      "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
      "fullname": "Siqi Sun",
      "name": "siqisun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01352",
      "authors": [
        {
          "_id": "6865cdc28c83dab5f72d1e18",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:50:33.676Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e19",
          "user": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "isPro": false,
            "fullname": "Liang Zeng",
            "user": "zengliangcs",
            "type": "user"
          },
          "name": "Liang Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T16:19:08.221Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1a",
          "user": {
            "_id": "658ceb595a8f8a309ea417a1",
            "avatarUrl": "/avatars/43bfa8c919aa802f2611439ebb7430b8.svg",
            "isPro": false,
            "fullname": "Ricky Shaw",
            "user": "RickyShaw999",
            "type": "user"
          },
          "name": "Yuzhen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:50:29.586Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1b",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1c",
          "name": "Jiacai Liu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1d",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1e",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1f",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e20",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e21",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e22",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e23",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T04:40:29.000Z",
      "submittedOnDailyAt": "2025-07-04T00:12:17.371Z",
      "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.",
      "upvotes": 23,
      "discussionId": "6865cdc28c83dab5f72d1e24",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-Reward-V2",
      "githubStars": 14
    },
    "publishedAt": "2025-07-02T00:40:29.000Z",
    "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
    "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01352.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02592",
      "authors": [
        {
          "_id": "686732329db35afc9c304cb4",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb5",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb6",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb7",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb8",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb9",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cba",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbb",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbc",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbd",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbe",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbf",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc0",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc1",
          "user": {
            "_id": "6622132f63598534f96ca29d",
            "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
            "isPro": false,
            "fullname": "Xixi Wu",
            "user": "xxwu",
            "type": "user"
          },
          "name": "Xixi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:47.564Z",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc2",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc3",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc4",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc5",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc6",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/9sUyYbIYfR5wDQMMft1io.png",
        "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/FLz6T05o_NbbQKrOQyTuL.png"
      ],
      "publishedAt": "2025-07-03T12:59:07.000Z",
      "submittedOnDailyAt": "2025-07-04T00:20:59.450Z",
      "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
      "submittedOnDailyBy": {
        "_id": "622f2feea32d46b4be9ed8c4",
        "avatarUrl": "/avatars/ba25eb941a7c9a414b7fd4818adfa26b.svg",
        "isPro": false,
        "fullname": "Litu Ou",
        "user": "learn3r",
        "type": "user"
      },
      "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
      "upvotes": 22,
      "discussionId": "686732339db35afc9c304cc7",
      "projectPage": "https://github.com/Alibaba-NLP/WebAgent"
    },
    "publishedAt": "2025-07-03T08:59:07.000Z",
    "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
    "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/9sUyYbIYfR5wDQMMft1io.png",
      "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/FLz6T05o_NbbQKrOQyTuL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f2feea32d46b4be9ed8c4",
      "avatarUrl": "/avatars/ba25eb941a7c9a414b7fd4818adfa26b.svg",
      "fullname": "Litu Ou",
      "name": "learn3r",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23918",
      "authors": [
        {
          "_id": "68674e689db35afc9c304d4c",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4d",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4e",
          "name": "Hangyu Guo",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4f",
          "name": "Zhenhua Liu",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d50",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d51",
          "user": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "isPro": false,
            "fullname": "Xiaoye Qu",
            "user": "Xiaoye08",
            "type": "user"
          },
          "name": "Xiaoye Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:48:48.311Z",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d52",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d53",
          "name": "Yanshu Li",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d54",
          "name": "Kaide Zeng",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d55",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d56",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d57",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d58",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d59",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d5a",
          "name": "Yi R. Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T14:48:35.000Z",
      "submittedOnDailyAt": "2025-07-04T02:20:13.561Z",
      "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers",
      "submittedOnDailyBy": {
        "_id": "64264095ba51f8a2136946a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
        "isPro": false,
        "fullname": "Zhaochen Su",
        "user": "Warrieryes",
        "type": "user"
      },
      "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.",
      "upvotes": 17,
      "discussionId": "68674e699db35afc9c304d5b",
      "githubRepo": "https://github.com/zhaochen0110/Awesome_Think_With_Images",
      "githubStars": 519
    },
    "publishedAt": "2025-06-30T10:48:35.000Z",
    "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers",
    "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23918.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64264095ba51f8a2136946a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
      "fullname": "Zhaochen Su",
      "name": "Warrieryes",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02652",
      "authors": [
        {
          "_id": "6867282b9db35afc9c304c83",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c84",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c85",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c86",
          "name": "Yuyao Zhang",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c87",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c88",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c89",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c8a",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:18:08.000Z",
      "submittedOnDailyAt": "2025-07-04T00:07:46.543Z",
      "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
      "submittedOnDailyBy": {
        "_id": "6695f14df0ffd8e3a379ad61",
        "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
        "isPro": false,
        "fullname": "Jiajie Jin",
        "user": "jinjiajie",
        "type": "user"
      },
      "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.",
      "upvotes": 10,
      "discussionId": "6867282c9db35afc9c304c8b"
    },
    "publishedAt": "2025-07-03T10:18:08.000Z",
    "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
    "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6695f14df0ffd8e3a379ad61",
      "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
      "fullname": "Jiajie Jin",
      "name": "jinjiajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02754",
      "authors": [
        {
          "_id": "686732e19db35afc9c304cc9",
          "name": "Aurko Roy",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cca",
          "name": "Timothy Chou",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccb",
          "name": "Sai Surya Duvvuri",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccc",
          "name": "Sijia Chen",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccd",
          "name": "Jiecao Yu",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cce",
          "name": "Xiaodong Wang",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccf",
          "name": "Manzil Zaheer",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cd0",
          "name": "Rohan Anil",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T16:16:34.000Z",
      "submittedOnDailyAt": "2025-07-04T00:19:14.552Z",
      "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
      "submittedOnDailyBy": {
        "_id": "651e96991b97c9f33d26bde6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
        "isPro": false,
        "fullname": "Elie Bakouch",
        "user": "eliebak",
        "type": "user"
      },
      "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n2-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
      "upvotes": 7,
      "discussionId": "686732e19db35afc9c304cd1"
    },
    "publishedAt": "2025-07-03T12:16:34.000Z",
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n2-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02754.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "651e96991b97c9f33d26bde6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
      "fullname": "Elie Bakouch",
      "name": "eliebak",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02694",
      "authors": [
        {
          "_id": "686762e49db35afc9c304d62",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d63",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d64",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d65",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d66",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T15:04:38.000Z",
      "submittedOnDailyAt": "2025-07-04T03:43:32.903Z",
      "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.",
      "upvotes": 6,
      "discussionId": "686762e49db35afc9c304d67"
    },
    "publishedAt": "2025-07-03T11:04:38.000Z",
    "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
    "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02726",
      "authors": [
        {
          "_id": "68673e349db35afc9c304d02",
          "name": "Matthieu Zimmer",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d03",
          "name": "Xiaotong Ji",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d04",
          "name": "Rasul Tutunov",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d05",
          "name": "Anthony Bordg",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d06",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d07",
          "name": "Haitham Bou Ammar",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631c375768f7da9ad2496bf6/j0vqZ_s8kw-ALYV8UCDlF.png"
      ],
      "publishedAt": "2025-07-03T15:41:38.000Z",
      "submittedOnDailyAt": "2025-07-04T01:09:09.946Z",
      "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": true,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
      "upvotes": 3,
      "discussionId": "68673e359db35afc9c304d08"
    },
    "publishedAt": "2025-07-03T11:41:38.000Z",
    "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
    "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631c375768f7da9ad2496bf6/j0vqZ_s8kw-ALYV8UCDlF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02092",
      "authors": [
        {
          "_id": "68676e4a9db35afc9c304d72",
          "name": "Alexi Gladstone",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d73",
          "name": "Ganesh Nanduru",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d74",
          "name": "Md Mofijul Islam",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d75",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d76",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d77",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:48:46.365Z",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d78",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d79",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d7a",
          "name": "Jundong Li",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d7b",
          "name": "Tariq Iqbal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T19:17:29.000Z",
      "submittedOnDailyAt": "2025-07-04T04:39:19.016Z",
      "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.",
      "upvotes": 2,
      "discussionId": "68676e4a9db35afc9c304d7c"
    },
    "publishedAt": "2025-07-02T15:17:29.000Z",
    "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
    "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01663",
      "authors": [
        {
          "_id": "6865e6588c83dab5f72d1e85",
          "name": "Zhenyu Han",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e86",
          "name": "Ansheng You",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e87",
          "name": "Haibo Wang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e88",
          "name": "Kui Luo",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e89",
          "name": "Guang Yang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8a",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8b",
          "name": "Menglong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8c",
          "name": "Sicheng Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8d",
          "name": "Zeshun Lan",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8e",
          "name": "Chunshi Deng",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8f",
          "name": "Huazhong Ji",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e90",
          "name": "Wenjie Liu",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e91",
          "name": "Yu Huang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e92",
          "name": "Yixiang Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e93",
          "name": "Chenyi Pan",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e94",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e95",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e96",
          "name": "Chunsheng Li",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e97",
          "name": "Jianping Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/_y3XJtovmLBm_ol-D36bj.jpeg"
      ],
      "publishedAt": "2025-07-02T12:45:34.000Z",
      "submittedOnDailyAt": "2025-07-04T01:49:59.664Z",
      "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
      "upvotes": 1,
      "discussionId": "6865e6598c83dab5f72d1e98"
    },
    "publishedAt": "2025-07-02T08:45:34.000Z",
    "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
    "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/_y3XJtovmLBm_ol-D36bj.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22813",
      "authors": [
        {
          "_id": "6867836c11736b002cf34d41",
          "name": "Zhuojun Ding",
          "hidden": false
        },
        {
          "_id": "6867836c11736b002cf34d42",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "6867836c11736b002cf34d43",
          "name": "Chenghao Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T08:28:52.000Z",
      "submittedOnDailyAt": "2025-07-04T06:06:27.989Z",
      "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "641aa5e391e3376a057bbd4c",
        "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
        "isPro": false,
        "fullname": "Chenghao Fan",
        "user": "Facico",
        "type": "user"
      },
      "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.",
      "upvotes": 1,
      "discussionId": "6867836c11736b002cf34d44",
      "githubRepo": "https://github.com/Ding-ZJ/SaM"
    },
    "publishedAt": "2025-06-28T04:28:52.000Z",
    "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
    "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  }
]