[
  {
    "paper": {
      "id": "2505.09388",
      "authors": [
        {
          "_id": "68299e3128752b51372d31ea",
          "user": {
            "_id": "62088594a5943c8a8fc94560",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
            "isPro": false,
            "fullname": "An Yang",
            "user": "yangapku",
            "type": "user"
          },
          "name": "An Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31eb",
          "user": {
            "_id": "6799128b9da39716ab1ebd95",
            "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
            "isPro": false,
            "fullname": "Anfeng Li",
            "user": "laf070810",
            "type": "user"
          },
          "name": "Anfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ec",
          "user": {
            "_id": "64b0a77df12b47366663884c",
            "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
            "isPro": false,
            "fullname": "Baosong Yang",
            "user": "Baosong",
            "type": "user"
          },
          "name": "Baosong Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ed",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ee",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ef",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f0",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f1",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f2",
          "name": "Chengen Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f3",
          "name": "Chenxu Lv",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f4",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f5",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f6",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f8",
          "name": "Feng Hu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f9",
          "name": "Hao Ge",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fa",
          "user": {
            "_id": "6436618aeef1f55654a9f458",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
            "isPro": false,
            "fullname": "Haoran Wei",
            "user": "HaoranWei",
            "type": "user"
          },
          "name": "Haoran Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fb",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fc",
          "user": {
            "_id": "63281d05ac205d01918b5fc7",
            "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
            "isPro": false,
            "fullname": "Jialong Tang",
            "user": "Jialong",
            "type": "user"
          },
          "name": "Jialong Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fd",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fe",
          "user": {
            "_id": "654bead777401b47e6424f88",
            "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
            "isPro": false,
            "fullname": "Jianhong Tu",
            "user": "ToviTu",
            "type": "user"
          },
          "name": "Jianhong Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ff",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3200",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3201",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3202",
          "name": "Jing Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3203",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3204",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3205",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3206",
          "name": "Keqin Bao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3207",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3208",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3209",
          "name": "Lianghao Deng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320a",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320b",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320c",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320d",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320e",
          "user": {
            "_id": "62f220ccee7d7af44979efc7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
            "isPro": false,
            "fullname": "Peng Wang",
            "user": "ZJUPeng",
            "type": "user"
          },
          "name": "Peng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320f",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3210",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3211",
          "user": {
            "_id": "6629ed94aabce1b25c3db90c",
            "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
            "isPro": false,
            "fullname": "Ruize Gao",
            "user": "gaoruize",
            "type": "user"
          },
          "name": "Ruize Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3212",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3213",
          "name": "Shuang Luo",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3214",
          "name": "Tianhao Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3215",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3216",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3217",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3218",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3219",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321a",
          "name": "Xuancheng Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321b",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321c",
          "name": "Yang Su",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321e",
          "name": "Yinger Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321f",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3220",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3221",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3222",
          "user": {
            "_id": "672c25ca8cfb61188128eb6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
            "isPro": false,
            "fullname": "Zeyu Cui",
            "user": "misakamage",
            "type": "user"
          },
          "name": "Zeyu Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3223",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3224",
          "name": "Zhipeng Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3225",
          "user": {
            "_id": "647ccbd6e07cf9bb2d485244",
            "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
            "isPro": false,
            "fullname": "Zihan Qiu",
            "user": "QwQZh",
            "type": "user"
          },
          "name": "Zihan Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T13:41:34.000Z",
      "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
      "title": "Qwen3 Technical Report",
      "submittedOnDailyBy": {
        "_id": "610b70452719facd4ea85e28",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
        "isPro": false,
        "fullname": "Chujie Zheng",
        "user": "chujiezheng",
        "type": "user"
      },
      "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
      "upvotes": 60,
      "discussionId": "68299e3228752b51372d325f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3/",
      "githubRepo": "https://github.com/QwenLM/Qwen3",
      "ai_keywords": [
        "large language models (LLMs)",
        "Mixture-of-Expert (MoE) architectures",
        "thinking mode",
        "non-thinking mode",
        "chat-optimized models",
        "dedicated reasoning models",
        "thinking budget mechanism",
        "computational resources adaptively",
        "inference",
        "latency",
        "performance",
        "code generation",
        "mathematical reasoning",
        "agent tasks",
        "multilingual support",
        "cross-lingual understanding",
        "generation capabilities"
      ]
    },
    "publishedAt": "2025-05-14T09:41:34.000Z",
    "title": "Qwen3 Technical Report",
    "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610b70452719facd4ea85e28",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
      "fullname": "Chujie Zheng",
      "name": "chujiezheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11409",
      "authors": [
        {
          "_id": "682abb7984695084c1a48eab",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eac",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48ead",
          "user": {
            "_id": "62b279e92375526ae51a537b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
            "isPro": false,
            "fullname": "Han Zhou",
            "user": "hzhouml",
            "type": "user"
          },
          "name": "Han Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eae",
          "user": {
            "_id": "65bf213f8467e2a3d6374d4b",
            "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
            "isPro": false,
            "fullname": "X Wan",
            "user": "masonxw",
            "type": "user"
          },
          "name": "Xingchen Wan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eaf",
          "user": {
            "_id": "63920dfac47e36ddeb8f1864",
            "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
            "isPro": false,
            "fullname": "Caiqi Zhang",
            "user": "caiqizh",
            "type": "user"
          },
          "name": "Caiqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb0",
          "user": {
            "_id": "617a6284941993035fbaf299",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
            "isPro": false,
            "fullname": "Anna Korhonen",
            "user": "akorhonen",
            "type": "user"
          },
          "name": "Anna Korhonen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb1",
          "user": {
            "_id": "6273e70dc8d55dd434bd8e52",
            "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
            "isPro": false,
            "fullname": "Ivan Vulić",
            "user": "ivulic",
            "type": "user"
          },
          "name": "Ivan Vulić",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
      ],
      "publishedAt": "2025-05-16T16:17:22.000Z",
      "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
      "title": "Visual Planning: Let's Think Only with Images",
      "submittedOnDailyBy": {
        "_id": "62b279e92375526ae51a537b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
        "isPro": false,
        "fullname": "Han Zhou",
        "user": "hzhouml",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
      "upvotes": 10,
      "discussionId": "682abb7c84695084c1a48fb4",
      "githubRepo": "https://github.com/yix8/VisualPlanning",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal extensions (MLLMs)",
        "machine reasoning",
        "visual information",
        "Visual Planning",
        "purely visual representations",
        "sequences of images",
        "step-by-step inference",
        "Visual Planning via Reinforcement Learning (VPRL)",
        "GRPO",
        "post-training large vision models",
        "planning",
        "visual navigation tasks",
        "FrozenLake",
        "Maze",
        "MiniBehavior",
        "text-only space",
        "intuitive, image-based inference"
      ]
    },
    "publishedAt": "2025-05-16T12:17:22.000Z",
    "title": "Visual Planning: Let's Think Only with Images",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b279e92375526ae51a537b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
      "fullname": "Han Zhou",
      "name": "hzhouml",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10962",
      "authors": [
        {
          "_id": "682ab4fe7a9f1a7ec9779dd6",
          "user": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/4c88cb518e000a475f8381573f21aa7f.svg",
            "isPro": false,
            "fullname": "Zhenwen Liang",
            "user": "invokerliang",
            "type": "user"
          },
          "name": "Zhenwen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:26.692Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd7",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:45.999Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd9",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dda",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddb",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:56.781Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddc",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T07:56:03.000Z",
      "submittedOnDailyAt": "2025-05-19T03:06:11.065Z",
      "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
      "submittedOnDailyBy": {
        "_id": "64c94eddcb2f1bf0e7db5a4d",
        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
        "isPro": false,
        "fullname": "Linfeng Song",
        "user": "freesunshine0316",
        "type": "user"
      },
      "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
      "upvotes": 5,
      "discussionId": "682ab4ff7a9f1a7ec9779e71",
      "ai_keywords": [
        "Automated Theorem Proving (ATP)",
        "large language models (LLMs)",
        "biased search guidance",
        "Multi-Perspective Search Prover (MPS-Prover)",
        "post-training data curation strategy",
        "multi-perspective tree search mechanism",
        "learned critic model",
        "heuristic rules",
        "tactic selection",
        "search robustness",
        "miniF2F",
        "ProofNet",
        "state-of-the-art performance",
        "formal reasoning"
      ]
    },
    "publishedAt": "2025-05-16T03:56:03.000Z",
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
    "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10962.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c94eddcb2f1bf0e7db5a4d",
      "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
      "fullname": "Linfeng Song",
      "name": "freesunshine0316",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11427",
      "authors": [
        {
          "_id": "682ad9809506a7e45a93be00",
          "user": {
            "_id": "6318e7a2acffc70bd4e057ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6318e7a2acffc70bd4e057ec/2m3XSbNLwv7Kmo8qfWq3L.jpeg",
            "isPro": false,
            "fullname": "Adrian Robert Minut",
            "user": "adrianrob",
            "type": "user"
          },
          "name": "Adrian Robert Minut",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:22.059Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be01",
          "user": {
            "_id": "63ab16a6d7ee953f604ecd52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
            "isPro": false,
            "fullname": "Tommaso Mencattini",
            "user": "tmencatt",
            "type": "user"
          },
          "name": "Tommaso Mencattini",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:21.898Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be02",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:26.518Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be03",
          "user": {
            "_id": "64256584daa3502ee3570b86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64256584daa3502ee3570b86/kui0eb59S5aTUeZIjawUj.jpeg",
            "isPro": false,
            "fullname": "Donato Crisostomi",
            "user": "crisostomi",
            "type": "user"
          },
          "name": "Donato Crisostomi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:28.737Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be04",
          "user": {
            "_id": "652681664e066bf73f8e2bd1",
            "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
            "isPro": false,
            "fullname": "Emanuele Rodola'",
            "user": "erodola",
            "type": "user"
          },
          "name": "Emanuele Rodolà",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:35.566Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T16:43:23.000Z",
      "submittedOnDailyAt": "2025-05-19T05:45:27.421Z",
      "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
      "submittedOnDailyBy": {
        "_id": "5e8ef1f14957053f606489e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
        "isPro": false,
        "fullname": "Andrea Santilli",
        "user": "teelinsan",
        "type": "user"
      },
      "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
      "upvotes": 4,
      "discussionId": "682ad9819506a7e45a93be38",
      "githubRepo": "https://github.com/tommasomncttn/mergenetic",
      "ai_keywords": [
        "model merging",
        "evolutionary algorithms",
        "Mergenetic",
        "fitness estimators",
        "evaluation costs"
      ]
    },
    "publishedAt": "2025-05-16T12:43:23.000Z",
    "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e8ef1f14957053f606489e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
      "fullname": "Andrea Santilli",
      "name": "teelinsan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11107",
      "authors": [
        {
          "_id": "682ad96cdc6d7453624831b9",
          "user": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "isPro": false,
            "fullname": "許湛然",
            "user": "Splend1dchan",
            "type": "user"
          },
          "name": "Chan-Jan Hsu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:15.798Z",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831ba",
          "name": "Davide Buffelli",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bb",
          "name": "Jamie McGowan",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bc",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bd",
          "name": "Yi-Chang Chen",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831be",
          "name": "Sattar Vakili",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bf",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T10:40:35.000Z",
      "submittedOnDailyAt": "2025-05-19T05:58:53.531Z",
      "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
      "submittedOnDailyBy": {
        "_id": "6213410828005421265b27d3",
        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
        "isPro": false,
        "fullname": "許湛然",
        "user": "Splend1dchan",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
      "upvotes": 3,
      "discussionId": "682ad96ddc6d7453624831f3",
      "ai_keywords": [
        "large language models (LLMs)",
        "reasoning through self-generated chains of thought",
        "reasoning agents",
        "turn-based manner",
        "Group Think",
        "concurrent reasoning agents",
        "think ers",
        "shared visibility",
        "reasoning trajectories",
        "token level",
        "reasoning thread",
        "fine-grained, token-level collaboration",
        "redundant reasoning",
        "edge inference",
        "modification",
        "LLMs",
        "local GPU",
        "evaluation strategy",
        "reasoning latency"
      ]
    },
    "publishedAt": "2025-05-16T06:40:35.000Z",
    "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
    "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11107.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6213410828005421265b27d3",
      "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
      "fullname": "許湛然",
      "name": "Splend1dchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11152",
      "authors": [
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e5",
          "user": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "isPro": false,
            "fullname": "Daniel Sungho Jung",
            "user": "dqj5182",
            "type": "user"
          },
          "name": "Daniel Sungho Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:43.502Z",
          "hidden": false
        },
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e6",
          "user": {
            "_id": "656056b21392aa3beb5de0bd",
            "avatarUrl": "/avatars/07f25b750ef308d65f2e6c82506e7816.svg",
            "isPro": false,
            "fullname": "Kyoung Mu  Lee ",
            "user": "kyoungmu",
            "type": "user"
          },
          "name": "Kyoung Mu Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:48.640Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:54:25.000Z",
      "submittedOnDailyAt": "2025-05-19T01:11:35.713Z",
      "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
      "submittedOnDailyBy": {
        "_id": "65601c6ee23401f82005e361",
        "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
        "isPro": false,
        "fullname": "Daniel Sungho Jung",
        "user": "dqj5182",
        "type": "user"
      },
      "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
      "upvotes": 2,
      "discussionId": "682a9a405e6f0c59f4d8a125",
      "projectPage": "https://haco-release.github.io/",
      "githubRepo": "https://github.com/dqj5182/HACO_RELEASE",
      "ai_keywords": [
        "dense hand contact estimation",
        "class imbalance issue",
        "spatial imbalance issue",
        "finger tips",
        "balanced contact sampling",
        "vertex-level class-balanced (VCB) loss",
        "contact distribution",
        "contact frequency"
      ]
    },
    "publishedAt": "2025-05-16T07:54:25.000Z",
    "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65601c6ee23401f82005e361",
      "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
      "fullname": "Daniel Sungho Jung",
      "name": "dqj5182",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11140",
      "authors": [
        {
          "_id": "682ad417500638b80a43471d",
          "user": {
            "_id": "60d33fbbd7b174177faabd4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
            "isPro": true,
            "fullname": "Mike Zhang",
            "user": "jjzha",
            "type": "user"
          },
          "name": "Mike Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:04.053Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471e",
          "user": {
            "_id": "678fa79005ae7fe48d03ba47",
            "avatarUrl": "/avatars/a78ab2b37fa3e18ace783f6f71f5a361.svg",
            "isPro": false,
            "fullname": "Johannes Bjerva",
            "user": "bjerva",
            "type": "user"
          },
          "name": "Johannes Bjerva",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:58.827Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471f",
          "user": {
            "_id": "60ed4c56abab3c2620df8ac8",
            "avatarUrl": "/avatars/ad5508c1c94a96f6d1290e4735e81b73.svg",
            "isPro": false,
            "fullname": "Russa Biswas",
            "user": "rubis",
            "type": "user"
          },
          "name": "Russa Biswas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:26:04.749Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:39:33.000Z",
      "submittedOnDailyAt": "2025-05-19T05:25:57.460Z",
      "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "60d33fbbd7b174177faabd4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
        "isPro": true,
        "fullname": "Mike Zhang",
        "user": "jjzha",
        "type": "user"
      },
      "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
      "upvotes": 1,
      "discussionId": "682ad418500638b80a434770",
      "githubRepo": "https://github.com/jjzha/fs1",
      "ai_keywords": [
        "large language model (LLM)",
        "reasoning capabilities",
        "mathematical reasoning",
        "length thinking process",
        "computational resources",
        "inference",
        "complex open-domain question-answering (QA)",
        "reasoning traces",
        "reasoning models",
        "QwQ-32B",
        "DeepSeek-R1-671B",
        "instruction-tuned variants",
        "Qwen2.5",
        "knowledge graphs",
        "paths",
        "reasoning traces",
        "baseline approaches",
        "instruction-tuned models",
        "benchmark",
        "datasets",
        "experimental runs",
        "factual accuracy",
        "test-time compute",
        "token budgets",
        "test-time scaling",
        "reasoning accuracy"
      ]
    },
    "publishedAt": "2025-05-16T07:39:33.000Z",
    "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d33fbbd7b174177faabd4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
      "fullname": "Mike Zhang",
      "name": "jjzha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07675",
      "authors": [
        {
          "_id": "6829dcab0daa5ccc817e6ec8",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ec9",
          "user": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "isPro": false,
            "fullname": "Dong-Bok Lee",
            "user": "dongboklee",
            "type": "user"
          },
          "name": "Dong Bok Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6eca",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ecb",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T15:39:51.000Z",
      "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
      "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
      "submittedOnDailyBy": {
        "_id": "64f000769e7770db74d44bba",
        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
        "isPro": false,
        "fullname": "Dong-Bok Lee",
        "user": "dongboklee",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
      "upvotes": 1,
      "discussionId": "6829dcad0daa5ccc817e6f40",
      "ai_keywords": [
        "Vision-language models (VLMs)",
        "knowledge distillation (KD)",
        "dual prediction heads",
        "gradient conflicts",
        "feature learning",
        "semi-supervised settings",
        "state-of-the-art performance",
        "ImageNet",
        "accuracy"
      ]
    },
    "publishedAt": "2025-05-12T11:39:51.000Z",
    "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
    "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f000769e7770db74d44bba",
      "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
      "fullname": "Dong-Bok Lee",
      "name": "dongboklee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11011",
      "authors": [
        {
          "_id": "682ae098730bd40a0755f87c",
          "name": "Darija Barak",
          "hidden": false
        },
        {
          "_id": "682ae098730bd40a0755f87d",
          "name": "Miguel Costa-Gomes",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:01:09.000Z",
      "submittedOnDailyAt": "2025-05-19T06:12:42.874Z",
      "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
      "upvotes": 0,
      "discussionId": "682ae099730bd40a0755f8b9",
      "ai_keywords": [
        "p-beauty contest",
        "Nash-equilibrium choices",
        "strategic reasoning ability",
        "reasoning ability",
        "propensity towards cooperation",
        "mechanism design"
      ]
    },
    "publishedAt": "2025-05-16T05:01:09.000Z",
    "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
    "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]