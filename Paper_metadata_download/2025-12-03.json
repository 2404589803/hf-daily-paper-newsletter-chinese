[
  {
    "paper": {
      "id": "2512.02556",
      "authors": [
        {
          "_id": "692fa6da26742347f61dab24",
          "name": "DeepSeek-AI",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab25",
          "name": "Aixin Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab26",
          "name": "Aoxue Mei",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab27",
          "name": "Bangcai Lin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab28",
          "name": "Bing Xue",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab29",
          "name": "Bingxuan Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2a",
          "name": "Bingzheng Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2b",
          "name": "Bochao Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2c",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2d",
          "name": "Chaofan Lin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2e",
          "name": "Chen Dong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab2f",
          "name": "Chengda Lu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab30",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab31",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab32",
          "name": "Chenhao Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab33",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab34",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab35",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab36",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab37",
          "name": "Deli Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab38",
          "name": "Erhang Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab39",
          "name": "Fangqi Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3a",
          "name": "Fangyun Lin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3b",
          "name": "Fucong Dai",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3c",
          "name": "Guangbo Hao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3d",
          "name": "Guanting Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3e",
          "name": "Guowei Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab3f",
          "name": "H. Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab40",
          "name": "Hanwei Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab41",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab42",
          "name": "Haofen Liang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab43",
          "name": "Haoran Wei",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab44",
          "name": "Haowei Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab45",
          "name": "Haowen Luo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab46",
          "name": "Haozhe Ji",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab47",
          "name": "Honghui Ding",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab48",
          "name": "Hongxuan Tang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab49",
          "name": "Huanqi Cao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4a",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4b",
          "name": "Hui Qu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4c",
          "name": "Hui Zeng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4d",
          "name": "Jialiang Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4e",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab4f",
          "name": "Jiaxin Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab50",
          "name": "Jiewen Hu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab51",
          "name": "Jingchang Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab52",
          "name": "Jingting Xiang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab53",
          "name": "Jingyang Yuan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab54",
          "name": "Jingyuan Cheng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab55",
          "name": "Jinhua Zhu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab56",
          "name": "Jun Ran",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab57",
          "name": "Junguang Jiang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab58",
          "name": "Junjie Qiu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab59",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5a",
          "name": "Junxiao Song",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5b",
          "name": "Kai Dong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5c",
          "name": "Kaige Gao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5d",
          "name": "Kang Guan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5e",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab5f",
          "name": "Kexing Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab60",
          "name": "Kezhao Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab61",
          "name": "Kuai Yu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab62",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab63",
          "name": "Lecong Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab64",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab65",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab66",
          "name": "Liangsheng Yin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab67",
          "name": "Lihua Guo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab68",
          "name": "Lingxiao Luo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab69",
          "name": "Linwang Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6a",
          "name": "Litong Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6b",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6c",
          "name": "M. S. Di",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6d",
          "name": "M. Y Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6e",
          "name": "Mingchuan Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab6f",
          "name": "Minghua Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab70",
          "name": "Minghui Tang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab71",
          "name": "Mingxu Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab72",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab73",
          "name": "Peixin Cong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab74",
          "name": "Peiyi Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab75",
          "name": "Qiancheng Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab76",
          "name": "Qihao Zhu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab77",
          "name": "Qingyang Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab78",
          "name": "Qinyu Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab79",
          "name": "Qiushi Du",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7a",
          "name": "Ruiling Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7b",
          "name": "Ruiqi Ge",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7c",
          "name": "Ruisong Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7d",
          "name": "Ruizhe Pan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7e",
          "name": "Runji Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab7f",
          "name": "Runqiu Yin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab80",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab81",
          "name": "Ruomeng Shen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab82",
          "name": "Ruoyu Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab83",
          "name": "S. H. Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab84",
          "name": "Shanghao Lu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab85",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab86",
          "name": "Shanhuang Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab87",
          "name": "Shaofei Cai",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab88",
          "name": "Shaoyuan Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab89",
          "name": "Shengding Hu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8a",
          "name": "Shengyu Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8b",
          "name": "Shiqiang Hu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8c",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8d",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8e",
          "name": "Shuiping Yu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab8f",
          "name": "Shunfeng Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab90",
          "name": "Shuting Pan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab91",
          "name": "Songyang Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab92",
          "name": "Tao Ni",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab93",
          "name": "Tao Yun",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab94",
          "name": "Tian Pei",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab95",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab96",
          "name": "Tianyuan Yue",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab97",
          "name": "Wangding Zeng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab98",
          "name": "Wen Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab99",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9a",
          "name": "Wenjie Pang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9b",
          "name": "Wenjing Luo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9c",
          "name": "Wenjun Gao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9d",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9e",
          "name": "Xi Gao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dab9f",
          "name": "Xiangwen Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba0",
          "name": "Xiao Bi",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba1",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba2",
          "name": "Xiaohan Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba3",
          "name": "Xiaokang Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba4",
          "name": "Xiaokang Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba5",
          "name": "Xiaotao Nie",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba6",
          "name": "Xin Cheng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba7",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba8",
          "name": "Xin Xie",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61daba9",
          "name": "Xingchao Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabaa",
          "name": "Xingkai Yu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabab",
          "name": "Xingyou Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabac",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabad",
          "name": "Xinyuan Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabae",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabaf",
          "name": "Xuecheng Su",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb0",
          "name": "Xuehai Pan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb1",
          "name": "Xuheng Lin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb2",
          "name": "Xuwei Fu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb3",
          "name": "Y. Q. Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb4",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb5",
          "name": "Yanhong Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb6",
          "name": "Yanru Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb7",
          "name": "Yao Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb8",
          "name": "Yao Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabb9",
          "name": "Yao Zhao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabba",
          "name": "Yaofeng Sun",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabbb",
          "name": "Yaohui Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabbc",
          "name": "Yi Qian",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabbd",
          "name": "Yi Yu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabbe",
          "name": "Yichao Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabbf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc0",
          "name": "Yifan Shi",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc1",
          "name": "Yiliang Xiong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc2",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc3",
          "name": "Ying Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc4",
          "name": "Yinmin Zhong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc5",
          "name": "Yishi Piao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc6",
          "name": "Yisong Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc7",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc8",
          "name": "Yixuan Tan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabc9",
          "name": "Yixuan Wei",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabca",
          "name": "Yiyang Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabcb",
          "name": "Yiyuan Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabcc",
          "name": "Yonglun Yang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabcd",
          "name": "Yongqiang Guo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabce",
          "name": "Yongtong Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabcf",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd0",
          "name": "Yuan Cheng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd1",
          "name": "Yuan Ou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd2",
          "name": "Yuanfan Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd3",
          "name": "Yuduan Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd4",
          "name": "Yue Gong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd5",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd6",
          "name": "Yuheng Zou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd7",
          "name": "Yukun Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd8",
          "name": "Yunfan Xiong",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabd9",
          "name": "Yuxiang Luo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabda",
          "name": "Yuxiang You",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabdb",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabdc",
          "name": "Yuyang Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabdd",
          "name": "Z. F. Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabde",
          "name": "Z. Z. Ren",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabdf",
          "name": "Zehua Zhao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe0",
          "name": "Zehui Ren",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe1",
          "name": "Zhangli Sha",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe2",
          "name": "Zhe Fu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe3",
          "name": "Zhean Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe4",
          "name": "Zhenda Xie",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe5",
          "name": "Zhengyan Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe6",
          "name": "Zhewen Hao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe7",
          "name": "Zhibin Gou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe8",
          "name": "Zhicheng Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabe9",
          "name": "Zhigang Yan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabea",
          "name": "Zhihong Shao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabeb",
          "name": "Zhixian Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabec",
          "name": "Zhiyu Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabed",
          "name": "Zhuoshu Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabee",
          "name": "Zhuping Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabef",
          "name": "Zian Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf0",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf1",
          "name": "Zihui Gu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf2",
          "name": "Zijia Zhu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf3",
          "name": "Zilin Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf4",
          "name": "Zipeng Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf5",
          "name": "Ziwei Xie",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf6",
          "name": "Ziyi Gao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf7",
          "name": "Zizheng Pan",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf8",
          "name": "Zongqing Yao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabf9",
          "name": "Bei Feng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabfa",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabfb",
          "name": "J. L. Cai",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabfc",
          "name": "Jiaqi Ni",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabfd",
          "name": "Lei Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabfe",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dabff",
          "name": "Ning Tian",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac00",
          "name": "R. J. Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac01",
          "name": "R. L. Jin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac02",
          "name": "S. S. Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac03",
          "name": "Shuang Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac04",
          "name": "Tianyu Sun",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac05",
          "name": "X. Q. Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac06",
          "name": "Xiangyue Jin",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac07",
          "name": "Xiaojin Shen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac08",
          "name": "Xiaosha Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac09",
          "name": "Xinnan Song",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0a",
          "name": "Xinyi Zhou",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0b",
          "name": "Y. X. Zhu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0c",
          "name": "Yanping Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0d",
          "name": "Yaohui Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0e",
          "name": "Yi Zheng",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac0f",
          "name": "Yuchen Zhu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac10",
          "name": "Yunxian Ma",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac11",
          "name": "Zhen Huang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac12",
          "name": "Zhipeng Xu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac13",
          "name": "Zhongyu Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac14",
          "name": "Dongjie Ji",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac15",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac16",
          "name": "Jianzhong Guo",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac17",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac18",
          "name": "Leyi Xia",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac19",
          "name": "Miaojun Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1a",
          "name": "Mingming Li",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1b",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1c",
          "name": "Ruyi Chen",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1d",
          "name": "Shangmian Sun",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1e",
          "name": "Shaoqing Wu",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac1f",
          "name": "Shengfeng Ye",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac20",
          "name": "T. Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac21",
          "name": "W. L. Xiao",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac22",
          "name": "Wei An",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac23",
          "name": "Xianzu Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac24",
          "name": "Xiaowen Sun",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac25",
          "name": "Xiaoxiang Wang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac26",
          "name": "Ying Tang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac27",
          "name": "Yukun Zha",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac28",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac29",
          "name": "Zhe Ju",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac2a",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "692fa6da26742347f61dac2b",
          "name": "Zihua Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T09:25:14.000Z",
      "submittedOnDailyAt": "2025-12-03T00:26:37.248Z",
      "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
      "upvotes": 48,
      "discussionId": "692fa6da26742347f61dac2c",
      "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.",
      "ai_keywords": [
        "DeepSeek Sparse Attention",
        "DSA",
        "reinforcement learning framework",
        "agentic task synthesis pipeline",
        "computational efficiency",
        "long-context scenarios",
        "gold-medal performance",
        "International Mathematical Olympiad",
        "International Olympiad in Informatics",
        "reasoning proficiency"
      ],
      "organization": {
        "_id": "652faff917096ceb6bf53f3f",
        "name": "deepseek-ai",
        "fullname": "DeepSeek",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
      }
    },
    "publishedAt": "2025-12-02T04:25:14.000Z",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 176
    },
    "organization": {
      "_id": "652faff917096ceb6bf53f3f",
      "name": "deepseek-ai",
      "fullname": "DeepSeek",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22609",
      "authors": [
        {
          "_id": "692fa76826742347f61dac2e",
          "name": "Bo Wang",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac2f",
          "name": "Jiehong Lin",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac30",
          "name": "Chenzhi Liu",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac31",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac32",
          "name": "Yifei Yu",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac33",
          "name": "Tianjia Liu",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac34",
          "name": "Zhongrui Wang",
          "hidden": false
        },
        {
          "_id": "692fa76826742347f61dac35",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T16:43:21.000Z",
      "submittedOnDailyAt": "2025-12-03T02:54:07.128Z",
      "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
      "submittedOnDailyBy": {
        "_id": "652845524def261e0febbf5b",
        "avatarUrl": "/avatars/92743e595cbce4e37a8fe7400b5051ba.svg",
        "isPro": false,
        "fullname": "Xinting Hu",
        "user": "kaleidudu",
        "type": "user"
      },
      "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
      "upvotes": 32,
      "discussionId": "692fa76826742347f61dac36",
      "ai_summary": "MG-Nav, a dual-scale framework for zero-shot visual navigation, combines global memory-guided planning with local geometry-enhanced control using a Sparse Spatial Memory Graph and a VGGT-adapter for robust navigation in unseen environments.",
      "ai_keywords": [
        "Sparse Spatial Memory Graph",
        "SMG",
        "keyframe",
        "object semantics",
        "image-to-instance hybrid retrieval",
        "waypoint",
        "point-goal mode",
        "image-goal mode",
        "obstacle-aware control",
        "VGGT-adapter",
        "3D-aware space",
        "HM3D",
        "MP3D",
        "zero-shot performance",
        "dynamic rearrangements"
      ],
      "organization": {
        "_id": "66deb312fd7d68a29348aa8d",
        "name": "TheHKU",
        "fullname": "Hong Kong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
      }
    },
    "publishedAt": "2025-11-27T11:43:21.000Z",
    "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
    "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652845524def261e0febbf5b",
      "avatarUrl": "/avatars/92743e595cbce4e37a8fe7400b5051ba.svg",
      "fullname": "Xinting Hu",
      "name": "kaleidudu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "66deb312fd7d68a29348aa8d",
      "name": "TheHKU",
      "fullname": "Hong Kong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66dc525add44163a31059cf6/kyqlTADY27mPRTqznqQFL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.23369",
      "authors": [
        {
          "_id": "692faf1e26742347f61dac5a",
          "name": "Haochen Tian",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac5b",
          "name": "Tianyu Li",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac5c",
          "name": "Haochen Liu",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac5d",
          "name": "Jiazhi Yang",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac5e",
          "name": "Yihang Qiu",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac5f",
          "name": "Guang Li",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac60",
          "name": "Junli Wang",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac61",
          "name": "Yinfeng Gao",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac62",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac63",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac64",
          "name": "Hangjun Ye",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac65",
          "name": "Tieniu Tan",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac66",
          "name": "Long Chen",
          "hidden": false
        },
        {
          "_id": "692faf1e26742347f61dac67",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-28T17:17:38.000Z",
      "submittedOnDailyAt": "2025-12-03T01:18:11.530Z",
      "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
      "submittedOnDailyBy": {
        "_id": "65c1d586d4c3b8dff20ea129",
        "avatarUrl": "/avatars/bf34d4189142fe48f46aa8351e99ff2d.svg",
        "isPro": false,
        "fullname": "HaochenLiu",
        "user": "georgeliu23333",
        "type": "user"
      },
      "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
      "upvotes": 29,
      "discussionId": "692faf1e26742347f61dac68",
      "projectPage": "https://opendrivelab.com/SimScale",
      "githubRepo": "https://github.com/OpenDriveLab/SimScale",
      "ai_summary": "A simulation framework enhances autonomous driving systems by generating diverse driving scenarios, improving robustness and generalization through co-training with real-world data.",
      "ai_keywords": [
        "neural rendering",
        "reactive environment",
        "multi-view observations",
        "ego trajectory",
        "pseudo-expert trajectory generation",
        "co-training strategy",
        "robustness",
        "generalization",
        "planning methods",
        "EPDMS",
        "navhard",
        "navtest",
        "SimScale",
        "policy improvement",
        "simulation data",
        "real-world data"
      ],
      "githubStars": 40,
      "organization": {
        "_id": "64e5798f5af55fb4d1f171d5",
        "name": "OpenDriveLab",
        "fullname": "OpenDriveLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/dwqGmx-hiCdAikyUuNLkO.png"
      }
    },
    "publishedAt": "2025-11-28T12:17:38.000Z",
    "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
    "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c1d586d4c3b8dff20ea129",
      "avatarUrl": "/avatars/bf34d4189142fe48f46aa8351e99ff2d.svg",
      "fullname": "HaochenLiu",
      "name": "georgeliu23333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "64e5798f5af55fb4d1f171d5",
      "name": "OpenDriveLab",
      "fullname": "OpenDriveLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/dwqGmx-hiCdAikyUuNLkO.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.03041",
      "authors": [
        {
          "_id": "692fcb3726742347f61dad18",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad19",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1a",
          "name": "Baolu Li",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1b",
          "name": "Weikang Bian",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1c",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1d",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1e",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad1f",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad20",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "692fcb3726742347f61dad21",
          "name": "Xu Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T18:59:48.000Z",
      "submittedOnDailyAt": "2025-12-03T03:10:24.737Z",
      "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
      "submittedOnDailyBy": {
        "_id": "646f3418a6a58aa29505fd30",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png",
        "isPro": false,
        "fullname": "QINGHE WANG",
        "user": "DecoderWQH666",
        "type": "user"
      },
      "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
      "upvotes": 24,
      "discussionId": "692fcb3726742347f61dad22",
      "projectPage": "https://qinghew.github.io/MultiShotMaster/",
      "ai_summary": "MultiShotMaster extends a single-shot model with novel RoPE variants for flexible and controllable multi-shot video generation, addressing data scarcity with an automated annotation pipeline.",
      "ai_keywords": [
        "RoPE",
        "Multi-Shot Narrative RoPE",
        "Spatiotemporal Position-Aware RoPE",
        "reference tokens",
        "cross-shot grounding signals",
        "reference images",
        "text-driven inter-shot consistency",
        "motion control",
        "background-driven customized scene"
      ],
      "organization": {
        "_id": "662c559b322afcbae51b3c8b",
        "name": "KlingTeam",
        "fullname": "Kling Team",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
      }
    },
    "publishedAt": "2025-12-02T13:59:48.000Z",
    "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646f3418a6a58aa29505fd30",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png",
      "fullname": "QINGHE WANG",
      "name": "DecoderWQH666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "662c559b322afcbae51b3c8b",
      "name": "KlingTeam",
      "fullname": "Kling Team",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02472",
      "authors": [
        {
          "_id": "692fb2c026742347f61dac94",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac95",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac96",
          "name": "Chengsong Huang",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac97",
          "name": "Kishan Panaganti",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac98",
          "name": "Tianqing Fang",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac99",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "692fb2c026742347f61dac9a",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T07:06:11.000Z",
      "submittedOnDailyAt": "2025-12-03T01:17:53.542Z",
      "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
      "submittedOnDailyBy": {
        "_id": "62ea79dd01ed9b0e8f61ccd3",
        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
        "isPro": false,
        "fullname": "Chengsong Huang",
        "user": "ChengsongHuang",
        "type": "user"
      },
      "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
      "upvotes": 22,
      "discussionId": "692fb2c026742347f61dac9b",
      "ai_summary": "R-Few, a guided Self-Play Challenger-Solver framework, enables stable and controllable model self-evolution with minimal human supervision, achieving performance improvements on math and reasoning benchmarks.",
      "ai_keywords": [
        "self-evolution",
        "superintelligence",
        "concept drift",
        "diversity collapse",
        "mis-evolution",
        "R-Few",
        "Self-Play Challenger-Solver",
        "in-context grounding",
        "mixed training",
        "synthetic question generation",
        "online curriculum",
        "Qwen3-8B-Base",
        "R-Zero",
        "General-Reasoner",
        "ablation studies",
        "co-evolutionary dynamics"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-02T02:06:11.000Z",
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02472.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ea79dd01ed9b0e8f61ccd3",
      "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
      "fullname": "Chengsong Huang",
      "name": "ChengsongHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02425",
      "authors": [
        {
          "_id": "692faddf26742347f61dac4c",
          "name": "Woongyeong Yeo",
          "hidden": false
        },
        {
          "_id": "692faddf26742347f61dac4d",
          "name": "Kangsan Kim",
          "hidden": false
        },
        {
          "_id": "692faddf26742347f61dac4e",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "692faddf26742347f61dac4f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T05:14:52.000Z",
      "submittedOnDailyAt": "2025-12-03T01:36:49.416Z",
      "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
      "submittedOnDailyBy": {
        "_id": "66d30f5fad293ffc4b7672bc",
        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
        "isPro": false,
        "fullname": "Woongyeong Yeo",
        "user": "wgcyeo",
        "type": "user"
      },
      "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
      "upvotes": 18,
      "discussionId": "692faddf26742347f61dac50",
      "projectPage": "https://worldmm.github.io",
      "githubRepo": "https://github.com/wgcyeo/WorldMM",
      "ai_summary": "WorldMM, a novel multimodal memory agent with episodic, semantic, and visual memory, outperforms existing methods in long video question-answering by adaptively retrieving from multiple temporal scales and memory sources.",
      "ai_keywords": [
        "video large language models",
        "memory-augmented methods",
        "textual summaries",
        "visual evidence",
        "multimodal memory agent",
        "episodic memory",
        "semantic memory",
        "visual memory",
        "adaptive retrieval agent",
        "long video question-answering benchmarks"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-12-02T00:14:52.000Z",
    "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
    "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d30f5fad293ffc4b7672bc",
      "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
      "fullname": "Woongyeong Yeo",
      "name": "wgcyeo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.03036",
      "authors": [
        {
          "_id": "692fc40826742347f61dacff",
          "name": "Mengchen Zhang",
          "hidden": false
        },
        {
          "_id": "692fc40826742347f61dad00",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "692fc40826742347f61dad01",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "692fc40826742347f61dad02",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "692fc40826742347f61dad03",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/sM-gbt0oJOGtwEChTx7ZI.mp4"
      ],
      "publishedAt": "2025-12-02T18:56:12.000Z",
      "submittedOnDailyAt": "2025-12-03T02:37:05.086Z",
      "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
      "submittedOnDailyBy": {
        "_id": "64de20c5808492ba6e65d124",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
        "isPro": false,
        "fullname": "Zhang Mengchen",
        "user": "Dubhe-zmc",
        "type": "user"
      },
      "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
      "upvotes": 16,
      "discussionId": "692fc40926742347f61dad04",
      "projectPage": "https://kszpxxzmc.github.io/ViSAudio-project/",
      "githubRepo": "https://github.com/kszpxxzmc/ViSAudio",
      "ai_summary": "An end-to-end framework, ViSAudio, generates high-quality binaural audio from silent video by employing conditional flow matching and a dual-branch architecture, outperforming existing methods in spatial immersion and consistency.",
      "ai_keywords": [
        "binaural spatial audio generation",
        "BiAudio dataset",
        "conditional flow matching",
        "dual-branch audio generation architecture",
        "conditional spacetime module",
        "spatio-temporal alignment"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2025-12-02T13:56:12.000Z",
    "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
    "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/sM-gbt0oJOGtwEChTx7ZI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de20c5808492ba6e65d124",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
      "fullname": "Zhang Mengchen",
      "name": "Dubhe-zmc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19433",
      "authors": [
        {
          "_id": "69268d88243b2216fb75caff",
          "user": {
            "_id": "642c25e68f90c557f7423eb0",
            "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
            "isPro": false,
            "fullname": "Dong Jing",
            "user": "Timsty",
            "type": "user"
          },
          "name": "Dong Jing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-02T08:49:57.902Z",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb00",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb01",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb02",
          "name": "Weiliang Tang",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb03",
          "name": "Zelong Sun",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb04",
          "name": "Yunchao Yao",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb05",
          "name": "Zhenyu Wei",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb06",
          "name": "Yunhui Liu",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb07",
          "name": "Zhiwu Lu",
          "hidden": false
        },
        {
          "_id": "69268d88243b2216fb75cb08",
          "name": "Mingyu Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T18:59:51.000Z",
      "submittedOnDailyAt": "2025-12-03T02:19:45.410Z",
      "title": "Mixture of Horizons in Action Chunking",
      "submittedOnDailyBy": {
        "_id": "642c25e68f90c557f7423eb0",
        "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
        "isPro": false,
        "fullname": "Dong Jing",
        "user": "Timsty",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies _0, _{0.5}, and one-step regression policy _{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, _{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
      "upvotes": 10,
      "discussionId": "69268d88243b2216fb75cb09",
      "projectPage": "https://timsty1.github.io/moh/",
      "githubRepo": "https://github.com/Timsty1/MixtureOfHorizons/tree/main",
      "ai_summary": "A mixture of horizons strategy in VLA models improves performance and generalizability by combining long-term foresight and short-term precision, achieving higher throughput and superior performance in robotic manipulation tasks.",
      "ai_keywords": [
        "Vision-language-action models",
        "action chunk length",
        "horizon",
        "mixture of horizons (MoH)",
        "action transformer",
        "cross-horizon consensus",
        "flow-based policies",
        "one-step regression policy",
        "LIBERO"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-11-24T13:59:51.000Z",
    "title": "Mixture of Horizons in Action Chunking",
    "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies _0, _{0.5}, and one-step regression policy _{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, _{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642c25e68f90c557f7423eb0",
      "avatarUrl": "/avatars/6f86a611994590b871e9ecfed61dc06f.svg",
      "fullname": "Dong Jing",
      "name": "Timsty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.02457",
      "authors": [
        {
          "_id": "692f9ccf26742347f61daad8",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daad9",
          "name": "Hao Lian",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daada",
          "name": "Dachao Hao",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daadb",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daadc",
          "name": "Qingyu Shi",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daadd",
          "name": "Biaolong Chen",
          "hidden": false
        },
        {
          "_id": "692f9ccf26742347f61daade",
          "name": "Hao Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T06:31:38.000Z",
      "submittedOnDailyAt": "2025-12-03T00:20:33.093Z",
      "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
      "submittedOnDailyBy": {
        "_id": "657a6eed1ccc3c2a5ea7b585",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
        "isPro": true,
        "fullname": "Jianzong Wu",
        "user": "jianzongwu",
        "type": "user"
      },
      "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
      "upvotes": 8,
      "discussionId": "692f9cd026742347f61daadf",
      "projectPage": "https://jianzongwu.github.io/projects/does-hearing-help-seeing",
      "githubRepo": "https://github.com/jianzongwu/Does-Hearing-Help-Seeing",
      "ai_summary": "Audio-video joint denoising improves video generation quality by regularizing video dynamics through audio as a privileged signal, as demonstrated by the AVFullDiT architecture.",
      "ai_keywords": [
        "Audio-Video Full DiT",
        "AVFullDiT",
        "text-to-video",
        "text-to-audio",
        "T2AV",
        "T2V",
        "joint denoising",
        "cross-modal co-training",
        "world models"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-12-02T01:31:38.000Z",
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a6eed1ccc3c2a5ea7b585",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
      "fullname": "Jianzong Wu",
      "name": "jianzongwu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.23127",
      "authors": [
        {
          "_id": "692ea20837312eaa83fd8988",
          "name": "Hongfei Zhang",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd8989",
          "name": "Kanghao Chen",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898a",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898c",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898d",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898e",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd898f",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "692ea20837312eaa83fd8990",
          "name": "Yingcong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-28T12:19:57.000Z",
      "submittedOnDailyAt": "2025-12-03T01:09:15.156Z",
      "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
      "submittedOnDailyBy": {
        "_id": "66864408c1d35ab079488662",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66864408c1d35ab079488662/U-btVsJ65j7m6vFLVIka8.jpeg",
        "isPro": false,
        "fullname": "Hongfei (Faye) Zhang",
        "user": "FayeHongfeiZhang",
        "type": "user"
      },
      "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
      "upvotes": 8,
      "discussionId": "692ea20937312eaa83fd8991",
      "projectPage": "https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
      "githubRepo": "https://github.com/EnVision-Research/DualCamCtrl",
      "ai_summary": "DualCamCtrl is a diffusion model for camera-controlled video generation that uses a dual-branch framework and Semantic Guided Mutual Alignment to improve consistency and disentangle appearance and geometry modeling.",
      "ai_keywords": [
        "diffusion model",
        "camera-controlled video generation",
        "ray-based conditions",
        "dual-branch framework",
        "Semantic Guided Mutual Alignment",
        "RGB",
        "depth sequences",
        "RGB-depth fusion",
        "appearance and geometry modeling",
        "denoising stages",
        "camera motion errors"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-11-28T07:19:57.000Z",
    "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
    "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66864408c1d35ab079488662",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66864408c1d35ab079488662/U-btVsJ65j7m6vFLVIka8.jpeg",
      "fullname": "Hongfei (Faye) Zhang",
      "name": "FayeHongfeiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01715",
      "authors": [
        {
          "_id": "692fac3b26742347f61dac38",
          "name": "Wanpeng Zhang",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac39",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3a",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3b",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3c",
          "name": "Yicheng Feng",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3d",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3e",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "692fac3b26742347f61dac3f",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/oemMx2I5b2nw8kp6EjaVc.mp4"
      ],
      "publishedAt": "2025-12-01T14:21:15.000Z",
      "submittedOnDailyAt": "2025-12-03T00:52:47.096Z",
      "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
      "submittedOnDailyBy": {
        "_id": "640dd700fdeaae139081f598",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
        "isPro": false,
        "fullname": "Wanpeng Zhang",
        "user": "zawnpn",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
      "upvotes": 7,
      "discussionId": "692fac3b26742347f61dac40",
      "projectPage": "https://beingbeyond.github.io/DiG-Flow/",
      "githubRepo": "https://github.com/BeingBeyond/DiG-Flow",
      "ai_summary": "DiG-Flow enhances VLA models' robustness by using geometric regularization to align observation and action embeddings, improving performance on complex tasks and with limited data.",
      "ai_keywords": [
        "flow matching",
        "distributional discrepancy",
        "transport cost",
        "empirical distributions",
        "observation embeddings",
        "action embeddings",
        "modulation weight",
        "monotone function",
        "residual updates",
        "contraction",
        "training objective",
        "inference refinement"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "687a8ba5aedd77694bc94386",
        "name": "BeingBeyond",
        "fullname": "BeingBeyond",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
      }
    },
    "publishedAt": "2025-12-01T09:21:15.000Z",
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/oemMx2I5b2nw8kp6EjaVc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640dd700fdeaae139081f598",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
      "fullname": "Wanpeng Zhang",
      "name": "zawnpn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "687a8ba5aedd77694bc94386",
      "name": "BeingBeyond",
      "fullname": "BeingBeyond",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02899",
      "authors": [
        {
          "_id": "692fad4226742347f61dac42",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac43",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac44",
          "name": "Songjie Wu",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac45",
          "name": "Junchao Yi",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac46",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac47",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac48",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "692fad4226742347f61dac49",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T16:05:21.000Z",
      "submittedOnDailyAt": "2025-12-03T00:54:18.768Z",
      "title": "Glance: Accelerating Diffusion Models with 1 Sample",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
      "upvotes": 6,
      "discussionId": "692fad4226742347f61dac4a",
      "ai_summary": "Using phase-aware LoRA adapters, diffusion models achieve efficient acceleration and strong generalization with minimal retraining.",
      "ai_keywords": [
        "diffusion models",
        "computational cost",
        "inference steps",
        "distillation",
        "compact student models",
        "retraining costs",
        "phase-aware strategy",
        "slow and fast denoising phases",
        "lightweight LoRA adapters",
        "Slow-LoRA",
        "Fast-LoRA",
        "acceleration",
        "visual quality",
        "benchmarks",
        "samples",
        "V100",
        "unseen prompts"
      ]
    },
    "publishedAt": "2025-12-02T11:05:21.000Z",
    "title": "Glance: Accelerating Diffusion Models with 1 Sample",
    "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02899.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8830
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01989",
      "authors": [
        {
          "_id": "692e9bc537312eaa83fd896b",
          "name": "Fengzhe Zhou",
          "hidden": false
        },
        {
          "_id": "692e9bc537312eaa83fd896c",
          "name": "Jiannan Huang",
          "hidden": false
        },
        {
          "_id": "692e9bc537312eaa83fd896d",
          "name": "Jialuo Li",
          "hidden": false
        },
        {
          "_id": "692e9bc537312eaa83fd896e",
          "name": "Deva Ramanan",
          "hidden": false
        },
        {
          "_id": "692e9bc537312eaa83fd896f",
          "name": "Humphrey Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T18:47:39.000Z",
      "submittedOnDailyAt": "2025-12-03T00:15:44.928Z",
      "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
      "upvotes": 4,
      "discussionId": "692e9bc637312eaa83fd8970",
      "githubRepo": "https://github.com/SHI-Labs/physical-ai-bench",
      "ai_summary": "PAI-Bench evaluates the perception and prediction capabilities of multi-modal large language models and video generative models, revealing limitations in physical coherence and causal reasoning.",
      "ai_keywords": [
        "multi-modal large language models",
        "video generative models",
        "video generation",
        "conditional video generation",
        "video understanding",
        "physical plausibility",
        "domain-specific reasoning",
        "physically coherent dynamics",
        "forecasting",
        "causal interpretation"
      ],
      "githubStars": 30
    },
    "publishedAt": "2025-12-01T13:47:39.000Z",
    "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01989.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 176
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22586",
      "authors": [
        {
          "_id": "692d22f24397b1ec214f690b",
          "name": "Yifan Du",
          "hidden": false
        },
        {
          "_id": "692d22f24397b1ec214f690c",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "692d22f24397b1ec214f690d",
          "name": "Yingqian Min",
          "hidden": false
        },
        {
          "_id": "692d22f24397b1ec214f690e",
          "name": "Yue Ling",
          "hidden": false
        },
        {
          "_id": "692d22f24397b1ec214f690f",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "692d22f24397b1ec214f6910",
          "name": "Youbin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T16:19:34.000Z",
      "submittedOnDailyAt": "2025-12-03T00:27:02.051Z",
      "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
      "submittedOnDailyBy": {
        "_id": "61d78857a21a9b49c7e8e4a9",
        "avatarUrl": "/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg",
        "isPro": false,
        "fullname": "Yifan Du",
        "user": "Richard1999",
        "type": "user"
      },
      "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
      "upvotes": 4,
      "discussionId": "692d22f24397b1ec214f6911",
      "ai_summary": "Investigating different Chain-of-Thought designs in vision-language models reveals that concise grounding steps are most effective for improving generalizable visual reasoning across various tasks.",
      "ai_keywords": [
        "Chain-of-Thought",
        "vision-language models",
        "visual reasoning",
        "CoT",
        "Grounding CoT",
        "Visual CoT",
        "maze-solving benchmark",
        "Qwen2.5-VL-7B",
        "SFT-then-RL",
        "visual reasoning tasks"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-27T11:19:34.000Z",
    "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
    "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61d78857a21a9b49c7e8e4a9",
      "avatarUrl": "/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg",
      "fullname": "Yifan Du",
      "name": "Richard1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01248",
      "authors": [
        {
          "_id": "692e868f37312eaa83fd8910",
          "name": "Junyuan Zhang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8911",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8912",
          "name": "Qintong Zhang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8913",
          "name": "Fan Wu",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8914",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8915",
          "name": "Jialin Lu",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8916",
          "name": "Junjie Shan",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8917",
          "name": "Ziqi Zhao",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8918",
          "name": "Shuya Yang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd8919",
          "name": "Ziling Wang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891a",
          "name": "Ziyang Miao",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891b",
          "name": "Huaping Zhong",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891c",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891d",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891e",
          "name": "Ka-Ho Chow",
          "hidden": false
        },
        {
          "_id": "692e868f37312eaa83fd891f",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T03:49:00.000Z",
      "submittedOnDailyAt": "2025-12-03T02:12:58.590Z",
      "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
      "submittedOnDailyBy": {
        "_id": "65411de281a8731a1c28293d",
        "avatarUrl": "/avatars/18607dcd65303e1f688fae8015f43314.svg",
        "isPro": false,
        "fullname": "junyuan",
        "user": "Carkham",
        "type": "user"
      },
      "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
      "upvotes": 3,
      "discussionId": "692e868f37312eaa83fd8920",
      "ai_summary": "TRivia, a self-supervised fine-tuning method, enables pretrained vision-language models to learn table recognition from unlabeled data using a question-answering-based reward mechanism, outperforming existing models on popular benchmarks.",
      "ai_keywords": [
        "self-supervised fine-tuning",
        "vision-language models",
        "Group Relative Policy Optimization",
        "attention-guided module",
        "question-answering-based reward mechanism",
        "table recognition",
        "TRivia",
        "TRivia-3B",
        "Gemini 2.5 Pro",
        "MinerU2.5"
      ]
    },
    "publishedAt": "2025-11-30T22:49:00.000Z",
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
    "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01248.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65411de281a8731a1c28293d",
      "avatarUrl": "/avatars/18607dcd65303e1f688fae8015f43314.svg",
      "fullname": "junyuan",
      "name": "Carkham",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00903",
      "authors": [
        {
          "_id": "692fbdef26742347f61dacaf",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb0",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb1",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb2",
          "name": "Zheng Zhu",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb3",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb4",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb5",
          "name": "Tianrun Chen",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb6",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb7",
          "name": "Haoyun Li",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb8",
          "name": "Zhehao Dong",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacb9",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacba",
          "name": "Yun Ye",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacbb",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacbc",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "692fbdef26742347f61dacbd",
          "name": "Wenjun Mei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-30T14:10:28.000Z",
      "submittedOnDailyAt": "2025-12-03T02:07:18.024Z",
      "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
      "submittedOnDailyBy": {
        "_id": "656e9b562cd7a3e348011d26",
        "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
        "isPro": false,
        "fullname": "Zheng Zhu",
        "user": "ZhengZhu",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
      "upvotes": 3,
      "discussionId": "692fbdef26742347f61dacbe",
      "projectPage": "https://swiftvla.github.io/",
      "githubRepo": "https://github.com/GigaAI-research/SwiftVLA",
      "ai_summary": "SwiftVLA enhances compact Vision-Language-Action models with 4D understanding using Fusion Tokens and a mask-and-reconstruct strategy, achieving high performance with reduced computational and memory demands.",
      "ai_keywords": [
        "pretrained Vision-Language Models",
        "VLA models",
        "spatiotemporal reasoning",
        "lightweight VLM",
        "3D inputs",
        "4D visual geometry transformer",
        "temporal cache",
        "Fusion Tokens",
        "future prediction objective",
        "mask-and-reconstruct strategy",
        "4D representations",
        "edge devices"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "68f5feb151a9558c3dc84362",
        "name": "GigaAI-Research",
        "fullname": "GigaAI-Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
      }
    },
    "publishedAt": "2025-11-30T09:10:28.000Z",
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656e9b562cd7a3e348011d26",
      "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
      "fullname": "Zheng Zhu",
      "name": "ZhengZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "68f5feb151a9558c3dc84362",
      "name": "GigaAI-Research",
      "fullname": "GigaAI-Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b68f3cdd7f21b758968b8d/8uazzXK5v3edLbCaNvmup.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22973",
      "authors": [
        {
          "_id": "692fc0c026742347f61dacc0",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc1",
          "name": "Shuning Chang",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc2",
          "name": "Yuanyu He",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc3",
          "name": "Yizeng Han",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc4",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc5",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "692fc0c026742347f61dacc6",
          "name": "Bohan Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/3Lvr1j1cc28m7caAw-606.mp4"
      ],
      "publishedAt": "2025-11-28T08:25:59.000Z",
      "submittedOnDailyAt": "2025-12-03T02:19:51.264Z",
      "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
      "upvotes": 2,
      "discussionId": "692fc0c126742347f61dacc7",
      "projectPage": "https://ziplab.co/BlockVid/",
      "githubRepo": "https://github.com/alibaba-damo-academy/Inferix/",
      "ai_summary": "BlockVid addresses challenges in block diffusion video generation by employing semantic-aware sparse KV caching, Block Forcing training, and noise scheduling to produce high-quality, coherent minute-long videos.",
      "ai_keywords": [
        "semi-autoregressive",
        "block diffusion",
        "diffusion models",
        "autoregressive models",
        "KV caching",
        "parallel sampling",
        "KV-cache-induced long-horizon error accumulation",
        "BlockVid",
        "semantic-aware sparse KV cache",
        "Block Forcing",
        "chunk-wise noise scheduling",
        "temporal consistency",
        "LV-Bench",
        "long-range coherence",
        "VBench",
        "VDE Subject",
        "VDE Clarity"
      ],
      "githubStars": 62,
      "organization": {
        "_id": "6808e7522a4d69d5111da55f",
        "name": "Alibaba-DAMO-Academy",
        "fullname": "DAMO Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
      }
    },
    "publishedAt": "2025-11-28T03:25:59.000Z",
    "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
    "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/3Lvr1j1cc28m7caAw-606.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22973.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "6808e7522a4d69d5111da55f",
      "name": "Alibaba-DAMO-Academy",
      "fullname": "DAMO Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.03040",
      "authors": [
        {
          "_id": "692fb17b26742347f61dac7e",
          "name": "Zeqi Xiao",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac7f",
          "name": "Yiwei Zhao",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac80",
          "name": "Lingxiao Li",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac81",
          "name": "Yushi Lan",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac82",
          "name": "Yu Ning",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac83",
          "name": "Rahul Garg",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac84",
          "name": "Roshni Cooper",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac85",
          "name": "Mohammad H. Taghavi",
          "hidden": false
        },
        {
          "_id": "692fb17b26742347f61dac86",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T18:59:44.000Z",
      "submittedOnDailyAt": "2025-12-03T01:13:12.275Z",
      "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
      "upvotes": 1,
      "discussionId": "692fb17c26742347f61dac87",
      "ai_summary": "Video4Spatial demonstrates that video diffusion models can perform complex spatial tasks using only visual data, achieving strong spatial understanding and generalization.",
      "ai_keywords": [
        "video diffusion models",
        "scene navigation",
        "object grounding",
        "semantic localization",
        "instruction following",
        "planning",
        "3D geometry",
        "video-only inputs",
        "spatial consistency",
        "generalization"
      ]
    },
    "publishedAt": "2025-12-02T13:59:44.000Z",
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8830
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02492",
      "authors": [
        {
          "_id": "692faf0326742347f61dac52",
          "name": "Jiahui Chen",
          "hidden": false
        },
        {
          "_id": "692faf0326742347f61dac53",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "692faf0326742347f61dac54",
          "name": "Runhua Shi",
          "hidden": false
        },
        {
          "_id": "692faf0326742347f61dac55",
          "name": "Huan Yang",
          "hidden": false
        },
        {
          "_id": "692faf0326742347f61dac56",
          "name": "Chaofan Ding",
          "hidden": false
        },
        {
          "_id": "692faf0326742347f61dac57",
          "name": "Zihao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T07:31:19.000Z",
      "submittedOnDailyAt": "2025-12-03T01:02:18.953Z",
      "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
      "upvotes": 1,
      "discussionId": "692faf0326742347f61dac58",
      "ai_summary": "YingVideo-MV generates high-quality music performance videos with synchronized camera motion using cascaded frameworks, audio semantic analysis, and temporal-aware diffusion Transformers.",
      "ai_keywords": [
        "diffusion model",
        "audio-driven avatar video generation",
        "audio semantic analysis",
        "shot planning module",
        "MV-Director",
        "temporal-aware diffusion Transformer architectures",
        "long-sequence consistency modeling",
        "Music-in-the-Wild Dataset",
        "camera adapter module",
        "latent noise",
        "time-aware dynamic window range strategy",
        "denoising ranges",
        "audio embedding",
        "music-motion-camera synchronization"
      ]
    },
    "publishedAt": "2025-12-02T02:31:19.000Z",
    "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8830
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02423",
      "authors": [
        {
          "_id": "692fa63626742347f61dab09",
          "name": "Haolong Yan",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0a",
          "name": "Yeqing Shen",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0b",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0c",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0d",
          "name": "Kaijun Tan",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0e",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab0f",
          "name": "Hongxin Li",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab10",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab11",
          "name": "Osamu Yoshie",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab12",
          "name": "Si Li",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab13",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "692fa63626742347f61dab14",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T05:11:23.000Z",
      "submittedOnDailyAt": "2025-12-03T00:24:05.516Z",
      "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
      "upvotes": 1,
      "discussionId": "692fa63726742347f61dab15",
      "ai_summary": "GUI Exploration Lab enables effective training and evaluation of GUI agents through simulation, leveraging supervised and reinforcement learning to enhance navigation capabilities.",
      "ai_keywords": [
        "GUI Exploration Lab",
        "supervised fine-tuning",
        "single-turn reinforcement learning",
        "multi-turn reinforcement learning",
        "GUI agent navigation",
        "screen navigation",
        "static benchmarks",
        "interactive benchmarks"
      ]
    },
    "publishedAt": "2025-12-02T00:11:23.000Z",
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02423.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 176
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01988",
      "authors": [
        {
          "_id": "692fe12026742347f61dadbb",
          "name": "Wei Tang",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadbc",
          "name": "Yanpeng Sun",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadbd",
          "name": "Shan Zhang",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadbe",
          "name": "Xiaofan Li",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadbf",
          "name": "Piotr Koniusz",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadc0",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadc1",
          "name": "Na Zhao",
          "hidden": false
        },
        {
          "_id": "692fe12026742347f61dadc2",
          "name": "Zechao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T18:45:30.000Z",
      "submittedOnDailyAt": "2025-12-03T04:36:51.749Z",
      "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
      "submittedOnDailyBy": {
        "_id": "64297212e5f33939cf3a3d9b",
        "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg",
        "isPro": false,
        "fullname": "yanpeng_sun",
        "user": "syp115",
        "type": "user"
      },
      "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
      "upvotes": 1,
      "discussionId": "692fe12126742347f61dadc3",
      "projectPage": "https://vi-ocean.github.io/projects/artemis/",
      "githubRepo": "https://github.com/WayneTomas/Artemis",
      "ai_summary": "Artemis, a perception-policy learning framework, enhances performance on visual tasks by using structured spatial reasoning with (label, bounding-box) pairs instead of linguistic intermediate reasoning.",
      "ai_keywords": [
        "reinforcement-learning frameworks",
        "visual perception policy",
        "intermediate reasoning chains",
        "natural language",
        "semantic reasoning",
        "spatial reasoning",
        "object-centric space",
        "structured proposal-based reasoning",
        "bounding-box",
        "verifiable visual state",
        "direct supervision",
        "ambiguity",
        "grounding",
        "detection",
        "counting",
        "geometric-perception",
        "MLLM benchmarks",
        "scalable perception policies"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-12-01T13:45:30.000Z",
    "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01988.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64297212e5f33939cf3a3d9b",
      "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg",
      "fullname": "yanpeng_sun",
      "name": "syp115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22146",
      "authors": [
        {
          "_id": "692fd6c726742347f61dad69",
          "name": "Kairong Han",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6a",
          "name": "Nuanqiao Shan",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6b",
          "name": "Ziyu Zhao",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6c",
          "name": "Zijing Hu",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6d",
          "name": "Xinpeng Dong",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6e",
          "name": "Junjian Ye",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad6f",
          "name": "Lujia Pan",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad70",
          "name": "Fei Wu",
          "hidden": false
        },
        {
          "_id": "692fd6c726742347f61dad71",
          "name": "Kun Kuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T06:33:33.000Z",
      "submittedOnDailyAt": "2025-12-03T03:52:11.482Z",
      "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
      "submittedOnDailyBy": {
        "_id": "68c6b73ad0bef8d72febf45d",
        "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
        "isPro": false,
        "fullname": "Kairong Han",
        "user": "Kairong-Han",
        "type": "user"
      },
      "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
      "upvotes": 1,
      "discussionId": "692fd6c726742347f61dad72",
      "ai_summary": "A Causal Concept-Guided Diffusion Language Model (C2DLM) improves reasoning capabilities by learning causal relationships between concepts, enhancing performance and training efficiency in downstream tasks.",
      "ai_keywords": [
        "autoregressive models",
        "diffusion language models",
        "causal knowledge",
        "next token prediction",
        "fully connected attention",
        "causal graph",
        "causal relationships",
        "causal inversion",
        "COT-OrderPerturb task",
        "downstream reasoning tasks"
      ]
    },
    "publishedAt": "2025-11-27T01:33:33.000Z",
    "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
    "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68c6b73ad0bef8d72febf45d",
      "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
      "fullname": "Kairong Han",
      "name": "Kairong-Han",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20344",
      "authors": [
        {
          "_id": "692eaf5537312eaa83fd899b",
          "user": {
            "_id": "6225c3ddc201a0ad62fee53d",
            "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
            "isPro": false,
            "fullname": "Taewhoo Lee",
            "user": "Taewhoo",
            "type": "user"
          },
          "name": "Taewhoo Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-02T16:50:15.761Z",
          "hidden": false
        },
        {
          "_id": "692eaf5537312eaa83fd899c",
          "name": "Minju Song",
          "hidden": false
        },
        {
          "_id": "692eaf5537312eaa83fd899d",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "692eaf5537312eaa83fd899e",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "692eaf5537312eaa83fd899f",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T14:23:58.000Z",
      "submittedOnDailyAt": "2025-12-03T03:44:17.639Z",
      "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6225c3ddc201a0ad62fee53d",
        "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
        "isPro": false,
        "fullname": "Taewhoo Lee",
        "user": "Taewhoo",
        "type": "user"
      },
      "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
      "upvotes": 1,
      "discussionId": "692eaf5537312eaa83fd89a0",
      "ai_summary": "LLMs can encode and apply high-level relational concepts in analogical reasoning but face limitations, particularly when relational information is missing or when transferring to new entities.",
      "ai_keywords": [
        "analogical reasoning",
        "LLMs",
        "task patterns",
        "surface-level concepts",
        "high-level relational concepts",
        "proportional and story analogies",
        "attributive information",
        "relational information",
        "mid-upper layers",
        "hidden representations",
        "token positions",
        "structural alignment"
      ]
    },
    "publishedAt": "2025-11-25T09:23:58.000Z",
    "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
    "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6225c3ddc201a0ad62fee53d",
      "avatarUrl": "/avatars/c42fef2c448deb8336d3aebc11bab148.svg",
      "fullname": "Taewhoo Lee",
      "name": "Taewhoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.02942",
      "authors": [
        {
          "_id": "692fa4dd26742347f61daaf1",
          "name": "Lanxiang Hu",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf2",
          "name": "Abhilash Shankarampeta",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf3",
          "name": "Yixin Huang",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf4",
          "name": "Zilin Dai",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf5",
          "name": "Haoyang Yu",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf6",
          "name": "Yujie Zhao",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf7",
          "name": "Haoqiang Kang",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf8",
          "name": "Daniel Zhao",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daaf9",
          "name": "Tajana Rosing",
          "hidden": false
        },
        {
          "_id": "692fa4dd26742347f61daafa",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-02T17:11:23.000Z",
      "submittedOnDailyAt": "2025-12-03T00:18:36.179Z",
      "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
      "upvotes": 0,
      "discussionId": "692fa4dd26742347f61daafb",
      "ai_summary": "VideoScience-Bench evaluates video models' scientific reasoning by assessing their ability to generate phenomena consistent with undergraduate-level physics and chemistry concepts.",
      "ai_keywords": [
        "zero-shot reasoning",
        "scientific reasoning",
        "VideoScience-Bench",
        "prompts",
        "scientific concepts",
        "T2V",
        "I2V",
        "Prompt Consistency",
        "Phenomenon Congruency",
        "Correct Dynamism",
        "Immutability",
        "Spatio-Temporal Continuity",
        "VLM-as-a-Judge"
      ]
    },
    "publishedAt": "2025-12-02T12:11:23.000Z",
    "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 176
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02790",
      "authors": [
        {
          "_id": "692fa53126742347f61daafd",
          "name": "Keming Ye",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61daafe",
          "name": "Zhipeng Huang",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61daaff",
          "name": "Canmiao Fu",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab00",
          "name": "Qingyang Liu",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab01",
          "name": "Jiani Cai",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab02",
          "name": "Zheqi Lv",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab03",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab04",
          "name": "Jing Lyu",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab05",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "692fa53126742347f61dab06",
          "name": "Shengyu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/DrQsXnJQZzdo8CTSQgen2.png"
      ],
      "publishedAt": "2025-12-01T17:45:44.000Z",
      "submittedOnDailyAt": "2025-12-03T00:19:53.098Z",
      "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
      "upvotes": 0,
      "discussionId": "692fa53126742347f61dab07",
      "ai_summary": "A lightweight data pipeline and benchmark are introduced to improve the quality and scale of image editing datasets and model evaluations, addressing the performance gap between closed-source and open-source models.",
      "ai_keywords": [
        "GPT-4o",
        "Nano Banana",
        "Seedream 4.0",
        "multimodal models",
        "Qwen-Verify",
        "UnicEdit-10M",
        "UnicBench",
        "Non-edit Consistency",
        "Reasoning Accuracy"
      ]
    },
    "publishedAt": "2025-12-01T12:45:44.000Z",
    "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
    "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/DrQsXnJQZzdo8CTSQgen2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 176
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.15948",
      "authors": [
        {
          "_id": "692fc10826742347f61dacc9",
          "name": "Raphael Ruschel",
          "hidden": false
        },
        {
          "_id": "692fc10826742347f61dacca",
          "name": "Hardikkumar Prajapati",
          "hidden": false
        },
        {
          "_id": "692fc10826742347f61daccb",
          "name": "Awsafur Rahman",
          "hidden": false
        },
        {
          "_id": "692fc10826742347f61daccc",
          "name": "B. S. Manjunath",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/dW4aVZP_cWfVUb9BrIu5V.png",
        "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/CuhWni1q3xKfIRNK1NEXu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/cy1dWRobfR3zQpcxXqUxV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/vy_6ImEUZKYASZKcSzZoT.png"
      ],
      "publishedAt": "2025-11-20T00:49:25.000Z",
      "submittedOnDailyAt": "2025-12-03T02:29:10.006Z",
      "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
      "submittedOnDailyBy": {
        "_id": "60d34de513f774189902f547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
        "isPro": false,
        "fullname": "Awsaf",
        "user": "awsaf49",
        "type": "user"
      },
      "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
      "upvotes": 0,
      "discussionId": "692fc10826742347f61daccd",
      "ai_summary": "Click2Graph is an interactive framework for panoptic video scene graph generation that combines user cues with dynamic interaction discovery and semantic classification for precise and controllable scene understanding.",
      "ai_keywords": [
        "Video Scene Graph Generation",
        "VSGG",
        "promptable segmentation models",
        "SAM2",
        "Panoptic Video Scene Graph Generation",
        "PVSG",
        "user interaction",
        "visual prompting",
        "Dynamic Interaction Discovery Module",
        "Semantic Classification Head",
        "scene graph",
        "openPVSG benchmark"
      ],
      "organization": {
        "_id": "691d9d63284266ada1eb632a",
        "name": "UCSantaBarbara",
        "fullname": "University of California, Santa Barbara",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/AoPyKoRF9O8PHOoaJOi7h.png"
      }
    },
    "publishedAt": "2025-11-19T19:49:25.000Z",
    "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
    "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/dW4aVZP_cWfVUb9BrIu5V.png",
      "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/CuhWni1q3xKfIRNK1NEXu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/cy1dWRobfR3zQpcxXqUxV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/vy_6ImEUZKYASZKcSzZoT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d34de513f774189902f547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
      "fullname": "Awsaf",
      "name": "awsaf49",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "691d9d63284266ada1eb632a",
      "name": "UCSantaBarbara",
      "fullname": "University of California, Santa Barbara",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/AoPyKoRF9O8PHOoaJOi7h.png"
    },
    "isAuthorParticipating": false
  }
]