[
  {
    "paper": {
      "id": "2510.22115",
      "authors": [
        {
          "_id": "690977f9812eca10f9cc61c7",
          "name": "Ling-Team",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61c8",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61c9",
          "name": "Ben Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ca",
          "name": "Binbin Hu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61cb",
          "name": "Bing Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61cc",
          "name": "Bingwei Zeng",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61cd",
          "name": "Borui Ye",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ce",
          "name": "Caizhi Tang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61cf",
          "name": "Changxin Tian",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d0",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d1",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d2",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d3",
          "name": "Chenchen Ju",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d4",
          "name": "Chenchen Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d5",
          "name": "Chengfu Tang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d6",
          "name": "Chili Fu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d7",
          "name": "Chunshao Ren",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d8",
          "name": "Chunwei Wu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61d9",
          "name": "Cong Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61da",
          "name": "Cunyin Peng",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61db",
          "name": "Dafeng Xu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61dc",
          "name": "Daixin Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61dd",
          "name": "Dalong Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61de",
          "name": "Dingnan Jin",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61df",
          "name": "Dingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e0",
          "name": "Dongke Hu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e1",
          "name": "Fangzheng Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e2",
          "name": "Feifan Wu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e3",
          "name": "Feng Zhu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e4",
          "name": "Gangshan Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e5",
          "name": "Haitao Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e6",
          "name": "Hailin Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e7",
          "name": "Hanxiao Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e8",
          "name": "Hanzi Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61e9",
          "name": "Hao Qian",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ea",
          "name": "Haoyi Yu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61eb",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ec",
          "name": "Hongliang Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ed",
          "name": "Hongzhi Luan",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ee",
          "name": "Huirong Dong",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ef",
          "name": "Huizhong Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f0",
          "name": "Jia Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f1",
          "name": "Jia Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f2",
          "name": "Jialong Zhu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f3",
          "name": "Jian Sha",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f4",
          "name": "Jianping Wei",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f5",
          "name": "Jiaolong Yang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f6",
          "name": "Jieyue Ma",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f7",
          "name": "Jiewei Wu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f8",
          "name": "Jinjing Huang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61f9",
          "name": "Jingyun Tian",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61fa",
          "name": "Jingyuan Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61fb",
          "name": "Jinquan Sun",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61fc",
          "name": "Juanhui Tu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61fd",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61fe",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc61ff",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6200",
          "name": "Junjie Ou",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6201",
          "name": "Junpeng Fang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6202",
          "name": "Kaihong Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6203",
          "name": "Kaiqin Hu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6204",
          "name": "Ke Shi",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6205",
          "name": "Kun Tang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6206",
          "name": "Kunlong Chen",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6207",
          "name": "Lanyin Mei",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6208",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6209",
          "name": "Lei Xu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620a",
          "name": "Libo Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620b",
          "name": "Lin Ju",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620c",
          "name": "Lin Yuan",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620d",
          "name": "Ling Zhong",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620e",
          "name": "Lintao Ma",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc620f",
          "name": "Lu Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6210",
          "name": "Lu Yu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6211",
          "name": "Lun Cai",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6212",
          "name": "Meiqi Zhu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6213",
          "name": "Mengying Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6214",
          "name": "Min Chen",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6215",
          "name": "Minghao Xue",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6216",
          "name": "Minghong Cai",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6217",
          "name": "Mingming Yin",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6218",
          "name": "Peijie Jiang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6219",
          "name": "Peilong Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621a",
          "name": "Pingping Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621b",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621c",
          "name": "Qing Cui",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621d",
          "name": "Qingxiang Huang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621e",
          "name": "Qingyuan Yang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc621f",
          "name": "Quankun Yu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6220",
          "name": "Shaowei Wei",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6221",
          "name": "Shijie Lian",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6222",
          "name": "Shoujian Zheng",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6223",
          "name": "Shun Song",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6224",
          "name": "Shungen Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6225",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6226",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6227",
          "name": "Song Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6228",
          "name": "Ting Guo",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6229",
          "name": "Tong Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622a",
          "name": "Wanli Gu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622b",
          "name": "Weichang Wu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622c",
          "name": "Weiguang Han",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622d",
          "name": "Wenjing Fang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622e",
          "name": "Wubin Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc622f",
          "name": "Xiang Shu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6230",
          "name": "Xiao Shi",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6231",
          "name": "Xiaoshun Lan",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6232",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6233",
          "name": "Xiaqing Sun",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6234",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6235",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6236",
          "name": "Xiong Xu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6237",
          "name": "Xudong Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6238",
          "name": "Xudong Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6239",
          "name": "Xuemin Yang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623a",
          "name": "Yajie Yang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623b",
          "name": "Yang Xiang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623c",
          "name": "Yanzhe Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623d",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623e",
          "name": "Yilong Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc623f",
          "name": "Yingxue Li",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6240",
          "name": "Yongzhen Guo",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6241",
          "name": "Yuzhuo Fu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6242",
          "name": "Yuanyuan Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6243",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6244",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6245",
          "name": "Yufeng Deng",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6246",
          "name": "Yun Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6247",
          "name": "Yunfei Xu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6248",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6249",
          "name": "Yuxiao He",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624a",
          "name": "Zengke Gui",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624b",
          "name": "Zhaoxin Huan",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624c",
          "name": "Zhaoyang Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624d",
          "name": "Zhibo Zhu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624e",
          "name": "Zhihao Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc624f",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6250",
          "name": "Zhoufei Wang",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6251",
          "name": "Zihang Zeng",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6252",
          "name": "Ziqi Liu",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6253",
          "name": "Zitao Xuan",
          "hidden": false
        },
        {
          "_id": "690977f9812eca10f9cc6254",
          "name": "Zuoli Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-25T01:51:37.000Z",
      "submittedOnDailyAt": "2025-11-04T01:24:23.418Z",
      "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
      "submittedOnDailyBy": {
        "_id": "677e8c5624bd3d7373584b0c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
        "isPro": false,
        "fullname": "Zhang Zhiqiang",
        "user": "zzqsmall",
        "type": "user"
      },
      "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
      "upvotes": 47,
      "discussionId": "690977f9812eca10f9cc6255",
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2025-10-24T21:51:37.000Z",
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
    "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677e8c5624bd3d7373584b0c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e8c5624bd3d7373584b0c/36DAI9UR_q3F4LqKZEjho.jpeg",
      "fullname": "Zhang Zhiqiang",
      "name": "zzqsmall",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24788",
      "authors": [
        {
          "_id": "69096b57812eca10f9cc6185",
          "name": "Xinjian Zhao",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc6186",
          "name": "Wei Pang",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc6187",
          "name": "Zhongkai Xue",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc6188",
          "name": "Xiangru Jian",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc6189",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc618a",
          "name": "Yaoyao Xu",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc618b",
          "name": "Xiaozhuang Song",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc618c",
          "name": "Shu Wu",
          "hidden": false
        },
        {
          "_id": "69096b57812eca10f9cc618d",
          "name": "Tianshu Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T05:11:44.000Z",
      "submittedOnDailyAt": "2025-11-04T02:03:48.972Z",
      "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "68380f4f231cf484dd4e87f4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
        "isPro": false,
        "fullname": "Xinjian Zhao",
        "user": "Xinjiansz",
        "type": "user"
      },
      "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
      "upvotes": 24,
      "discussionId": "69096b57812eca10f9cc618e"
    },
    "publishedAt": "2025-10-27T01:11:44.000Z",
    "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
    "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24788.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "68380f4f231cf484dd4e87f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xp34hfiSLf-DiE1DVVhHk.png",
      "fullname": "Xinjian Zhao",
      "name": "Xinjiansz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.00086",
      "authors": [
        {
          "_id": "69096c6c812eca10f9cc6190",
          "name": "Fali Wang",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6191",
          "name": "Jihai Chen",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6192",
          "name": "Shuhua Yang",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6193",
          "name": "Runxue Bao",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6194",
          "name": "Tianxiang Zhao",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6195",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6196",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6197",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6198",
          "name": "Qi He",
          "hidden": false
        },
        {
          "_id": "69096c6c812eca10f9cc6199",
          "name": "Suhang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T22:14:25.000Z",
      "submittedOnDailyAt": "2025-11-04T00:31:32.141Z",
      "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
      "submittedOnDailyBy": {
        "_id": "644a8ca97c5c68c7762906a0",
        "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
        "isPro": false,
        "fullname": "Fali Wang",
        "user": "FairyFali",
        "type": "user"
      },
      "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
      "upvotes": 21,
      "discussionId": "69096c6c812eca10f9cc619a",
      "organization": {
        "_id": "623c72b6483fb88b35620a27",
        "name": "PennState",
        "fullname": "Pennsylvania State University"
      }
    },
    "publishedAt": "2025-10-29T18:14:25.000Z",
    "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
    "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a8ca97c5c68c7762906a0",
      "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
      "fullname": "Fali Wang",
      "name": "FairyFali",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "623c72b6483fb88b35620a27",
      "name": "PennState",
      "fullname": "Pennsylvania State University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01678",
      "authors": [
        {
          "_id": "6909a211812eca10f9cc63ca",
          "name": "Ropeway Liu",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63cb",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63cc",
          "name": "Bo Dong",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63cd",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63ce",
          "name": "Jinwang Wang",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63cf",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63d0",
          "name": "Yan Xing",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63d1",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "6909a211812eca10f9cc63d2",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T15:41:41.000Z",
      "submittedOnDailyAt": "2025-11-04T04:21:00.451Z",
      "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
      "upvotes": 20,
      "discussionId": "6909a212812eca10f9cc63d3",
      "organization": {
        "_id": "6808e7522a4d69d5111da55f",
        "name": "Alibaba-DAMO-Academy",
        "fullname": "DAMO Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
      }
    },
    "publishedAt": "2025-11-03T10:41:41.000Z",
    "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
    "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01678.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "6808e7522a4d69d5111da55f",
      "name": "Alibaba-DAMO-Academy",
      "fullname": "DAMO Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01163",
      "authors": [
        {
          "_id": "690986f4812eca10f9cc6384",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc6385",
          "name": "Wei Chow",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc6386",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc6387",
          "name": "Ziqiao Ma",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc6388",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc6389",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc638a",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc638b",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc638c",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "690986f4812eca10f9cc638d",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T02:27:46.000Z",
      "submittedOnDailyAt": "2025-11-04T02:28:41.642Z",
      "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6646d5819bb34d2b6b7455d3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
        "isPro": false,
        "fullname": "Yongyuan Liang",
        "user": "cheryyunl",
        "type": "user"
      },
      "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
      "upvotes": 13,
      "discussionId": "690986f4812eca10f9cc638e"
    },
    "publishedAt": "2025-11-02T21:27:46.000Z",
    "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6646d5819bb34d2b6b7455d3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
      "fullname": "Yongyuan Liang",
      "name": "cheryyunl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01295",
      "authors": [
        {
          "_id": "69097a04812eca10f9cc62fe",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc62ff",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6300",
          "name": "Chenglin Li",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6301",
          "name": "Zheming Liang",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6302",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6303",
          "name": "Yang Jiao",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6304",
          "name": "Zhipeng Wei",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6305",
          "name": "Chao Gong",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6306",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6307",
          "name": "Jingjing Chen",
          "hidden": false
        },
        {
          "_id": "69097a04812eca10f9cc6308",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T07:24:57.000Z",
      "submittedOnDailyAt": "2025-11-04T01:29:52.436Z",
      "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
        "isPro": false,
        "fullname": "SII-Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
      "upvotes": 11,
      "discussionId": "69097a04812eca10f9cc6309",
      "projectPage": "https://maplebb.github.io/UniREditBench/",
      "githubRepo": "https://github.com/Maplebb/UniREditBench",
      "githubStars": 5,
      "organization": {
        "_id": "643cb0625fcffe09fb6ca688",
        "name": "Fudan-University",
        "fullname": "Fudan University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
      }
    },
    "publishedAt": "2025-11-03T02:24:57.000Z",
    "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
    "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
      "fullname": "SII-Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "organization": {
      "_id": "643cb0625fcffe09fb6ca688",
      "name": "Fudan-University",
      "fullname": "Fudan University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26236",
      "authors": [
        {
          "_id": "6904bbe336711a7e91ed4d94",
          "user": {
            "_id": "68f621035356f4ab64fa04a2",
            "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
            "isPro": false,
            "fullname": "Kyungmin Lee",
            "user": "Kyungminn",
            "type": "user"
          },
          "name": "Kyungmin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-03T20:54:32.642Z",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d95",
          "name": "Sibeen Kim",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d96",
          "name": "Minho Park",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d97",
          "name": "Hyunseung Kim",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d98",
          "name": "Dongyoon Hwang",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d99",
          "name": "Hojoon Lee",
          "hidden": false
        },
        {
          "_id": "6904bbe336711a7e91ed4d9a",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/gVSPzM9IOvUGTSsjcfZkk.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/sZNMyVxYxz2QqZ9yq4t1t.mp4"
      ],
      "publishedAt": "2025-10-30T08:13:12.000Z",
      "submittedOnDailyAt": "2025-11-04T04:38:29.362Z",
      "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
      "submittedOnDailyBy": {
        "_id": "68f621035356f4ab64fa04a2",
        "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
        "isPro": false,
        "fullname": "Kyungmin Lee",
        "user": "Kyungminn",
        "type": "user"
      },
      "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.",
      "upvotes": 11,
      "discussionId": "6904bbe336711a7e91ed4d9b",
      "projectPage": "https://davian-robotics.github.io/PHUMA/",
      "githubRepo": "https://github.com/davian-robotics/PHUMA",
      "githubStars": 76,
      "organization": {
        "_id": "68f61ab10a6265597402e1b1",
        "name": "DAVIAN-Robotics",
        "fullname": "DAVIAN Robotics",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
      }
    },
    "publishedAt": "2025-10-30T04:13:12.000Z",
    "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/gVSPzM9IOvUGTSsjcfZkk.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/68f621035356f4ab64fa04a2/sZNMyVxYxz2QqZ9yq4t1t.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68f621035356f4ab64fa04a2",
      "avatarUrl": "/avatars/29fbe7a537a71160b2c301efe7c830d1.svg",
      "fullname": "Kyungmin Lee",
      "name": "Kyungminn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68f61ab10a6265597402e1b1",
      "name": "DAVIAN-Robotics",
      "fullname": "DAVIAN Robotics",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630461624ec2dfa82a5ad7e7/0NA6xGIRlNalvgIxvZIXD.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.27571",
      "authors": [
        {
          "_id": "6908681a812eca10f9cc5f5a",
          "name": "Zhuoning Guo",
          "hidden": false
        },
        {
          "_id": "6908681a812eca10f9cc5f5b",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6908681a812eca10f9cc5f5c",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6908681a812eca10f9cc5f5d",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "6908681a812eca10f9cc5f5e",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6908681a812eca10f9cc5f5f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T15:54:48.000Z",
      "submittedOnDailyAt": "2025-11-04T01:06:37.116Z",
      "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.",
      "upvotes": 10,
      "discussionId": "6908681b812eca10f9cc5f60",
      "projectPage": "https://gzn00417.github.io/GVE/",
      "organization": {
        "_id": "661f98de142a51d630dbbcc4",
        "name": "Alibaba-NLP",
        "fullname": "Alibaba-NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
      }
    },
    "publishedAt": "2025-10-31T11:54:48.000Z",
    "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
    "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27571.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "661f98de142a51d630dbbcc4",
      "name": "Alibaba-NLP",
      "fullname": "Alibaba-NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27363",
      "authors": [
        {
          "_id": "6908501d812eca10f9cc5efe",
          "user": {
            "_id": "65db23d1f386d08eb0d1cec5",
            "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
            "isPro": false,
            "fullname": "Mengjie Deng",
            "user": "MengjieDeng",
            "type": "user"
          },
          "name": "Mengjie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-03T20:51:52.660Z",
          "hidden": false
        },
        {
          "_id": "6908501d812eca10f9cc5eff",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6908501d812eca10f9cc5f00",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T10:51:27.000Z",
      "submittedOnDailyAt": "2025-11-04T04:26:55.171Z",
      "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
      "submittedOnDailyBy": {
        "_id": "65db23d1f386d08eb0d1cec5",
        "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
        "isPro": false,
        "fullname": "Mengjie Deng",
        "user": "MengjieDeng",
        "type": "user"
      },
      "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
      "upvotes": 9,
      "discussionId": "6908501e812eca10f9cc5f01",
      "githubRepo": "https://github.com/dengmengjie/ToolScope",
      "githubStars": 1
    },
    "publishedAt": "2025-10-31T06:51:27.000Z",
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
    "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27363.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65db23d1f386d08eb0d1cec5",
      "avatarUrl": "/avatars/b495ec5b35b15fea245ef490b83d1856.svg",
      "fullname": "Mengjie Deng",
      "name": "MengjieDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.00062",
      "authors": [
        {
          "_id": "69097a8f812eca10f9cc630b",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc630d",
          "name": "Arslan Ali",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc630e",
          "name": "Junjie Bai",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc630f",
          "name": "Maciej Bala",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6310",
          "name": "Yogesh Balaji",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6311",
          "name": "Aaron Blakeman",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6312",
          "name": "Tiffany Cai",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6313",
          "name": "Jiaxin Cao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6314",
          "name": "Tianshi Cao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6315",
          "name": "Elizabeth Cha",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6316",
          "name": "Yu-Wei Chao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6317",
          "name": "Prithvijit Chattopadhyay",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6318",
          "name": "Mike Chen",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6319",
          "name": "Yongxin Chen",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631a",
          "name": "Yu Chen",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631b",
          "name": "Shuai Cheng",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631c",
          "name": "Yin Cui",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631d",
          "name": "Jenna Diamond",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631e",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc631f",
          "name": "Jiaojiao Fan",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6320",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6321",
          "name": "Liang Feng",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6322",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6323",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6324",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6325",
          "name": "Ruiyuan Gao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6326",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6327",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6328",
          "name": "Aryaman Gupta",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6329",
          "name": "Siddharth Gururani",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632a",
          "name": "Imad El Hanafi",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632b",
          "name": "Ali Hassani",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632c",
          "name": "Zekun Hao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632d",
          "name": "Jacob Huffman",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632e",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc632f",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6330",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6331",
          "name": "Grace Lam",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6332",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6333",
          "name": "Zhaoshuo Li",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6334",
          "name": "Maosheng Liao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6335",
          "name": "Chen-Hsuan Lin",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6336",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6337",
          "name": "Yen-Chen Lin",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6338",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6339",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633a",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633b",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633c",
          "name": "Alice Luo",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633d",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633e",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc633f",
          "name": "Kaichun Mo",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6340",
          "name": "Seungjun Nah",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6341",
          "name": "Yashraj Narang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6342",
          "name": "Abhijeet Panaskar",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6343",
          "name": "Lindsey Pavao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6344",
          "name": "Trung Pham",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6345",
          "name": "Morteza Ramezanali",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6346",
          "name": "Fitsum Reda",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6347",
          "name": "Scott Reed",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6348",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6349",
          "name": "Haonan Shao",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634a",
          "name": "Yue Shen",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634b",
          "name": "Stella Shi",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634c",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634d",
          "name": "Bartosz Stefaniak",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634e",
          "name": "Shangkun Sun",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc634f",
          "name": "Shitao Tang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6350",
          "name": "Sameena Tasmeen",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6351",
          "name": "Lyne Tchapmi",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6352",
          "name": "Wei-Cheng Tseng",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6353",
          "name": "Jibin Varghese",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6354",
          "name": "Andrew Z. Wang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6355",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6356",
          "name": "Haoxiang Wang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6357",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6358",
          "name": "Ting-Chun Wang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6359",
          "name": "Fangyin Wei",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635a",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635b",
          "name": "Dinghao Yang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635c",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635d",
          "name": "Haotian Ye",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635e",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc635f",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6360",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6361",
          "name": "Qinsheng Zhang",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6362",
          "name": "Kaiwen Zheng",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6363",
          "name": "Andrew Zhu",
          "hidden": false
        },
        {
          "_id": "69097a8f812eca10f9cc6364",
          "name": "Yuke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T22:44:13.000Z",
      "submittedOnDailyAt": "2025-11-04T01:31:46.720Z",
      "title": "World Simulation with Video Foundation Models for Physical AI",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.",
      "upvotes": 8,
      "discussionId": "69097a90812eca10f9cc6365"
    },
    "publishedAt": "2025-10-28T18:44:13.000Z",
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00062.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01833",
      "authors": [
        {
          "_id": "6909788f812eca10f9cc6257",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc6258",
          "name": "Jike Zhong",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc6259",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625a",
          "name": "Haoquan Zhang",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625b",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625c",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625d",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625e",
          "name": "Konstantinos Psounis",
          "hidden": false
        },
        {
          "_id": "6909788f812eca10f9cc625f",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T18:40:17.000Z",
      "submittedOnDailyAt": "2025-11-04T01:22:59.659Z",
      "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-with-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-with-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce TIR-Bench, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.",
      "upvotes": 7,
      "discussionId": "6909788f812eca10f9cc6260",
      "githubRepo": "https://github.com/agents-x-project/TIR-Bench",
      "githubStars": 2
    },
    "publishedAt": "2025-11-03T13:40:17.000Z",
    "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
    "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-with-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-with-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce TIR-Bench, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01618",
      "authors": [
        {
          "_id": "6909a0a0812eca10f9cc63ba",
          "name": "Xiaoyu Zhan",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63bb",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63bc",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63bd",
          "name": "Xinyu Fu",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63be",
          "name": "Changfeng Ma",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63bf",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c0",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c1",
          "name": "Shaohui Lin",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c2",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c3",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c4",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c5",
          "name": "Yuanqi Li",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c6",
          "name": "Jie Guo",
          "hidden": false
        },
        {
          "_id": "6909a0a0812eca10f9cc63c7",
          "name": "Yanwen Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T14:27:00.000Z",
      "submittedOnDailyAt": "2025-11-04T04:15:47.258Z",
      "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "67dc162ec8c00778e8689f42",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
        "isPro": false,
        "fullname": "Wenxuan Huang",
        "user": "Osilly",
        "type": "user"
      },
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
      "upvotes": 7,
      "discussionId": "6909a0a0812eca10f9cc63c8"
    },
    "publishedAt": "2025-11-03T09:27:00.000Z",
    "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dc162ec8c00778e8689f42",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
      "fullname": "Wenxuan Huang",
      "name": "Osilly",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26865",
      "authors": [
        {
          "_id": "69080eb6812eca10f9cc5de7",
          "name": "Fenfen Lin",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5de8",
          "name": "Yesheng Liu",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5de9",
          "name": "Haiyu Xu",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5dea",
          "name": "Chen Yue",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5deb",
          "user": {
            "_id": "65b21047f5d76208991e463e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
            "isPro": false,
            "fullname": "Zheqi He",
            "user": "philokey",
            "type": "user"
          },
          "name": "Zheqi He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-03T20:52:48.573Z",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5dec",
          "name": "Mingxuan Zhao",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5ded",
          "name": "Miguel Hu Chen",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5dee",
          "name": "Jiakang Liu",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5def",
          "name": "JG Yao",
          "hidden": false
        },
        {
          "_id": "69080eb6812eca10f9cc5df0",
          "name": "Xi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:20:51.000Z",
      "submittedOnDailyAt": "2025-11-04T04:03:17.287Z",
      "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
      "submittedOnDailyBy": {
        "_id": "65b21047f5d76208991e463e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
        "isPro": false,
        "fullname": "Zheqi He",
        "user": "philokey",
        "type": "user"
      },
      "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.",
      "upvotes": 7,
      "discussionId": "69080eb6812eca10f9cc5df1",
      "projectPage": "https://flageval-baai.github.io/MeasureBenchPage/",
      "githubRepo": "https://github.com/flageval-baai/MeasureBench",
      "githubStars": 2,
      "organization": {
        "_id": "61be9739d2f9358e24ca0a4f",
        "name": "BAAI",
        "fullname": "Beijing Academy of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
      }
    },
    "publishedAt": "2025-10-30T13:20:51.000Z",
    "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
    "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b21047f5d76208991e463e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b21047f5d76208991e463e/6ML4lLz-vUr1HdWR3Jo-L.jpeg",
      "fullname": "Zheqi He",
      "name": "philokey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "61be9739d2f9358e24ca0a4f",
      "name": "BAAI",
      "fullname": "Beijing Academy of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.00279",
      "authors": [
        {
          "_id": "690979c6812eca10f9cc6278",
          "name": "Meituan LongCat Team",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6279",
          "name": "Bairui Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627a",
          "name": "Bayan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627b",
          "name": "Bin Xiao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627c",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627d",
          "name": "Bolin Rong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627e",
          "name": "Borun Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc627f",
          "name": "Chang Wan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6280",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6281",
          "name": "Chen Huang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6282",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6283",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6284",
          "name": "Chengxu Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6285",
          "name": "Chengzuo Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6286",
          "name": "Cong Han",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6287",
          "name": "Dandan Peng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6288",
          "name": "Delian Ruan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6289",
          "name": "Detai Xin",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628a",
          "name": "Disong Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628b",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628c",
          "name": "Fanfan Liu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628d",
          "name": "Fengjiao Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628e",
          "name": "Fengyu Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc628f",
          "name": "Gan Dong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6290",
          "name": "Gang Huang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6291",
          "name": "Gang Xu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6292",
          "name": "Guanglu Wan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6293",
          "name": "Guoqiang Tan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6294",
          "name": "Guoqiao Yu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6295",
          "name": "Haibo Qiu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6296",
          "name": "Hao Lu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6297",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6298",
          "name": "Hongyu Xiang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc6299",
          "name": "Jiaheng Wu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629b",
          "name": "Jiaxing Liu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629c",
          "name": "Jing Huang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629d",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629e",
          "name": "Jinrui Ding",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc629f",
          "name": "Juchao Jiang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a0",
          "name": "Jun Kuang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a1",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a2",
          "name": "Junhui Mei",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a3",
          "name": "Ke Ding",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a4",
          "name": "Kefeng Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a5",
          "name": "Lei Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a6",
          "name": "Liang Shi",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a7",
          "name": "Limeng Qiao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a8",
          "name": "Liming Zheng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62a9",
          "name": "Lin Ma",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62aa",
          "name": "Liuyang Guo",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ab",
          "name": "Liya Ma",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ac",
          "name": "Luying Sun",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ad",
          "name": "Man Gao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ae",
          "name": "Mengshen Zhu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62af",
          "name": "Miao Cao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b0",
          "name": "Minliang Lin",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b1",
          "name": "Nuo Xu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b2",
          "name": "Peng Shi",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b4",
          "name": "Qian Fang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b5",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b6",
          "name": "Qian Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b7",
          "name": "Quanxiu Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b8",
          "name": "Rongxiang Weng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62b9",
          "name": "Rongxin Guo",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ba",
          "name": "Ruoxuan Liang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62bb",
          "name": "Senbin Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62bc",
          "name": "Shanbo Xu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62bd",
          "name": "Shanglin Lei",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62be",
          "name": "Shengze Ye",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62bf",
          "name": "Shimin Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c0",
          "name": "Shuaiqi Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c1",
          "name": "Shujie Hu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c2",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c3",
          "name": "Siqi Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c4",
          "name": "Siyu Xu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c5",
          "name": "Siyu Ren",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c6",
          "name": "Song Li",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c7",
          "name": "Songxiang Liu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c8",
          "name": "Tianhao Bai",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62c9",
          "name": "Tianye Dai",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ca",
          "name": "Wei Hong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62cb",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62cc",
          "name": "Weixiao Zhao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62cd",
          "name": "Wengang Cao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ce",
          "name": "Wenlong Zhu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62cf",
          "name": "Wenlong He",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d0",
          "name": "Xi Su",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d1",
          "name": "Xi Nan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d2",
          "name": "Xiaohan Zhao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d3",
          "name": "Xiaohao Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d4",
          "name": "Xiaoyu Zhao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d5",
          "name": "Xiaoyu Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d6",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d7",
          "name": "Xin Pan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d8",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62d9",
          "name": "Xiusong Sun",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62da",
          "name": "Xu Xiang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62db",
          "name": "Xudong Xing",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62dc",
          "name": "Xuezhi Cao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62dd",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62de",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62df",
          "name": "Yanli Tan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e0",
          "name": "Yao Yao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e1",
          "name": "Yerui Sun",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e2",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e3",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e4",
          "name": "Yin Gong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e5",
          "name": "Yining Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e6",
          "name": "Yitian Chen",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e7",
          "name": "Yiyang Gan",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e8",
          "name": "Yuchen Tang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62e9",
          "name": "Yuchen Xie",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ea",
          "name": "Yueqian Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62eb",
          "name": "Yuewen Zheng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ec",
          "name": "Yufei Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ed",
          "name": "Yufeng Zhong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ee",
          "name": "Yulei Qian",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62ef",
          "name": "Yuqi Peng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f0",
          "name": "Yuwei Jiang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f1",
          "name": "Zeyang Hu",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f2",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f3",
          "name": "Zhengkun Tian",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f4",
          "name": "Zhiqing Hong",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f5",
          "name": "Zhixiong Zeng",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f6",
          "name": "Zhuqi Mi",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f7",
          "name": "Ziran Li",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f8",
          "name": "Ziwen Wang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62f9",
          "name": "Ziyi Zhao",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62fa",
          "name": "Ziyuan Zhuang",
          "hidden": false
        },
        {
          "_id": "690979c6812eca10f9cc62fb",
          "name": "Zizhe Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T21:58:15.000Z",
      "submittedOnDailyAt": "2025-11-04T01:28:08.212Z",
      "title": "LongCat-Flash-Omni Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.",
      "upvotes": 6,
      "discussionId": "690979c6812eca10f9cc62fc"
    },
    "publishedAt": "2025-10-31T17:58:15.000Z",
    "title": "LongCat-Flash-Omni Technical Report",
    "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01857",
      "authors": [
        {
          "_id": "69098243812eca10f9cc637f",
          "name": "Reza Esfandiarpoor",
          "hidden": false
        },
        {
          "_id": "69098243812eca10f9cc6380",
          "name": "Max Zuo",
          "hidden": false
        },
        {
          "_id": "69098243812eca10f9cc6381",
          "name": "Stephen H. Bach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T18:59:57.000Z",
      "submittedOnDailyAt": "2025-11-04T02:05:54.867Z",
      "title": "Trove: A Flexible Toolkit for Dense Retrieval",
      "submittedOnDailyBy": {
        "_id": "64fbb457c7f04f7cee8624e0",
        "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
        "isPro": false,
        "fullname": "Max Zuo",
        "user": "zuom",
        "type": "user"
      },
      "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that\nsimplifies research experiments without sacrificing flexibility or speed. For\nthe first time, we introduce efficient data management features that load and\nprocess (filter, select, transform, and combine) retrieval datasets on the fly,\nwith just a few lines of code. This gives users the flexibility to easily\nexperiment with different dataset configurations without the need to compute\nand store multiple copies of large datasets. Trove is highly customizable: in\naddition to many built-in options, it allows users to freely modify existing\ncomponents or replace them entirely with user-defined objects. It also provides\na low-code and unified pipeline for evaluation and hard negative mining, which\nsupports multi-node execution without any code changes. Trove's data management\nfeatures reduce memory consumption by a factor of 2.6. Moreover, Trove's\neasy-to-use inference pipeline incurs no overhead, and inference times decrease\nlinearly with the number of available nodes. Most importantly, we demonstrate\nhow Trove simplifies retrieval experiments and allows for arbitrary\ncustomizations, thus facilitating exploratory research.",
      "upvotes": 4,
      "discussionId": "69098243812eca10f9cc6382",
      "projectPage": "https://ir-trove.dev/",
      "githubRepo": "https://github.com/BatsResearch/trove",
      "githubStars": 33,
      "organization": {
        "_id": "650d8bcd5085c0ce1f286c12",
        "name": "BatsResearch",
        "fullname": "Bats Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
      }
    },
    "publishedAt": "2025-11-03T13:59:57.000Z",
    "title": "Trove: A Flexible Toolkit for Dense Retrieval",
    "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that\nsimplifies research experiments without sacrificing flexibility or speed. For\nthe first time, we introduce efficient data management features that load and\nprocess (filter, select, transform, and combine) retrieval datasets on the fly,\nwith just a few lines of code. This gives users the flexibility to easily\nexperiment with different dataset configurations without the need to compute\nand store multiple copies of large datasets. Trove is highly customizable: in\naddition to many built-in options, it allows users to freely modify existing\ncomponents or replace them entirely with user-defined objects. It also provides\na low-code and unified pipeline for evaluation and hard negative mining, which\nsupports multi-node execution without any code changes. Trove's data management\nfeatures reduce memory consumption by a factor of 2.6. Moreover, Trove's\neasy-to-use inference pipeline incurs no overhead, and inference times decrease\nlinearly with the number of available nodes. Most importantly, we demonstrate\nhow Trove simplifies retrieval experiments and allows for arbitrary\ncustomizations, thus facilitating exploratory research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fbb457c7f04f7cee8624e0",
      "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
      "fullname": "Max Zuo",
      "name": "zuom",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "650d8bcd5085c0ce1f286c12",
      "name": "BatsResearch",
      "fullname": "Bats Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26491",
      "authors": [
        {
          "_id": "6905baa1b26ae8b17699fd58",
          "user": {
            "_id": "6421c749eaad1bcb28b11206",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
            "isPro": false,
            "fullname": "Erle Zhu",
            "user": "lez3f",
            "type": "user"
          },
          "name": "Erle Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-03T20:53:52.262Z",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd59",
          "name": "Dazhi Jiang",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5a",
          "name": "Yuan Wang",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5b",
          "name": "Xujun Li",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5c",
          "name": "Jiale Cheng",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5d",
          "name": "Yuxian Gu",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5e",
          "name": "Yilin Niu",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd5f",
          "name": "Aohan Zeng",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd60",
          "name": "Jie Tang",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd61",
          "name": "Minlie Huang",
          "hidden": false
        },
        {
          "_id": "6905baa1b26ae8b17699fd62",
          "name": "Hongning Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T13:40:52.000Z",
      "submittedOnDailyAt": "2025-11-04T02:09:51.474Z",
      "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
      "submittedOnDailyBy": {
        "_id": "6421c749eaad1bcb28b11206",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
        "isPro": false,
        "fullname": "Erle Zhu",
        "user": "lez3f",
        "type": "user"
      },
      "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
      "upvotes": 4,
      "discussionId": "6905baa1b26ae8b17699fd63"
    },
    "publishedAt": "2025-10-30T09:40:52.000Z",
    "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
    "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6421c749eaad1bcb28b11206",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6421c749eaad1bcb28b11206/jvaBUG7Z5fv6lJMoH301q.jpeg",
      "fullname": "Erle Zhu",
      "name": "lez3f",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.01846",
      "authors": [
        {
          "_id": "6909790d812eca10f9cc6262",
          "name": "Thang Luong",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6263",
          "name": "Dawsen Hwang",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6264",
          "name": "Hoang H. Nguyen",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6265",
          "name": "Golnaz Ghiasi",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6266",
          "name": "Yuri Chervonyi",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6267",
          "name": "Insuk Seo",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6268",
          "name": "Junsu Kim",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6269",
          "name": "Garrett Bingham",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626a",
          "name": "Jonathan Lee",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626b",
          "name": "Swaroop Mishra",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626c",
          "name": "Alex Zhai",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626d",
          "name": "Clara Huiyi Hu",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626e",
          "name": "Henryk Michalewski",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc626f",
          "name": "Jimin Kim",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6270",
          "name": "Jeonghyun Ahn",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6271",
          "name": "Junhwi Bae",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6272",
          "name": "Xingyou Song",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6273",
          "name": "Trieu H. Trinh",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6274",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "6909790d812eca10f9cc6275",
          "name": "Junehyuk Jung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T18:53:02.000Z",
      "submittedOnDailyAt": "2025-11-04T01:25:07.006Z",
      "title": "Towards Robust Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.",
      "upvotes": 3,
      "discussionId": "6909790d812eca10f9cc6276",
      "projectPage": "https://imobench.github.io/"
    },
    "publishedAt": "2025-11-03T13:53:02.000Z",
    "title": "Towards Robust Mathematical Reasoning",
    "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01775",
      "authors": [
        {
          "_id": "69097bc4812eca10f9cc6367",
          "name": "Zhen Chen",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc6368",
          "name": "Qing Xu",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc6369",
          "name": "Jinlin Wu",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636a",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636b",
          "name": "Yuhao Zhai",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636c",
          "name": "Geng Guo",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636d",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636e",
          "name": "Yinlu Ding",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc636f",
          "name": "Nassir Navab",
          "hidden": false
        },
        {
          "_id": "69097bc4812eca10f9cc6370",
          "name": "Jiebo Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T17:28:54.000Z",
      "submittedOnDailyAt": "2025-11-04T01:36:35.253Z",
      "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
      "upvotes": 2,
      "discussionId": "69097bc4812eca10f9cc6371"
    },
    "publishedAt": "2025-11-03T12:28:54.000Z",
    "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
    "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01775.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.00405",
      "authors": [
        {
          "_id": "69097f4d812eca10f9cc6373",
          "name": "Zhibin Lan",
          "hidden": false
        },
        {
          "_id": "69097f4d812eca10f9cc6374",
          "name": "Liqiang Niu",
          "hidden": false
        },
        {
          "_id": "69097f4d812eca10f9cc6375",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "69097f4d812eca10f9cc6376",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "69097f4d812eca10f9cc6377",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-01T05:04:23.000Z",
      "submittedOnDailyAt": "2025-11-04T02:10:14.506Z",
      "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
      "submittedOnDailyBy": {
        "_id": "6626449503e1f561573d30e9",
        "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
        "isPro": false,
        "fullname": "Zhibin Lan",
        "user": "zhibinlan",
        "type": "user"
      },
      "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
      "upvotes": 2,
      "discussionId": "69097f4d812eca10f9cc6378",
      "githubRepo": "https://github.com/XMUDeepLIT/UME-R1",
      "githubStars": 1
    },
    "publishedAt": "2025-11-01T01:04:23.000Z",
    "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
    "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6626449503e1f561573d30e9",
      "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
      "fullname": "Zhibin Lan",
      "name": "zhibinlan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01718",
      "authors": [
        {
          "_id": "69096c89812eca10f9cc619c",
          "name": "Jiayi Chen",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc619d",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc619e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc619f",
          "name": "Ziyang Zhou",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc61a0",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc61a1",
          "name": "Feilong Tang",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc61a2",
          "name": "Donglin Wang",
          "hidden": false
        },
        {
          "_id": "69096c89812eca10f9cc61a3",
          "name": "Haoang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T16:26:54.000Z",
      "submittedOnDailyAt": "2025-11-04T00:35:20.084Z",
      "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
      "submittedOnDailyBy": {
        "_id": "66a0a3405c5e2a42a214c70f",
        "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
        "isPro": false,
        "fullname": "Wenxuan Song",
        "user": "Wenxuan123",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
      "upvotes": 1,
      "discussionId": "69096c8a812eca10f9cc61a4",
      "projectPage": "https://irpn-eai.github.io/UD-VLA.github.io/",
      "githubRepo": "https://github.com/OpenHelix-Team/UD-VLA",
      "githubStars": 0,
      "organization": {
        "_id": "65ad19cac14c3cf579ad9b68",
        "name": "HKUSTGZ",
        "fullname": "HKUSTGZ"
      }
    },
    "publishedAt": "2025-11-03T11:26:54.000Z",
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
    "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a0a3405c5e2a42a214c70f",
      "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
      "fullname": "Wenxuan Song",
      "name": "Wenxuan123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "65ad19cac14c3cf579ad9b68",
      "name": "HKUSTGZ",
      "fullname": "HKUSTGZ"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01266",
      "authors": [
        {
          "_id": "6909a508812eca10f9cc63de",
          "name": "Joonghyuk Shin",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63df",
          "name": "Zhengqi Li",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63e0",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63e1",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63e2",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63e3",
          "name": "Eli Schechtman",
          "hidden": false
        },
        {
          "_id": "6909a508812eca10f9cc63e4",
          "name": "Xun Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T06:37:53.000Z",
      "submittedOnDailyAt": "2025-11-04T04:33:51.430Z",
      "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
      "upvotes": 1,
      "discussionId": "6909a509812eca10f9cc63e5"
    },
    "publishedAt": "2025-11-03T01:37:53.000Z",
    "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
    "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01266.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.00602",
      "authors": [
        {
          "_id": "6909a672812eca10f9cc63f7",
          "name": "Wai-Chung Kwan",
          "hidden": false
        },
        {
          "_id": "6909a672812eca10f9cc63f8",
          "name": "Joshua Ong Jun Leang",
          "hidden": false
        },
        {
          "_id": "6909a672812eca10f9cc63f9",
          "name": "Pavlos Vougiouklis",
          "hidden": false
        },
        {
          "_id": "6909a672812eca10f9cc63fa",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "6909a672812eca10f9cc63fb",
          "name": "Marco Valentino",
          "hidden": false
        },
        {
          "_id": "6909a672812eca10f9cc63fc",
          "name": "Pasquale Minervini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-01T16:08:28.000Z",
      "submittedOnDailyAt": "2025-11-04T04:40:55.345Z",
      "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
      "submittedOnDailyBy": {
        "_id": "61001311e043e15c13412d30",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
        "isPro": false,
        "fullname": "Pasquale Minervini",
        "user": "pminervini",
        "type": "user"
      },
      "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.",
      "upvotes": 1,
      "discussionId": "6909a672812eca10f9cc63fd",
      "organization": {
        "_id": "652e72b5fd5e3a357cf6f844",
        "name": "EdinburghNLP",
        "fullname": "EdinburghNLP - Natural Language Processing Group at the University of Edinburgh",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fbfd09ee366524fe8e97cd3/KBva4SboTuDXRdYqWZsCX.png"
      }
    },
    "publishedAt": "2025-11-01T12:08:28.000Z",
    "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
    "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.00602.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61001311e043e15c13412d30",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
      "fullname": "Pasquale Minervini",
      "name": "pminervini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 45
    },
    "organization": {
      "_id": "652e72b5fd5e3a357cf6f844",
      "name": "EdinburghNLP",
      "fullname": "EdinburghNLP - Natural Language Processing Group at the University of Edinburgh",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5fbfd09ee366524fe8e97cd3/KBva4SboTuDXRdYqWZsCX.png"
    },
    "isAuthorParticipating": false
  }
]