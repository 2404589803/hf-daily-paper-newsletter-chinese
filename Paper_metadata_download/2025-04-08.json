[
  {
    "paper": {
      "id": "2504.05298",
      "authors": [
        {
          "_id": "67f4a0ccfefcc542f83f84e7",
          "name": "Karan Dalal",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e8",
          "name": "Daniel Koceja",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e9",
          "name": "Gashon Hussein",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ea",
          "name": "Jiarui Xu",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84eb",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:29.995Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ec",
          "name": "Youjin Song",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ed",
          "name": "Shihao Han",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ee",
          "name": "Ka Chun Cheung",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ef",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f0",
          "name": "Carlos Guestrin",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f1",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f2",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f3",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f4",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f5",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
      ],
      "publishedAt": "2025-04-07T17:56:31.000Z",
      "submittedOnDailyAt": "2025-04-08T02:38:37.647Z",
      "title": "One-Minute Video Generation with Test-Time Training",
      "submittedOnDailyBy": {
        "_id": "638fe91639f7e2a7f9d2a8c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
        "isPro": false,
        "fullname": "Yue Zhao",
        "user": "zhaoyue-zephyrus",
        "type": "user"
      },
      "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
      "upvotes": 21,
      "discussionId": "67f4a0cefefcc542f83f8592",
      "projectPage": "https://test-time-training.github.io/video-dit/",
      "githubRepo": "https://github.com/test-time-training/ttt-video-dit"
    },
    "publishedAt": "2025-04-07T13:56:31.000Z",
    "title": "One-Minute Video Generation with Test-Time Training",
    "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05305",
      "authors": [
        {
          "_id": "67f495437e1624ebbaf2d90e",
          "user": {
            "_id": "64842d1edc475c315e41123a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
            "isPro": false,
            "fullname": "Sangbeom Lim",
            "user": "SammyLim",
            "type": "user"
          },
          "name": "Sangbeom Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:37.341Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d90f",
          "user": {
            "_id": "64c2c45ae818eec6128fdda3",
            "avatarUrl": "/avatars/d4399e25e6399345e263c7902789047e.svg",
            "isPro": false,
            "fullname": "Jun-Wan KIM",
            "user": "junwann",
            "type": "user"
          },
          "name": "Junwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:34.302Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d910",
          "name": "Heeji Yoon",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d911",
          "user": {
            "_id": "65d02dc017e2b305e0d7bf4f",
            "avatarUrl": "/avatars/2a50fd0541e7b0e200c577a661956696.svg",
            "isPro": false,
            "fullname": "Jaewoo Jung",
            "user": "crepejung00",
            "type": "user"
          },
          "name": "Jaewoo Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:32.321Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d912",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-08T01:57:21.829Z",
      "title": "URECA: Unique Region Caption Anything",
      "submittedOnDailyBy": {
        "_id": "64842d1edc475c315e41123a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
        "isPro": false,
        "fullname": "Sangbeom Lim",
        "user": "SammyLim",
        "type": "user"
      },
      "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
      "upvotes": 11,
      "discussionId": "67f495477e1624ebbaf2da2f",
      "projectPage": "https://cvlab-kaist.github.io/URECA/",
      "githubRepo": "https://github.com/cvlab-kaist/URECA"
    },
    "publishedAt": "2025-04-07T13:59:44.000Z",
    "title": "URECA: Unique Region Caption Anything",
    "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05305.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64842d1edc475c315e41123a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
      "fullname": "Sangbeom Lim",
      "name": "SammyLim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02828",
      "authors": [
        {
          "_id": "67f01efe81f4f7a1b43f4930",
          "user": {
            "_id": "6279a4f6812ee439d9c72d3f",
            "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
            "isPro": false,
            "fullname": "Jinqi Luo",
            "user": "peterljq",
            "type": "user"
          },
          "name": "Jinqi Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:28.131Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4931",
          "name": "Tianjiao Ding",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4932",
          "name": "Kwan Ho Ryan Chan",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4933",
          "name": "Hancheng Min",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4934",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4935",
          "name": "René Vidal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-08T02:59:02.267Z",
      "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
      "submittedOnDailyBy": {
        "_id": "6279a4f6812ee439d9c72d3f",
        "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
        "isPro": false,
        "fullname": "Jinqi Luo",
        "user": "peterljq",
        "type": "user"
      },
      "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
      "upvotes": 10,
      "discussionId": "67f01eff81f4f7a1b43f4971",
      "projectPage": "https://peterljq.github.io/project/colan",
      "githubRepo": "https://github.com/peterljq/Concept-Lancet",
      "ai_keywords": [
        "diffusion models",
        "image editing",
        "text embedding",
        "score space",
        "latent space",
        "sparse linear combination",
        "visual concepts",
        "concept transplantation",
        "conceptual representation dataset",
        "CoLan-150K",
        "CoLan"
      ]
    },
    "publishedAt": "2025-04-03T13:59:58.000Z",
    "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
    "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02828.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6279a4f6812ee439d9c72d3f",
      "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
      "fullname": "Jinqi Luo",
      "name": "peterljq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04718",
      "authors": [
        {
          "_id": "67f48eff463c4d1c4ae5284b",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284c",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284d",
          "name": "Jaewoong Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T04:01:17.000Z",
      "submittedOnDailyAt": "2025-04-08T06:04:55.039Z",
      "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
      "upvotes": 8,
      "discussionId": "67f48f00463c4d1c4ae52882"
    },
    "publishedAt": "2025-04-07T00:01:17.000Z",
    "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
    "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05288",
      "authors": [
        {
          "_id": "67f4adf70864398533373364",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373365",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373366",
          "name": "Benlin Liu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373367",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373368",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:39:31.000Z",
      "submittedOnDailyAt": "2025-04-08T03:33:51.436Z",
      "title": "LiveVQA: Live Visual Knowledge Seeking",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.",
      "upvotes": 6,
      "discussionId": "67f4adfb086439853337346e"
    },
    "publishedAt": "2025-04-07T13:39:31.000Z",
    "title": "LiveVQA: Live Visual Knowledge Seeking",
    "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04823",
      "authors": [
        {
          "_id": "67f4a8ead83d88e30c450332",
          "user": {
            "_id": "6411c22dd52c57f628f7c331",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
            "isPro": false,
            "fullname": "ruikang liu",
            "user": "ruikangliu",
            "type": "user"
          },
          "name": "Ruikang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:24.004Z",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450333",
          "name": "Yuxuan Sun",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450334",
          "name": "Manyi Zhang",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450335",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450336",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450337",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450338",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450339",
          "name": "Lu Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:22:45.000Z",
      "submittedOnDailyAt": "2025-04-08T03:14:58.007Z",
      "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
      "submittedOnDailyBy": {
        "_id": "6411c22dd52c57f628f7c331",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
        "isPro": false,
        "fullname": "ruikang liu",
        "user": "ruikangliu",
        "type": "user"
      },
      "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
      "upvotes": 6,
      "discussionId": "67f4a8efd83d88e30c45043c"
    },
    "publishedAt": "2025-04-07T04:22:45.000Z",
    "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
    "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6411c22dd52c57f628f7c331",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
      "fullname": "ruikang liu",
      "name": "ruikangliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05304",
      "authors": [
        {
          "_id": "67f48c11412c65a9d4e3e552",
          "user": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "isPro": false,
            "fullname": "Hansheng Chen",
            "user": "Lakonik",
            "type": "user"
          },
          "name": "Hansheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:39.675Z",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e553",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e554",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e555",
          "name": "Zexiang Xu",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e556",
          "name": "Fujun Luan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e557",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e558",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e559",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-08T01:13:17.992Z",
      "title": "Gaussian Mixture Flow Matching Models",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an L_2 denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256times256.",
      "upvotes": 2,
      "discussionId": "67f48c13412c65a9d4e3e5d7",
      "githubRepo": "https://github.com/Lakonik/GMFlow"
    },
    "publishedAt": "2025-04-07T13:59:42.000Z",
    "title": "Gaussian Mixture Flow Matching Models",
    "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an L_2 denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256times256.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05304.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05299",
      "authors": [
        {
          "_id": "67f4cf5b504263bce1236d87",
          "name": "Andrés Marafioti",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d88",
          "name": "Orr Zohar",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d89",
          "name": "Miquel Farré",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8a",
          "name": "Merve Noyan",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8b",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8c",
          "name": "Pedro Cuenca",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8d",
          "name": "Cyril Zakka",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8e",
          "name": "Loubna Ben Allal",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8f",
          "name": "Anton Lozhkov",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d90",
          "name": "Nouamane Tazi",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d91",
          "name": "Vaibhav Srivastav",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d92",
          "name": "Joshua Lochner",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d93",
          "name": "Hugo Larcher",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d94",
          "name": "Mathieu Morlon",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d95",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d96",
          "name": "Leandro von Werra",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d97",
          "name": "Thomas Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:58:57.000Z",
      "submittedOnDailyAt": "2025-04-08T06:10:52.675Z",
      "title": "SmolVLM: Redefining small and efficient multimodal models",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
      "upvotes": 2,
      "discussionId": "67f4cf5d504263bce1236dda"
    },
    "publishedAt": "2025-04-07T13:58:57.000Z",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05299.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04715",
      "authors": [
        {
          "_id": "67f4a59bf51362f606eb84e8",
          "name": "Will Cai",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84e9",
          "name": "Tianneng Shi",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84ea",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84eb",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T03:57:41.000Z",
      "submittedOnDailyAt": "2025-04-08T02:57:43.185Z",
      "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit",
      "upvotes": 2,
      "discussionId": "67f4a59cf51362f606eb8529",
      "githubRepo": "https://github.com/sunblaze-ucb/llm-api-audit"
    },
    "publishedAt": "2025-04-06T23:57:41.000Z",
    "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
    "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03964",
      "authors": [
        {
          "_id": "67f4889c7e1624ebbaef710d",
          "user": {
            "_id": "652ee41e7b0079ff035b2269",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
            "isPro": false,
            "fullname": "Simon Lee",
            "user": "Simonlee711",
            "type": "user"
          },
          "name": "Simon A. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:44.271Z",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710e",
          "name": "Anthony Wu",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710f",
          "name": "Jeffrey N. Chiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T22:14:12.000Z",
      "submittedOnDailyAt": "2025-04-08T05:40:27.532Z",
      "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
      "submittedOnDailyBy": {
        "_id": "652ee41e7b0079ff035b2269",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
        "isPro": false,
        "fullname": "Simon Lee",
        "user": "Simonlee711",
        "type": "user"
      },
      "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
      "upvotes": 1,
      "discussionId": "67f4889d7e1624ebbaef715a",
      "githubRepo": "https://github.com/Simonlee711/Clinical_ModernBERT"
    },
    "publishedAt": "2025-04-04T18:14:12.000Z",
    "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
    "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ee41e7b0079ff035b2269",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
      "fullname": "Simon Lee",
      "name": "Simonlee711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03193",
      "authors": [
        {
          "_id": "67f3f25019592b36b6d8b30c",
          "user": {
            "_id": "6436290e76dbfd731bcf1f55",
            "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "XinNUS",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:52:23.735Z",
          "hidden": false
        },
        {
          "_id": "67f3f25019592b36b6d8b30d",
          "name": "Robby T. Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T05:44:45.000Z",
      "submittedOnDailyAt": "2025-04-08T06:12:20.759Z",
      "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
      "submittedOnDailyBy": {
        "_id": "6436290e76dbfd731bcf1f55",
        "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "XinNUS",
        "type": "user"
      },
      "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.",
      "upvotes": 1,
      "discussionId": "67f3f25219592b36b6d8b3d0",
      "projectPage": "https://devinxzhang.github.io/MFuser_ProjPage/",
      "githubRepo": "https://github.com/devinxzhang/MFuser"
    },
    "publishedAt": "2025-04-04T01:44:45.000Z",
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
    "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436290e76dbfd731bcf1f55",
      "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
      "fullname": "Xin Zhang",
      "name": "XinNUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02812",
      "authors": [
        {
          "_id": "67f4b54ddf6757586b52a4eb",
          "name": "Van Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ec",
          "name": "Stephen Tyree",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ed",
          "name": "Andrew Guo",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ee",
          "name": "Mederic Fourmy",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ef",
          "name": "Anas Gouda",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f0",
          "name": "Taeyeop Lee",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f1",
          "name": "Sungphill Moon",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f2",
          "name": "Hyeontae Son",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f3",
          "name": "Lukas Ranftl",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f4",
          "name": "Jonathan Tremblay",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f5",
          "name": "Eric Brachmann",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f6",
          "name": "Bertram Drost",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f7",
          "name": "Vincent Lepetit",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f8",
          "name": "Carsten Rother",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f9",
          "name": "Stan Birchfield",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fa",
          "name": "Jiri Matas",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fb",
          "name": "Yann Labbe",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fc",
          "name": "Martin Sundermeyer",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fd",
          "name": "Tomas Hodan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:55:19.000Z",
      "submittedOnDailyAt": "2025-04-08T04:04:46.852Z",
      "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
      "submittedOnDailyBy": {
        "_id": "67400e2bf2d0992e1f373b9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
        "isPro": false,
        "fullname": "hugw",
        "user": "hugw",
        "type": "user"
      },
      "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/",
      "upvotes": 1,
      "discussionId": "67f4b552df6757586b52a613"
    },
    "publishedAt": "2025-04-03T13:55:19.000Z",
    "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
    "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67400e2bf2d0992e1f373b9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
      "fullname": "hugw",
      "name": "hugw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03770",
      "authors": [
        {
          "_id": "67f4a80a261cb1c328d9b0a2",
          "name": "Yi Nian",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a3",
          "user": {
            "_id": "6614243f67d7bfc73afc6b77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
            "isPro": false,
            "fullname": "Shenzhe Zhu",
            "user": "Chouoftears",
            "type": "user"
          },
          "name": "Shenzhe Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:27.601Z",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a4",
          "name": "Yuehan Qin",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a5",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a6",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a7",
          "name": "Chaowei Xiao",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a8",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
      ],
      "publishedAt": "2025-04-03T05:00:28.000Z",
      "submittedOnDailyAt": "2025-04-08T03:10:00.355Z",
      "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
      "submittedOnDailyBy": {
        "_id": "6614243f67d7bfc73afc6b77",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
        "isPro": false,
        "fullname": "Shenzhe Zhu",
        "user": "Chouoftears",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but\nalso pose significant risks of generating harmful content, particularly through\njailbreak attacks. Jailbreak attacks refer to intentional manipulations that\nbypass safety mechanisms in models, leading to the generation of inappropriate\nor unsafe content. Detecting such attacks is critical to ensuring the\nresponsible deployment of MLLMs. Existing jailbreak detection methods face\nthree primary challenges: (1) Many rely on model hidden states or gradients,\nlimiting their applicability to white-box models, where the internal workings\nof the model are accessible; (2) They involve high computational overhead from\nuncertainty-based analysis, which limits real-time detection, and (3) They\nrequire fully labeled harmful datasets, which are often scarce in real-world\nsettings. To address these issues, we introduce a test-time adaptive framework\ncalled JAILDAM. Our method leverages a memory-based approach guided by\npolicy-driven unsafe knowledge representations, eliminating the need for\nexplicit exposure to harmful data. By dynamically updating unsafe knowledge\nduring test-time, our framework improves generalization to unseen jailbreak\nstrategies while maintaining efficiency. Experiments on multiple VLM jailbreak\nbenchmarks demonstrate that JAILDAM delivers state-of-the-art performance in\nharmful content detection, improving both accuracy and speed.",
      "upvotes": 1,
      "discussionId": "67f4a80b261cb1c328d9b0e9",
      "githubRepo": "https://github.com/ShenzheZhu/JailDAM"
    },
    "publishedAt": "2025-04-03T01:00:28.000Z",
    "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
    "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but\nalso pose significant risks of generating harmful content, particularly through\njailbreak attacks. Jailbreak attacks refer to intentional manipulations that\nbypass safety mechanisms in models, leading to the generation of inappropriate\nor unsafe content. Detecting such attacks is critical to ensuring the\nresponsible deployment of MLLMs. Existing jailbreak detection methods face\nthree primary challenges: (1) Many rely on model hidden states or gradients,\nlimiting their applicability to white-box models, where the internal workings\nof the model are accessible; (2) They involve high computational overhead from\nuncertainty-based analysis, which limits real-time detection, and (3) They\nrequire fully labeled harmful datasets, which are often scarce in real-world\nsettings. To address these issues, we introduce a test-time adaptive framework\ncalled JAILDAM. Our method leverages a memory-based approach guided by\npolicy-driven unsafe knowledge representations, eliminating the need for\nexplicit exposure to harmful data. By dynamically updating unsafe knowledge\nduring test-time, our framework improves generalization to unseen jailbreak\nstrategies while maintaining efficiency. Experiments on multiple VLM jailbreak\nbenchmarks demonstrate that JAILDAM delivers state-of-the-art performance in\nharmful content detection, improving both accuracy and speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03770.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614243f67d7bfc73afc6b77",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
      "fullname": "Shenzhe Zhu",
      "name": "Chouoftears",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02882",
      "authors": [
        {
          "_id": "67f4d15a8f00d281c155880f",
          "name": "Sunghee Jung",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558810",
          "name": "Donghun Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558811",
          "name": "Shinbok Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558812",
          "name": "Gaeun Seo",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558813",
          "name": "Daniel Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558814",
          "name": "Byeongil Ko",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558815",
          "name": "Junrae Cho",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558816",
          "name": "Kihyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558817",
          "name": "Eunggyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558818",
          "name": "Myeongcheol Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:47:28.000Z",
      "submittedOnDailyAt": "2025-04-08T06:11:32.041Z",
      "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
      "submittedOnDailyBy": {
        "_id": "65e30342e8b017ee1384824c",
        "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
        "isPro": false,
        "fullname": "Sunghee Jung",
        "user": "hash2430",
        "type": "user"
      },
      "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
      "upvotes": 1,
      "discussionId": "67f4d15c8f00d281c1558870"
    },
    "publishedAt": "2025-04-02T01:47:28.000Z",
    "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
    "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e30342e8b017ee1384824c",
      "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
      "fullname": "Sunghee Jung",
      "name": "hash2430",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]