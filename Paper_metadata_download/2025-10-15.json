[
  {
    "paper": {
      "id": "2510.12586",
      "authors": [
        {
          "_id": "68eefdd0486b78128f0e3374",
          "user": {
            "_id": "648c6537aeff9347218f49f2",
            "avatarUrl": "/avatars/1891855926eec77f91a389755998212f.svg",
            "isPro": true,
            "fullname": "Jiachen Lei",
            "user": "jiachenlei",
            "type": "user"
          },
          "name": "Jiachen Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:13:00.511Z",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e3375",
          "name": "Keli Liu",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e3376",
          "name": "Julius Berner",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e3377",
          "name": "Haiming Yu",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e3378",
          "name": "Hongkai Zheng",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e3379",
          "name": "Jiahong Wu",
          "hidden": false
        },
        {
          "_id": "68eefdd0486b78128f0e337a",
          "name": "Xiangxiang Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T14:41:16.000Z",
      "submittedOnDailyAt": "2025-10-15T00:25:14.164Z",
      "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
      "upvotes": 67,
      "discussionId": "68eefdd1486b78128f0e337b",
      "githubRepo": "https://github.com/AMAP-ML/EPG",
      "githubStars": 48,
      "organization": {
        "_id": "67d11771890254196d3174e5",
        "name": "GD-ML",
        "fullname": "AMAP-ML",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
      }
    },
    "publishedAt": "2025-10-14T10:41:16.000Z",
    "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training",
    "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12586.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67d11771890254196d3174e5",
      "name": "GD-ML",
      "fullname": "AMAP-ML",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09116",
      "authors": [
        {
          "_id": "68edd273de1fee572713a963",
          "user": {
            "_id": "67e659905c1ed903debed72a",
            "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
            "isPro": false,
            "fullname": "zez",
            "user": "Everything-is-Ok",
            "type": "user"
          },
          "name": "Enze Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:47.167Z",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a964",
          "user": {
            "_id": "6852d8a124a68790bddff660",
            "avatarUrl": "/avatars/0c07abf23c1a099335a7fa0ca15db7ca.svg",
            "isPro": false,
            "fullname": "Jiaying Wang",
            "user": "winniwang",
            "type": "user"
          },
          "name": "Jiaying Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:42.425Z",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a965",
          "user": {
            "_id": "663adb42e14047f710dc1d29",
            "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
            "isPro": false,
            "fullname": "Mengxi Xiao",
            "user": "ElsaShaw",
            "type": "user"
          },
          "name": "Mengxi Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:44.631Z",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a966",
          "name": "Jifei Liu",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a967",
          "name": "Ziyan Kuang",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a968",
          "name": "Rui Dong",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a969",
          "name": "Eric Dong",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a96a",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a96b",
          "name": "Min Peng",
          "hidden": false
        },
        {
          "_id": "68edd273de1fee572713a96c",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T08:10:10.000Z",
      "submittedOnDailyAt": "2025-10-15T00:43:50.761Z",
      "title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation",
      "submittedOnDailyBy": {
        "_id": "67e659905c1ed903debed72a",
        "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
        "isPro": false,
        "fullname": "zez",
        "user": "Everything-is-Ok",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have substantially advanced machine translation\n(MT), yet their effectiveness in translating web novels remains unclear.\nExisting benchmarks rely on surface-level metrics that fail to capture the\ndistinctive traits of this genre. To address these gaps, we introduce DITING,\nthe first comprehensive evaluation framework for web novel translation,\nassessing narrative and cultural fidelity across six dimensions: idiom\ntranslation, lexical ambiguity, terminology localization, tense consistency,\nzero-pronoun resolution, and cultural safety, supported by over 18K\nexpert-annotated Chinese-English sentence pairs. We further propose AgentEval,\na reasoning-driven multi-agent evaluation framework that simulates expert\ndeliberation to assess translation quality beyond lexical overlap, achieving\nthe highest correlation with human judgments among seven tested automatic\nmetrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation\ndataset of 300 sentence pairs annotated with error labels and scalar quality\nscores. Comprehensive evaluation of fourteen open, closed, and commercial\nmodels reveals that Chinese-trained LLMs surpass larger foreign counterparts,\nand that DeepSeek-V3 delivers the most faithful and stylistically coherent\ntranslations. Our work establishes a new paradigm for exploring LLM-based web\nnovel translation and provides public resources to advance future research.",
      "upvotes": 51,
      "discussionId": "68edd274de1fee572713a96d",
      "githubRepo": "https://github.com/WHUNextGen/DITING",
      "ai_summary": "A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.",
      "ai_keywords": [
        "large language models",
        "machine translation",
        "web novels",
        "DITING",
        "narrative fidelity",
        "cultural fidelity",
        "idiom translation",
        "lexical ambiguity",
        "terminology localization",
        "tense consistency",
        "zero-pronoun resolution",
        "cultural safety",
        "AgentEval",
        "reasoning-driven",
        "multi-agent evaluation",
        "MetricAlign",
        "meta-evaluation",
        "DeepSeek-V3",
        "stylistic coherence"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6693770d0bf3f05db3017f31",
        "name": "NextGenWhu",
        "fullname": "CLAIN-WHU",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
      }
    },
    "publishedAt": "2025-10-10T04:10:10.000Z",
    "title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation",
    "summary": "Large language models (LLMs) have substantially advanced machine translation\n(MT), yet their effectiveness in translating web novels remains unclear.\nExisting benchmarks rely on surface-level metrics that fail to capture the\ndistinctive traits of this genre. To address these gaps, we introduce DITING,\nthe first comprehensive evaluation framework for web novel translation,\nassessing narrative and cultural fidelity across six dimensions: idiom\ntranslation, lexical ambiguity, terminology localization, tense consistency,\nzero-pronoun resolution, and cultural safety, supported by over 18K\nexpert-annotated Chinese-English sentence pairs. We further propose AgentEval,\na reasoning-driven multi-agent evaluation framework that simulates expert\ndeliberation to assess translation quality beyond lexical overlap, achieving\nthe highest correlation with human judgments among seven tested automatic\nmetrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation\ndataset of 300 sentence pairs annotated with error labels and scalar quality\nscores. Comprehensive evaluation of fourteen open, closed, and commercial\nmodels reveals that Chinese-trained LLMs surpass larger foreign counterparts,\nand that DeepSeek-V3 delivers the most faithful and stylistically coherent\ntranslations. Our work establishes a new paradigm for exploring LLM-based web\nnovel translation and provides public resources to advance future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e659905c1ed903debed72a",
      "avatarUrl": "/avatars/b112b1f5903806e55aabd16aafdeee10.svg",
      "fullname": "zez",
      "name": "Everything-is-Ok",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6693770d0bf3f05db3017f31",
      "name": "NextGenWhu",
      "fullname": "CLAIN-WHU",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11693",
      "authors": [
        {
          "_id": "68ef04bc486b78128f0e33fc",
          "user": {
            "_id": "63108cc834c7d77420b0fd68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
            "isPro": false,
            "fullname": "Chenghao Xiao",
            "user": "gowitheflow",
            "type": "user"
          },
          "name": "Chenghao Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:42.130Z",
          "hidden": false
        },
        {
          "_id": "68ef04bc486b78128f0e33fd",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:34.613Z",
          "hidden": false
        },
        {
          "_id": "68ef04bc486b78128f0e33fe",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:44.546Z",
          "hidden": false
        },
        {
          "_id": "68ef04bc486b78128f0e33ff",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "68ef04bc486b78128f0e3400",
          "name": "Mahani Aljunied",
          "hidden": false
        },
        {
          "_id": "68ef04bc486b78128f0e3401",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:46.630Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:53:52.000Z",
      "submittedOnDailyAt": "2025-10-15T00:59:38.833Z",
      "title": "Scaling Language-Centric Omnimodal Representation Learning",
      "submittedOnDailyBy": {
        "_id": "63108cc834c7d77420b0fd68",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
        "isPro": false,
        "fullname": "Chenghao Xiao",
        "user": "gowitheflow",
        "type": "user"
      },
      "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
      "upvotes": 50,
      "discussionId": "68ef04bc486b78128f0e3402",
      "projectPage": "https://huggingface.co/LCO-Embedding",
      "githubRepo": "https://github.com/LCO-Embedding/LCO-Embedding",
      "githubStars": 11,
      "organization": {
        "_id": "6808e7522a4d69d5111da55f",
        "name": "Alibaba-DAMO-Academy",
        "fullname": "DAMO Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
      }
    },
    "publishedAt": "2025-10-13T13:53:52.000Z",
    "title": "Scaling Language-Centric Omnimodal Representation Learning",
    "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11693.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63108cc834c7d77420b0fd68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63108cc834c7d77420b0fd68/taDnqEmcI9Rhe3uzcPEE3.jpeg",
      "fullname": "Chenghao Xiao",
      "name": "gowitheflow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "organization": {
      "_id": "6808e7522a4d69d5111da55f",
      "name": "Alibaba-DAMO-Academy",
      "fullname": "DAMO Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12747",
      "authors": [
        {
          "_id": "68eeff63486b78128f0e3393",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:58.427Z",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3394",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3395",
          "name": "Xin Cai",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3396",
          "name": "Xiaohui Li",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3397",
          "name": "Yihao Liu",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3398",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "68eeff63486b78128f0e3399",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:25:54.000Z",
      "submittedOnDailyAt": "2025-10-15T00:27:04.930Z",
      "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
      "upvotes": 24,
      "discussionId": "68eeff63486b78128f0e339a",
      "projectPage": "https://zhuang2002.github.io/FlashVSR/",
      "githubRepo": "https://github.com/OpenImagingLab/FlashVSR",
      "githubStars": 34
    },
    "publishedAt": "2025-10-14T13:25:54.000Z",
    "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution",
    "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12747.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12798",
      "authors": [
        {
          "_id": "68eefe5c486b78128f0e337d",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e337e",
          "name": "Junan Huo",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e337f",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3380",
          "name": "Yuda Xiong",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3381",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3382",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3383",
          "name": "Tianhe Ren",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3384",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "68eefe5c486b78128f0e3385",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:59:54.000Z",
      "submittedOnDailyAt": "2025-10-15T00:22:45.065Z",
      "title": "Detect Anything via Next Point Prediction",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.",
      "upvotes": 23,
      "discussionId": "68eefe5c486b78128f0e3386",
      "projectPage": "https://rex-omni.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Omni",
      "githubStars": 40,
      "organization": {
        "_id": "64390caac62f375ba47da58f",
        "name": "IDEA-Research",
        "fullname": "IDEA-Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438ff69e1acfc375c693ab1/CSi9zEks9X2rRdCXoEeU_.png"
      }
    },
    "publishedAt": "2025-10-14T13:59:54.000Z",
    "title": "Detect Anything via Next Point Prediction",
    "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12798.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "organization": {
      "_id": "64390caac62f375ba47da58f",
      "name": "IDEA-Research",
      "fullname": "IDEA-Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438ff69e1acfc375c693ab1/CSi9zEks9X2rRdCXoEeU_.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11057",
      "authors": [
        {
          "_id": "68ef3928486b78128f0e356b",
          "name": "Youngrok Park",
          "hidden": false
        },
        {
          "_id": "68ef3928486b78128f0e356c",
          "name": "Hojung Jung",
          "hidden": false
        },
        {
          "_id": "68ef3928486b78128f0e356d",
          "name": "Sangmin Bae",
          "hidden": false
        },
        {
          "_id": "68ef3928486b78128f0e356e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T06:46:57.000Z",
      "submittedOnDailyAt": "2025-10-15T04:44:04.629Z",
      "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "6602ca1e10a1441af41637be",
        "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
        "isPro": false,
        "fullname": "Sangmin Bae",
        "user": "raymin0223",
        "type": "user"
      },
      "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.",
      "upvotes": 23,
      "discussionId": "68ef3928486b78128f0e356f",
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-10-13T02:46:57.000Z",
    "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
    "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11057.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602ca1e10a1441af41637be",
      "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
      "fullname": "Sangmin Bae",
      "name": "raymin0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12773",
      "authors": [
        {
          "_id": "68ef08b2486b78128f0e340b",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:39.051Z",
          "hidden": false
        },
        {
          "_id": "68ef08b2486b78128f0e340c",
          "name": "Martin Gubri",
          "hidden": false
        },
        {
          "_id": "68ef08b2486b78128f0e340d",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "68ef08b2486b78128f0e340e",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "68ef08b2486b78128f0e340f",
          "name": "Seong Joon Oh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/AY9q3n-wpcp0MXUL4SMEY.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/2n_XtdcAhxlT-jbMl_QWd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/ALKTlIvZzbZkwAQ-eELxU.png"
      ],
      "publishedAt": "2025-10-14T17:51:26.000Z",
      "submittedOnDailyAt": "2025-10-15T01:09:24.997Z",
      "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
      "upvotes": 22,
      "discussionId": "68ef08b2486b78128f0e3410",
      "githubRepo": "https://github.com/parameterlab/dr-llm",
      "githubStars": 4,
      "organization": {
        "_id": "673bcad43a2fd2a3b41f64e3",
        "name": "parameterlab",
        "fullname": "Parameter Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
      }
    },
    "publishedAt": "2025-10-14T13:51:26.000Z",
    "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/AY9q3n-wpcp0MXUL4SMEY.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/2n_XtdcAhxlT-jbMl_QWd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/ALKTlIvZzbZkwAQ-eELxU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12773.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 51
    },
    "organization": {
      "_id": "673bcad43a2fd2a3b41f64e3",
      "name": "parameterlab",
      "fullname": "Parameter Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12399",
      "authors": [
        {
          "_id": "68ef02c7486b78128f0e33dd",
          "user": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
            "isPro": false,
            "fullname": "YuyaoGe",
            "user": "YuyaoGe",
            "type": "user"
          },
          "name": "Yuyao Ge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:48.718Z",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33de",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:36.646Z",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33df",
          "name": "Zenghao Duan",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e0",
          "name": "Tianhao Li",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e1",
          "name": "Yujia Zheng",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e2",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e3",
          "name": "Lexin Wang",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e4",
          "name": "Jiayu Yao",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e5",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e6",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e7",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e8",
          "name": "Fangda Guo",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33e9",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33ea",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "68ef02c7486b78128f0e33eb",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T11:26:56.000Z",
      "submittedOnDailyAt": "2025-10-15T00:41:50.264Z",
      "title": "A Survey of Vibe Coding with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "656ad93853703dd78f3de7b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
        "isPro": false,
        "fullname": "YuyaoGe",
        "user": "YuyaoGe",
        "type": "user"
      },
      "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
      "upvotes": 21,
      "discussionId": "68ef02c8486b78128f0e33ec",
      "githubRepo": "https://github.com/YuyaoGe/Awesome-Vibe-Coding",
      "githubStars": 7,
      "organization": {
        "_id": "68ef0ab704f0f03d81964936",
        "name": "ict-cas",
        "fullname": "Institute of Computing Technology, Chinese Academy of Sciences",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"
      }
    },
    "publishedAt": "2025-10-14T07:26:56.000Z",
    "title": "A Survey of Vibe Coding with Large Language Models",
    "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656ad93853703dd78f3de7b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/JkSiNFe7LApux9YBmMPKk.png",
      "fullname": "YuyaoGe",
      "name": "YuyaoGe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "68ef0ab704f0f03d81964936",
      "name": "ict-cas",
      "fullname": "Institute of Computing Technology, Chinese Academy of Sciences",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12276",
      "authors": [
        {
          "_id": "68ef0057486b78128f0e33b0",
          "name": "Fuhao Li",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b1",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b2",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b3",
          "user": {
            "_id": "66a4ed2a9ba24c30408441b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a4ed2a9ba24c30408441b0/K9Z4MK3Do2CXjXr7XQDdu.png",
            "isPro": false,
            "fullname": "Jingbo Wang",
            "user": "hhhJB",
            "type": "user"
          },
          "name": "Jingbo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:53.766Z",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b4",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b5",
          "name": "Donglin Wang",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b6",
          "name": "Long Zeng",
          "hidden": false
        },
        {
          "_id": "68ef0057486b78128f0e33b7",
          "name": "Haoang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T08:27:10.000Z",
      "submittedOnDailyAt": "2025-10-15T01:16:56.905Z",
      "title": "Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model",
      "submittedOnDailyBy": {
        "_id": "66a0a3405c5e2a42a214c70f",
        "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
        "isPro": false,
        "fullname": "Wenxuan Song",
        "user": "Wenxuan123",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/",
      "upvotes": 18,
      "discussionId": "68ef0057486b78128f0e33b8",
      "projectPage": "https://spatial-forcing.github.io/",
      "githubRepo": "https://github.com/OpenHelix-Team/Spatial-Forcing",
      "githubStars": 18,
      "organization": {
        "_id": "65ad19cac14c3cf579ad9b68",
        "name": "HKUSTGZ",
        "fullname": "HKUSTGZ"
      }
    },
    "publishedAt": "2025-10-14T04:27:10.000Z",
    "title": "Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model",
    "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a0a3405c5e2a42a214c70f",
      "avatarUrl": "/avatars/52b9ee7f899ee5431ed37fd1db378d9e.svg",
      "fullname": "Wenxuan Song",
      "name": "Wenxuan123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "65ad19cac14c3cf579ad9b68",
      "name": "HKUSTGZ",
      "fullname": "HKUSTGZ"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12693",
      "authors": [
        {
          "_id": "68ef0007486b78128f0e339c",
          "user": {
            "_id": "6700b1f93381f2db06857fb5",
            "avatarUrl": "/avatars/c8b9ec7c00773c5a4055ba50de0c6b2f.svg",
            "isPro": false,
            "fullname": "Hanyang Chen",
            "user": "Hanyang81",
            "type": "user"
          },
          "name": "Hanyang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:38.765Z",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e339d",
          "name": "Mark Zhao",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e339e",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:56.270Z",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e339f",
          "name": "Qinwei Ma",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a0",
          "name": "Ke Yang",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a1",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a2",
          "name": "Kangrui Wang",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a3",
          "name": "Hao Bai",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a4",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a5",
          "name": "Rui Pan",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a6",
          "name": "Mengchao Zhang",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a7",
          "name": "Jose Barreiros",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a8",
          "name": "Aykut Onol",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33a9",
          "name": "ChengXiang Zhai",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33aa",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33ab",
          "name": "Manling Li",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33ac",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "68ef0007486b78128f0e33ad",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T16:25:46.000Z",
      "submittedOnDailyAt": "2025-10-15T00:49:47.828Z",
      "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\nEmbodied Reasoning Agent (ERA), a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, Embodied Prior Learning, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.",
      "upvotes": 16,
      "discussionId": "68ef0007486b78128f0e33ae",
      "projectPage": "https://embodied-reasoning-agent.github.io",
      "organization": {
        "_id": "65448bef5b5d9185ba3202b9",
        "name": "UIUC-CS",
        "fullname": "University of Illinois at Urbana-Champaign",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
      }
    },
    "publishedAt": "2025-10-14T12:25:46.000Z",
    "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning",
    "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\nEmbodied Reasoning Agent (ERA), a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, Embodied Prior Learning, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "65448bef5b5d9185ba3202b9",
      "name": "UIUC-CS",
      "fullname": "University of Illinois at Urbana-Champaign",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12784",
      "authors": [
        {
          "_id": "68ef011a486b78128f0e33ce",
          "name": "Weiyang Jin",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33cf",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33d0",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33d1",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33d2",
          "name": "Aoxue Li",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33d3",
          "name": "Shenghua Gao",
          "hidden": false
        },
        {
          "_id": "68ef011a486b78128f0e33d4",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/_qfb3S1n3HUhiypTBo-ax.png"
      ],
      "publishedAt": "2025-10-14T17:56:11.000Z",
      "submittedOnDailyAt": "2025-10-15T00:35:56.518Z",
      "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "66608add236f958513d21d2e",
        "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
        "isPro": false,
        "fullname": "Weiyang Jin",
        "user": "Wayne-King",
        "type": "user"
      },
      "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a global reward\nensures the correctness of the overall visual semantics and layout, while a\nlocal reward refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82\nto 46.75. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
      "upvotes": 12,
      "discussionId": "68ef011a486b78128f0e33d5",
      "projectPage": "https://waynejin0918.github.io/srum_web/",
      "githubRepo": "https://github.com/WayneJin0918/SRUM",
      "githubStars": 21,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-14T13:56:11.000Z",
    "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
    "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a global reward\nensures the correctness of the overall visual semantics and layout, while a\nlocal reward refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82\nto 46.75. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/_qfb3S1n3HUhiypTBo-ax.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12784.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66608add236f958513d21d2e",
      "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
      "fullname": "Weiyang Jin",
      "name": "Wayne-King",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12635",
      "authors": [
        {
          "_id": "68eef8ab486b78128f0e3304",
          "user": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "isPro": false,
            "fullname": "Yuxiang Zhang",
            "user": "TokerZ",
            "type": "user"
          },
          "name": "Yuxiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:13:02.636Z",
          "hidden": false
        },
        {
          "_id": "68eef8ab486b78128f0e3305",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "68eef8ab486b78128f0e3306",
          "name": "Ye Ma",
          "hidden": false
        },
        {
          "_id": "68eef8ab486b78128f0e3307",
          "name": "Xueyuan Lin",
          "hidden": false
        },
        {
          "_id": "68eef8ab486b78128f0e3308",
          "user": {
            "_id": "665ace40ddba0f825f3a4941",
            "avatarUrl": "/avatars/ed7e209e14b5ae495aa7f4baec980b38.svg",
            "isPro": false,
            "fullname": "Shangxi Wu",
            "user": "KirinNg",
            "type": "user"
          },
          "name": "Shangxi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:51.674Z",
          "hidden": false
        },
        {
          "_id": "68eef8ab486b78128f0e3309",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T15:29:57.000Z",
      "submittedOnDailyAt": "2025-10-15T00:24:53.390Z",
      "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
      "submittedOnDailyBy": {
        "_id": "645b4a2978730bcc103dfe4d",
        "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
        "isPro": false,
        "fullname": "Yuxiang Zhang",
        "user": "TokerZ",
        "type": "user"
      },
      "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
      "upvotes": 11,
      "discussionId": "68eef8ab486b78128f0e330a"
    },
    "publishedAt": "2025-10-14T11:29:57.000Z",
    "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
    "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4a2978730bcc103dfe4d",
      "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
      "fullname": "Yuxiang Zhang",
      "name": "TokerZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11683",
      "authors": [
        {
          "_id": "68edd3f6de1fee572713a979",
          "name": "Nianyi Lin",
          "hidden": false
        },
        {
          "_id": "68edd3f6de1fee572713a97a",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "68edd3f6de1fee572713a97b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "68edd3f6de1fee572713a97c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:47:50.000Z",
      "submittedOnDailyAt": "2025-10-15T00:52:19.998Z",
      "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "66cdd285c51a915bd5f2d017",
        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
        "isPro": false,
        "fullname": "Jiajie Zhang",
        "user": "NeoZ123",
        "type": "user"
      },
      "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose Boundary-Guided Policy Optimization (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
      "upvotes": 11,
      "discussionId": "68edd3f6de1fee572713a97d",
      "githubRepo": "https://github.com/THU-KEG/BGPO",
      "ai_summary": "Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.",
      "ai_keywords": [
        "reinforcement learning",
        "diffusion large language models",
        "likelihood functions",
        "evidence lower bounds",
        "Monte Carlo sampling",
        "gradient computation",
        "memory overhead",
        "Boundary-Guided Policy Optimization",
        "on-policy training"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "62ad27f19096e7f9ecb1853a",
        "name": "zai-org",
        "fullname": "Z.ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
      }
    },
    "publishedAt": "2025-10-13T13:47:50.000Z",
    "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
    "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose Boundary-Guided Policy Optimization (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cdd285c51a915bd5f2d017",
      "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
      "fullname": "Jiajie Zhang",
      "name": "NeoZ123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "62ad27f19096e7f9ecb1853a",
      "name": "zai-org",
      "fullname": "Z.ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12789",
      "authors": [
        {
          "_id": "68eefdae486b78128f0e336d",
          "name": "Kevin Li",
          "hidden": false
        },
        {
          "_id": "68eefdae486b78128f0e336e",
          "user": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "isPro": false,
            "fullname": "Manuel Brack",
            "user": "mbrack",
            "type": "user"
          },
          "name": "Manuel Brack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:40.869Z",
          "hidden": false
        },
        {
          "_id": "68eefdae486b78128f0e336f",
          "name": "Sudeep Katakol",
          "hidden": false
        },
        {
          "_id": "68eefdae486b78128f0e3370",
          "name": "Hareesh Ravi",
          "hidden": false
        },
        {
          "_id": "68eefdae486b78128f0e3371",
          "name": "Ajinkya Kale",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:57:56.000Z",
      "submittedOnDailyAt": "2025-10-15T00:19:45.469Z",
      "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
      "upvotes": 10,
      "discussionId": "68eefdae486b78128f0e3372",
      "projectPage": "https://thekevinli.github.io/unifusion/",
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-10-14T13:57:56.000Z",
    "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12709",
      "authors": [
        {
          "_id": "68ef0081486b78128f0e33ba",
          "name": "Lin Lin",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33bb",
          "name": "Jiefeng Long",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33bc",
          "name": "Zhihe Wan",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33bd",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33be",
          "name": "Dingkang Yang",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33bf",
          "name": "Shuang Yang",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c0",
          "name": "Yueyang Yao",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c1",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c2",
          "name": "Zirui Guo",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c3",
          "name": "Shengqiang Li",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c4",
          "name": "Weiran Li",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c5",
          "name": "Hanyu Li",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c6",
          "name": "Yaling Mou",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c7",
          "name": "Yan Qiu",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c8",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33c9",
          "name": "Xiao Liang",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33ca",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "68ef0081486b78128f0e33cb",
          "name": "Chao Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T16:43:22.000Z",
      "submittedOnDailyAt": "2025-10-15T00:31:59.900Z",
      "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.",
      "upvotes": 7,
      "discussionId": "68ef0081486b78128f0e33cc",
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2025-10-14T12:43:22.000Z",
    "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
    "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12709.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12801",
      "authors": [
        {
          "_id": "68eefd3a486b78128f0e3361",
          "name": "Kartik Narayan",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3362",
          "name": "Yang Xu",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3363",
          "name": "Tian Cao",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3364",
          "name": "Kavya Nerella",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3365",
          "name": "Vishal M. Patel",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3366",
          "name": "Navid Shiee",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3367",
          "name": "Peter Grasch",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3368",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e3369",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68eefd3a486b78128f0e336a",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:59:58.000Z",
      "submittedOnDailyAt": "2025-10-15T00:17:51.493Z",
      "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
      "upvotes": 4,
      "discussionId": "68eefd3a486b78128f0e336b",
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-10-14T13:59:58.000Z",
    "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
    "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12225",
      "authors": [
        {
          "_id": "68ef03f8486b78128f0e33ee",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33ef",
          "name": "Devandra Singh Sachan",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33f0",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33f1",
          "name": "Aditya Grover",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33f2",
          "name": "Gargi Ghosh",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33f3",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68ef03f8486b78128f0e33f4",
          "name": "Ramakanth Pasunuru",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T07:23:44.000Z",
      "submittedOnDailyAt": "2025-10-15T00:47:46.720Z",
      "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.",
      "upvotes": 4,
      "discussionId": "68ef03f9486b78128f0e33f5",
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-10-14T03:23:44.000Z",
    "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
    "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11919",
      "authors": [
        {
          "_id": "68eefecd486b78128f0e3388",
          "name": "Armel Zebaze",
          "hidden": false
        },
        {
          "_id": "68eefecd486b78128f0e3389",
          "name": "Rachel Bawden",
          "hidden": false
        },
        {
          "_id": "68eefecd486b78128f0e338a",
          "name": "Benoît Sagot",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T20:41:01.000Z",
      "submittedOnDailyAt": "2025-10-15T00:26:03.399Z",
      "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens",
      "submittedOnDailyBy": {
        "_id": "63952b31ee411d4c6fba891c",
        "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
        "isPro": false,
        "fullname": "Armel Randy Zebaze",
        "user": "ArmelRandy",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.",
      "upvotes": 4,
      "discussionId": "68eefecd486b78128f0e338b",
      "githubRepo": "https://github.com/ArmelRandy/llm-reasoning-mt",
      "githubStars": 0,
      "organization": {
        "_id": "602ba30dc4f8038e9a1e0a60",
        "name": "almanach",
        "fullname": "ALMAnaCH (Inria)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613472488646-602ba2a739515f8d31237967.png"
      }
    },
    "publishedAt": "2025-10-13T16:41:01.000Z",
    "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens",
    "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11919.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63952b31ee411d4c6fba891c",
      "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
      "fullname": "Armel Randy Zebaze",
      "name": "ArmelRandy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "602ba30dc4f8038e9a1e0a60",
      "name": "almanach",
      "fullname": "ALMAnaCH (Inria)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613472488646-602ba2a739515f8d31237967.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12777",
      "authors": [
        {
          "_id": "68ef0228486b78128f0e33d7",
          "user": {
            "_id": "6471c12a0c2b5fdaf1f07c45",
            "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
            "isPro": false,
            "fullname": "Stefan Baumann",
            "user": "stefan-baumann",
            "type": "user"
          },
          "name": "Stefan Andreas Baumann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T03:12:51.560Z",
          "hidden": false
        },
        {
          "_id": "68ef0228486b78128f0e33d8",
          "name": "Nick Stracke",
          "hidden": false
        },
        {
          "_id": "68ef0228486b78128f0e33d9",
          "name": "Timy Phan",
          "hidden": false
        },
        {
          "_id": "68ef0228486b78128f0e33da",
          "name": "Björn Ommer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6471c12a0c2b5fdaf1f07c45/ksU6k1ub2r8-LwZtZBsz1.png"
      ],
      "publishedAt": "2025-10-14T17:52:17.000Z",
      "submittedOnDailyAt": "2025-10-15T00:42:42.381Z",
      "title": "What If : Understanding Motion Through Sparse Interactions",
      "submittedOnDailyBy": {
        "_id": "6471c12a0c2b5fdaf1f07c45",
        "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
        "isPro": false,
        "fullname": "Stefan Baumann",
        "user": "stefan-baumann",
        "type": "user"
      },
      "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
      "upvotes": 2,
      "discussionId": "68ef0228486b78128f0e33db",
      "projectPage": "https://compvis.github.io/flow-poke-transformer/",
      "githubRepo": "https://github.com/CompVis/flow-poke-transformer",
      "githubStars": 3,
      "organization": {
        "_id": "62cfeeb73c54a34d508b82a9",
        "name": "CompVis",
        "fullname": "CompVis",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
      }
    },
    "publishedAt": "2025-10-14T13:52:17.000Z",
    "title": "What If : Understanding Motion Through Sparse Interactions",
    "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6471c12a0c2b5fdaf1f07c45/ksU6k1ub2r8-LwZtZBsz1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12777.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471c12a0c2b5fdaf1f07c45",
      "avatarUrl": "/avatars/6783067212d24d1e716a8b4c64df61b4.svg",
      "fullname": "Stefan Baumann",
      "name": "stefan-baumann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "62cfeeb73c54a34d508b82a9",
      "name": "CompVis",
      "fullname": "CompVis",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1657794102363-5e3aec01f55e2b62848a5217.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.12793",
      "authors": [
        {
          "_id": "68ef2552486b78128f0e34f0",
          "name": "Long Cui",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f1",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f2",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f3",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f4",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f5",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f6",
          "name": "Yanting Zhang",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f7",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68ef2552486b78128f0e34f8",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:58:10.000Z",
      "submittedOnDailyAt": "2025-10-15T03:14:40.386Z",
      "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
      "submittedOnDailyBy": {
        "_id": "68d8ba59f2f999edd0e25eea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d8ba59f2f999edd0e25eea/9GLAoaqtxo8c5LQtBt2oX.png",
        "isPro": false,
        "fullname": "Long Cui",
        "user": "CuiLong7",
        "type": "user"
      },
      "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
      "upvotes": 1,
      "discussionId": "68ef2553486b78128f0e34f9",
      "organization": {
        "_id": "64006c57a3b8fe3ac0e9af7c",
        "name": "OpenGVLab",
        "fullname": "OpenGVLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
      }
    },
    "publishedAt": "2025-10-14T13:58:10.000Z",
    "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
    "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12793.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68d8ba59f2f999edd0e25eea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d8ba59f2f999edd0e25eea/9GLAoaqtxo8c5LQtBt2oX.png",
      "fullname": "Long Cui",
      "name": "CuiLong7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64006c57a3b8fe3ac0e9af7c",
      "name": "OpenGVLab",
      "fullname": "OpenGVLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12403",
      "authors": [
        {
          "_id": "68ef4853486b78128f0e35d3",
          "name": "Francesco Capuano",
          "hidden": false
        },
        {
          "_id": "68ef4853486b78128f0e35d4",
          "name": "Caroline Pascal",
          "hidden": false
        },
        {
          "_id": "68ef4853486b78128f0e35d5",
          "name": "Adil Zouitine",
          "hidden": false
        },
        {
          "_id": "68ef4853486b78128f0e35d6",
          "name": "Thomas Wolf",
          "hidden": false
        },
        {
          "_id": "68ef4853486b78128f0e35d7",
          "name": "Michel Aractingi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T11:36:46.000Z",
      "submittedOnDailyAt": "2025-10-15T05:38:53.854Z",
      "title": "Robot Learning: A Tutorial",
      "submittedOnDailyBy": {
        "_id": "63d67eac6f49aa8230601996",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
        "isPro": false,
        "fullname": "Francesco Capuano",
        "user": "fracapuano",
        "type": "user"
      },
      "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\nlerobot.",
      "upvotes": 1,
      "discussionId": "68ef4853486b78128f0e35d8",
      "ai_summary": "Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Behavioral Cloning",
        "language-conditioned models"
      ],
      "organization": {
        "_id": "65f08dbd118cb0547bb49092",
        "name": "lerobot",
        "fullname": "LeRobot",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631ce4b244503b72277fc89f/pcLUTLsvMQiR-ujlTgLYF.png"
      }
    },
    "publishedAt": "2025-10-14T07:36:46.000Z",
    "title": "Robot Learning: A Tutorial",
    "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\nlerobot.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d67eac6f49aa8230601996",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
      "fullname": "Francesco Capuano",
      "name": "fracapuano",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 78
    },
    "organization": {
      "_id": "65f08dbd118cb0547bb49092",
      "name": "lerobot",
      "fullname": "LeRobot",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631ce4b244503b72277fc89f/pcLUTLsvMQiR-ujlTgLYF.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12088",
      "authors": [
        {
          "_id": "68ef4328486b78128f0e35a6",
          "name": "Zaid Khan",
          "hidden": false
        },
        {
          "_id": "68ef4328486b78128f0e35a7",
          "name": "Archiki Prasad",
          "hidden": false
        },
        {
          "_id": "68ef4328486b78128f0e35a8",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "68ef4328486b78128f0e35a9",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "68ef4328486b78128f0e35aa",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T02:49:32.000Z",
      "submittedOnDailyAt": "2025-10-15T05:17:01.654Z",
      "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration",
      "submittedOnDailyBy": {
        "_id": "6301c3e0a123c93a5fb295ff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
        "isPro": false,
        "fullname": "Zaid Khan",
        "user": "codezakh",
        "type": "user"
      },
      "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.",
      "upvotes": 1,
      "discussionId": "68ef4328486b78128f0e35ab",
      "projectPage": "https://onelife-worldmodel.github.io",
      "githubRepo": "https://github.com/codezakh/onelife",
      "ai_summary": "OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.",
      "ai_keywords": [
        "conditionally-activated programmatic laws",
        "probabilistic programming framework",
        "precondition-effect structure",
        "dynamic computation graph",
        "state ranking",
        "state fidelity",
        "Crafter-OO",
        "object-oriented symbolic state",
        "pure transition function",
        "simulated rollouts"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-10-13T22:49:32.000Z",
    "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration",
    "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301c3e0a123c93a5fb295ff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
      "fullname": "Zaid Khan",
      "name": "codezakh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11606",
      "authors": [
        {
          "_id": "68ee35b69b77b5223f666173",
          "user": {
            "_id": "682c163fa17480053339f270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
            "isPro": false,
            "fullname": "Yicheng Xu",
            "user": "linghan199",
            "type": "user"
          },
          "name": "Yicheng Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T14:24:10.982Z",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666174",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666175",
          "name": "Jiashuo Yu",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666176",
          "name": "Ziang Yan",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666177",
          "name": "Tianxiang Jiang",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666178",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f666179",
          "name": "Qingsong Zhao",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f66617a",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f66617b",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f66617c",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f66617d",
          "name": "Manabu Okumura",
          "hidden": false
        },
        {
          "_id": "68ee35b69b77b5223f66617e",
          "name": "Yi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T16:45:28.000Z",
      "submittedOnDailyAt": "2025-10-15T03:27:19.691Z",
      "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
      "submittedOnDailyBy": {
        "_id": "682c163fa17480053339f270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
        "isPro": false,
        "fullname": "Yicheng Xu",
        "user": "linghan199",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
      "upvotes": 1,
      "discussionId": "68ee35b69b77b5223f66617f",
      "githubRepo": "https://github.com/OpenGVLab/ExpVid",
      "ai_summary": "ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "ExpVid",
        "benchmark",
        "scientific experiment videos",
        "three-level task hierarchy",
        "fine-grained perception",
        "procedural understanding",
        "scientific reasoning",
        "vision-centric annotation pipeline",
        "automated generation",
        "multi-disciplinary expert validation",
        "coarse-grained recognition",
        "fine details",
        "state changes",
        "high-order reasoning",
        "proprietary models",
        "open-source models"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "64006c57a3b8fe3ac0e9af7c",
        "name": "OpenGVLab",
        "fullname": "OpenGVLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
      }
    },
    "publishedAt": "2025-10-13T12:45:28.000Z",
    "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "682c163fa17480053339f270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UQZW2bM9UEtlVHDUq0yel.png",
      "fullname": "Yicheng Xu",
      "name": "linghan199",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64006c57a3b8fe3ac0e9af7c",
      "name": "OpenGVLab",
      "fullname": "OpenGVLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11545",
      "authors": [
        {
          "_id": "68ef4a3a486b78128f0e35da",
          "name": "Jiayu Ding",
          "hidden": false
        },
        {
          "_id": "68ef4a3a486b78128f0e35db",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "68ef4a3a486b78128f0e35dc",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "68ef4a3a486b78128f0e35dd",
          "name": "Nanning Zheng",
          "hidden": false
        },
        {
          "_id": "68ef4a3a486b78128f0e35de",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T15:42:11.000Z",
      "submittedOnDailyAt": "2025-10-15T06:08:27.466Z",
      "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
      "submittedOnDailyBy": {
        "_id": "667d9460a5d952954ce9f1c4",
        "avatarUrl": "/avatars/340b2503acad0cd692f46e21b9358bd0.svg",
        "isPro": false,
        "fullname": "Jiayu Ding",
        "user": "JiayuDing",
        "type": "user"
      },
      "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
      "upvotes": 1,
      "discussionId": "68ef4a3b486b78128f0e35df",
      "ai_summary": "PART reformulates reasoning traces to preserve information while disrupting unauthorized distillation in Large Language Models.",
      "ai_keywords": [
        "Large Language Models",
        "reasoning chains",
        "reasoning traces",
        "unauthorized distillation",
        "information-preserving antidistillation",
        "self-talk behaviors",
        "sub-conclusions",
        "auxiliary model",
        "reasoning benchmarks",
        "AIME 2024"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-13T11:42:11.000Z",
    "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
    "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667d9460a5d952954ce9f1c4",
      "avatarUrl": "/avatars/340b2503acad0cd692f46e21b9358bd0.svg",
      "fullname": "Jiayu Ding",
      "name": "JiayuDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09062",
      "authors": [
        {
          "_id": "68eda21ede1fee572713a67d",
          "user": {
            "_id": "64b6076821c601b486d217a3",
            "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
            "isPro": false,
            "fullname": "Chung-En, Sun",
            "user": "cesun",
            "type": "user"
          },
          "name": "Chung-En Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:19.643Z",
          "hidden": false
        },
        {
          "_id": "68eda21ede1fee572713a67e",
          "name": "Ge Yan",
          "hidden": false
        },
        {
          "_id": "68eda21ede1fee572713a67f",
          "name": "Akshay Kulkarni",
          "hidden": false
        },
        {
          "_id": "68eda21ede1fee572713a680",
          "name": "Tsui-Wei Weng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T07:08:44.000Z",
      "submittedOnDailyAt": "2025-10-15T05:52:19.331Z",
      "title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with\n  Reliability, Faithfulness, and Interpretability",
      "submittedOnDailyBy": {
        "_id": "64b6076821c601b486d217a3",
        "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
        "isPro": false,
        "fullname": "Chung-En, Sun",
        "user": "cesun",
        "type": "user"
      },
      "summary": "Recent advances in long chain-of-thought (CoT) reasoning have largely\nprioritized answer accuracy and token efficiency, while overlooking aspects\ncritical to trustworthiness. We argue that usable reasoning systems must be\ntrustworthy, characterized by three properties: interpretability, faithfulness,\nand reliability. To this end, we propose ReFIne, a new training framework that\nintegrates supervised fine-tuning with GRPO to encourage models to: (i) improve\ninterpretability by producing structured, tag-based traces with high-level\nplanning that are easier for humans to follow; (ii) enhance faithfulness by\nexplicitly disclosing the decisive information guiding each solution, with\nconsistent cross-section references; and (iii) promote reliability by providing\nself-assessments of both the derivation's soundness and the confidence of the\nfinal answer. We apply ReFIne to the Qwen3 models at multiple scales\n(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.\nOur experimental results show that ReFIne models generate clearer and\nbetter-structured reasoning traces (interpretability +44.0%), more faithfully\nexpose their underlying decision process (faithfulness +18.8%), and offer\ninformative confidence estimates (reliability +42.4%). These findings highlight\nan overlooked but important direction: reasoning models should be optimized not\nonly for accuracy, but also for broader dimensions of trustworthiness. Our code\nis available at:\nhttps://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine",
      "upvotes": 1,
      "discussionId": "68eda21ede1fee572713a681",
      "ai_summary": "ReFIne, a new training framework, enhances the trustworthiness of reasoning models by improving interpretability, faithfulness, and reliability through structured traces, decisive information disclosure, and confidence estimates.",
      "ai_keywords": [
        "long chain-of-thought reasoning",
        "CoT reasoning",
        "ReFIne",
        "supervised fine-tuning",
        "GRPO",
        "interpretability",
        "faithfulness",
        "reliability",
        "structured traces",
        "high-level planning",
        "cross-section references",
        "self-assessments",
        "derivation soundness",
        "confidence estimates",
        "Qwen3 models",
        "mathematical benchmarks"
      ]
    },
    "publishedAt": "2025-10-10T03:08:44.000Z",
    "title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with\n  Reliability, Faithfulness, and Interpretability",
    "summary": "Recent advances in long chain-of-thought (CoT) reasoning have largely\nprioritized answer accuracy and token efficiency, while overlooking aspects\ncritical to trustworthiness. We argue that usable reasoning systems must be\ntrustworthy, characterized by three properties: interpretability, faithfulness,\nand reliability. To this end, we propose ReFIne, a new training framework that\nintegrates supervised fine-tuning with GRPO to encourage models to: (i) improve\ninterpretability by producing structured, tag-based traces with high-level\nplanning that are easier for humans to follow; (ii) enhance faithfulness by\nexplicitly disclosing the decisive information guiding each solution, with\nconsistent cross-section references; and (iii) promote reliability by providing\nself-assessments of both the derivation's soundness and the confidence of the\nfinal answer. We apply ReFIne to the Qwen3 models at multiple scales\n(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.\nOur experimental results show that ReFIne models generate clearer and\nbetter-structured reasoning traces (interpretability +44.0%), more faithfully\nexpose their underlying decision process (faithfulness +18.8%), and offer\ninformative confidence estimates (reliability +42.4%). These findings highlight\nan overlooked but important direction: reasoning models should be optimized not\nonly for accuracy, but also for broader dimensions of trustworthiness. Our code\nis available at:\nhttps://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09062.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6076821c601b486d217a3",
      "avatarUrl": "/avatars/bfd680028e10683b6c0544eb24006246.svg",
      "fullname": "Chung-En, Sun",
      "name": "cesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.08783",
      "authors": [
        {
          "_id": "68ef3a71486b78128f0e3571",
          "name": "Reuben A. Luera",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3572",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3573",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T07:07:09.832Z",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3574",
          "name": "Samyadeep Basu",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3575",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3576",
          "name": "Subhojyoti Mukherjee",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3577",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3578",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e3579",
          "name": "Jihyung Kil",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357a",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357b",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357c",
          "name": "Jiuxiang Gu",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357d",
          "name": "Zichao Wang",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357e",
          "name": "Cindy Xiong Bearfield",
          "hidden": false
        },
        {
          "_id": "68ef3a71486b78128f0e357f",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T20:00:41.000Z",
      "submittedOnDailyAt": "2025-10-15T04:38:58.638Z",
      "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "In an ideal design pipeline, user interface (UI) design is intertwined with\nuser research to validate decisions, yet studies are often resource-constrained\nduring early exploration. Recent advances in multimodal large language models\n(MLLMs) offer a promising opportunity to act as early evaluators, helping\ndesigners narrow options before formal testing. Unlike prior work that\nemphasizes user behavior in narrow domains such as e-commerce with metrics like\nclicks or conversions, we focus on subjective user evaluations across varied\ninterfaces. We investigate whether MLLMs can mimic human preferences when\nevaluating individual UIs and comparing them. Using data from a crowdsourcing\nplatform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and\nexamine alignment with human judgments on multiple UI factors. Our results show\nthat MLLMs approximate human preferences on some dimensions but diverge on\nothers, underscoring both their potential and limitations in supplementing\nearly UX research.",
      "upvotes": 1,
      "discussionId": "68ef3a72486b78128f0e3580"
    },
    "publishedAt": "2025-10-09T16:00:41.000Z",
    "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces",
    "summary": "In an ideal design pipeline, user interface (UI) design is intertwined with\nuser research to validate decisions, yet studies are often resource-constrained\nduring early exploration. Recent advances in multimodal large language models\n(MLLMs) offer a promising opportunity to act as early evaluators, helping\ndesigners narrow options before formal testing. Unlike prior work that\nemphasizes user behavior in narrow domains such as e-commerce with metrics like\nclicks or conversions, we focus on subjective user evaluations across varied\ninterfaces. We investigate whether MLLMs can mimic human preferences when\nevaluating individual UIs and comparing them. Using data from a crowdsourcing\nplatform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and\nexamine alignment with human judgments on multiple UI factors. Our results show\nthat MLLMs approximate human preferences on some dimensions but diverge on\nothers, underscoring both their potential and limitations in supplementing\nearly UX research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  }
]