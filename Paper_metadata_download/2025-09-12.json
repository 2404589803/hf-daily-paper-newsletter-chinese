[
  {
    "paper": {
      "id": "2509.08519",
      "authors": [
        {
          "_id": "68c2624629b8ec9932cd09ea",
          "name": "Liyang Chen",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09eb",
          "name": "Tianxiang Ma",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09ec",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09ed",
          "name": "Bingchuan Li",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09ee",
          "name": "Zhuowei Chen",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09ef",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09f0",
          "name": "Xu He",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09f1",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09f2",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "68c2624629b8ec9932cd09f3",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6804ce31d205d72ddbeec8a0/fw6mAAW3nySgkfKJ1DiH2.mp4"
      ],
      "publishedAt": "2025-09-10T11:54:29.000Z",
      "submittedOnDailyAt": "2025-09-12T03:41:05.784Z",
      "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning",
      "submittedOnDailyBy": {
        "_id": "6804ce31d205d72ddbeec8a0",
        "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
        "isPro": false,
        "fullname": "Tianxiang Ma",
        "user": "TianxiangMa",
        "type": "user"
      },
      "summary": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo.",
      "upvotes": 63,
      "discussionId": "68c2624629b8ec9932cd09f4",
      "projectPage": "https://phantom-video.github.io/HuMo/",
      "githubRepo": "https://github.com/Phantom-video/HuMo",
      "ai_summary": "HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.",
      "ai_keywords": [
        "human-centric video generation",
        "multimodal inputs",
        "text",
        "image",
        "audio",
        "high-quality dataset",
        "two-stage progressive multimodal training",
        "minimal-invasive image injection",
        "audio cross-attention layer",
        "focus-by-predicting strategy",
        "time-adaptive Classifier-Free Guidance",
        "denoising steps"
      ],
      "githubStars": 91
    },
    "publishedAt": "2025-09-10T07:54:29.000Z",
    "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning",
    "summary": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6804ce31d205d72ddbeec8a0/fw6mAAW3nySgkfKJ1DiH2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08519.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6804ce31d205d72ddbeec8a0",
      "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
      "fullname": "Tianxiang Ma",
      "name": "TianxiangMa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09174",
      "authors": [
        {
          "_id": "68c38053fc1747b9124039ea",
          "name": "Yuhao Zhang",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039eb",
          "name": "Yuhao Du",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ec",
          "name": "Zhanchen Dai",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ed",
          "name": "Xiangnan Ma",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ee",
          "name": "Kaiqi Kou",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ef",
          "name": "Benyou Wang",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039f0",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66975b9f8031bf92b428e138/fpbLmvS4cUyXEqNZlqSY3.mp4"
      ],
      "publishedAt": "2025-09-11T06:17:59.000Z",
      "submittedOnDailyAt": "2025-09-12T00:46:24.478Z",
      "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
      "submittedOnDailyBy": {
        "_id": "66975b9f8031bf92b428e138",
        "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
        "isPro": false,
        "fullname": "Yuhao Zhang",
        "user": "Yoohao",
        "type": "user"
      },
      "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
      "upvotes": 46,
      "discussionId": "68c38053fc1747b9124039f1",
      "projectPage": "https://freedomintelligence.github.io/EchoX/",
      "githubRepo": "https://github.com/FreedomIntelligence/EchoX",
      "ai_summary": "EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.",
      "ai_keywords": [
        "speech-to-speech large language models",
        "SLLMs",
        "text-based large language models",
        "LLMs",
        "acoustic-semantic gap",
        "semantic representations",
        "speech training targets",
        "acoustic learning",
        "semantic learning",
        "knowledge-based question-answering benchmarks"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-11T02:17:59.000Z",
    "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
    "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66975b9f8031bf92b428e138/fpbLmvS4cUyXEqNZlqSY3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09174.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09674",
      "authors": [
        {
          "_id": "68c37bb3fc1747b912403994",
          "name": "Haozhan Li",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403995",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403996",
          "name": "Jiale Yu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403997",
          "name": "Yuhao Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403998",
          "name": "Zhaohui Yang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403999",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399a",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399b",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399c",
          "name": "Tianxing Chen",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399d",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399e",
          "name": "Dehui Wang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399f",
          "name": "Dingxiang Luo",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a0",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a1",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a2",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a3",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a4",
          "name": "Shanghang Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a5",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a6",
          "name": "Yao Mu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a7",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a8",
          "name": "Ning Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:17.000Z",
      "submittedOnDailyAt": "2025-09-12T00:32:42.595Z",
      "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "662f638ba9891e43cc4c5125",
        "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
        "isPro": true,
        "fullname": "Li Haozhan",
        "user": "Haozhan72",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
      "upvotes": 42,
      "discussionId": "68c37bb3fc1747b9124039a9",
      "githubRepo": "https://github.com/PRIME-RL/SimpleVLA-RL",
      "ai_summary": "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "VLA models",
        "trajectory sampling",
        "parallelization",
        "multi-environment rendering",
        "loss computation",
        "LIBERO",
        "RoboTwin",
        "step-by-step reasoning",
        "distribution shift",
        "pushcut"
      ],
      "githubStars": 446
    },
    "publishedAt": "2025-09-11T13:59:17.000Z",
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662f638ba9891e43cc4c5125",
      "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
      "fullname": "Li Haozhan",
      "name": "Haozhan72",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09595",
      "authors": [
        {
          "_id": "68c37f08fc1747b9124039ce",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039cf",
          "name": "Jiwen Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d0",
          "name": "Wenyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d1",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d2",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d3",
          "name": "Liyuan Cui",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d4",
          "name": "Mingming Lao",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d5",
          "name": "Yingchao Shao",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d6",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d7",
          "name": "Xiaohan Li",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d8",
          "name": "Ming Chen",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d9",
          "name": "Xiaoqiang Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039da",
          "name": "Yu-Shen Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039db",
          "name": "Pengfei Wan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T16:34:57.000Z",
      "submittedOnDailyAt": "2025-09-12T00:31:52.896Z",
      "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
      "upvotes": 26,
      "discussionId": "68c37f08fc1747b9124039dc",
      "ai_summary": "Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.",
      "ai_keywords": [
        "multimodal large language model",
        "blueprint video",
        "high-level semantics",
        "character motion",
        "emotions",
        "blueprint keyframes",
        "first-last frame strategy",
        "global-to-local framework",
        "parallel architecture",
        "lip synchronization accuracy",
        "emotion and dynamic expressiveness",
        "instruction controllability",
        "identity preservation",
        "cross-domain generalization"
      ]
    },
    "publishedAt": "2025-09-11T12:34:57.000Z",
    "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
    "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09265",
      "authors": [
        {
          "_id": "68c37f86fc1747b9124039de",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039df",
          "name": "Jiacai Liu",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e0",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e1",
          "name": "Yingru Li",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e2",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e3",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e4",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e5",
          "name": "Lin Zhang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e6",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e7",
          "name": "Ke Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T08:50:01.000Z",
      "submittedOnDailyAt": "2025-09-12T00:36:00.113Z",
      "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
      "upvotes": 26,
      "discussionId": "68c37f86fc1747b9124039e8",
      "projectPage": "https://empgseed-seed.github.io/",
      "ai_summary": "Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sparse rewards",
        "dense reward signals",
        "inverse reinforcement learning",
        "Process Reward Models",
        "policy gradients",
        "entropy",
        "Entropy-Modulated Policy Gradients (EMPG)",
        "WebShop",
        "ALFWorld",
        "Deep Search"
      ]
    },
    "publishedAt": "2025-09-11T04:50:01.000Z",
    "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
    "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09680",
      "authors": [
        {
          "_id": "68c37e4cfc1747b9124039ab",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ac",
          "name": "Aldrich Yu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ad",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ae",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039af",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b0",
          "name": "Yuxuan Cai",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b1",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b2",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b3",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:59.000Z",
      "submittedOnDailyAt": "2025-09-12T00:28:54.761Z",
      "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
      "upvotes": 19,
      "discussionId": "68c37e4dfc1747b9124039b5",
      "projectPage": "https://flux-reason-6m.github.io/",
      "githubRepo": "https://github.com/rongyaofang/prism-bench",
      "ai_summary": "FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.",
      "ai_keywords": [
        "text-to-image (T2I) models",
        "FLUX-Reason-6M",
        "PRISM-Bench",
        "Generation Chain-of-Thought (GCoT)",
        "vision-language models",
        "prompt-image alignment",
        "image aesthetics"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-09-11T13:59:59.000Z",
    "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09666",
      "authors": [
        {
          "_id": "68c3b899fc1747b912403acb",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403acc",
          "name": "Kaiqing Lin",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403acd",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ace",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403acf",
          "name": "Hui Han",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad0",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad1",
          "name": "Hao Liu",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad2",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad3",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad4",
          "name": "Xue Xu",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad5",
          "name": "Xinyan Xiao",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad6",
          "name": "Jingdong Wang",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad7",
          "name": "Haifeng Wang",
          "hidden": false
        },
        {
          "_id": "68c3b899fc1747b912403ad8",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/DTzxWlEUl9PlV5z4x7b-N.jpeg"
      ],
      "publishedAt": "2025-09-11T17:57:59.000Z",
      "submittedOnDailyAt": "2025-09-12T04:45:44.849Z",
      "title": "Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?",
      "submittedOnDailyBy": {
        "_id": "6367a8175bb06007ea099b8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
        "isPro": false,
        "fullname": "linbin",
        "user": "LanguageBind",
        "type": "user"
      },
      "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder\nlens-understanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. Using\nreconstruction fidelity as the unified training objective, we enforce the\ncoherent bidirectional information flow between the understanding and\ngeneration processes, bringing mutual gains. To implement this, we propose UAE,\na novel framework for unified multimodal learning. We begin by pre-training the\ndecoder with large-scale long-context image captions to capture fine-grained\nsemantic and complex spatial relationships. We then propose Unified-GRPO via\nreinforcement learning (RL), which covers three stages: (1) A cold-start phase\nto gently initialize both encoder and decoder with a semantic reconstruction\nloss; (2) Generation for Understanding, where the encoder is trained to\ngenerate informative captions that maximize the decoder's reconstruction\nquality, enhancing its visual understanding; (3) Understanding for Generation,\nwhere the decoder is refined to reconstruct from these captions, forcing it to\nleverage every detail and improving its long-context instruction following and\ngeneration fidelity. For evaluation, we introduce Unified-Bench, the first\nbenchmark tailored to assess the degree of unification of the UMMs. A\nsurprising \"aha moment\" arises within the multimodal learning domain: as RL\nprogresses, the encoder autonomously produces more descriptive captions, while\nthe decoder simultaneously demonstrates a profound ability to understand these\nintricate descriptions, resulting in reconstructions of striking fidelity.",
      "upvotes": 18,
      "discussionId": "68c3b899fc1747b912403ad9",
      "ai_summary": "A novel framework UAE uses reinforcement learning to unify image-to-text and text-to-image processes, enhancing mutual understanding and generation fidelity.",
      "ai_keywords": [
        "Auto-Encoder",
        "encoder",
        "decoder",
        "reconstruction fidelity",
        "unified multimodal learning",
        "UAE",
        "Unified-GRPO",
        "reinforcement learning",
        "semantic reconstruction loss",
        "Unified-Bench",
        "multimodal learning"
      ]
    },
    "publishedAt": "2025-09-11T13:57:59.000Z",
    "title": "Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?",
    "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder\nlens-understanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. Using\nreconstruction fidelity as the unified training objective, we enforce the\ncoherent bidirectional information flow between the understanding and\ngeneration processes, bringing mutual gains. To implement this, we propose UAE,\na novel framework for unified multimodal learning. We begin by pre-training the\ndecoder with large-scale long-context image captions to capture fine-grained\nsemantic and complex spatial relationships. We then propose Unified-GRPO via\nreinforcement learning (RL), which covers three stages: (1) A cold-start phase\nto gently initialize both encoder and decoder with a semantic reconstruction\nloss; (2) Generation for Understanding, where the encoder is trained to\ngenerate informative captions that maximize the decoder's reconstruction\nquality, enhancing its visual understanding; (3) Understanding for Generation,\nwhere the decoder is refined to reconstruct from these captions, forcing it to\nleverage every detail and improving its long-context instruction following and\ngeneration fidelity. For evaluation, we introduce Unified-Bench, the first\nbenchmark tailored to assess the degree of unification of the UMMs. A\nsurprising \"aha moment\" arises within the multimodal learning domain: as RL\nprogresses, the encoder autonomously produces more descriptive captions, while\nthe decoder simultaneously demonstrates a profound ability to understand these\nintricate descriptions, resulting in reconstructions of striking fidelity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/DTzxWlEUl9PlV5z4x7b-N.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6367a8175bb06007ea099b8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
      "fullname": "linbin",
      "name": "LanguageBind",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09676",
      "authors": [
        {
          "_id": "68c37eb7fc1747b9124039bd",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039be",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039bf",
          "name": "Rujie Zheng",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c0",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c1",
          "name": "Jian Gao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c2",
          "name": "Lin-Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c3",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c4",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c5",
          "name": "Chang Zeng",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c6",
          "name": "Yanxi Zhou",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c7",
          "name": "Xiaoxiao Long",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c8",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c9",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039ca",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039cb",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:31.000Z",
      "submittedOnDailyAt": "2025-09-12T00:30:37.532Z",
      "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Significant progress has been made in spatial intelligence, spanning both\nspatial reconstruction and world exploration. However, the scalability and\nreal-world fidelity of current models remain severely constrained by the\nscarcity of large-scale, high-quality training data. While several datasets\nprovide camera pose information, they are typically limited in scale,\ndiversity, and annotation richness, particularly for real-world dynamic scenes\nwith ground-truth camera motion. To this end, we collect SpatialVID, a\ndataset consists of a large corpus of in-the-wild videos with diverse scenes,\ncamera movements and dense 3D annotations such as per-frame camera poses,\ndepth, and motion instructions. Specifically, we collect more than 21,000 hours\nof raw video, and process them into 2.7 million clips through a hierarchical\nfiltering pipeline, totaling 7,089 hours of dynamic content. A subsequent\nannotation pipeline enriches these clips with detailed spatial and semantic\ninformation, including camera poses, depth maps, dynamic masks, structured\ncaptions, and serialized motion instructions. Analysis of SpatialVID's data\nstatistics reveals a richness and diversity that directly foster improved model\ngeneralization and performance, establishing it as a key asset for the video\nand 3D vision research community.",
      "upvotes": 11,
      "discussionId": "68c37eb8fc1747b9124039cc",
      "projectPage": "https://nju-3dv.github.io/projects/SpatialVID/",
      "ai_summary": "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.",
      "ai_keywords": [
        "spatialVID",
        "camera poses",
        "depth maps",
        "dynamic masks",
        "structured captions",
        "motion instructions",
        "video and 3D vision"
      ]
    },
    "publishedAt": "2025-09-11T13:59:31.000Z",
    "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
    "summary": "Significant progress has been made in spatial intelligence, spanning both\nspatial reconstruction and world exploration. However, the scalability and\nreal-world fidelity of current models remain severely constrained by the\nscarcity of large-scale, high-quality training data. While several datasets\nprovide camera pose information, they are typically limited in scale,\ndiversity, and annotation richness, particularly for real-world dynamic scenes\nwith ground-truth camera motion. To this end, we collect SpatialVID, a\ndataset consists of a large corpus of in-the-wild videos with diverse scenes,\ncamera movements and dense 3D annotations such as per-frame camera poses,\ndepth, and motion instructions. Specifically, we collect more than 21,000 hours\nof raw video, and process them into 2.7 million clips through a hierarchical\nfiltering pipeline, totaling 7,089 hours of dynamic content. A subsequent\nannotation pipeline enriches these clips with detailed spatial and semantic\ninformation, including camera poses, depth maps, dynamic masks, structured\ncaptions, and serialized motion instructions. Analysis of SpatialVID's data\nstatistics reveals a richness and diversity that directly foster improved model\ngeneralization and performance, establishing it as a key asset for the video\nand 3D vision research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09676.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09372",
      "authors": [
        {
          "_id": "68c3cc49fc1747b912403b06",
          "name": "Yihao Wang",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b07",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b08",
          "name": "Lingxiao Li",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b09",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0a",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0b",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0c",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0d",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0e",
          "name": "Wei Zhao",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b0f",
          "name": "Pengxu Hou",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b10",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b11",
          "name": "Yifan Tang",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b12",
          "name": "Wenhui Wang",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b13",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b14",
          "name": "Jianyi Liu",
          "hidden": false
        },
        {
          "_id": "68c3cc49fc1747b912403b15",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T11:42:21.000Z",
      "submittedOnDailyAt": "2025-09-12T06:04:56.756Z",
      "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.",
      "upvotes": 10,
      "discussionId": "68c3cc4afc1747b912403b16",
      "projectPage": "https://vla-adapter.github.io/",
      "ai_summary": "VLA-Adapter reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving state-of-the-art performance and fast inference speed with minimal computational resources.",
      "ai_keywords": [
        "VLA models",
        "Vision-Language Model (VLM)",
        "robotic data",
        "VL conditions",
        "Policy module",
        "Bridge Attention",
        "parameter backbone",
        "simulated benchmarks",
        "real-world benchmarks",
        "inference speed",
        "consumer-grade GPU"
      ]
    },
    "publishedAt": "2025-09-11T07:42:21.000Z",
    "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
    "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09286",
      "authors": [
        {
          "_id": "68c3905efc1747b912403a5c",
          "name": "Bohao Tang",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a5d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a5e",
          "name": "Fei Zhang",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a5f",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a60",
          "name": "Ethan Chern",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a61",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a62",
          "name": "Zhixin Wang",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a63",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "68c3905efc1747b912403a64",
          "name": "Ya Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T09:22:16.000Z",
      "submittedOnDailyAt": "2025-09-12T01:46:56.969Z",
      "title": "Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.",
      "upvotes": 7,
      "discussionId": "68c3905efc1747b912403a65",
      "githubRepo": "https://github.com/Aphelios-Tang/Code-as-Thought",
      "ai_summary": "VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.",
      "ai_keywords": [
        "Code-as-Thought",
        "Visual Programmability",
        "direct visual reasoning",
        "reinforcement learning",
        "dual-reward system",
        "numerical hallucination",
        "chart-understanding benchmarks"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-09-11T05:22:16.000Z",
    "title": "Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding",
    "summary": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09118",
      "authors": [
        {
          "_id": "68c375f3fc1747b91240397f",
          "name": "Tianlu Zheng",
          "hidden": false
        },
        {
          "_id": "68c375f3fc1747b912403980",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "68c375f3fc1747b912403981",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68c375f3fc1747b912403982",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "68c375f3fc1747b912403983",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "68c375f3fc1747b912403984",
          "name": "Qichuan Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T03:06:22.000Z",
      "submittedOnDailyAt": "2025-09-12T00:58:37.868Z",
      "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks.",
      "upvotes": 5,
      "discussionId": "68c375f4fc1747b912403985",
      "githubRepo": "https://github.com/Multimodal-Representation-Learning-MRL/GA-DMS",
      "ai_summary": "GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training",
        "CLIP",
        "person representation learning",
        "global contrastive learning",
        "local features",
        "fine-grained matching",
        "noise-resistant data construction",
        "MLLMs",
        "WebPerson",
        "GA-DMS",
        "Gradient-Attention Guided Dual-Masking Synergetic",
        "cross-modal alignment",
        "gradient-attention similarity score",
        "masked token prediction",
        "fine-grained semantic representation learning"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-09-10T23:06:22.000Z",
    "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval",
    "summary": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09118.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01964",
      "authors": [
        {
          "_id": "68b7fb01295f15ff60911548",
          "user": {
            "_id": "64f955c582673b2a07fbf0ad",
            "avatarUrl": "/avatars/1c98c8be61f6580c1e4ee698fa5c0716.svg",
            "isPro": false,
            "fullname": "hongyu",
            "user": "learn12138",
            "type": "user"
          },
          "name": "Hongyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-03T08:25:43.578Z",
          "hidden": false
        },
        {
          "_id": "68b7fb01295f15ff60911549",
          "name": "Chaofeng Chen",
          "hidden": false
        },
        {
          "_id": "68b7fb01295f15ff6091154a",
          "name": "Xiaoming Li",
          "hidden": false
        },
        {
          "_id": "68b7fb01295f15ff6091154b",
          "name": "Guangming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T05:12:52.000Z",
      "submittedOnDailyAt": "2025-09-12T03:06:33.102Z",
      "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
      "submittedOnDailyBy": {
        "_id": "64f955c582673b2a07fbf0ad",
        "avatarUrl": "/avatars/1c98c8be61f6580c1e4ee698fa5c0716.svg",
        "isPro": false,
        "fullname": "hongyu",
        "user": "learn12138",
        "type": "user"
      },
      "summary": "Gaussian Splatting (GS), a recent technique for converting discrete points\ninto continuous spatial representations, has shown promising results in 3D\nscene modeling and 2D image super-resolution. In this paper, we explore its\nuntapped potential for image inpainting, which demands both locally coherent\npixel synthesis and globally consistent semantic restoration. We propose the\nfirst image inpainting framework based on 2D Gaussian Splatting, which encodes\nincomplete images into a continuous field of 2D Gaussian splat coefficients and\nreconstructs the final image via a differentiable rasterization process. The\ncontinuous rendering paradigm of GS inherently promotes pixel-level coherence\nin the inpainted results. To improve efficiency and scalability, we introduce a\npatch-wise rasterization strategy that reduces memory overhead and accelerates\ninference. For global semantic consistency, we incorporate features from a\npretrained DINO model. We observe that DINO's global features are naturally\nrobust to small missing regions and can be effectively adapted to guide\nsemantic alignment in large-mask scenarios, ensuring that the inpainted content\nremains contextually consistent with the surrounding scene. Extensive\nexperiments on standard benchmarks demonstrate that our method achieves\ncompetitive performance in both quantitative metrics and perceptual quality,\nestablishing a new direction for applying Gaussian Splatting to 2D image\nprocessing.",
      "upvotes": 4,
      "discussionId": "68b7fb01295f15ff6091154c",
      "ai_summary": "A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.",
      "ai_keywords": [
        "Gaussian Splatting",
        "2D Gaussian splat coefficients",
        "differentiable rasterization",
        "patch-wise rasterization",
        "DINO model",
        "global semantic consistency",
        "image inpainting",
        "perceptual quality"
      ]
    },
    "publishedAt": "2025-09-02T01:12:52.000Z",
    "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
    "summary": "Gaussian Splatting (GS), a recent technique for converting discrete points\ninto continuous spatial representations, has shown promising results in 3D\nscene modeling and 2D image super-resolution. In this paper, we explore its\nuntapped potential for image inpainting, which demands both locally coherent\npixel synthesis and globally consistent semantic restoration. We propose the\nfirst image inpainting framework based on 2D Gaussian Splatting, which encodes\nincomplete images into a continuous field of 2D Gaussian splat coefficients and\nreconstructs the final image via a differentiable rasterization process. The\ncontinuous rendering paradigm of GS inherently promotes pixel-level coherence\nin the inpainted results. To improve efficiency and scalability, we introduce a\npatch-wise rasterization strategy that reduces memory overhead and accelerates\ninference. For global semantic consistency, we incorporate features from a\npretrained DINO model. We observe that DINO's global features are naturally\nrobust to small missing regions and can be effectively adapted to guide\nsemantic alignment in large-mask scenarios, ensuring that the inpainted content\nremains contextually consistent with the surrounding scene. Extensive\nexperiments on standard benchmarks demonstrate that our method achieves\ncompetitive performance in both quantitative metrics and perceptual quality,\nestablishing a new direction for applying Gaussian Splatting to 2D image\nprocessing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f955c582673b2a07fbf0ad",
      "avatarUrl": "/avatars/1c98c8be61f6580c1e4ee698fa5c0716.svg",
      "fullname": "hongyu",
      "name": "learn12138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.09614",
      "authors": [
        {
          "_id": "68c381befc1747b9124039f9",
          "name": "Jielin Qiu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fa",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fb",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fc",
          "name": "Rithesh Murthy",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fd",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fe",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039ff",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a00",
          "name": "Ming Zhu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a01",
          "name": "Liangwei Yang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a02",
          "name": "Juntao Tan",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a03",
          "name": "Zhepeng Cen",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a04",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a05",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a06",
          "name": "Weiran Yao",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a07",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a08",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a09",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T16:55:04.000Z",
      "submittedOnDailyAt": "2025-09-12T00:43:38.492Z",
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
      "upvotes": 2,
      "discussionId": "68c381bffc1747b912403a0a",
      "githubRepo": "https://github.com/SalesforceAIResearch/LoCoBench",
      "ai_summary": "LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.",
      "ai_keywords": [
        "long-context language models",
        "LoCoBench",
        "code evaluation benchmarks",
        "long-context capabilities",
        "codebases",
        "cross-file refactoring",
        "multi-session development",
        "bug investigation",
        "feature implementation",
        "code comprehension",
        "integration testing",
        "security analysis",
        "LoCoBench Score (LCBS)"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-11T12:55:04.000Z",
    "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
    "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09332",
      "authors": [
        {
          "_id": "68c382b8fc1747b912403a1c",
          "name": "Yuecheng Liu",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a1d",
          "name": "Dafeng Chi",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a1e",
          "name": "Shiguang Wu",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a1f",
          "name": "Zhanguang Zhang",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a20",
          "name": "Yuzheng Zhuang",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a21",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a22",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a23",
          "name": "Lingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a24",
          "name": "Pengwei Xie",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a25",
          "name": "David Gamaliel Arcos Bravo",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a26",
          "name": "Yingxue Zhang",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a27",
          "name": "Jianye Hao",
          "hidden": false
        },
        {
          "_id": "68c382b8fc1747b912403a28",
          "name": "Xingyue Quan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T10:32:22.000Z",
      "submittedOnDailyAt": "2025-09-12T00:48:32.921Z",
      "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io",
      "upvotes": 2,
      "discussionId": "68c382b8fc1747b912403a29",
      "projectPage": "https://omnieva.github.io/",
      "ai_summary": "OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "embodied intelligence",
        "multimodal understanding",
        "reasoning",
        "interaction",
        "spatial decision-making",
        "Geometric Adaptability Gap",
        "Embodiment Constraint Gap",
        "Task-Adaptive 3D Grounding",
        "gated router",
        "3D fusion",
        "context-aware 3D grounding",
        "Embodiment-Aware Reasoning",
        "goal-directed",
        "executable",
        "embodied benchmarks",
        "primitive tasks",
        "composite tasks"
      ]
    },
    "publishedAt": "2025-09-11T06:32:22.000Z",
    "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
    "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.07430",
      "authors": [
        {
          "_id": "68c2652d29b8ec9932cd09f6",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09f7",
          "name": "Jiaran Hao",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09f8",
          "name": "Jason Klein Liu",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09f9",
          "name": "Zhijian Zhou",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09fa",
          "name": "Xiaoyu Tan",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09fb",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09fc",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09fd",
          "name": "Shirui Pan",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09fe",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "68c2652d29b8ec9932cd09ff",
          "name": "Yuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-09T06:34:32.000Z",
      "submittedOnDailyAt": "2025-09-12T02:09:47.311Z",
      "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward",
      "submittedOnDailyBy": {
        "_id": "65164444bc0631719873af81",
        "avatarUrl": "/avatars/dab8b90db8bbd00806268fe276e3ea36.svg",
        "isPro": false,
        "fullname": "Wei Pang",
        "user": "weipang142857",
        "type": "user"
      },
      "summary": "A central paradox in fine-tuning Large Language Models (LLMs) with\nReinforcement Learning with Verifiable Reward (RLVR) is the frequent\ndegradation of multi-attempt performance (Pass@k) despite improvements in\nsingle-attempt accuracy (Pass@1). This is often accompanied by catastrophic\nforgetting, where models lose previously acquired skills. While various methods\nhave been proposed, the choice and function of the divergence term have been\nsurprisingly unexamined as a proactive solution. We argue that standard RLVR\nobjectives -- both those using the mode-seeking reverse KL-divergence and those\nforgoing a divergence term entirely -- lack a crucial mechanism for knowledge\nretention. The reverse-KL actively accelerates this decay by narrowing the\npolicy, while its absence provides no safeguard against the model drifting from\nits diverse knowledge base. We propose a fundamental shift in perspective:\nusing the divergence term itself as the solution. Our framework,\nDiversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences\n(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By\ncontinuously referencing the initial policy, this approach forces the model to\nmaintain broad solution coverage. Extensive experiments on math and SQL\ngeneration demonstrate that DPH-RL not only resolves the Pass@k degradation but\nimproves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is\nmore training-efficient because it computes f-divergence using generator\nfunctions, requiring only sampling from the initial policy and no online\nreference model. Our work highlights a crucial, overlooked axis for improving\nRLVR, demonstrating that the proper selection of a divergence measure is a\npowerful tool for building more general and diverse reasoning models.",
      "upvotes": 2,
      "discussionId": "68c2652d29b8ec9932cd0a00",
      "ai_summary": "A new framework, DPH-RL, uses mass-covering f-divergences to address Pass@k degradation and catastrophic forgetting in fine-tuning LLMs with RLVR, improving both Pass@1 and Pass@k.",
      "ai_keywords": [
        "Large Language Models",
        "Reinforcement Learning with Verifiable Reward",
        "RLVR",
        "multi-attempt performance",
        "Pass@k",
        "single-attempt accuracy",
        "Pass@1",
        "catastrophic forgetting",
        "reverse KL-divergence",
        "divergence term",
        "knowledge retention",
        "mass-covering f-divergences",
        "forward-KL",
        "JS-divergence",
        "rehearsal mechanism",
        "initial policy",
        "solution coverage",
        "training-efficiency",
        "generator functions",
        "online reference model"
      ]
    },
    "publishedAt": "2025-09-09T02:34:32.000Z",
    "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward",
    "summary": "A central paradox in fine-tuning Large Language Models (LLMs) with\nReinforcement Learning with Verifiable Reward (RLVR) is the frequent\ndegradation of multi-attempt performance (Pass@k) despite improvements in\nsingle-attempt accuracy (Pass@1). This is often accompanied by catastrophic\nforgetting, where models lose previously acquired skills. While various methods\nhave been proposed, the choice and function of the divergence term have been\nsurprisingly unexamined as a proactive solution. We argue that standard RLVR\nobjectives -- both those using the mode-seeking reverse KL-divergence and those\nforgoing a divergence term entirely -- lack a crucial mechanism for knowledge\nretention. The reverse-KL actively accelerates this decay by narrowing the\npolicy, while its absence provides no safeguard against the model drifting from\nits diverse knowledge base. We propose a fundamental shift in perspective:\nusing the divergence term itself as the solution. Our framework,\nDiversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences\n(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By\ncontinuously referencing the initial policy, this approach forces the model to\nmaintain broad solution coverage. Extensive experiments on math and SQL\ngeneration demonstrate that DPH-RL not only resolves the Pass@k degradation but\nimproves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is\nmore training-efficient because it computes f-divergence using generator\nfunctions, requiring only sampling from the initial policy and no online\nreference model. Our work highlights a crucial, overlooked axis for improving\nRLVR, demonstrating that the proper selection of a divergence measure is a\npowerful tool for building more general and diverse reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65164444bc0631719873af81",
      "avatarUrl": "/avatars/dab8b90db8bbd00806268fe276e3ea36.svg",
      "fullname": "Wei Pang",
      "name": "weipang142857",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]