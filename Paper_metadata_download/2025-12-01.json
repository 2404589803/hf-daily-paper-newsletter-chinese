[
  {
    "paper": {
      "id": "2511.22699",
      "authors": [
        {
          "_id": "692d06234397b1ec214f6788",
          "name": "Z-Image Team",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6789",
          "name": "Huanqia Cai",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678a",
          "name": "Sihan Cao",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678b",
          "name": "Ruoyi Du",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678c",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678d",
          "name": "Steven Hoi",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678e",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f678f",
          "name": "Zhaohui Hou",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6790",
          "name": "Dengyang Jiang",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6791",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6792",
          "name": "Liangchen Li",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6793",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6794",
          "name": "Zhong-Yu Li",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6795",
          "name": "David Liu",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6796",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6797",
          "name": "Junhan Shi",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6798",
          "name": "Qilong Wu",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f6799",
          "name": "Feng Yu",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f679a",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f679b",
          "name": "Shifeng Zhang",
          "hidden": false
        },
        {
          "_id": "692d06234397b1ec214f679c",
          "name": "Shilin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T18:52:07.000Z",
      "submittedOnDailyAt": "2025-12-01T00:38:17.269Z",
      "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
      "submittedOnDailyBy": {
        "_id": "6285a9133ab6642179158944",
        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
        "isPro": false,
        "fullname": "Zhen Li",
        "user": "Paper99",
        "type": "user"
      },
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "upvotes": 42,
      "discussionId": "692d06234397b1ec214f679d",
      "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/",
      "githubRepo": "https://github.com/Tongyi-MAI/Z-Image",
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "ai_keywords": [
        "Scalable Single-Stream Diffusion Transformer",
        "S3-DiT",
        "diffusion transformer",
        "omni-pre-training",
        "instruction-following capabilities",
        "photorealistic image generation",
        "bilingual text rendering",
        "distillation scheme",
        "reward post-training",
        "H800 GPU",
        "VRAM"
      ],
      "githubStars": 2931,
      "organization": {
        "_id": "6925b20fed452d1567c012d3",
        "name": "Tongyi-MAI",
        "fullname": "Tongyi-MAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
      }
    },
    "publishedAt": "2025-11-27T13:52:07.000Z",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6285a9133ab6642179158944",
      "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
      "fullname": "Zhen Li",
      "name": "Paper99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "organization": {
      "_id": "6925b20fed452d1567c012d3",
      "name": "Tongyi-MAI",
      "fullname": "Tongyi-MAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22625",
      "authors": [
        {
          "_id": "692d118f4397b1ec214f683f",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6840",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6841",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6842",
          "name": "Zhibo Wang",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6843",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6844",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6845",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6846",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6847",
          "name": "Aojie Li",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6848",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f6849",
          "name": "Pengtao Chen",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f684a",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f684b",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f684c",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "692d118f4397b1ec214f684d",
          "name": "Gang Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ePq9HeiskK7FFzwEgqppg.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uyf0WIc1F6Zpdth1OHtkl.png"
      ],
      "publishedAt": "2025-11-27T17:02:48.000Z",
      "submittedOnDailyAt": "2025-12-01T01:25:40.597Z",
      "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
      "upvotes": 30,
      "discussionId": "692d118f4397b1ec214f684e",
      "ai_summary": "Integrating reasoning mechanisms into image editing models enhances performance by improving instruction understanding and result correction.",
      "ai_keywords": [
        "multimodal large language model",
        "MLLM",
        "diffusion decoder",
        "Step1X-Edit",
        "Qwen-Image-Edit",
        "thinking mechanism",
        "reflection",
        "image editing",
        "instruction understanding",
        "editing accuracy",
        "world knowledge",
        "ImgEdit",
        "GEdit",
        "Kris",
        "DiT"
      ],
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2025-11-27T12:02:48.000Z",
    "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
    "summary": "Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ePq9HeiskK7FFzwEgqppg.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uyf0WIc1F6Zpdth1OHtkl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 172
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.23475",
      "authors": [
        {
          "_id": "692d0d8f4397b1ec214f67e7",
          "name": "Zhizhou Zhong",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67e8",
          "name": "Yicheng Ji",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67e9",
          "name": "Zhe Kong",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67ea",
          "name": "Yiying Liu",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67eb",
          "name": "Jiarui Wang",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67ec",
          "name": "Jiasun Feng",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67ed",
          "name": "Lupeng Liu",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67ee",
          "name": "Xiangyi Wang",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67ef",
          "name": "Yanjia Li",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f0",
          "name": "Yuqing She",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f1",
          "name": "Ying Qin",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f2",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f3",
          "name": "Shuiyang Mao",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f4",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "692d0d8f4397b1ec214f67f5",
          "name": "Wenhan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-28T18:59:01.000Z",
      "submittedOnDailyAt": "2025-12-01T01:08:00.505Z",
      "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
      "upvotes": 22,
      "discussionId": "692d0d904397b1ec214f67f6",
      "projectPage": "https://hkust-c4g.github.io/AnyTalker-homepage/",
      "githubRepo": "https://github.com/HKUST-C4G/AnyTalker",
      "ai_summary": "The proposed AnyTalker framework generates high-quality multi-person talking videos by extending Diffusion Transformer with identity-aware attention, leveraging single-person videos for training, and using a specialized dataset for evaluation.",
      "ai_keywords": [
        "Diffusion Transformer",
        "identity-aware attention",
        "multi-person video generation",
        "audio-driven generation",
        "multi-stream processing",
        "lip synchronization",
        "visual quality",
        "interactivity",
        "dataset"
      ],
      "githubStars": 63
    },
    "publishedAt": "2025-11-28T13:59:01.000Z",
    "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
    "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23475.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 172
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.23199",
      "authors": [
        {
          "_id": "692d09fb4397b1ec214f67b2",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "692d09fb4397b1ec214f67b3",
          "name": "Zeqing Wang",
          "hidden": false
        },
        {
          "_id": "692d09fb4397b1ec214f67b4",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "692d09fb4397b1ec214f67b5",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "692d09fb4397b1ec214f67b6",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/TtAxSiJXuWb3AASpinIdd.mp4"
      ],
      "publishedAt": "2025-11-28T14:03:39.000Z",
      "submittedOnDailyAt": "2025-12-01T02:09:07.640Z",
      "title": "Vision Bridge Transformer at Scale",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
      "upvotes": 21,
      "discussionId": "692d09fb4397b1ec214f67b7",
      "projectPage": "https://yuanshi9815.github.io/ViBT_homepage/",
      "githubRepo": "https://github.com/Yuanshi9815/ViBT",
      "ai_summary": "Bridge Models, instantiated as Vision Bridge Transformer (ViBT), efficiently translate data through direct modeling of input-to-output trajectories, achieving robust performance in image and video editing tasks at large scales.",
      "ai_keywords": [
        "Vision Bridge Transformer (ViBT)",
        "Brownian Bridge Models",
        "diffusion models",
        "data-to-data translation",
        "variance-stabilized velocity-matching objective"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-11-28T09:03:39.000Z",
    "title": "Vision Bridge Transformer at Scale",
    "summary": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/TtAxSiJXuWb3AASpinIdd.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22663",
      "authors": [
        {
          "_id": "692cff484397b1ec214f676e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f676f",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6770",
          "name": "Hongyu Li",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6771",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6772",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6773",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6774",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6775",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6776",
          "name": "Ying Luo",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6777",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6778",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f6779",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "692cff484397b1ec214f677a",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T17:55:25.000Z",
      "submittedOnDailyAt": "2025-12-01T00:57:45.124Z",
      "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
      "upvotes": 16,
      "discussionId": "692cff484397b1ec214f677b",
      "projectPage": "https://zhengdian1.github.io/AIA-project/",
      "githubRepo": "https://github.com/zhengdian1/AIA",
      "ai_summary": "The proposed Attention Interaction Alignment (AIA) loss improves cross-modal attention and performance in unified multimodal models for image generation and understanding without decoupling.",
      "ai_keywords": [
        "multimodal models",
        "image generation",
        "understanding",
        "task-specific multimodal interaction",
        "cross-modal attention",
        "Attention Interaction Alignment (AIA) loss",
        "SFT",
        "post-training stage",
        "Qwen-VL",
        "HunyuanImage",
        "Emu3",
        "Janus-Pro"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-11-27T12:55:25.000Z",
    "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
    "summary": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.18822",
      "authors": [
        {
          "_id": "692d00ed4397b1ec214f677d",
          "name": "Zhennan Chen",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f677e",
          "name": "Junwei Zhu",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f677f",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6780",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6781",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6782",
          "name": "Hanzhen Zhao",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6783",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6784",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "692d00ed4397b1ec214f6785",
          "name": "Ying Tai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T06:55:49.000Z",
      "submittedOnDailyAt": "2025-12-01T00:32:27.778Z",
      "title": "DiP: Taming Diffusion Models in Pixel Space",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10times faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256times256.",
      "upvotes": 12,
      "discussionId": "692d00ed4397b1ec214f6786",
      "ai_summary": "DiP, a pixel space diffusion framework, combines a Diffusion Transformer and a Patch Detailer Head to achieve computational efficiency and high-quality image generation without using VAEs.",
      "ai_keywords": [
        "diffusion models",
        "latent diffusion models (LDMs)",
        "pixel space models",
        "variational autoencoders (VAEs)",
        "diffusion transformer (DiT)",
        "patch detailer head",
        "FID score"
      ]
    },
    "publishedAt": "2025-11-24T01:55:49.000Z",
    "title": "DiP: Taming Diffusion Models in Pixel Space",
    "summary": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10times faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256times256.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18822.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.23319",
      "authors": [
        {
          "_id": "692d0cae4397b1ec214f67d1",
          "name": "Xiang Hu",
          "hidden": false
        },
        {
          "_id": "692d0cae4397b1ec214f67d2",
          "name": "Zhanchao Zhou",
          "hidden": false
        },
        {
          "_id": "692d0cae4397b1ec214f67d3",
          "name": "Ruiqi Liang",
          "hidden": false
        },
        {
          "_id": "692d0cae4397b1ec214f67d4",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "692d0cae4397b1ec214f67d5",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "692d0cae4397b1ec214f67d6",
          "name": "Jianguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-28T16:17:53.000Z",
      "submittedOnDailyAt": "2025-12-01T01:13:38.539Z",
      "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "671b3d82101c0d9f78749256",
        "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
        "isPro": false,
        "fullname": "Zhanchao Zhou",
        "user": "Zcchill",
        "type": "user"
      },
      "summary": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: sparsity, random-access flexibility, and length generalization. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
      "upvotes": 9,
      "discussionId": "692d0cae4397b1ec214f67d7",
      "ai_summary": "Hierarchical Sparse Attention (HSA) is integrated into Transformers to efficiently handle ultra-long contexts, achieving high accuracy on in-context retrieval tasks.",
      "ai_keywords": [
        "Hierarchical Sparse Attention (HSA)",
        "Transformers",
        "ultra-long context modeling",
        "sparsity",
        "random-access flexibility",
        "length generalization",
        "MoE model"
      ]
    },
    "publishedAt": "2025-11-28T11:17:53.000Z",
    "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
    "summary": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: sparsity, random-access flexibility, and length generalization. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23319.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671b3d82101c0d9f78749256",
      "avatarUrl": "/avatars/902c1006280c6552885b0b946cd2bb16.svg",
      "fullname": "Zhanchao Zhou",
      "name": "Zcchill",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22677",
      "authors": [
        {
          "_id": "692d12574397b1ec214f6850",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6851",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6852",
          "name": "David Liu",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6853",
          "name": "Ruoyi Du",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6854",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6855",
          "name": "Qilong Wu",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6856",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6857",
          "name": "Sihan Cao",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6858",
          "name": "Shifeng Zhang",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f6859",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "692d12574397b1ec214f685a",
          "name": "Steven Hoi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T18:24:28.000Z",
      "submittedOnDailyAt": "2025-12-01T01:29:35.579Z",
      "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
      "submittedOnDailyBy": {
        "_id": "646f1bef075e11ca78da3bb7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
        "isPro": false,
        "fullname": "Dongyang Liu (Chris Liu)",
        "user": "Cxxs",
        "type": "user"
      },
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "upvotes": 8,
      "discussionId": "692d12574397b1ec214f685b",
      "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/",
      "githubRepo": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "ai_keywords": [
        "diffusion model distillation",
        "Distribution Matching Distillation (DMD)",
        "CFG Augmentation (CA)",
        "text-to-image generation",
        "configuration (CFG)",
        "distribution matching",
        "noise schedules",
        "Z-Image"
      ],
      "githubStars": 2971,
      "organization": {
        "_id": "6925b20fed452d1567c012d3",
        "name": "Tongyi-MAI",
        "fullname": "Tongyi-MAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
      }
    },
    "publishedAt": "2025-11-27T13:24:28.000Z",
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646f1bef075e11ca78da3bb7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
      "fullname": "Dongyang Liu (Chris Liu)",
      "name": "Cxxs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "organization": {
      "_id": "6925b20fed452d1567c012d3",
      "name": "Tongyi-MAI",
      "fullname": "Tongyi-MAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22475",
      "authors": [
        {
          "_id": "692d299e4397b1ec214f691e",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "692d299e4397b1ec214f691f",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "692d299e4397b1ec214f6920",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "692d299e4397b1ec214f6921",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "692d299e4397b1ec214f6922",
          "name": "Haoqi Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/uc23yr5kEb1qqOWcBU5L7.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/MkfCAMwjnNhnbUbKKZ_Fs.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/1oSfGtXDH38xBlwAu9ePx.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/TaRPCQ3TALjIZAjNya8uT.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/daHOcFerXxpRywo10ktxq.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/a2-ziDK-yiFI7yS9KGtu_.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/SryxcuiU-3xj5BSrbTJH3.png"
      ],
      "publishedAt": "2025-11-27T14:04:08.000Z",
      "submittedOnDailyAt": "2025-12-01T03:52:14.845Z",
      "title": "Adversarial Flow Models",
      "submittedOnDailyBy": {
        "_id": "645863f7dc18eb1a9b5d29df",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg",
        "isPro": false,
        "fullname": "Peter Lin",
        "user": "PeterL1n",
        "type": "user"
      },
      "summary": "We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.",
      "upvotes": 8,
      "discussionId": "692d299e4397b1ec214f6923",
      "ai_summary": "Adversarial flow models unify adversarial and flow-based generative models, offering stable training, efficient generation, and high performance on image datasets.",
      "ai_keywords": [
        "adversarial flow models",
        "generative models",
        "adversarial models",
        "flow models",
        "adversarial training",
        "generator",
        "noise-to-data mapping",
        "optimal transport",
        "flow-matching models",
        "consistency-based methods",
        "one-step or few-step generation",
        "probability flow",
        "ImageNet-256px",
        "FID",
        "end-to-end training",
        "depth repetition",
        "forward pass"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-27T09:04:08.000Z",
    "title": "Adversarial Flow Models",
    "summary": "We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/uc23yr5kEb1qqOWcBU5L7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/MkfCAMwjnNhnbUbKKZ_Fs.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/1oSfGtXDH38xBlwAu9ePx.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/TaRPCQ3TALjIZAjNya8uT.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/daHOcFerXxpRywo10ktxq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/a2-ziDK-yiFI7yS9KGtu_.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645863f7dc18eb1a9b5d29df/SryxcuiU-3xj5BSrbTJH3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22475.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645863f7dc18eb1a9b5d29df",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg",
      "fullname": "Peter Lin",
      "name": "PeterL1n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 51
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22815",
      "authors": [
        {
          "_id": "692d10e04397b1ec214f6835",
          "name": "Yu-Cheng Chou",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f6836",
          "name": "Xingrui Wang",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f6837",
          "name": "Yitong Li",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f6838",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f6839",
          "name": "Hanting Liu",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f683a",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f683b",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "692d10e04397b1ec214f683c",
          "name": "Junfei Xiao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4Tma8QScvmFvb5RVRp-gK.mp4"
      ],
      "publishedAt": "2025-11-28T00:27:46.000Z",
      "submittedOnDailyAt": "2025-12-01T01:23:31.270Z",
      "title": "Captain Safari: A World Engine",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
      "upvotes": 5,
      "discussionId": "692d10e14397b1ec214f683d",
      "projectPage": "https://johnson111788.github.io/open-safari/",
      "ai_summary": "Captain Safari, a pose-conditioned world engine using a dynamic local memory and retriever for pose-aligned world tokens, generates high-quality, 3D-consistent long videos with accurate camera maneuvers, outperforming existing methods across quality, consistency, and trajectory following.",
      "ai_keywords": [
        "pose-conditioned world engine",
        "world memory",
        "dynamic local memory",
        "retriever",
        "pose-aligned world tokens",
        "video generation",
        "FPV dataset",
        "geometric validation",
        "kinematic validation",
        "MEt3R",
        "AUC@30",
        "FVD",
        "human study"
      ]
    },
    "publishedAt": "2025-11-27T19:27:46.000Z",
    "title": "Captain Safari: A World Engine",
    "summary": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4Tma8QScvmFvb5RVRp-gK.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22815.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 172
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22787",
      "authors": [
        {
          "_id": "692d20864397b1ec214f68e7",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68e8",
          "name": "Junyeong Park",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68e9",
          "name": "Na Min An",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68ea",
          "name": "Junseong Kim",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68eb",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68ec",
          "name": "Jiho Jin",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68ed",
          "name": "Julia Kruk",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68ee",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68ef",
          "name": "Srikant Panda",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68f0",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68f1",
          "name": "Hyunjung Shim",
          "hidden": false
        },
        {
          "_id": "692d20864397b1ec214f68f2",
          "name": "Alice Oh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T22:23:08.000Z",
      "submittedOnDailyAt": "2025-12-01T02:30:27.546Z",
      "title": "World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.",
      "upvotes": 5,
      "discussionId": "692d20864397b1ec214f68f3",
      "ai_summary": "LVLMs struggle with preserving cultural identities in mixed visual scenes, but supervised fine-tuning with culture mixing datasets improves their performance.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "Visual Question Answering",
        "VQA",
        "diffusion-generated",
        "culture mixing",
        "food-only",
        "food+food",
        "food+background",
        "food+food+background",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-11-27T17:23:08.000Z",
    "title": "World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models",
    "summary": "In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22787.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22173",
      "authors": [
        {
          "_id": "692d1e1d4397b1ec214f68b8",
          "name": "Young-Jun Lee",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68b9",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68ba",
          "name": "Byung-Kwan Lee",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68bb",
          "name": "Minkyeong Moon",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68bc",
          "name": "Yechan Hwang",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68bd",
          "name": "Jong Myoung Kim",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68be",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68bf",
          "name": "Sean Welleck",
          "hidden": false
        },
        {
          "_id": "692d1e1d4397b1ec214f68c0",
          "name": "Ho-Jin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T07:20:52.000Z",
      "submittedOnDailyAt": "2025-12-01T02:55:33.725Z",
      "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
      "submittedOnDailyBy": {
        "_id": "6469949654873f0043b09c22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
        "isPro": true,
        "fullname": "Seungone Kim",
        "user": "seungone",
        "type": "user"
      },
      "summary": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
      "upvotes": 5,
      "discussionId": "692d1e1d4397b1ec214f68c1",
      "projectPage": "https://passing2961.github.io/refinebench-page/",
      "githubRepo": "https://github.com/RefineBench/refinebench-eval",
      "ai_summary": "RefineBench evaluates language models' ability to self-refine and improve their responses through self-reflection and guided refinement across various challenging tasks, highlighting the need for significant improvements in self-refinement capabilities.",
      "ai_keywords": [
        "RefineBench",
        "chains-of-thought",
        "self-reflection",
        "self-refinement",
        "guided refinement",
        "natural language feedback"
      ],
      "organization": {
        "_id": "691d9a1012cc4d473e1c862f",
        "name": "CarnegieMellonU",
        "fullname": "Carnegie Mellon University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
      }
    },
    "publishedAt": "2025-11-27T02:20:52.000Z",
    "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
    "summary": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6469949654873f0043b09c22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
      "fullname": "Seungone Kim",
      "name": "seungone",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "organization": {
      "_id": "691d9a1012cc4d473e1c862f",
      "name": "CarnegieMellonU",
      "fullname": "Carnegie Mellon University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22134",
      "authors": [
        {
          "_id": "692d0e944397b1ec214f67f8",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67f9",
          "name": "Zhuoyang Liu",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67fa",
          "name": "Jiaming Liu",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67fb",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67fc",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67fd",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67fe",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f67ff",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f6800",
          "name": "Shanghang Zhang",
          "hidden": false
        },
        {
          "_id": "692d0e944397b1ec214f6801",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T06:03:53.000Z",
      "submittedOnDailyAt": "2025-12-01T01:13:33.835Z",
      "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
      "submittedOnDailyBy": {
        "_id": "64b0a5037a475fba70a7260d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
        "isPro": false,
        "fullname": "Zhen Fang",
        "user": "CostaliyA",
        "type": "user"
      },
      "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
      "upvotes": 5,
      "discussionId": "692d0e954397b1ec214f6802",
      "projectPage": "https://costaliya.github.io/DualVLA/",
      "ai_summary": "DualVLA enhances action performance while preserving reasoning capability in Vision-Language-Action models through dual-layer data pruning and dual-teacher adaptive distillation, achieving high success rates in multimodal benchmarks.",
      "ai_keywords": [
        "Vision-Language-Action (VLA)",
        "DualVLA",
        "dual-layer data pruning",
        "dual-teacher adaptive distillation",
        "VLA Score",
        "reasoning",
        "intention",
        "action",
        "alignment",
        "SimplerEnv",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-11-27T01:03:53.000Z",
    "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
    "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22134.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b0a5037a475fba70a7260d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
      "fullname": "Zhen Fang",
      "name": "CostaliyA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22281",
      "authors": [
        {
          "_id": "692d0f6f4397b1ec214f681c",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "692d0f6f4397b1ec214f681d",
          "name": "Shunqi Mao",
          "hidden": false
        },
        {
          "_id": "692d0f6f4397b1ec214f681e",
          "name": "Zhuonan Liang",
          "hidden": false
        },
        {
          "_id": "692d0f6f4397b1ec214f681f",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "692d0f6f4397b1ec214f6820",
          "name": "Weidong Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T10:04:44.000Z",
      "submittedOnDailyAt": "2025-12-01T01:16:01.722Z",
      "title": "The Collapse of Patches",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .",
      "upvotes": 3,
      "discussionId": "692d0f6f4397b1ec214f6821",
      "githubRepo": "https://github.com/wguo-ai/CoP",
      "ai_summary": "Patch collapse, a phenomenon where observing certain image patches reduces uncertainty in others, improves masked image modeling and promotes vision efficiency through selective patch exposure.",
      "ai_keywords": [
        "autoencoder",
        "PageRank score",
        "patch collapse",
        "autoregressive image generation",
        "MAR",
        "Vision Transformers",
        "image classification",
        "masked image modeling"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-11-27T05:04:44.000Z",
    "title": "The Collapse of Patches",
    "summary": "Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22281.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 172
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22055",
      "authors": [
        {
          "_id": "692d1b3c4397b1ec214f68a0",
          "name": "Jing Hao",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a1",
          "name": "Yuci Liang",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a2",
          "name": "Lizhuo Lin",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a3",
          "name": "Yuxuan Fan",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a4",
          "name": "Wenkai Zhou",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a5",
          "name": "Kaixin Guo",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a6",
          "name": "Zanting Ye",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a7",
          "name": "Yanpeng Sun",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a8",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68a9",
          "name": "Yanqi Yang",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68aa",
          "name": "Qiankun Li",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68ab",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68ac",
          "name": "James Kit-Hon Tsoi",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68ad",
          "name": "Linlin Shen",
          "hidden": false
        },
        {
          "_id": "692d1b3c4397b1ec214f68ae",
          "name": "Kuo Feng Hung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T03:21:20.000Z",
      "submittedOnDailyAt": "2025-12-01T02:08:03.867Z",
      "title": "OralGPT-Omni: A Versatile Dental Multimodal Large Language Model",
      "submittedOnDailyBy": {
        "_id": "6433a530b0dee98d17844636",
        "avatarUrl": "/avatars/3cca4504f482dfb5e3fcfe9ee9fbaf51.svg",
        "isPro": false,
        "fullname": "Jing Hao",
        "user": "Bryceee",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.",
      "upvotes": 3,
      "discussionId": "692d1b3c4397b1ec214f68af",
      "ai_summary": "OralGPT-Omni, a dental-specialized multimodal large language model, enhances dental image analysis through TRACE-CoT reasoning supervision and achieves superior performance on MMOral-Uni benchmark compared to GPT-5.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "dental-specialized MLLM",
        "TRACE-CoT",
        "chain-of-thought dataset",
        "four-stage training paradigm",
        "unified multimodal benchmark",
        "dental image analysis",
        "MMOral-Uni",
        "MMOral-OPG",
        "intelligent dentistry"
      ],
      "organization": {
        "_id": "68d1f768cb23cd33ffb519a3",
        "name": "OralGPT",
        "fullname": "OralGPT-Series",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6433a530b0dee98d17844636/Ww8y-_8xVHkANz3DwUl1k.png"
      }
    },
    "publishedAt": "2025-11-26T22:21:20.000Z",
    "title": "OralGPT-Omni: A Versatile Dental Multimodal Large Language Model",
    "summary": "Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6433a530b0dee98d17844636",
      "avatarUrl": "/avatars/3cca4504f482dfb5e3fcfe9ee9fbaf51.svg",
      "fullname": "Jing Hao",
      "name": "Bryceee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "68d1f768cb23cd33ffb519a3",
      "name": "OralGPT",
      "fullname": "OralGPT-Series",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6433a530b0dee98d17844636/Ww8y-_8xVHkANz3DwUl1k.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22688",
      "authors": [
        {
          "_id": "692d0ac14397b1ec214f67b9",
          "name": "Amirmojtaba Sabour",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67ba",
          "name": "Michael S. Albergo",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67bb",
          "name": "Carles Domingo-Enrich",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67bc",
          "name": "Nicholas M. Boffi",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67bd",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67be",
          "name": "Karsten Kreis",
          "hidden": false
        },
        {
          "_id": "692d0ac14397b1ec214f67bf",
          "name": "Eric Vanden-Eijnden",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/v4LaQJJywUY72czXHHlAV.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/xGWJLesdUtQKHyfpGBNWx.jpeg"
      ],
      "publishedAt": "2025-11-27T18:44:12.000Z",
      "submittedOnDailyAt": "2025-12-01T00:59:28.500Z",
      "title": "Test-time scaling of diffusions with flow maps",
      "submittedOnDailyBy": {
        "_id": "656015f28138827138c70858",
        "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
        "isPro": false,
        "fullname": "Amirmojtaba Sabour",
        "user": "amsabour",
        "type": "user"
      },
      "summary": "A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.",
      "upvotes": 2,
      "discussionId": "692d0ac14397b1ec214f67c0",
      "projectPage": "https://flow-map-trajectory-tilting.github.io/",
      "ai_summary": "The proposed Flow Map Trajectory Tilting (FMTT) algorithm improves diffusion models at test-time by leveraging flow maps to better align with user-specified rewards, enabling more effective sampling and image editing.",
      "ai_keywords": [
        "diffusion models",
        "gradient of the reward",
        "flow map",
        "velocity field",
        "Flow Map Trajectory Tilting (FMTT)",
        "importance weighting",
        "reward-tilted distribution",
        "vision language models"
      ]
    },
    "publishedAt": "2025-11-27T13:44:12.000Z",
    "title": "Test-time scaling of diffusions with flow maps",
    "summary": "A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/v4LaQJJywUY72czXHHlAV.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/xGWJLesdUtQKHyfpGBNWx.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22688.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656015f28138827138c70858",
      "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
      "fullname": "Amirmojtaba Sabour",
      "name": "amsabour",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.21750",
      "authors": [
        {
          "_id": "692d0d2b4397b1ec214f67d9",
          "name": "Di Feng",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67da",
          "name": "Kaixin Ma",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67db",
          "name": "Feng Nan",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67dc",
          "name": "Haofeng Chen",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67dd",
          "name": "Bohan Zhai",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67de",
          "name": "David Griffiths",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67df",
          "name": "Mingfei Gao",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67e0",
          "name": "Zhe Gan",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67e1",
          "name": "Eshan Verma",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67e2",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67e3",
          "name": "Zhifeng Chen",
          "hidden": false
        },
        {
          "_id": "692d0d2b4397b1ec214f67e4",
          "name": "Afshin Dehghan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-23T16:53:16.000Z",
      "submittedOnDailyAt": "2025-12-01T01:06:30.013Z",
      "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
      "upvotes": 2,
      "discussionId": "692d0d2b4397b1ec214f67e5",
      "ai_summary": "A benchmark evaluates schema-grounded information extraction and reasoning over visual inputs for multimodal large language models, revealing gaps and guiding future improvements.",
      "ai_keywords": [
        "multimodal large language models",
        "schema-grounded information extraction",
        "reasoning over visual inputs",
        "SO-Bench",
        "JSON schemas",
        "image-schema pairs",
        "structured generation",
        "multimodal structured reasoning"
      ],
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-11-23T11:53:16.000Z",
    "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
    "summary": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21750.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 172
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22176",
      "authors": [
        {
          "_id": "692d08524397b1ec214f67ab",
          "name": "Lukas Struppek",
          "hidden": false
        },
        {
          "_id": "692d08524397b1ec214f67ac",
          "name": "Dominik Hintersdorf",
          "hidden": false
        },
        {
          "_id": "692d08524397b1ec214f67ad",
          "name": "Hannah Struppek",
          "hidden": false
        },
        {
          "_id": "692d08524397b1ec214f67ae",
          "name": "Daniel Neider",
          "hidden": false
        },
        {
          "_id": "692d08524397b1ec214f67af",
          "name": "Kristian Kersting",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T07:31:52.000Z",
      "submittedOnDailyAt": "2025-12-01T00:47:11.520Z",
      "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
      "submittedOnDailyBy": {
        "_id": "6305ca82d37ce67e0e48aadb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg",
        "isPro": false,
        "fullname": "Lukas Struppek",
        "user": "lukas-struppek",
        "type": "user"
      },
      "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
      "upvotes": 1,
      "discussionId": "692d08524397b1ec214f67b0",
      "ai_summary": "F-CoT, an input-centric approach inspired by cognitive psychology, reduces token usage in large language models by structuring context and focusing reasoning, maintaining accuracy on arithmetic problems.",
      "ai_keywords": [
        "Focused Chain-of-Thought (F-CoT)",
        "cognitive psychology",
        "chain-of-thought traces",
        "reinforcement learning",
        "supervised fine-tuning",
        "attention",
        "reasoning paths"
      ]
    },
    "publishedAt": "2025-11-27T02:31:52.000Z",
    "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
    "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22176.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6305ca82d37ce67e0e48aadb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg",
      "fullname": "Lukas Struppek",
      "name": "lukas-struppek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.13344",
      "authors": [
        {
          "_id": "692d316c4397b1ec214f694e",
          "name": "Ori Meiraz",
          "hidden": false
        },
        {
          "_id": "692d316c4397b1ec214f694f",
          "name": "Sharon Shalev",
          "hidden": false
        },
        {
          "_id": "692d316c4397b1ec214f6950",
          "name": "Avishai Weizman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-17T13:11:11.000Z",
      "submittedOnDailyAt": "2025-12-01T03:41:24.268Z",
      "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
      "submittedOnDailyBy": {
        "_id": "652ac7194267f8c8024dec1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
        "isPro": false,
        "fullname": "avishai",
        "user": "avishai111",
        "type": "user"
      },
      "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
      "upvotes": 1,
      "discussionId": "692d316d4397b1ec214f6951",
      "ai_summary": "A Mixture-of-Experts framework with adaptive routing among multiple YOLOv9-T experts improves object detection performance by achieving higher mAP and AR.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "adaptive routing",
        "YOLOv9-T",
        "dynamic feature specialization",
        "mean Average Precision",
        "Average Recall"
      ]
    },
    "publishedAt": "2025-11-17T08:11:11.000Z",
    "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ac7194267f8c8024dec1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
      "fullname": "avishai",
      "name": "avishai111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22805",
      "authors": [
        {
          "_id": "692d2a4e4397b1ec214f6925",
          "name": "Yiming Chen",
          "hidden": false
        },
        {
          "_id": "692d2a4e4397b1ec214f6926",
          "name": "Junlin Han",
          "hidden": false
        },
        {
          "_id": "692d2a4e4397b1ec214f6927",
          "name": "Tianyi Bai",
          "hidden": false
        },
        {
          "_id": "692d2a4e4397b1ec214f6928",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "692d2a4e4397b1ec214f6929",
          "name": "Filippos Kokkinos",
          "hidden": false
        },
        {
          "_id": "692d2a4e4397b1ec214f692a",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T23:30:24.000Z",
      "submittedOnDailyAt": "2025-12-01T03:11:20.023Z",
      "title": "From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images",
      "submittedOnDailyBy": {
        "_id": "636e6ee287545ca5a136b4c3",
        "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
        "isPro": false,
        "fullname": "Junlin Han",
        "user": "Junlinh",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.",
      "upvotes": 0,
      "discussionId": "692d2a4e4397b1ec214f692b",
      "ai_summary": "CogIP-Bench evaluates MLLMs on image cognitive properties, revealing a gap that post-training can bridge, enhancing human-like perception and improving creative tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "CogIP-Bench",
        "image cognitive properties",
        "post-training phase",
        "human perception",
        "cognitive alignment",
        "image generation pipeline"
      ]
    },
    "publishedAt": "2025-11-27T18:30:24.000Z",
    "title": "From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images",
    "summary": "While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636e6ee287545ca5a136b4c3",
      "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
      "fullname": "Junlin Han",
      "name": "Junlinh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22265",
      "authors": [
        {
          "_id": "692d07304397b1ec214f679f",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a0",
          "name": "Lixu Wang",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a1",
          "name": "Jiaqi Wu",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a2",
          "name": "Jin Song",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a3",
          "name": "Simin Chen",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a4",
          "name": "Zehua Wang",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a5",
          "name": "Zijian Tian",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a6",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a7",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "692d07304397b1ec214f67a8",
          "name": "Xiaoxiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T09:42:22.000Z",
      "submittedOnDailyAt": "2025-12-01T00:44:36.289Z",
      "title": "FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning",
      "submittedOnDailyBy": {
        "_id": "668bb3b14c25c09b01815a55",
        "avatarUrl": "/avatars/5d46301dd5d7641e3da05b0ad560efee.svg",
        "isPro": false,
        "fullname": "Yuan Yao",
        "user": "yyyaoyuan",
        "type": "user"
      },
      "summary": "Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.",
      "upvotes": 0,
      "discussionId": "692d07304397b1ec214f67a9",
      "ai_summary": "FedRE, a federated learning framework, uses entangled representations and labels to enhance performance, privacy, and reduce communication overhead in model-heterogeneous environments.",
      "ai_keywords": [
        "Federated learning",
        "model-heterogeneous FL",
        "entangled representation",
        "one-hot label encodings",
        "entangled-label encoding",
        "global classifier",
        "overconfidence",
        "decision boundaries",
        "representation inversion attacks",
        "communication overhead"
      ]
    },
    "publishedAt": "2025-11-27T04:42:22.000Z",
    "title": "FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning",
    "summary": "Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668bb3b14c25c09b01815a55",
      "avatarUrl": "/avatars/5d46301dd5d7641e3da05b0ad560efee.svg",
      "fullname": "Yuan Yao",
      "name": "yyyaoyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.13944",
      "authors": [
        {
          "_id": "692d30c74397b1ec214f6948",
          "name": "Noam Glazner",
          "hidden": false
        },
        {
          "_id": "692d30c74397b1ec214f6949",
          "name": "Noam Tsfaty",
          "hidden": false
        },
        {
          "_id": "692d30c74397b1ec214f694a",
          "name": "Sharon Shalev",
          "hidden": false
        },
        {
          "_id": "692d30c74397b1ec214f694b",
          "name": "Avishai Weizman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-17T21:57:46.000Z",
      "submittedOnDailyAt": "2025-12-01T03:40:23.952Z",
      "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
      "submittedOnDailyBy": {
        "_id": "652ac7194267f8c8024dec1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
        "isPro": false,
        "fullname": "avishai",
        "user": "avishai111",
        "type": "user"
      },
      "summary": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.",
      "upvotes": 0,
      "discussionId": "692d30c74397b1ec214f694c",
      "ai_summary": "A cluster-based frame selection strategy groups visually similar frames to create more representative and balanced dataset partitions, reducing information leakage in video-derived frames datasets.",
      "ai_keywords": [
        "cluster-based frame selection",
        "information leakage",
        "video-derived frames datasets"
      ]
    },
    "publishedAt": "2025-11-17T16:57:46.000Z",
    "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
    "summary": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ac7194267f8c8024dec1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
      "fullname": "avishai",
      "name": "avishai111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.13276",
      "authors": [
        {
          "_id": "692d31aa4397b1ec214f6953",
          "name": "Noam Tsfaty",
          "hidden": false
        },
        {
          "_id": "692d31aa4397b1ec214f6954",
          "name": "Avishai Weizman",
          "hidden": false
        },
        {
          "_id": "692d31aa4397b1ec214f6955",
          "name": "Liav Cohen",
          "hidden": false
        },
        {
          "_id": "692d31aa4397b1ec214f6956",
          "name": "Moshe Tshuva",
          "hidden": false
        },
        {
          "_id": "692d31aa4397b1ec214f6957",
          "name": "Yehudit Aperstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-17T11:47:28.000Z",
      "submittedOnDailyAt": "2025-12-01T03:42:00.426Z",
      "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
      "submittedOnDailyBy": {
        "_id": "652ac7194267f8c8024dec1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
        "isPro": false,
        "fullname": "avishai",
        "user": "avishai111",
        "type": "user"
      },
      "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
      "upvotes": 0,
      "discussionId": "692d31aa4397b1ec214f6958",
      "ai_summary": "A dual-backbone framework using convolutional and transformer representations with top-k pooling detects anomalies in surveillance videos with 90.7% AUC on UCF-Crime.",
      "ai_keywords": [
        "convolutional",
        "transformer",
        "top-k pooling"
      ]
    },
    "publishedAt": "2025-11-17T06:47:28.000Z",
    "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
    "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ac7194267f8c8024dec1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ac7194267f8c8024dec1a/kh2oFpYkzrLUy43cOQeqK.jpeg",
      "fullname": "avishai",
      "name": "avishai111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]