[
  {
    "paper": {
      "id": "2506.15154",
      "authors": [
        {
          "_id": "685393f499bf39f9665c79db",
          "name": "Anuradha Chopra",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dc",
          "name": "Abhinaba Roy",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dd",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
      ],
      "publishedAt": "2025-06-18T05:51:36.000Z",
      "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
      "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
      "upvotes": 2,
      "discussionId": "685393f599bf39f9665c79de",
      "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
      "ai_keywords": [
        "multi-task music captioning",
        "SonicVerse",
        "caption generation",
        "key detection",
        "vocals detection",
        "projection-based architecture",
        "language tokens",
        "auxiliary heads",
        "time-informed descriptions",
        "large-language model",
        "MusicBench dataset",
        "MIRFLEX",
        "music feature extractor"
      ]
    },
    "publishedAt": "2025-06-18T01:51:36.000Z",
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14837",
      "authors": [
        {
          "_id": "6854ea7a7bc8d012d4ca998d",
          "name": "Chengzhi Xu",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998e",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998f",
          "name": "Lai Wei",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9990",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9991",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:10:16.000Z",
      "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
      "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
      "upvotes": 0,
      "discussionId": "6854ea7a7bc8d012d4ca9992",
      "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "visual understanding",
        "code translation",
        "structured instruction",
        "description instruction",
        "difference instruction",
        "language representations",
        "initial code generation",
        "iterative refinement",
        "Qwen2-VL",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-06-15T10:10:16.000Z",
    "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
    "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]