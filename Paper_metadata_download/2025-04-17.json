[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "name": "Chenrui Fan",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "name": "Kwesi Cobbina",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "name": "Shweta Bhardwaj",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
      "upvotes": 16,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "name": "Lingen Li",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
      "upvotes": 15,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "name": "Xingxing Zhang",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T Technical Report",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
      "upvotes": 8,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "tkoniy",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "name": "Qilong Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "name": "Peiqi Yuan",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "name": "Yitao Zheng",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "name": "Runzhong Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "name": "Haotian Liu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "name": "Bo Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "tkoniy",
        "type": "user"
      },
      "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
      "upvotes": 7,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "tkoniy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "name": "Ram Mohan Rao Kadiyala",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "name": "Siddartha Pullakhandam",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "name": "Drishti Sharma",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "name": "Jebish Purbey",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "name": "Ashay Srivastava",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "name": "Subhasya TippaReddy",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "name": "Arvind Reddy Bobbili",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "name": "Modabbir Adeeb",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "name": "Srinadh Vura",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "Robust and Fine-Grained Detection of AI Generated Texts",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "name": "Yunzhong Hou",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "name": "Zhenchang Xing",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "name": "Liang Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
      "upvotes": 1,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  }
]