[
    {
        "paper": {
            "id": "2411.17465",
            "authors": [
                {
                    "_id": "67468ee8e36c38e099c5c0d1",
                    "name": "Kevin Qinghong Lin",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d2",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d3",
                    "name": "Difei Gao",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d4",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d5",
                    "name": "Shiwei Wu",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d6",
                    "name": "Zechen Bai",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d7",
                    "name": "Weixian Lei",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d8",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67468ee8e36c38e099c5c0d9",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T14:29:47.000Z",
            "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
            "summary": "Building Graphical User Interface (GUI) assistants holds significant promise\nfor enhancing human workflow productivity. While most agents are\nlanguage-based, relying on closed-source API with text-rich meta-information\n(e.g., HTML or accessibility tree), they show limitations in perceiving UI\nvisuals as humans do, highlighting the need for GUI visual agents. In this\nwork, we develop a vision-language-action model in digital world, namely\nShowUI, which features the following innovations: (i) UI-Guided Visual Token\nSelection to reduce computational costs by formulating screenshots as an UI\nconnected graph, adaptively identifying their redundant relationship and serve\nas the criteria for token selection during self-attention blocks; (ii)\nInterleaved Vision-Language-Action Streaming that flexibly unifies diverse\nneeds within GUI tasks, enabling effective management of visual-action history\nin navigation or pairing multi-turn query-action sequences per screenshot to\nenhance training efficiency; (iii) Small-scale High-quality GUI\nInstruction-following Datasets by careful data curation and employing a\nresampling strategy to address significant data type imbalances. With above\ncomponents, ShowUI, a lightweight 2B model using 256K data, achieves a strong\n75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection\nfurther reduces 33% of redundant visual tokens during training and speeds up\nthe performance by 1.4x. Navigation experiments across web Mind2Web, mobile\nAITW, and online MiniWob environments further underscore the effectiveness and\npotential of our model in advancing GUI visual agents. The models are available\nat https://github.com/showlab/ShowUI.",
            "upvotes": 43,
            "discussionId": "67468eede36c38e099c5c2c5"
        },
        "publishedAt": "2024-11-27T01:49:11.399Z",
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/MHiPEEwE4IR6Fh0--KjDZ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/-kVzsyDwR2X-MW-6jiZ10.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17465.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "fullname": "Qinghong Lin",
            "name": "KevinQHLin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        }
    },
    {
        "paper": {
            "id": "2411.16819",
            "authors": [
                {
                    "_id": "6746c37efb7a1de1dca06a6d",
                    "name": "Noam Rotstein",
                    "hidden": false
                },
                {
                    "_id": "6746c37efb7a1de1dca06a6e",
                    "name": "Gal Yona",
                    "hidden": false
                },
                {
                    "_id": "6746c37efb7a1de1dca06a6f",
                    "name": "Daniel Silver",
                    "hidden": false
                },
                {
                    "_id": "6746c37efb7a1de1dca06a70",
                    "name": "Roy Velich",
                    "hidden": false
                },
                {
                    "_id": "6746c37efb7a1de1dca06a71",
                    "name": "David Bensa√Ød",
                    "hidden": false
                },
                {
                    "_id": "6746c37efb7a1de1dca06a72",
                    "name": "Ron Kimmel",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T16:41:45.000Z",
            "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
            "summary": "Recent advances in image editing, driven by image diffusion models, have\nshown remarkable progress. However, significant challenges remain, as these\nmodels often struggle to follow complex edit instructions accurately and\nfrequently compromise fidelity by altering key elements of the original image.\nSimultaneously, video generation has made remarkable strides, with models that\neffectively function as consistent and continuous world simulators. In this\npaper, we propose merging these two fields by utilizing image-to-video models\nfor image editing. We reformulate image editing as a temporal process, using\npretrained video models to create smooth transitions from the original image to\nthe desired edit. This approach traverses the image manifold continuously,\nensuring consistent edits while preserving the original image's key aspects.\nOur approach achieves state-of-the-art results on text-based image editing,\ndemonstrating significant improvements in both edit accuracy and image\npreservation.",
            "upvotes": 21,
            "discussionId": "6746c381fb7a1de1dca06b71"
        },
        "publishedAt": "2024-11-27T05:38:35.508Z",
        "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16819.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
            "fullname": "noam rotstein",
            "name": "noamrot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.17116",
            "authors": [
                {
                    "_id": "67467818ee80399924ef20cc",
                    "name": "Shantanu Acharya",
                    "hidden": false
                },
                {
                    "_id": "67467818ee80399924ef20cd",
                    "name": "Fei Jia",
                    "hidden": false
                },
                {
                    "_id": "67467818ee80399924ef20ce",
                    "name": "Boris Ginsburg",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T05:10:04.000Z",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
            "upvotes": 21,
            "discussionId": "67467819ee80399924ef20f2"
        },
        "publishedAt": "2024-11-27T00:10:11.516Z",
        "title": "Star Attention: Efficient LLM Inference over Long Sequences",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17116.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/c2ecdf2d4eb8ca1c14ba427933cc38b0.svg",
            "fullname": "Shantanu Acharya",
            "name": "shantanuacharya",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.17686",
            "authors": [
                {
                    "_id": "67469c4e599a83a637c9fa74",
                    "name": "Yuhang Han",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa75",
                    "name": "Xuyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa76",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa77",
                    "name": "Donglin Wang",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa78",
                    "name": "Honggang Chen",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa79",
                    "name": "Qingsen Yan",
                    "hidden": false
                },
                {
                    "_id": "67469c4e599a83a637c9fa7a",
                    "name": "Siteng Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T18:53:51.000Z",
            "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration",
            "summary": "To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/.",
            "upvotes": 14,
            "discussionId": "67469c4f599a83a637c9fb01"
        },
        "publishedAt": "2024-11-27T02:43:53.365Z",
        "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17686.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.15296",
            "authors": [
                {
                    "_id": "67456b5485003a3689267f80",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f81",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-26T06:31:50.432Z",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f82",
                    "name": "Shukang Yin",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f83",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f84",
                    "name": "Xinyu Fang",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f85",
                    "name": "Sirui Zhao",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f86",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f87",
                    "name": "Xing Sun",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f88",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f89",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f8a",
                    "name": "Caifeng Shan",
                    "hidden": false
                },
                {
                    "_id": "67456b5485003a3689267f8b",
                    "name": "Ran He",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T18:59:54.000Z",
            "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
            "summary": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal\nLarge Language Models (MLLMs) have garnered increased attention from both\nindustry and academia. Building upon pre-trained LLMs, this family of models\nfurther develops multimodal perception and reasoning capabilities that are\nimpressive, such as writing code given a flow chart or creating stories based\non an image. In the development process, evaluation is critical since it\nprovides intuitive feedback and guidance on improving models. Distinct from the\ntraditional train-eval-test paradigm that only favors a single task like image\nclassification, the versatility of MLLMs has spurred the rise of various new\nbenchmarks and evaluation methods. In this paper, we aim to present a\ncomprehensive survey of MLLM evaluation, discussing four key aspects: 1) the\nsummarised benchmarks types divided by the evaluation capabilities, including\nfoundation capabilities, model self-analysis, and extented applications; 2) the\ntypical process of benchmark counstruction, consisting of data collection,\nannotation, and precautions; 3) the systematic evaluation manner composed of\njudge, metric, and toolkit; 4) the outlook for the next benchmark. This work\naims to offer researchers an easy grasp of how to effectively evaluate MLLMs\naccording to different needs and to inspire better evaluation methods, thereby\ndriving the progress of MLLM research.",
            "upvotes": 13,
            "discussionId": "67456b5685003a3689267ff8"
        },
        "publishedAt": "2024-11-27T02:14:18.842Z",
        "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15296.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.14740",
            "authors": [
                {
                    "_id": "674559365ac931cfc8367ddb",
                    "name": "Xin Yu",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367ddc",
                    "user": {
                        "_id": "64ad15a05d48838462e46687",
                        "avatarUrl": "/avatars/5f0d0233e99fdca3af8dca45341085be.svg",
                        "isPro": false,
                        "fullname": "Ze Yuan",
                        "user": "yuanze1024",
                        "type": "user"
                    },
                    "name": "Ze Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-26T21:20:58.452Z",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367ddd",
                    "name": "Yuan-Chen Guo",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367dde",
                    "name": "Ying-Tian Liu",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367ddf",
                    "name": "JianHui Liu",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367de0",
                    "user": {
                        "_id": "64d71083a787c9bc7b9f1238",
                        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                        "isPro": false,
                        "fullname": "YG",
                        "user": "Lp256",
                        "type": "user"
                    },
                    "name": "Yangguang Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-26T05:14:35.007Z",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367de1",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367de2",
                    "name": "Ding Liang",
                    "hidden": false
                },
                {
                    "_id": "674559365ac931cfc8367de3",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T05:22:11.000Z",
            "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
            "summary": "While high-quality texture maps are essential for realistic 3D asset\nrendering, few studies have explored learning directly in the texture space,\nespecially on large-scale datasets. In this work, we depart from the\nconventional approach of relying on pre-trained 2D diffusion models for\ntest-time optimization of 3D textures. Instead, we focus on the fundamental\nproblem of learning in the UV texture space itself. For the first time, we\ntrain a large diffusion model capable of directly generating high-resolution\ntexture maps in a feed-forward manner. To facilitate efficient learning in\nhigh-resolution UV spaces, we propose a scalable network architecture that\ninterleaves convolutions on UV maps with attention layers on point clouds.\nLeveraging this architectural design, we train a 700 million parameter\ndiffusion model that can generate UV texture maps guided by text prompts and\nsingle-view images. Once trained, our model naturally supports various extended\napplications, including text-guided texture inpainting, sparse-view texture\ncompletion, and text-driven texture synthesis. Project page is at\nhttp://cvmi-lab.github.io/TEXGen/.",
            "upvotes": 11,
            "discussionId": "6745593b5ac931cfc8367ed0"
        },
        "publishedAt": "2024-11-27T00:54:27.269Z",
        "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14740.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5f0d0233e99fdca3af8dca45341085be.svg",
            "fullname": "Ze Yuan",
            "name": "yuanze1024",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.17673",
            "authors": [
                {
                    "_id": "6746935b7f9c1728d230630d",
                    "name": "Yael Vinker",
                    "hidden": false
                },
                {
                    "_id": "6746935b7f9c1728d230630e",
                    "name": "Tamar Rott Shaham",
                    "hidden": false
                },
                {
                    "_id": "6746935b7f9c1728d230630f",
                    "name": "Kristine Zheng",
                    "hidden": false
                },
                {
                    "_id": "6746935b7f9c1728d2306310",
                    "name": "Alex Zhao",
                    "hidden": false
                },
                {
                    "_id": "6746935b7f9c1728d2306311",
                    "name": "Judith E Fan",
                    "hidden": false
                },
                {
                    "_id": "6746935b7f9c1728d2306312",
                    "name": "Antonio Torralba",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T18:32:06.000Z",
            "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
            "summary": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid\nexploration and visual communication that spans various disciplines. While\nartificial systems have driven substantial advances in content creation and\nhuman-computer interaction, capturing the dynamic and abstract nature of human\nsketching remains challenging. In this work, we introduce SketchAgent, a\nlanguage-driven, sequential sketch generation method that enables users to\ncreate, modify, and refine sketches through dynamic, conversational\ninteractions. Our approach requires no training or fine-tuning. Instead, we\nleverage the sequential nature and rich prior knowledge of off-the-shelf\nmultimodal large language models (LLMs). We present an intuitive sketching\nlanguage, introduced to the model through in-context examples, enabling it to\n\"draw\" using string-based actions. These are processed into vector graphics and\nthen rendered to create a sketch on a pixel canvas, which can be accessed again\nfor further tasks. By drawing stroke by stroke, our agent captures the\nevolving, dynamic qualities intrinsic to sketching. We demonstrate that\nSketchAgent can generate sketches from diverse prompts, engage in\ndialogue-driven drawing, and collaborate meaningfully with human users.",
            "upvotes": 10,
            "discussionId": "6746935f7f9c1728d23063c2"
        },
        "publishedAt": "2024-11-27T02:06:33.615Z",
        "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620df756dbac1855c00529d4/7dHEMc2T5mw_3EoooJZJi.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/620df756dbac1855c00529d4/wWLc3eSTgEJzxStOX19IP.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17673.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/0a5bda350f46e152baab4c8eedbb85f3.svg",
            "fullname": "Yael Vinker",
            "name": "yaelvinker",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.17467",
            "authors": [
                {
                    "_id": "6746871ae2e2e857eac6df53",
                    "name": "Xuweiyi Chen",
                    "hidden": false
                },
                {
                    "_id": "6746871ae2e2e857eac6df54",
                    "name": "Zezhou Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T18:59:57.000Z",
            "title": "Learning 3D Representations from Procedural 3D Programs",
            "summary": "Self-supervised learning has emerged as a promising approach for acquiring\ntransferable 3D representations from unlabeled 3D point clouds. Unlike 2D\nimages, which are widely accessible, acquiring 3D assets requires specialized\nexpertise or professional 3D scanning equipment, making it difficult to scale\nand raising copyright concerns. To address these challenges, we propose\nlearning 3D representations from procedural 3D programs that automatically\ngenerate 3D shapes using simple primitives and augmentations.\n  Remarkably, despite lacking semantic content, the 3D representations learned\nfrom this synthesized dataset perform on par with state-of-the-art\nrepresentations learned from semantically recognizable 3D models (e.g.,\nairplanes) across various downstream 3D tasks, including shape classification,\npart segmentation, and masked point cloud completion. Our analysis further\nsuggests that current self-supervised learning methods primarily capture\ngeometric structures rather than high-level semantics.",
            "upvotes": 7,
            "discussionId": "6746871be2e2e857eac6dfa3"
        },
        "publishedAt": "2024-11-27T01:15:00.168Z",
        "title": "Learning 3D Representations from Procedural 3D Programs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17467.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
            "fullname": "Xuweiyi Chen",
            "name": "Xuweiyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.17451",
            "authors": [
                {
                    "_id": "6746f9c205afcd8837d520b7",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520b8",
                    "name": "Yuancheng Wei",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520b9",
                    "name": "Zhihui Xie",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520ba",
                    "name": "Xuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520bb",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520bc",
                    "name": "Peiyi Wang",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520bd",
                    "name": "Chenxin An",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520be",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520bf",
                    "name": "Sujian Li",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520c0",
                    "name": "Bill Yuchen Lin",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520c1",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "6746f9c205afcd8837d520c2",
                    "name": "Qi Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T14:08:34.000Z",
            "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models",
            "summary": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline combining sample\nselection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe model limitations. Comprehensive evaluation\nacross 16 leading large vision-language models, demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N\nsampling with VL-GenRMs. Analysis experiments uncover three critical insights\nfor improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs.",
            "upvotes": 6,
            "discussionId": "6746f9c405afcd8837d52182"
        },
        "publishedAt": "2024-11-27T09:22:41.770Z",
        "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17451.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "fullname": "Lei Li",
            "name": "tobiaslee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.15411",
            "authors": [
                {
                    "_id": "674689ccfe2f63e2200c520c",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c520d",
                    "name": "Qing Liu",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c520e",
                    "name": "Lingzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c520f",
                    "name": "Jing Shi",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c5210",
                    "name": "Zhifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c5211",
                    "name": "Yilin Wang",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c5212",
                    "name": "Jianming Zhang",
                    "hidden": false
                },
                {
                    "_id": "674689ccfe2f63e2200c5213",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-23T02:20:32.000Z",
            "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You\n  Want at Any Granularity",
            "summary": "The advent of large Vision-Language Models (VLMs) has significantly advanced\nmultimodal tasks, enabling more sophisticated and accurate reasoning across\nvarious applications, including image and video captioning, visual question\nanswering, and cross-modal retrieval. Despite their superior capabilities, VLMs\nstruggle with fine-grained image regional composition information perception.\nSpecifically, they have difficulty accurately aligning the segmentation masks\nwith the corresponding semantics and precisely describing the compositional\naspects of the referred regions.\n  However, compositionality - the ability to understand and generate novel\ncombinations of known visual and textual components - is critical for\nfacilitating coherent reasoning and understanding across modalities by VLMs. To\naddress this issue, we propose FINECAPTION, a novel VLM that can recognize\narbitrary masks as referential inputs and process high-resolution images for\ncompositional image captioning at different granularity levels. To support this\nendeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region\ncompositional image captioning, which introduces the task of compositional\nattribute-aware regional image captioning.\n  Empirical results demonstrate the effectiveness of our proposed model\ncompared to other state-of-the-art VLMs. Additionally, we analyze the\ncapabilities of current VLMs in recognizing various visual prompts for\ncompositional region image captioning, highlighting areas for improvement in\nVLM design and training.",
            "upvotes": 6,
            "discussionId": "674689d0fe2f63e2200c52f1"
        },
        "publishedAt": "2024-11-27T01:25:01.664Z",
        "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15411.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
            "fullname": "HangHua",
            "name": "hhua2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.16856",
            "authors": [
                {
                    "_id": "674693666b75c045a3f54a54",
                    "name": "Yongwei Chen",
                    "hidden": false
                },
                {
                    "_id": "674693666b75c045a3f54a55",
                    "name": "Yushi Lan",
                    "hidden": false
                },
                {
                    "_id": "674693666b75c045a3f54a56",
                    "name": "Shangchen Zhou",
                    "hidden": false
                },
                {
                    "_id": "674693666b75c045a3f54a57",
                    "name": "Tengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "674693666b75c045a3f54a58",
                    "name": "XIngang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T19:00:05.000Z",
            "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via\n  Multi-scale 3D VQVAE",
            "summary": "Autoregressive models have demonstrated remarkable success across various\nfields, from large language models (LLMs) to large multimodal models (LMMs) and\n2D content generation, moving closer to artificial general intelligence (AGI).\nDespite these advances, applying autoregressive approaches to 3D object\ngeneration and understanding remains largely unexplored. This paper introduces\nScale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale\n3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects for\nefficient autoregressive generation and detailed understanding. By predicting\nthe next scale in a multi-scale latent representation instead of the next\nsingle token, SAR3D reduces generation time significantly, achieving fast 3D\nobject generation in just 0.82 seconds on an A6000 GPU. Additionally, given the\ntokens enriched with hierarchical 3D-aware information, we finetune a\npretrained LLM on them, enabling multimodal comprehension of 3D content. Our\nexperiments show that SAR3D surpasses current 3D generation methods in both\nspeed and quality and allows LLMs to interpret and caption 3D models\ncomprehensively.",
            "upvotes": 5,
            "discussionId": "674693676b75c045a3f54b0e"
        },
        "publishedAt": "2024-11-27T02:22:07.376Z",
        "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6458c809b1202755ec5cf33a/B3BGqmVVU8QCMePcYTNHv.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16856.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/0cd92dbb4e85484926c65e5582d89e55.svg",
            "fullname": "cyw-3d",
            "name": "cyw-3d",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.17223",
            "authors": [
                {
                    "_id": "6746801be0ff542317fa9902",
                    "name": "Yicheng Yang",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9903",
                    "name": "Pengxiang Li",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9904",
                    "name": "Lu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9905",
                    "name": "Liqian Ma",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9906",
                    "name": "Ping Hu",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9907",
                    "name": "Siyu Du",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9908",
                    "name": "Yunzhi Zhuge",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa9909",
                    "name": "Xu Jia",
                    "hidden": false
                },
                {
                    "_id": "6746801be0ff542317fa990a",
                    "name": "Huchuan Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T08:44:47.000Z",
            "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in\n  Customized Image Inpainting",
            "summary": "Subject-driven image inpainting has emerged as a popular task in image\nediting alongside recent advancements in diffusion models. Previous methods\nprimarily focus on identity preservation but struggle to maintain the\neditability of inserted objects. In response, this paper introduces DreamMix, a\ndiffusion-based generative model adept at inserting target objects into given\nscenes at user-specified locations while concurrently enabling arbitrary\ntext-driven modifications to their attributes. In particular, we leverage\nadvanced foundational inpainting models and introduce a disentangled\nlocal-global inpainting framework to balance precise local object insertion\nwith effective global visual coherence. Additionally, we propose an Attribute\nDecoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to\nimprove the diversity and discriminative capability of the text-based attribute\nguidance, respectively. Extensive experiments demonstrate that DreamMix\neffectively balances identity preservation and attribute editability across\nvarious application scenarios, including object insertion, attribute editing,\nand small object inpainting. Our code is publicly available at\nhttps://github.com/mycfhs/DreamMix.",
            "upvotes": 5,
            "discussionId": "67468021e0ff542317fa9ae4"
        },
        "publishedAt": "2024-11-27T00:45:45.589Z",
        "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17223.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.16173",
            "authors": [
                {
                    "_id": "6746a1f0844d4bb7f7b79703",
                    "name": "Junho Kim",
                    "hidden": false
                },
                {
                    "_id": "6746a1f0844d4bb7f7b79704",
                    "name": "Hyunjun Kim",
                    "hidden": false
                },
                {
                    "_id": "6746a1f0844d4bb7f7b79705",
                    "name": "Hosu Lee",
                    "hidden": false
                },
                {
                    "_id": "6746a1f0844d4bb7f7b79706",
                    "name": "Yong Man Ro",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T08:04:47.000Z",
            "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis",
            "summary": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.",
            "upvotes": 3,
            "discussionId": "6746a1f1844d4bb7f7b7976c"
        },
        "publishedAt": "2024-11-27T03:18:46.578Z",
        "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16173.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
            "fullname": "Junho Kim",
            "name": "arkimjh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.17691",
            "authors": [
                {
                    "_id": "67469174db412d1e84f9d8be",
                    "name": "Xu Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67469174db412d1e84f9d8bf",
                    "name": "Tao Ge",
                    "hidden": false
                },
                {
                    "_id": "67469174db412d1e84f9d8c0",
                    "name": "Thomas Hartvigsen",
                    "hidden": false
                },
                {
                    "_id": "67469174db412d1e84f9d8c1",
                    "name": "Zhisong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67469174db412d1e84f9d8c2",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "67469174db412d1e84f9d8c3",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T18:57:58.000Z",
            "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens",
            "summary": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.",
            "upvotes": 3,
            "discussionId": "67469175db412d1e84f9d915"
        },
        "publishedAt": "2024-11-27T02:07:48.252Z",
        "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17691.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/17add4cd02e183f9b9d4feb7b6c27c21.svg",
            "fullname": "Tao Ge",
            "name": "sggetao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.16801",
            "authors": [
                {
                    "_id": "67470ec05535474406e7bede",
                    "name": "Yisol Choi",
                    "hidden": false
                },
                {
                    "_id": "67470ec05535474406e7bedf",
                    "name": "Sangkyung Kwak",
                    "hidden": false
                },
                {
                    "_id": "67470ec05535474406e7bee0",
                    "name": "Sihyun Yu",
                    "hidden": false
                },
                {
                    "_id": "67470ec05535474406e7bee1",
                    "user": {
                        "_id": "651f8790559843c34d3d2c22",
                        "avatarUrl": "/avatars/cbc4189247ed39b761f7c8ffe91d75a7.svg",
                        "isPro": false,
                        "fullname": "Hyungwon Choi",
                        "user": "hyungwonc",
                        "type": "user"
                    },
                    "name": "Hyungwon Choi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-27T12:21:26.273Z",
                    "hidden": false
                },
                {
                    "_id": "67470ec05535474406e7bee2",
                    "name": "Jinwoo Shin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-25T12:37:13.000Z",
            "title": "Controllable Human Image Generation with Personalized Multi-Garments",
            "summary": "We present BootComp, a novel framework based on text-to-image diffusion\nmodels for controllable human image generation with multiple reference\ngarments. Here, the main bottleneck is data acquisition for training:\ncollecting a large-scale dataset of high-quality reference garment images per\nhuman subject is quite challenging, i.e., ideally, one needs to manually gather\nevery single garment photograph worn by each human. To address this, we propose\na data generation pipeline to construct a large synthetic dataset, consisting\nof human and multiple-garment pairs, by introducing a model to extract any\nreference garment images from each human image. To ensure data quality, we also\npropose a filtering strategy to remove undesirable generated data based on\nmeasuring perceptual similarities between the garment presented in human image\nand extracted garment. Finally, by utilizing the constructed synthetic dataset,\nwe train a diffusion model having two parallel denoising paths that use\nmultiple garment images as conditions to generate human images while preserving\ntheir fine-grained details. We further show the wide-applicability of our\nframework by adapting it to different types of reference-based generation in\nthe fashion domain, including virtual try-on, and controllable human image\ngeneration with other conditions, e.g., pose, face, etc.",
            "upvotes": 2,
            "discussionId": "67470ec65535474406e7c3b3"
        },
        "publishedAt": "2024-11-27T11:03:53.175Z",
        "title": "Controllable Human Image Generation with Personalized Multi-Garments",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16801.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2a1daf0fee29302c470cffa8e74a0fdb.svg",
            "fullname": "Yisol Choi",
            "name": "yisol",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 132
        }
    },
    {
        "paper": {
            "id": "2411.14721",
            "authors": [
                {
                    "_id": "6746d91b1c6c176064e0d558",
                    "name": "Jiatong Li",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d559",
                    "name": "Yunqing Liu",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55a",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55b",
                    "name": "Jingdi Le",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55c",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "qq8933",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-27T08:32:29.658Z",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55d",
                    "name": "Wenqi Fan",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55e",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d55f",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "6746d91b1c6c176064e0d560",
                    "name": "Qing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T04:28:56.000Z",
            "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts",
            "summary": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.",
            "upvotes": 2,
            "discussionId": "6746d91d1c6c176064e0d60d"
        },
        "publishedAt": "2024-11-27T07:03:21.740Z",
        "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14721.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e13c7398f77e7e0bd5eed03102aa5c36.svg",
            "fullname": "Jiatong LI",
            "name": "phenixace",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.16754",
            "authors": [
                {
                    "_id": "6746a7c80a7a7bc04551f5c0",
                    "name": "Nasrin Imanpour",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c1",
                    "name": "Shashwat Bajpai",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c2",
                    "name": "Subhankar Ghosh",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c3",
                    "name": "Sainath Reddy Sankepally",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c4",
                    "name": "Abhilekh Borah",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c5",
                    "name": "Hasnat Md Abdullah",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c6",
                    "name": "Nishoak Kosaraju",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c7",
                    "name": "Shreyas Dixit",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c8",
                    "name": "Ashhar Aziz",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5c9",
                    "name": "Shwetangshu Biswas",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5ca",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5cb",
                    "name": "Aman Chadha",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5cc",
                    "name": "Amit Sheth",
                    "hidden": false
                },
                {
                    "_id": "6746a7c80a7a7bc04551f5cd",
                    "name": "Amitava Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-24T06:03:49.000Z",
            "title": "Visual Counter Turing Test (VCT^2): Discovering the Challenges for\n  AI-Generated Image Detection and Introducing Visual AI Index (V_AI)",
            "summary": "The proliferation of AI techniques for image generation, coupled with their\nincreasing accessibility, has raised significant concerns about the potential\nmisuse of these images to spread misinformation. Recent AI-generated image\ndetection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake\nImage Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE,\nOCC-CLIP, De-Fake, and Deep Fake Detection. However, we argue that the current\nstate-of-the-art AGID techniques are inadequate for effectively detecting\ncontemporary AI-generated images and advocate for a comprehensive reevaluation\nof these methods. We introduce the Visual Counter Turing Test (VCT^2), a\nbenchmark comprising ~130K images generated by contemporary text-to-image\nmodels (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E\n3, and Midjourney 6). VCT^2 includes two sets of prompts sourced from tweets by\nthe New York Times Twitter account and captions from the MS COCO dataset. We\nalso evaluate the performance of the aforementioned AGID techniques on the\nVCT^2 benchmark, highlighting their ineffectiveness in detecting AI-generated\nimages. As image-generative AI models continue to evolve, the need for a\nquantifiable framework to evaluate these models becomes increasingly critical.\nTo meet this need, we propose the Visual AI Index (V_AI), which assesses\ngenerated images from various visual perspectives, including texture complexity\nand object coherence, setting a new standard for evaluating image-generative AI\nmodels. To foster research in this domain, we make our\nhttps://huggingface.co/datasets/anonymous1233/COCO_AI and\nhttps://huggingface.co/datasets/anonymous1233/twitter_AI datasets publicly\navailable.",
            "upvotes": 1,
            "discussionId": "6746a7cb0a7a7bc04551f71d"
        },
        "publishedAt": "2024-11-27T03:36:26.814Z",
        "title": "Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16754.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.17383",
            "authors": [
                {
                    "_id": "67468e323f6c325cb30981b3",
                    "name": "Ziyi Xu",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b4",
                    "name": "Ziyao Huang",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b5",
                    "name": "Juan Cao",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b6",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b7",
                    "name": "Xiaodong Cun",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b8",
                    "name": "Qing Shuai",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981b9",
                    "name": "Yuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981ba",
                    "name": "Linchao Bao",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981bb",
                    "name": "Jintao Li",
                    "hidden": false
                },
                {
                    "_id": "67468e323f6c325cb30981bc",
                    "name": "Fan Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T12:42:13.000Z",
            "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via\n  Human-Object Interacting Video Generation",
            "summary": "The automatic generation of anchor-style product promotion videos presents\npromising opportunities in online commerce, advertising, and consumer\nengagement. However, this remains a challenging task despite significant\nadvancements in pose-guided human video generation. In addressing this\nchallenge, we identify the integration of human-object interactions (HOI) into\npose-guided human video generation as a core issue. To this end, we introduce\nAnchorCrafter, a novel diffusion-based system designed to generate 2D videos\nfeaturing a target human and a customized object, achieving high visual\nfidelity and controllable interactions. Specifically, we propose two key\ninnovations: the HOI-appearance perception, which enhances object appearance\nrecognition from arbitrary multi-view perspectives and disentangles object and\nhuman appearance, and the HOI-motion injection, which enables complex\nhuman-object interactions by overcoming challenges in object trajectory\nconditioning and inter-occlusion management. Additionally, we introduce the\nHOI-region reweighting loss, a training objective that enhances the learning of\nobject details. Extensive experiments demonstrate that our proposed system\noutperforms existing methods in preserving object appearance and shape\nawareness, while simultaneously maintaining consistency in human appearance and\nmotion. Project page: https://cangcz.github.io/Anchor-Crafter/",
            "upvotes": 1,
            "discussionId": "67468e363f6c325cb3098436"
        },
        "publishedAt": "2024-11-27T01:43:06.128Z",
        "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17383.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5242
        }
    }
]