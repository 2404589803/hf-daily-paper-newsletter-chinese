[
  {
    "paper": {
      "id": "2502.19613",
      "authors": [
        {
          "_id": "67c12987505a88e4a185e0d7",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d8",
          "name": "Hanning Zhang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d9",
          "name": "Chenlu Ye",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0da",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0db",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0dc",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T23:01:16.000Z",
      "title": "Self-rewarding correction for mathematical reasoning",
      "summary": "We study self-rewarding reasoning large language models (LLMs), which can\nsimultaneously generate step-by-step reasoning and evaluate the correctness of\ntheir outputs during the inference time-without external feedback. This\nintegrated approach allows a single model to independently guide its reasoning\nprocess, offering computational advantages for model deployment. We\nparticularly focus on the representative task of self-correction, where models\nautonomously detect errors in their responses, revise outputs, and decide when\nto terminate iterative refinement loops. To enable this, we propose a\ntwo-staged algorithmic framework for constructing self-rewarding reasoning\nmodels using only self-generated data. In the first stage, we employ sequential\nrejection sampling to synthesize long chain-of-thought trajectories that\nincorporate both self-rewarding and self-correction mechanisms. Fine-tuning\nmodels on these curated data allows them to learn the patterns of\nself-rewarding and self-correction. In the second stage, we further enhance the\nmodels' ability to assess response accuracy and refine outputs through\nreinforcement learning with rule-based signals. Experiments with Llama-3 and\nQwen-2.5 demonstrate that our approach surpasses intrinsic self-correction\ncapabilities and achieves performance comparable to systems that rely on\nexternal reward models.",
      "upvotes": 39,
      "discussionId": "67c12989505a88e4a185e115"
    },
    "publishedAt": "2025-02-27T22:15:54.222Z",
    "title": "Self-rewarding correction for mathematical reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20395",
      "authors": [
        {
          "_id": "67c12b5def9af74902537b98",
          "name": "Zhongyang Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b99",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b9a",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:32.000Z",
      "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
      "summary": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters.",
      "upvotes": 19,
      "discussionId": "67c12b5eef9af74902537c00"
    },
    "publishedAt": "2025-02-27T22:27:24.486Z",
    "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/PaZkWIhqZBRCSfBA-k4OX.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FASlyPDiSb9VHZaeWMj9H.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/kGeIJVMDDAbIassiuYIb2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/Tw2Bf_RsFTPARKLJWIlKM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20082",
      "authors": [
        {
          "_id": "67c12b6d25c74ee5b6e2ce8e",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce8f",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce90",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce91",
          "name": "Gaokai Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce92",
          "name": "Gilsinia Lopez",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce93",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce94",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce95",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T13:41:07.000Z",
      "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
      "summary": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.",
      "upvotes": 17,
      "discussionId": "67c12b6e25c74ee5b6e2ceb5"
    },
    "publishedAt": "2025-02-27T22:22:53.713Z",
    "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16645",
      "authors": [
        {
          "_id": "67c12e60d8247a49b805694f",
          "name": "Chenlong Wang",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056950",
          "name": "Zhaoyang Chu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056951",
          "name": "Zhengxiang Cheng",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056952",
          "name": "Xuyi Yang",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056953",
          "name": "Kaiyue Qiu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056954",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056955",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056956",
          "name": "Xuanhua Shi",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056957",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T16:46:18.000Z",
      "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale",
      "summary": "Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.",
      "upvotes": 11,
      "discussionId": "67c12e61d8247a49b805698f"
    },
    "publishedAt": "2025-02-27T23:04:14.619Z",
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20238",
      "authors": [
        {
          "_id": "67c15306333e2f71f01c8e35",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e36",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e37",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e38",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e39",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3a",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3b",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3c",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3d",
          "name": "Yu Rong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:23:25.000Z",
      "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving",
      "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.",
      "upvotes": 9,
      "discussionId": "67c15307333e2f71f01c8ebc"
    },
    "publishedAt": "2025-02-28T01:14:11.268Z",
    "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e85b3edb3767299865e0e3",
      "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
      "fullname": "Chen",
      "name": "Guizhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16944",
      "authors": [
        {
          "_id": "67be807e8a5a805423137ca2",
          "name": "Chenghua Huang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca3",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca4",
          "user": {
            "_id": "669dcf6200970c3b27aafa5d",
            "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
            "isPro": false,
            "fullname": "kaikai yang",
            "user": "keanudicap",
            "type": "user"
          },
          "name": "Fangkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:17:46.382Z",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca5",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca6",
          "name": "Zhixu Li",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca7",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca8",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca9",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137caa",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T08:11:33.000Z",
      "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value\n  Guidance",
      "summary": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human\nFeedback (RLHF) is essential for aligning large language models (LLMs) with\nhuman preferences. It requires joint training of an actor and critic with a\npretrained, fixed reward model for guidance. This approach increases\ncomputational complexity and instability due to actor-critic interdependence.\nAdditionally, PPO lacks access to true environment rewards in LLM tasks,\nlimiting its adaptability. Under such conditions, pretraining a value model or\na reward model becomes equivalent, as both provide fixed supervisory signals\nwithout new ground-truth feedback. To address these issues, we propose\nDecoupled Value Policy Optimization (DVPO), a lean framework that\nreplaces traditional reward modeling with a pretrained global value model\n(GVM). The GVM is conditioned on policy trajectories and predicts token-level\nreturn-to-go estimates. By decoupling value model from policy training (via\nfrozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence,\nreducing GPU memory usage by 40\\% and training time by 35\\% compared to\nconventional RLHF. Experiments across benchmarks show DVPO outperforms\nefficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in\nperformance.",
      "upvotes": 6,
      "discussionId": "67be807e8a5a805423137cc2"
    },
    "publishedAt": "2025-02-28T01:55:41.427Z",
    "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669dcf6200970c3b27aafa5d",
      "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
      "fullname": "kaikai yang",
      "name": "keanudicap",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20321",
      "authors": [
        {
          "_id": "67c13c68d8247a49b808fdac",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdad",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdae",
          "name": "Junfeng Wu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdaf",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb0",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb1",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb2",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb3",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:47:01.000Z",
      "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
      "summary": "The representation disparity between visual generation and understanding\nimposes a critical gap in integrating these capabilities into a single\nframework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer\nthat encodes fine-grained details for generation while also capturing\nhigh-level semantics for understanding. Despite recent studies have shown that\nthese objectives could induce loss conflicts in training, we reveal that the\nunderlying bottleneck stems from limited representational capacity of discrete\ntokens. We address this by introducing multi-codebook quantization, which\ndivides vector quantization with several independent sub-codebooks to expand\nthe latent feature space, while avoiding training instability caused by\noverlarge codebooks. Our method significantly raises the upper limit of unified\ndiscrete tokenizers to match or even surpass domain-specific continuous\ntokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus\n0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on\nImageNet. Our code is available at https://github.com/FoundationVision/UniTok.",
      "upvotes": 6,
      "discussionId": "67c13c6ad8247a49b8090003"
    },
    "publishedAt": "2025-02-27T23:34:45.416Z",
    "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6344dcb1cd37e44d9ed46508",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
      "fullname": "Yi Jiang",
      "name": "JiangYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20307",
      "authors": [
        {
          "_id": "67c1460201cef6d4b9b9ac73",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac74",
          "name": "Jianfei Yuan",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac75",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac76",
          "name": "Yong Zhang",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac77",
          "name": "Xiaodong Cun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac78",
          "name": "Chi-Man Pun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac79",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:33:51.000Z",
      "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
      "summary": "We present Mobius, a novel method to generate seamlessly looping videos from\ntext descriptions directly without any user annotations, thereby creating new\nvisual materials for the multi-media presentation. Our method repurposes the\npre-trained video latent diffusion model for generating looping videos from\ntext prompts without any training. During inference, we first construct a\nlatent cycle by connecting the starting and ending noise of the videos. Given\nthat the temporal consistency can be maintained by the context of the video\ndiffusion model, we perform multi-frame latent denoising by gradually shifting\nthe first-frame latent to the end in each step. As a result, the denoising\ncontext varies in each step while maintaining consistency throughout the\ninference process. Moreover, the latent cycle in our method can be of any\nlength. This extends our latent-shifting approach to generate seamless looping\nvideos beyond the scope of the video diffusion model's context. Unlike previous\ncinemagraphs, the proposed method does not require an image as appearance,\nwhich will restrict the motions of the generated results. Instead, our method\ncan produce more dynamic motion and better visual quality. We conduct multiple\nexperiments and comparisons to verify the effectiveness of the proposed method,\ndemonstrating its efficacy in different scenarios. All the code will be made\navailable.",
      "upvotes": 4,
      "discussionId": "67c1460501cef6d4b9b9addf"
    },
    "publishedAt": "2025-02-28T00:14:01.841Z",
    "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20126",
      "authors": [
        {
          "_id": "67c14524af5eaa8dd062a216",
          "name": "Sotiris Anagnostidis",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a217",
          "name": "Gregor Bachmann",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a218",
          "name": "Yeongmin Kim",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a219",
          "name": "Jonas Kohler",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21a",
          "name": "Markos Georgopoulos",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21b",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21c",
          "name": "Yuming Du",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21d",
          "name": "Albert Pumarola",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21e",
          "name": "Ali Thabet",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21f",
          "name": "Edgar Schönfeld",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:16:56.000Z",
      "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\n  Samples with Less Compute",
      "summary": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into flexible ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\nflexible model can generate images without any drop in quality, while\nreducing the required FLOPs by more than 40\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to 75\\% less compute without\ncompromising performance.",
      "upvotes": 4,
      "discussionId": "67c14529af5eaa8dd062a38c"
    },
    "publishedAt": "2025-02-28T00:10:30.864Z",
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20127",
      "authors": [
        {
          "_id": "67c12de08cd49ca63e230b99",
          "name": "Zexiong Ma",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9a",
          "name": "Chao Peng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9b",
          "name": "Pengfei Gao",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9c",
          "name": "Xiangxin Meng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9d",
          "name": "Yanzhen Zou",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9e",
          "name": "Bing Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:19:45.000Z",
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "summary": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
      "upvotes": 4,
      "discussionId": "67c12de08cd49ca63e230bd1"
    },
    "publishedAt": "2025-02-27T22:38:04.562Z",
    "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654da66fb36f85a025bc24b6",
      "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
      "fullname": "Zexiong Ma",
      "name": "mizersy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19735",
      "authors": [
        {
          "_id": "67c1438fd7ffcd1cab1fc412",
          "user": {
            "_id": "6727998d4fc2e4f7cc0c85d3",
            "avatarUrl": "/avatars/ac18eaadd606f7fae64996502f393cf2.svg",
            "isPro": false,
            "fullname": "he",
            "user": "boommmmm",
            "type": "user"
          },
          "name": "Minggui He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-28T05:03:12.675Z",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc413",
          "name": "Yilun Liu",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc414",
          "name": "Shimin Tao",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc415",
          "name": "Yuanchang Luo",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc416",
          "name": "Hongyong Zeng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc417",
          "name": "Chang Su",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc418",
          "name": "Li Zhang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc419",
          "name": "Hongxia Ma",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41a",
          "name": "Daimeng Wei",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41b",
          "name": "Weibin Meng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41c",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41d",
          "name": "Boxing Chen",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41e",
          "name": "Osamu Yoshie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T03:57:00.000Z",
      "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning",
      "summary": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to catastrophic forgetting,\nlimiting their adaptability to diverse translation scenarios. This paper\nintroduces R1-Translator (R1-T1), a novel framework to achieve inference-time\nreasoning for general MT via reinforcement learning (RL) with human-aligned\nCoTs comprising six common patterns. Our approach pioneers three innovations:\n(1) extending reasoning-based translation beyond MT sub-tasks to six languages\nand diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);\n(2) formalizing six expert-curated CoT templates that mirror hybrid human\nstrategies like context-aware paraphrasing and back translation; and (3)\nenabling self-evolving CoT discovery and anti-forgetting adaptation through RL\nwith KL-constrained rewards. Experimental results indicate a steady translation\nperformance improvement in 21 languages and 80 translation directions on\nFlores-101 test set, especially on the 15 languages unseen from training, with\nits general multilingual abilities preserved compared with plain SFT.",
      "upvotes": 2,
      "discussionId": "67c14390d7ffcd1cab1fc479"
    },
    "publishedAt": "2025-02-28T00:03:34.893Z",
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6245
    },
    "isAuthorParticipating": false
  }
]