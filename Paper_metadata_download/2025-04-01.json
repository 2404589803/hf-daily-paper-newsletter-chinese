[
  {
    "paper": {
      "id": "2503.23307",
      "authors": [
        {
          "_id": "67eb4bd0eca57c4eebbb343a",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "isPro": true,
            "fullname": "Cong Wei",
            "user": "lim142857",
            "type": "user"
          },
          "name": "Cong Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343b",
          "name": "Bo Sun",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343c",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343d",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343e",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343f",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3440",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3441",
          "name": "Luxin Zhang",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3442",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3443",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3444",
          "name": "Animesh Sinha",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3445",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3446",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T04:22:09.000Z",
      "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
      "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
      "submittedOnDailyBy": {
        "_id": "64f8e358766ff9f3d2b0de84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
        "isPro": true,
        "fullname": "Cong Wei",
        "user": "lim142857",
        "type": "user"
      },
      "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
      "upvotes": 24,
      "discussionId": "67eb4bd3eca57c4eebbb34c7",
      "projectPage": "https://congwei1230.github.io/MoCha/",
      "ai_keywords": [
        "speech-video window attention mechanism",
        "speech-labeled video datasets",
        "text-labeled video data",
        "structured prompt templates",
        "character tags",
        "multi-character conversation",
        "turn-based dialogue",
        "context-aware conversations",
        "cinematic coherence"
      ]
    },
    "publishedAt": "2025-03-30T00:22:09.000Z",
    "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8e358766ff9f3d2b0de84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
      "fullname": "Cong Wei",
      "name": "lim142857",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24235",
      "authors": [
        {
          "_id": "67eb57023475e7b135788500",
          "name": "Qiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788501",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788502",
          "user": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "isPro": false,
            "fullname": "RubinSun",
            "user": "RubinSun",
            "type": "user"
          },
          "name": "Zexu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788503",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788504",
          "name": "Weixu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788505",
          "name": "Zhihan Guo",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788506",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788507",
          "name": "Irwin King",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788508",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788509",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T15:46:15.000Z",
      "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
      "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "62a42f22c683d02f5b63320c",
        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
        "isPro": false,
        "fullname": "Qiyuan Zhang",
        "user": "DonJoey",
        "type": "user"
      },
      "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
      "upvotes": 19,
      "discussionId": "67eb57053475e7b135788624",
      "ai_keywords": [
        "test-time scaling",
        "test-time computing",
        "large language models",
        "specialized reasoning tasks",
        "open-ended Q&A",
        "multidimensional framework",
        "what to scale",
        "how to scale",
        "where to scale",
        "how well to scale",
        "assessment aspects",
        "functional roles",
        "developmental trajectories",
        "practical deployment",
        "open challenges",
        "attributions"
      ]
    },
    "publishedAt": "2025-03-31T11:46:15.000Z",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23461",
      "authors": [
        {
          "_id": "67eb594988a08fae617242f1",
          "name": "Nikai Du",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f2",
          "user": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "zhen-nan",
            "type": "user"
          },
          "name": "Zhennan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f3",
          "name": "Zhizhou Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f4",
          "name": "Shan Gao",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f6",
          "name": "Zhengkai Jiang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f8",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T14:36:55.000Z",
      "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
      "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
      "upvotes": 19,
      "discussionId": "67eb594b88a08fae617243ac",
      "projectPage": "https://dnknju.github.io/textcrafter-vue/",
      "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
      "ai_keywords": [
        "complex visual text",
        "TextCrafter",
        "multi-visual text rendering",
        "progressive strategy",
        "token focus enhancement",
        "CVTG-2K",
        "generative models",
        "CVTG tasks",
        "state-of-the-art approaches"
      ]
    },
    "publishedAt": "2025-03-30T10:36:55.000Z",
    "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
    "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24388",
      "authors": [
        {
          "_id": "67eb544113ca8dcb9ccb991b",
          "name": "Zhonghan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991c",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991d",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991f",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9920",
          "name": "Gaoang Wang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9921",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:52.000Z",
      "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
      "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
      "upvotes": 18,
      "discussionId": "67eb544213ca8dcb9ccb9963"
    },
    "publishedAt": "2025-03-31T13:59:52.000Z",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24370",
      "authors": [
        {
          "_id": "67eb4fff13ca8dcb9cca5f9b",
          "user": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "isPro": false,
            "fullname": "Tong Wu",
            "user": "tongwu2020",
            "type": "user"
          },
          "name": "Tong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9c",
          "name": "Chong Xiang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9d",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9e",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:50:13.000Z",
      "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
      "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
      "upvotes": 10,
      "discussionId": "67eb500013ca8dcb9cca5fe0",
      "ai_keywords": [
        "Reasoning-enhanced large language models (LLMs)",
        "intermediate reasoning steps",
        "Thinking Intervention",
        "thinking tokens",
        "instruction following",
        "IFEval",
        "instruction hierarchy",
        "SEP",
        "safety alignment",
        "XSTest",
        "SORRY-Bench",
        "open-source DeepSeek R1 models"
      ]
    },
    "publishedAt": "2025-03-31T13:50:13.000Z",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24364",
      "authors": [
        {
          "_id": "67eb6e6088a08fae617860f3",
          "user": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "isPro": false,
            "fullname": "Łukasz Borchmann",
            "user": "Borchmann",
            "type": "user"
          },
          "name": "Łukasz Borchmann",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
          "hidden": false
        },
        {
          "_id": "67eb6e6088a08fae617860f4",
          "name": "Marek Wydmuch",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
      ],
      "publishedAt": "2025-03-31T17:43:36.000Z",
      "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
      "title": "Query and Conquer: Execution-Guided SQL Generation",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "Łukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
      "upvotes": 7,
      "discussionId": "67eb6e6188a08fae6178613f",
      "ai_keywords": [
        "text-to-SQL",
        "execution results",
        "semantically consistent",
        "query",
        "candidates",
        "models",
        "reasoning methods",
        "o1",
        "o3-mini",
        "DeepSeek R1",
        "inference cost",
        "SQL generation"
      ]
    },
    "publishedAt": "2025-03-31T13:43:36.000Z",
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "Łukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24115",
      "authors": [
        {
          "_id": "67eb5116d3a707c0a5b02bd1",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd2",
          "user": {
            "_id": "6385f7b969634850f8ddd541",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
            "isPro": false,
            "fullname": "Peidong Wang",
            "user": "WDong",
            "type": "user"
          },
          "name": "Peidong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd3",
          "name": "Minhua Huang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd4",
          "name": "Jingpeng Wang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd5",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd6",
          "name": "Xiangzhao Lv",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd7",
          "name": "Yachun Pang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd8",
          "name": "Yin Yang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd9",
          "name": "Wenjie Tang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bda",
          "name": "Yuchen Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
      ],
      "publishedAt": "2025-03-31T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
      "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
      "upvotes": 7,
      "discussionId": "67eb5117d3a707c0a5b02c4c",
      "ai_keywords": [
        "automatically speech recognition (ASR)",
        "text-to-speech (TTS)",
        "large language model (LLM)",
        "self-instruction sampling",
        "multi-agent adversarial synthesis",
        "supervised fine-tuning (SFT)",
        "hybrid real/synthetic data"
      ]
    },
    "publishedAt": "2025-03-31T10:06:17.000Z",
    "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
    "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23284",
      "authors": [
        {
          "_id": "67eb5280aeab4ce97de07134",
          "user": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "isPro": false,
            "fullname": "Feng-Lin Liu",
            "user": "Okrin",
            "type": "user"
          },
          "name": "Feng-Lin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07135",
          "name": "Hongbo Fu",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07136",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07137",
          "name": "Weicai Ye",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07138",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07139",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de0713a",
          "name": "Lin Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T02:44:09.000Z",
      "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
      "title": "SketchVideo: Sketch-based Video Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "6424538b9f9e65b42389920e",
        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
        "isPro": false,
        "fullname": "Feng-Lin Liu",
        "user": "Okrin",
        "type": "user"
      },
      "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
      "upvotes": 7,
      "discussionId": "67eb5286aeab4ce97de07320",
      "githubRepo": "https://github.com/IGLICT/SketchVideo",
      "ai_keywords": [
        "DiT video generation model",
        "memory-efficient control structure",
        "sketch control blocks",
        "residual features",
        "skipped DiT blocks",
        "temporally sparse sketch conditions",
        "inter-frame attention mechanism",
        "keyframes",
        "video insertion module",
        "spatial feature",
        "dynamic motion",
        "latent fusion",
        "SketchVideo"
      ]
    },
    "publishedAt": "2025-03-29T22:44:09.000Z",
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424538b9f9e65b42389920e",
      "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
      "fullname": "Feng-Lin Liu",
      "name": "Okrin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18809",
      "authors": [
        {
          "_id": "67eaa0f83ace6eb46745a9fe",
          "user": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "isPro": false,
            "fullname": "Augusto B. Corrêa",
            "user": "abcorrea",
            "type": "user"
          },
          "name": "Augusto B. Corrêa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745a9ff",
          "user": {
            "_id": "662fb9c891587703a677856e",
            "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
            "isPro": false,
            "fullname": "Andre Grahl Pereira",
            "user": "andregrahl",
            "type": "user"
          },
          "name": "André G. Pereira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745aa00",
          "user": {
            "_id": "66f3dfd4b8703dde248f6d26",
            "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
            "isPro": false,
            "fullname": "Jendrik Seipp",
            "user": "jendrikseipp",
            "type": "user"
          },
          "name": "Jendrik Seipp",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:50:20.000Z",
      "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
      "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
      "submittedOnDailyBy": {
        "_id": "674f43d6df6fa102409f6d1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
        "isPro": false,
        "fullname": "Augusto B. Corrêa",
        "user": "abcorrea",
        "type": "user"
      },
      "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
      "upvotes": 7,
      "discussionId": "67eaa0f93ace6eb46745aa3e",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought prompting",
        "fine-tuning",
        "reasoning",
        "planning domain",
        "domain-dependent heuristic functions",
        "Python code",
        "greedy best-first search",
        "state-of-the-art domain-independent heuristics",
        "domain-dependent planning",
        "unoptimized Python planner",
        "highly optimized C++ code",
        "planning heuristic function programs"
      ]
    },
    "publishedAt": "2025-03-24T11:50:20.000Z",
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
    "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674f43d6df6fa102409f6d1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
      "fullname": "Augusto B. Corrêa",
      "name": "abcorrea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23077",
      "authors": [
        {
          "_id": "67eb58c71e23a7499b683cce",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683ccf",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd0",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd1",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd2",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd3",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd4",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd5",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd6",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T13:27:46.000Z",
      "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
      "title": "Efficient Inference for Large Reasoning Models: A Survey",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
      "upvotes": 4,
      "discussionId": "67eb58c81e23a7499b683d12",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "explicit compact Chain-of-Thought (CoT)",
        "implicit latent CoT",
        "model merging",
        "agent routers"
      ]
    },
    "publishedAt": "2025-03-29T09:27:46.000Z",
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24290",
      "authors": [
        {
          "_id": "67eb762381e530baa56dc830",
          "name": "Jingcheng Hu",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc831",
          "name": "Yinmin Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc832",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc833",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc834",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc835",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:36:05.000Z",
      "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
      "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
      "upvotes": 3,
      "discussionId": "67eb762481e530baa56dc872",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vanilla PPO",
        "GAE ($\\lambda=1$, $\\gamma=1$)",
        "rule-based rewards",
        "KL regularization",
        "response length",
        "benchmark performance",
        "AIME2024",
        "MATH500",
        "GPQA Diamond benchmark",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-31T12:36:05.000Z",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6539
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23829",
      "authors": [
        {
          "_id": "67eb759cb9fa8908e1934f21",
          "name": "Yi Su",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f22",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f23",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f24",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f25",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f26",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f27",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f28",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T08:22:49.000Z",
      "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
      "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
      "upvotes": 3,
      "discussionId": "67eb759db9fa8908e1934f62",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable rewards (RLVR)",
        "mathematical reasoning",
        "coding tasks",
        "well-structured reference answers",
        "diverse domains",
        "medicine",
        "chemistry",
        "psychology",
        "economics",
        "large language models (LLMs)",
        "binary judgments",
        "domain-specific reward models",
        "model-based soft scoring",
        "distilled generative reward model",
        "effective cross-domain verifier",
        "reward signals",
        "fine-tuning",
        "base 7B model",
        "RL algorithms",
        "state-of-the-art open-source aligned LLMs",
        "Qwen2.5-72B-Instruct",
        "DeepSeek-R1-Distill-Qwen-32B",
        "free-form answer settings",
        "robustness",
        "scalability",
        "real-world applications",
        "noisy labels",
        "weak labels"
      ]
    },
    "publishedAt": "2025-03-31T04:22:49.000Z",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6539
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23730",
      "authors": [
        {
          "_id": "67eb567141abf40cd86e0e15",
          "user": {
            "_id": "67038a66eb760972bcb62c70",
            "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
            "isPro": false,
            "fullname": "김윤식",
            "user": "yoonshik1205",
            "type": "user"
          },
          "name": "Yoonshik Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
          "hidden": false
        },
        {
          "_id": "67eb567141abf40cd86e0e16",
          "user": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "isPro": true,
            "fullname": "Jaeyoon Jung",
            "user": "lastdefiance20",
            "type": "user"
          },
          "name": "Jaeyoon Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:04:25.000Z",
      "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
      "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": true,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
      "upvotes": 2,
      "discussionId": "67eb567341abf40cd86e0e63",
      "githubRepo": "https://github.com/maum-ai/KOFFVQA",
      "ai_keywords": [
        "Large Vision-Language Models (VLMs)",
        "visual question answering benchmark",
        "grading criteria"
      ]
    },
    "publishedAt": "2025-03-31T01:04:25.000Z",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21694",
      "authors": [
        {
          "_id": "67eb92defa85fe030e2db9e2",
          "name": "Zhiyuan Ma",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e3",
          "name": "Xinyue Liang",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e4",
          "name": "Rongyuan Wu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e5",
          "name": "Xiangyu Zhu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e6",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e7",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
      ],
      "publishedAt": "2025-03-27T16:59:15.000Z",
      "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
      "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
      "submittedOnDailyBy": {
        "_id": "64295d1f4e073875f6a605ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
        "isPro": true,
        "fullname": "Zhiyuan Ma",
        "user": "ZhiyuanthePony",
        "type": "user"
      },
      "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
      "upvotes": 2,
      "discussionId": "67eb92e2fa85fe030e2dbc04",
      "projectPage": "https://theericma.github.io/TriplaneTurbo/",
      "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
      "ai_keywords": [
        "diffusion models",
        "Stable Diffusion (SD)",
        "3D representations",
        "Progressive Rendering Distillation (PRD)",
        "U-Net",
        "latent from random noise",
        "denoise the latent",
        "3D output",
        "Multi-view diffusion models",
        "MVDream",
        "RichDreamer",
        "score distillation",
        "text-consistent textures",
        "geometries",
        "Triplane generator",
        "TriplaneTurbo",
        "high-quality 3D meshes"
      ]
    },
    "publishedAt": "2025-03-27T12:59:15.000Z",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
    "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64295d1f4e073875f6a605ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
      "fullname": "Zhiyuan Ma",
      "name": "ZhiyuanthePony",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20286",
      "authors": [
        {
          "_id": "67eaa88c40bebc3127ade04c",
          "user": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "isPro": false,
            "fullname": "Zhenyu Liang",
            "user": "ZhenyuLiang",
            "type": "user"
          },
          "name": "Zhenyu Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04e",
          "name": "Naiwei Yu",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04f",
          "name": "Kebin Sun",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade050",
          "name": "Ran Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T07:30:23.000Z",
      "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
      "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
      "submittedOnDailyBy": {
        "_id": "67e77099284080c98d8c9bfc",
        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
        "isPro": false,
        "fullname": "Zhenyu Liang",
        "user": "ZhenyuLiang",
        "type": "user"
      },
      "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
      "upvotes": 1,
      "discussionId": "67eaa88e40bebc3127ade0eb",
      "githubRepo": "https://github.com/EMI-Group/evomo",
      "ai_keywords": [
        "evolutionary multiobjective optimization (EMO)",
        "parallelism",
        "scalability",
        "GPU",
        "tensorization",
        "tensor representations",
        "NSGA-III",
        "MOEA/D",
        "HypE",
        "GPU-accelerated physics engine",
        "multiobjective robot control benchmark",
        "population sizes",
        "high-quality solutions",
        "diverse behaviors"
      ]
    },
    "publishedAt": "2025-03-26T03:30:23.000Z",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
    "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e77099284080c98d8c9bfc",
      "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
      "fullname": "Zhenyu Liang",
      "name": "ZhenyuLiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14941",
      "authors": [
        {
          "_id": "67eb932522a341478ae86cb6",
          "user": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "isPro": false,
            "fullname": "Qihui Zhang",
            "user": "77Hui",
            "type": "user"
          },
          "name": "Qihui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb7",
          "name": "Munan Ning",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb8",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb9",
          "name": "Yanbo Wang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cba",
          "name": "Jiayi Ye",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbb",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbc",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbd",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbe",
          "name": "Yibing Song",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbf",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T07:15:41.000Z",
      "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
      "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
      "submittedOnDailyBy": {
        "_id": "67a99d1fef1439e285c4cbec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
        "isPro": false,
        "fullname": "Qihui Zhang",
        "user": "77Hui",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
      "upvotes": 1,
      "discussionId": "67eb932622a341478ae86d15",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Question Answering (VQA)",
        "Q&A pairs",
        "MLLM-as-judge",
        "Unsupervised Peer review MLLM Evaluation (UPME)",
        "vision-language scoring system",
        "response correctness",
        "visual understanding and reasoning",
        "image-text correlation",
        "Pearson correlation",
        "MMstar dataset",
        "ScienceQA dataset"
      ]
    },
    "publishedAt": "2025-03-19T03:15:41.000Z",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a99d1fef1439e285c4cbec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
      "fullname": "Qihui Zhang",
      "name": "77Hui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]